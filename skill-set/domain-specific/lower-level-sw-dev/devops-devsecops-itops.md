#	DevOps & Site Reliability Engineering (SRE) & DevSecOps & ITOps & IT operations analytics & AIOps


##	DevOps




A DevOps toolchain is a set or combination of tools that aid in the delivery, development, and management of software applications throughout the systems development life cycle, as coordinated by an organisation that uses DevOps practices.

Generally, DevOps tools fit into one or more activities, which supports specific DevOps initiatives: Plan, Create, Verify, Package, Release, Configure, Monitor, and Version Control.


DevOps include:
+ Microservices
+ DevOps automation
+ Automation with version control
	- Many organizations use version control to power DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD. 




DevOps life cycle:
+ continuous development
+ continuous integration
+ continuous testing
+ continuous monitoring
+ continuous feedback
+ continuous deployment
+ continuous operations



Alternate DevOps life cycle, exploiting real-time communication:
+ continuous development/build
+ continuous integration
+ continuous deployment
+ continuous operations
+ continuous feedback
+ continuous planning







DevOps is at the intersection of:
+ software development
+ software quality assurance
+ ITOps



***BizDevOps is DevOps that account for business needs.***
+ Or, ***BizDevOps*** is ***DevOps 2.0***.
+ Collaborate with business units using agile methods
	- Business unites have to contribute to reduce the product/service backlog
	- DevOps teams have to be more responsible for the business aspects of what they do.







+ skill set:
	- Experience with Kubernetes and Docker.
	- Experience with Elasticsearch, Redis and/or Memcached.
+ skill set:
	- An understanding of several of these methodologies and tools:
		* Software development methods such as Agile, Scrum, Lean, Waterfall
		* Software project tools like JIRA, Pivotal Tracker, Trello, Asana, and MS Project
		* Continuous integration and build automation with Jenkins, TeamCity, TFS, TravisCI, CircleCI
	- Experience configuring the following technologies:
		* LDAP, ActiveDirectory, and other SAML/Single-Sign-on services
		* VMware vSphere, ESXi, AWS, Azure, GCP, and other virtual infrastructure tools
+ Familiarity with configuration/orchestration management software such as Puppet, Chef, Ansible, or Salt.
+ developer experience (DX) lead
	- current tech stack:
		* Golang
		* TypeScript
		* GCP
		* Docker
		* Terraform
		* Serverless
		* K8s
		* VueJS
		* gRPC, gRPC Remote Procedure Calls, general-purpose RPC infrastructure, Stubby
	- Gopher communication protocol
+ skill set:
	- maintain and improve Torq's automation infrastructure and CI/CD pipelines
	- improve test's infrastructure to support scale, reduce delivery times and improve the overall quality of all product aspects
	- develop E2E (UI and API) tests for Torq's management app and microservices (Python)
	- contribute to integration test framework (TypeScript) - cross platform and cross browsers tests
	- develop performance tests over k6
	- experience with UI automation frameworks
		* Playwright or Puppeteer (Node.js libraries, headless browser and usage), CDP based
		* CDP, content delivery platform - SaaS content service
		* CMS, content management system
		* CDP, continuous data protection, continuous backup, or real-time backup
		* CDP, customer data protection
	- experience writing Python-based frameworks for test automation, Python, unittest
	- experience with writing JavaScript/TypeScript -based automation framework, Jest, Mocha
	- hands-on experience working with Linux and modern apps using Docker containers and k8s
	- working knowledge of CI/CD, and cloud deployment and testing
	- experience with testing gRPC microservices
+ skill set:
	- scalable configs as code
	- developer-facing tooling
	- drive, implement, support, and maintain infrastructure services with:
		* Kubernetes
		* Envoy
		* Kafka
		* Cassandra
	- collaborate and evangelize the right cloud solutions throughout the business by creating a visionary direction and road map for infrastructure-as-a-service, IaaS
	- implement automation to perform the day to day operations/functions of the cloud platform, working across all teams
	- develop, monitor, and build alerts around error conditions and performance
	- work in a fast-paced environment while participating in conceptualizing and building CI/CD pipelines
	- be on-call as needed to support the infratructure and our systems, and drive philosophies around site reliability
	- experience in infrastructure engineering
	- experience with public cloud computing services, such as AWS and GCP
	- experience with container technologies
	- experience working with high availability and scalable SaaS (or consumer technologies)
	- experience deploying highly available and scalable, secure and reliable services with automatic failover using containers and container orchestration tools like K8s
	- use of service meshes
	- cloud formation via Terraform and Ansible
	- experience deploying container applications with helm charts
	- experience with:
		* monitoring tools:
			+ Datalog
		* on-call tooling
			+ PagerDuty
	- experience architecting, implementing, and managing environments in AWS
	- experience implementing AWS services, such as:
		* EC2 Load Balancing
		* VPC
		* Route 53
		* Direct Connect
		* NAT Gateway
		* VPN
		* EC2 Networking
		* Transit Gateway
		* Global Accelerator
+ skill set:
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
+ Specific technologies, like Spring, docker, Kubernetes, etc. are, of course, also a great help
+ skill set:
	- Excellent solid understanding of Apache Pulsar, RabbitMQ, or Apache Kafka.
	- experience in stream processing platform, such as Flink, Storm, Spark or equivalent
+ skill set:
	- Senior DevOps Engineer, Cloud IaaS (US Remote Available)
	- Splunk's IT Operations team's exciting and meaningful mission: Build, scale and maintain Splunk’s Infrastructure for all Splunkers. While various Engineering groups focus on building our products, IT Ops serves as the backbone operational support for Splunkers across the globe.
	- We are actively seeking DevOps Engineers with a real passion for automation to help build scalable tools to run our distributed systems. You will be responsible for expanding and supporting the infrastructure platform services we provide to Splunk, as well as engaging with other teams to help improve efficiency and optimize our infrastructure. You're also an individual who’s motivated by technology and enjoys automation and problem-solving. We work hard, we like to challenge the status quo, and we enjoy having fun!
	- Support and maintain IT Public/Private Cloud Infrastructure, including our virtualization and container platforms
	- Be a part of the On-Call for production issues during shift or as required.
	- Take on performance and stability issues using a wide variety of tools, including Splunk
	- Ensure that day-to-day operational requirements and SLAs are met
	- Work with key business partners to understand their requirements and recommend potential solutions, and secure resources to deliver
	- Maintain critical services and provide visibility to internal teams
	- Seek opportunities to improve or optimize processes through automation
	- 5 + years of experience as a DevOps Engineer administering/managing an AWS Public Cloud platform
	- 3+ years of experience providing automation with a major scripting language such as shell, python or go.
	- 5 + years of experience providing Linux systems administration/engineering
	- 1+ years of experience with Configuration Management tools like Ansible, Puppet or Chef
	- CI/CD pipeline tool experience (e.g. Jenkins, GitLab, etc)
	- Experience with Hashicorp toolsets Terraform, Vault, and Packer.
	- A deep understanding of networking concepts and internet protocols
	- Familiarity with Observability concepts and tools
	- Experience with Kubernetes and containers
	- Good understanding of cloud infrastructure security concepts
	- Ability to provide reliable technical support and mentorship on complex issues in a high velocity, dynamic environment
	- Ability to communicate complex technical concepts clearly to customers and upper management
+ skill set:
	- Senior Site Reliability Engineer (remote Spain)
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via Terraform as well as Kubernetes, Helm and Istio
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- Infrastructure as code experience with in one or more of Terraform, Ansible, Puppet or Salt.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio
	- Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Software Engineer - Developer Platform Infrastructure
	- We are looking for a Senior Software Engineer to help lead, design and build the next generation of our CI/CD and tools offerings. You will be working on the core infrastructure platform enabling the next generation of Splunk’s offerings.
	- CI/CD and tooling expertise. Cloud, container and virtualization experience. Innovating and scaling secure services on-prem and different cloud providers is a plus. You will use Kubernetes, Docker, UCP, AWS/GCP, Jenkins, Gitlab, Ansible.
	- Data structures and algorithms. A solid grasp of data structures, algorithms, and RESTful APIs.
	- Observability infrastructure expertise to ensure 24/7 operational excellence and data driven decision making.
	- Ability to work with multiple programming languages. We have code in several languages, ranging from Go to Python.
	- BS/MS degree in EE or CS or a related technical field or 3+ years of progressive experience.
	- Desire to learn and adapt. You will constantly be learning new areas and new technologies.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. We want you to be excited and have utmost ownership of your projects.
	- Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and observability.
	- Opportunities to develop and grow as an engineer. We are always expanding into new areas, working with open-source projects and contributing back, and exploring new technologies.
	- A team of incredibly capable and dedicated peers, all the way from engineering to product management and customer support.
	- Breadth and depth. You are interested to work on an area that dynamically scales to meet the needs of Splunk’s offerings.
	- You want to go deep into optimizing how we automate every manual process and tedious task we encounter.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe that mentors help both sides of the equation.
	- Fun. We have frequent group outings and team building events. We are committed to having every employee want to give it their all, be respectful and a part of the family, and have a smile on their face while doing it.
+ skill set:
	- Software Engineer- Tooling and Infrastructure
	- Splunk is looking for a seasoned professional engineer to join the effort to define and build the future of Splunk. Splunk is rapidly expanding their presence in the cloud, and we are looking for engineers who are interested in being founding members of the Deployment Tooling team (D4S)  that defines and builds tools to optimize how Splunk services are deployed to the public cloud.  This is a great opportunity to both lead and to learn.
	- In this role you will help Splunk to orchestrate deployments of its multi-tenant cloud platform across multiple regions, and to manage continuous deployment to these regions via Argo CD to provide for automated rolling deployments.  We are looking for candidates who have experience transitioning from operationalized deploys to automated deploys.  This is a position with broad impact–what you build will be used across all of Splunk cloud.  You need to be able to build robust solutions that are easy to use and provide exponential impact to an organization.
	- Opportunities to develop and grow as an engineer. We are at the forefront of our industry, always expanding into new areas, and working with open source & new technologies.
	- A set of talented and dedicated peers, all the way from engineering and QA to product management and customer support.
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both mentor and mentee alike.
	- An open, collaborative and supportive work environment. We embody the scrum values.  We also have a number of Employee Resource Groups for employees of all backgrounds.
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Experience in systems-level programming and distributed systems. You have knowledge of operating systems, networking and network protocols, messaging, consensus, failure modes, and parallel programming.
	- Demonstrated ability to advocate for simple and clear APIs for complex functionality. You have an API-first mentality, with the ability to build straightforward APIs to help services configure the service mesh routing, maintaining API contracts, etc.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. You actively own your work and be excited about your projects.
	- Ability to work with Golang and Python. Most of our services are written in Golang and tools are written in Python; if you are an expert at another language we can consider you, but you will be expected to program in Python and Golang.
	- Requires 3-5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience
	- 3-5 years experience in programming languages: Python, GoLang, Java, C, 
	- 2 years building and deploying cloud-native applications/services on AWS or other cloud services like GCP, Azure, etc.
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
+ skill set:
	- Principal Software Engineer - Analytics Platform (Remote)
	- We are seeking a Principal Software Engineer to help lead, design, develop and deliver Splunk's User Behavior Analytics (UBA) security analytics solution that detects known and unknown security threats at scale using big data and machine learning techniques. UBA helps security analysts quickly identify and resolve threats; delivered on customer managed resources using Kubernetes and Spark to run innovative stream processing and machine learning algorithms in near real-time.
	- We are a passionate team who care deeply about our customers and our teammates. In this role, you will work directly with Product Management, our Design Team, our Customers and other engineering teams to help derive the best experience for the customer. We have a lean process that focuses on empowering and serving our engineers as opposed to just directing them.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real world use-cases in order to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality.
	- Champion, coach and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 12+ years of Software Development experience.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience using languages such as Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, Hadoop, Cassandra.
	- Exposure to working with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Knowledge of distributed computing architectures and principles that solve for scalability, performance, redundancy and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
	- Background in developing machine learning products for the Security market a plus.
+ skill set:
	- Comfortable with Linux, Docker, AWS, GIT, Artifactory in terms of both tools and systems administration
	- Previous experience in design and implementation of solutions to evaluate and improve performance: availability, reliability, interoperability, scalability of SaaS / Cloud Native / Bigdata Platform and application with microservice architecture
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Confluence
	- Jira
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




















##	Site Reliability Engineering (SRE)





Layers of a pyramid for Site Reliability Engineering (SRE).
+ product (design and management)
+ (product/software) development
+ (infrastructure) capacity planning
+ (software) testing and release management
+ postmortem analysis, or root-cause analysis (RCA)
+ incident response
+ (cloud computing or online Web) service monitoring






+ skill set:
	- Experience monitoring cloud environments using tools like Splunk, VictorOps and Nagios
	- Knowledge of best practices related to security, performance, and disaster recovery.
	- Skilled in identifying performance bottlenecks, spotting anomalous system behavior, and determining the root cause of incidents.
+ skill set:
	- Senior Site Reliability Engineer
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via ***Terraform*** as well as ***Kubernetes, Helm and Istio***
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- Infrastructure as code experience with in one or more of ***Terraform, Ansible, Puppet or Salt***.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including ***Kubernetes, Helm, and Istio***
	- Experience operating one or more OSS technologies like ***Kafka, Cassandra, Zookeeper***; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Principal Site Reliability Engineer, Splunk Observability - remote Spain
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Principal Site Reliability Engineer, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- ***Responsible for automating & operationalizing cloud provider infrastructure via Terraform, Kubernetes, Helm and Istio***
	- ***Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.***
	- ***Own & operate critical back-end open-source services like Cassandra, Kafka, Elasticsearch, MongoDB, and Zookeeper.***
	- Build tools and design processes that help improve observability and system resiliency.
	- ***Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.***
	- Implement service level metrics & service level objectives that act as service-level health indicators.
	- ***Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.***
	- Strong coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code experience within one or more of Terraform, Ansible, Puppet or Salt.***
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- ***Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)***
	- ***Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio***
	- ***Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus***
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 12+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Site Reliability Engineer (Stack Automation Service Team)
	- Splunk's Cloud group is looking for an experienced Site Reliability Engineer to join a team that is responsible for Cloud’s operational infrastructure and delivery. As a member of the Stack Automation Service team, you will be responsible for maintaining and troubleshooting Splunk's SaaS system, monitoring system stability and performance, troubleshooting complex problems, performing Amazon instance maintenance and system upgrades, and managing Amazon server/storage deployments, all while collaborating with various other Splunk Cloud teams. This is a fantastic opportunity to work with an exceptional team, grow your cloud experience, and help drive the growth of Splunk Cloud.
	- Puppet experience. You have at least 2 years of Puppet experience, including writing Puppet code and configuration management.
	- Python or Bash scripting experience. You will develop scripts and tools in Python/Bash.
	- AWS experience. Knowledge of Amazon EC2 including machine image management and storage, as well as an understanding of Amazon EC2 regional centers, availability zones, and HA strategies
	- Unix/Linux. You will use a command line terminal frequently.
	- Multi-tenant infrastructure experience. Experience supporting customer facing multi-tenant infrastructure (SaaS) or similar cloud related services
	- Software Development and Data Structures/Algorithms. We code primarily in Golang and Ruby, and work with RESTful APIs.
	- Cloud and container experience. Building and scaling secure services on different cloud providers.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and disaster recovery.
	- Problem Solving. You are able to fix a product outage, skilled in identifying performance bottlenecks, spotting anomalous system behavior, and figuring out the root cause of incidents.
	- Desire to learn and adapt. Our team has many projects going on at once, and you'll have the opportunity to learn to navigate new code and features.
	- Passion. We want you to actively own your work and be excited about your projects.
	- ***Kubernetes experience. Working in Kubernetes systems with experience in kubectl and docker containers.***
	- Terraform experience. Any prior work with Terraform is a plus.
	- ***Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















##	DevSecOps




DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term "shift left" can be used. Security is tested in three main areas: static, software composition, and dynamic.











+ skill set:
	- Senior Client Platform Engineer
	- Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
	- Are you the kind of systems engineer that has a passion for administering enterprise software using best of breed technologies? Are you self-motivated and require minimal supervision? Do you put together a rolling 12-month roadmap to execute against? If so, then this is the dream job you've been looking for. 
	- Splunk is looking for a highly skilled Senior Systems Engineer focusing on endpoint security, and configuration management. You should be comfortable delivering at a high level in a fast paced and growing environment. You will drive standardization and management for our endpoints along with a number of enterprise applications and services. This role provides high visibility and impact to both the CIO and CISO organizations.
	- Architecture, design, integration, implementation, operation, and support of enterprise-wide applications and services for our Windows fleet.
	- Assisting in developing long-term strategies and capacity planning for meeting future end user needs
	- Configuration Management for Windows using industry standard tools to meet Security requirements and comply with CIS benchmarks
	- Managing configuration of our endpoint security software such as endpoint detection and response, application allow/block lists, and host-based intrusion detection software
	- Partnering with the Security Engineering leads to coordinate efforts, initiatives, and roadmaps
	- Administer enterprise software including deployment and package management
	- Write scripts/policies that automate application and settings distribution using internal tools
	- Manage transition plans for major upgrades or patches
	- Integrate with other internal systems and tools
	- Manage and report on application performance against KPI’s
	- Work as the escalation point between various support teams for issues on the client platform
	- Work as a liaison from the Splunker Technology Success org to other IT Service organizations to deliver feature enhancements and best in class solutions through shared products and goals
	- Drive client security models and best practices in an enterprise environment
	- Drive business decisions through data using tools like Splunk
	- Diagnose and investigate unique and complex systemic problems
	- Develop solutions that meet the business needs to complex customer requirements
	- 10+ years of overall IT experience; 5+ years experience of providing application support and engineering
	- Experience with implementing security standards and compliance across a huge enterprise organization
	- Knowledge of bash/python scripting
	- Experience with Endpoint Management platforms such as WorkspaceONE/InTune/LANdesk/Kace/etc.
	- Experience with DevOps platforms such as Puppet/Salt/Chef
	- Ability to work in high pressure, highly flexible environment against both short and long term requirements
	- Passionate about technology and solving IT operations-focused problems
+ skill set:
	- Software Engineer - Analytics Platform (Remote)
	- Are you passionate about working on products that make a difference for your customers? Do you enjoy building large scale applications that are powered by huge data sets? Do you value working in an environment where you're empowered to make key technical decisions across a full stack of technologies? If so, a role on the Splunk Security Analytics team might be a great fit for you.
	- As a Software Engineer on the Security Content Engineering team, your primary focus is content distribution, content improvement, and content assurance. You will contribute to build and maintain the content distribution system and support content improvements.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify, and improve designs. Help put together a plan for how to make it happen using Agile Methodologies.
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality while continuously deployed in the cloud to our customers.
	- Participate in the software development lifecycle by writing code, tests, documentation; support the sprint management process; and communicate effectively with peers and managers.
	- 2+ years of Software Engineering experience.
	- Bachelor's degree in Computer Science or equivalent training and work experience.
	- Proficient with Java or Python programming.
	- Engagement with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Exposure to an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Versed with CI/CD frameworks and experience with automation.
	- Strong oral and written communication skills, including a demonstrated ability to prepare documentation and presentations for technical and non-technical audiences.
	- Background in developing products for the Security market, a plus.
+ skill set:
	- Senior Software Engineer - Analytics Platform (Remote)
	- Enterprise Security behavioral analytics service (Advanced Analytics) is Splunk’s next-generation, cloud-native, multi-tenant analytics solution that detects known and unknown security threats at petabyte scale. Advanced Analytics detects cybersecurity threats by using stream and batch processing, and building large scale analytics infrastructure for petabyte scale data ingestion, processing, storage, and analysis. Advanced Analytics will power large and medium scale enterprises to combat security threats, protect brand reputation and protect intellectual property.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep aware of security trends in the industry and bring that knowledge back to the team.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best software development practices.
	- Champion, coach, and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 8+ years of experience in Enterprise Software Engineering.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience with Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, HBase, Cassandra, MongoDB.
	- Exposure to working with cloud environments (AWS, Azure, GCP) and container ecosystems (Docker and Kubernetes).
	- Knowledge of distributed computing architectures and principles that solve for scalability, consistency, availability, performance, and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












##	ITOps





ITOps (for data centers):
+ maintain core infrastructure and associated services needed to support AI and machine learning models running in business applications
+ enterprise AI role:
	- provide highly available, secure infrastructure to operate performant models in enterprise application to scale, 24x7
	- deliver performance metrics for deployed models and applications
	- plan and execute infrastructure evolution to support AI technologies
+ for ModelOps
	- centralized catalog of all:
		* models
		* model runtime requirements
		* lineage
		* operational history
	- regardless of tools to create AI and ML models, support standardized models on any infrastructure (prem, cloud, or hybrid) that can be consistently:
		* deployed
		* monitored
		* controlled
	- provide automated alerts regarding model performance and behavior
	- automated processes and approvals for:
		* deployment
		* testing
		* refresh
		* monitoring
	- real-time visibility to performance of deployed models
+ Information Technologies Operations (ITOps) is the process responsible for acquiring, designing, deploying, configuring, and maintaining the physical and virtual components that comprise your IT infrastructure
+ While ITOps takes a broad view of the entire technology landscape that your organization relies on to conduct its business mission, DevOps focuses on the task at hand.
	- DevOps teams don’t always have the visibility or awareness of the downstream implications on the enterprise stack.
	- And this is where can modern ITOps team can help.
+ ITOps (or) Tech Ops is the most traditional Ops that refers to managing all the physical and software components of an organization’s IT environment.
	- It is responsible for the smooth running of a business by handling applications, delivery, maintaining services, and the underlying technologies administrated by a company's IT members to its internal or external clients.
	- It includes:
		* maintaining networks
		* ensuring security
		* managing data center
		* system administrators
		* regulatory compliance
		* licensing
		* managing software
		* managing help desk
		* supports IT infrastructure and systems for daily business operations
	- IT department can effortlessly manage all the IT operations and make the job easier by using IT Operations.
		* It guides the business to be more secure, swift, and productive.
+ includes:
	- networking operations
	- deploying, maintaining, and configuring applications
	- overseeing both physical and virtual components of a company's IT environment
+ includes:
	- administration and maintenance,
	- network management,
	- systems management,
	- technical support (help desk)



ITOps processes consists of 5 main aspects/phase:
+ Run Solutions
	- The primary purpose of having ITOps teams is to run solutions such as data backups, configurations, handling servers, and restoring systems after an outage or update.
	- The aim is to optimize the performance and allocate the proper resource for the most effective delivery.
+ Infrastructure
	- Managing the IT infrastructure includes maintaining, provisioning, setting up, and updating all the software and hardware applications and cloud resources of the company's IT infrastructure.
	- These components include:
		* operating systems
		* hypervisors
		* network infrastructure
		* platforms
		* container environments
		* physical servers
		* application software
	- The maintaining oversight of IT infrastructure is made on-premises data center or in the cloud.
+ Network
	- Network management is responsible for managing all network functions for internal or external IT communications.
	- Network management is also involved in configuring and managing telecommunication lines.
	- In addition, it allows authorized customers to secure remote access to the company's network.
+ Security
	- Security management is an integral function of IT service management (ITSM).
		* It deals with:
			+ securing the hardware and software assets
			+ implementing security within development operations
			+ managing access control
			+ ensuring that security standards have reached across the IT environment of the organization
+ Problem Solving
	- Event management or incident management is also known as Problem-solving, it can be divided into 2 types – preventive measures and reactive measures.
		* Preventive Measures
			+ Preventive measures reduce the possibility of disasters and find the solution to anticipate and avoid any negative impacts on the IT environment.
		* Reactive Measures
			+ This concept refers to cyber-attacks, critical situations, and other issue when implementing disaster recovery plans and help desk management services.




ITOps tasks:
+ Network infrastructure:
	- Configuring and managing all networking functions for internal and external IT communications
	- Configuring and managing telecommunication lines
	- Managing firewall ports to allow the network to communicate with outside servers
	- Providing authorized users secure remote access to the organization’s network
	- Monitoring network health and performance, detecting anomalies, and preventing or quickly resolving issues, which may include building and managing a network operations center (NOC, pronounced “knock”), a centralized physical location from which ITOps teams can continuously monitor a network
+ Server and device management:
	- Configuring, maintaining and managing servers for infrastructure and applications
	- Managing network and individual storage to ensure they meet application requirements
	- Setting up and authorizing email and file servers
	- Provisioning and managing company-approved PCs
	- Provisioning and managing cell phones and other mobile devices
	- Managing licensing and desktop, laptop and mobile device software
+ Computer operations and help desk:
	- Managing data center locations and equipment
	- Operating the help desk
	- Creating, authorizing and managing all user profiles on organizational systems
	- Providing network configuration auditing information to regulatory agencies, business partners and other outside entities
	- Ensuring high availability of the network and disaster recovery plans
	- Alerting users when a major incident impacts network services
	- Instituting regular backups to facilitate data recovery when needed
	- Maintaining the ITIL for the organization








ITOps addresses:
+ infrastructure capacity
+ infrastructure availability
+ infrastructure security


KPIs for ITOps are based on:
+ application performance
+ infrastructure availability
+ infrastructure security
+ infrastructure cost




*Information Technology Infrastructure Library, ITIL*:
+ functions of *IT Operations Management framework*
	- ITOps, or IT operations, refers to the processes and services administered by an organization's IT staff to its internal or external clients.
	- applications management
	- technical management
	- service desk







+ skill set:
	- 4+ years of experience in developing, planning, and and administering VMware ESXI and vCenter deployments
	- Experience with vSphere, vSAN, and other VMware products and platforms
	- Experience with troubleshooting for virtualized platform
	- Ability to provide and manage virtual machine configurations, including networking, storage and security settings and deploy it to hosts upon request
	- Knowledge of data center networking, including TCP/IP, switching/routing, ports and protocols, firewall concepts, or load-balancing
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Knowledge of data center architecture: power, cooling, and networking.
	- Significant experience working with data center hardware and writing software to make that easier, faster, and less manual effort
	- Familiarity with best practices for hardware acceptance testing






















##	 IT operations analytics, ITOA


IT operations analytics (ITOA) is an approach or method to retrieve, analyze, and report data for IT operations.
+ Or, known as:
	- advanced operational analytics
	- IT data analytics
+ ITOA may apply big data analytics to large datasets to produce business insights.
+ ***ITOA is different than AIOps, which focuses on applying artificial intelligence and machine learning to the applications of ITOA.***
+ The use of mathematical algorithms and other innovations to extract meaningful information from the sea of raw data collected by management and monitoring technologies.



Context:
+ (IT) systems management
	- Fault, Configuration, Accounting, Performance, Security (FCAPS)
	- Distributed Management Task Force (DMTF)





applications of ITOA systems:
+ Root cause analysis:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored can help users pinpoint fine-grained and previously unknown root causes of overall system behavior pathologies.
+ Proactive control of service performance and availability:
	- Predicts future system states and the impact of those states on performance.
+ Problem assignment:
	- Determines how problems may be resolved or, at least, direct the results of inferences to the most appropriate individuals or communities in the enterprise for problem resolution.
+ Service impact analysis:
	- When multiple root causes are known, the analytics system's output is used to determine and rank the relative impact, so that resources can be devoted to correcting the fault in the most timely and cost-effective way possible.
+ Complement best-of-breed technology:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored are used to correct or extend the outputs of other discovery-oriented tools to improve the fidelity of information used in operational tasks (e.g., service dependency maps, application runtime architecture topologies, network topologies).
+ Real time application behavior learning:
	- Learns & correlates the behavior of Application based on user pattern and underlying Infrastructure on various application patterns, create metrics of such correlated patterns and store it for further analysis.
+ Dynamically baselines threshold:
	- Learns behavior of Infrastructure on various application user patterns and determines the Optimal behavior of the Infra and technological components, bench marks and baselines the low and high water mark for the specific environments and dynamically changes the bench mark baselines with the changing infra and user patterns without any manual intervention




Types of ITOA:
+ Log analysis
+ Unstructured text indexing, search and inference (UTISI)
+ Topological analysis (TA)
+ Multidimensional database search and analysis (MDSA)
+ Complex operations event processing (COEP)
+ Statistical pattern discovery and recognition (SPDR)

















+ skill set:
	- experience with large-scale distributed storage and database systems
		* SQL
		* NoSQL
		* MySQL
		* Cassandra
	- data processing experience with building and maintaining large-scale and/or real-time complex data processing pipelines using:
		* Kafka
		* Hadoop
		* Hive
		* Storm
		* Zookeeper
	- experience with developing complex software systems scaling to substantial data volumes or millions of users with production quality deployment, monitoring, and reliability
	- experience running scalable (thousands of RPS) and reliable (three 9's) systems
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




##	 Artificial Intelligence for IT Operations, AIOps



Artificial Intelligence for IT Operations (AIOps) is an industry category for machine learning analytics technology that enhances IT operations analytics.


Goals of AIOps:
+ enable IT transformation
+ receive continuous insights that provide continuous fixes and improvements via automation



AIOps can be viewed as CI/CD for core IT functions.
+ Given the inherent nature of IT operations, which is closely tied to cloud deployment and the management of distributed applications, AIOps has increasingly led to the coalescence of machine learning and cloud research.



main aspects of an AIOps platform:
+ machine learning
+ big data

In order to collect observational data and engagement data that can be found inside a big data platform, which requires a shift away from sectionally segregated IT data, a holistic machine learning and analytics strategy is implemented with the combined IT data.





AIOps tasks include:
+ automation
+ performance monitoring
+ event correlations



AIOps process:
+ The *normalized data* is suitable to be processed through machine learning algorithms to *automatically reduce noise* and *identify the probable root cause of incidents*.
	- The main output of such stage is the ***detection of any abnormal behavior from users, devices or applications***.
+ Noise reduction can be done by various methods, but most of the research in the field points to the following actions:
	- Analysis of all incoming alerts;
	- Remove duplicates;
	- Identify the false positives;
	- Early *anomaly, fault and failure* (AFF) detection and analysis.[13]
+ ***Anomaly detection*** - another step in any AIOps process is based on the analysis of past behavior of users, equipment and applications. Anything that strays from that behavior baseline is considered unusual and flagged as abnormal.
+ ***Root cause determination*** is usually done by passing incoming alerts through algorithms that take into consideration correlated events as well as topology dependencies. The algorithms on which AI are basing their functioning can be influenced directly, essentially by "training" them.





AIOps platforms enabling IT operations management (ITOM)
+ inputs:
	- historic data
	- real-time streaming data
	- vendor-agnostic data ingestion
+ input types:
	- logs
	- metrics
	- wire data
	- document text
+ observe (monitoring)
+ act (IT Service Management process, ITSM)
+ engage (monitoring)
+ machine learning with *Big Data*
+ outputs:
	- historical analysis
	- anomaly detection
	- performance analysis
	- correlation and contextualization


Factors of IT maturity:
+ organizational structures
+ processes and practices
+ skills and knowledge,
+ tools and policies
+ systems and data
+ documents and agreements



cause-and-effect sequence
+ outcomes
+ capabilities
+ IT maturity
+ IT excellence
+ business maturity, innovation, and productivity




Role of IT suppert services:
+ aware
	- The organization is aware of its chaotic stage and needs.
	- IT capabilities are unstable and success depends on the individuals' effort and technical knowledge.
+ committed
	- IT operations are more process-oriented, thus more reputable.
	- Success depends on process adherence and point collaboration.
	- This level is a stable plateau where IT can "keep the lights on."
+ proactive
	- IT organization is recognized as "mature."
	- At this level, IT has reached a tipping point from which a path to high IT maturity is accessible.
+ aligned
	- IT is a highly efficient internal service provider, offering a stable portfolio of optimized services.
+ business partner
	- IT is a trusted partner and innovator for the business












 










Applications of AIOps:
+ analysis of large and unconnected datasets, or large number of un-normalized databases
	- including aggregated data
+ Automation of tasks (DevOps)
+ Machine learning platforms
+ Augmented reality
+ Agent-based simulations
+ Internet of things (IoT)
+ AI Optimized Hardware
+ Natural language generation
+ Streaming data platforms
+ Conversational BI and analytics













+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










##	CloudOps



CloudOps include:
+ cloud-specific flexible provisioning and scalability of environments,
+ built-in task automation.













##	NoOps



NoOps (or) No Operation is the new idea that completely automates a software environment from the underlying system infrastructure through technologies including machine learning (ML), and Artificial Intelligence (AI). As a result, there is no need for any operation team to manage software in-house.

With NoOps, developers can concentrate solely on writing and improving the software product’s code that improves the resources like management, security, infrastructure, product, and operations part of the lifecycle. Additionally, the service providers offer developers to develop software like resources, backups, patches, and the right cloud infrastructure to work independently without any interference.

***Serverless architecture*** is the best example for NoOps software. The developer’s team aims to create their application and deploy them in ***serverless computing*** without interfering any operational or infrastructure considerations.

Operating the right tools in NoOps can achieve a faster deployment process than DevOps by running Platform as a Service (PaaS) or Function as a Service (FaaS) in the cloud. Moreover, NoOps can easily be adaptable for Product as a Service companies, small-scale applications, and start-ups.