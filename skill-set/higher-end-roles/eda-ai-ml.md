#	EDA, AI, (Applied) Machine Learning, MLOps, ModelOps, Data Science, Data Engineering, DataOps, MIS, & Corporate Research Labs



##	Important Information about Innovation Management


Important [technology roadmaps](https://en.wikipedia.org/wiki/Technology_roadmap) to pay attention to, so that I can create my own:
+ Semiconductor pathfinding for research and development (R&D) activities
	- References:
		* [Semiconductor Pathfinding and Development](https://www.thermofisher.com/us/en/home/semiconductors/pathfinding.html)
			+ Directed from Thermo Fisher Scientific -> Applications & Techniques -> Industrial & Applied Sciences -> ***Semiconductor Analysis*** -> Applications -> Semiconductor Pathfinding and Development
				- https://www.thermofisher.com/us/en/home/semiconductors.html#applications
			+ Use cases:
				- [NEXS Software, CAD navigation and design debug solutions](https://www.thermofisher.com/us/en/home/electron-microscopy/products/software-em-3d-vis/nexs-software.html) 
				- Circuit edit and nanoprobing
					* [use of high-resolution focused ion beams (FiBs) and advanced chemistry to perform "nanosurgery" on semiconductor devices](https://www.thermofisher.com/us/en/home/semiconductors/circuit-edit.html)
				- defect localization and analysis
				- physical and chemical characterization
+ additional resources for:
	- environmental scanning
		* horizon scanning, horizon scan
			+ trend analysis
			+ issue tree, logic tree
			+ scenario planning, scenario thinking, scenario analysis, scenario prediction, scenario method
			+ morphological analysis, or general morphological analysis, in the context of problem solving
		* analyze the [***market environment*** and ***business environment***](https://en.wikipedia.org/wiki/Market_environment)
	- determine scientific lacuna, or knowledge gaps in science and ***engineering***
	- technological readiness assessment, TRA
		* determine the [technological readiness level, TRL](https://en.wikipedia.org/wiki/Technology_readiness_level)
			+ basic technology research, TRLs 1-2
			+ research to prove feasibility, TRLs 2-4
			+ technology development, TRLs 3-5
			+ technology demonstration, TRLs 5-7
			+ system/subsystem development, TRLs 6-9
				- system test, launch, and operations, TRLs 8-9
	- [technological innovation system](https://en.wikipedia.org/wiki/Technological_innovation_system)
		* Entrepreneurial activities
		* Knowledge development
		* Knowledge diffusion / knowledge exchange through networks
		* Guidance of the search
		* Market formation
		* Resource mobilization
		* Support from advocacy coalitions
	- ethics in EDA and machine learning research
		* [differential technological development](https://en.wikipedia.org/wiki/Differential_technological_development)
		* [proactionary principle](https://en.wikipedia.org/wiki/Proactionary_principle)
		* [precautionary principle, or precautionary approach](https://en.wikipedia.org/wiki/Precautionary_principle)
		* [postcautionary principle](https://en.wikipedia.org/wiki/Postcautionary_principle)
	- list of research labs, such as corporate research labs, research universities, and non-profit research institutes, that perform [***exploratory engineering***](https://en.wikipedia.org/wiki/Exploratory_engineering)
	- [technology forecasting](https://en.wikipedia.org/wiki/Technology_forecasting)
		* [technology scouting](https://en.wikipedia.org/wiki/Technology_scouting)
			+ part of [corporate foresight](https://en.wikipedia.org/wiki/Corporate_foresight)
			+ improves [***competitive intelligence***, CI](https://en.wikipedia.org/wiki/Competitive_intelligence) to improve competitive strategy
		* part of [Technology management](https://en.wikipedia.org/wiki/Technology_management)
			+ part of [engineering management](https://en.wikipedia.org/wiki/Engineering_management)
			+ technology strategy
			+ technology forecasting
				- technology scouting
			+ technology roadmap
				- map technologies to business and market needs
			+ technology project portfolio
			+ [innovation management](https://en.wikipedia.org/wiki/Innovation_management)
				- Knowledge management tools
				- Market intelligence techniques
				- Cooperative and networking tools
				- Human resources management techniques
				- Interface management approaches
				- Creativity development techniques
				- Process improvement techniques
				- Innovation project management techniques
				- Design and product development management tools
				- Business creation tools
	- technology assessment
	- ***cross-impact analysis***
	- [frugal innovation/engineering](https://en.wikipedia.org/wiki/Frugal_innovation)
		* jugaad, or jugaar, Hindi for a stop-gap solution
		* inclusive innovation
		* catalytic innovation
		* reverse innovation
		* bottom of the pyramid (BOP) innovation
			+ bottom of the income pyramid
			+ bottom of the wealth pyramid





##	EDA, Electronic Design Automation


Skills for EDA software development, and other high-end software development:
+ Production quality coding standards and patterns.
+ Build system experience, like:
	- Apache Buildr, historic open-source build system, Rake-based, gives the full power of scripting in Ruby with integral support for most abilities wanted in a build system
	- ***Bazel***
		* For automating the building and testing of software.
		* Derived from the Google internal tool, Blaze.
		* for multiple programming languages
		* Build systems most similar to Bazel are:
			+ Pants
			+ Buck
			+ Please
	- Blaze, predecessor to Bazel
	- boost.build, for C++ projects, cross-platform, based on Perforce Jam
	- Buildout, a Python-based build system for creating, assembling and deploying applications from multiple parts
	- ***CMake***
	- Gradle, for JVM software, and C and C++
	- ***Jenkins***, an extensible continuous-integration engine, forked from Hudson
	- language-specific build systems:
		* Ant and Maven for Java
			+ ***Apache Ant***, popular for Java platform development and uses an XML file format
			+ ***Apache Maven***, a Java platform tool for dependency management and automated software build
		* ***Leiningen*** for Clojure
		* ***sbt*** for Scala
	- ***Maven*** (for JVM software, and other languages, such as C\# and Ruby)
	- ***Meson*** is a software tool for automating the building (compiling) of software. 
	- ***SCons***, Python-based, with integrated functionality similar to autoconf/automake
	- Stack, a tool to ***build Haskell projects***, manage their dependencies (compilers and libraries), and for testing and benchmarking
	- tinyrick, a ***Rust build tool***
	- ***Travis CI***, a hosted continuous-integration service
	- ***Waf***, a Python-based tool for configuring, compiling and installing applications. It is a replacement for other tools such as Autotools, Scons, CMake or Ant
	- Turbo, or Turborepo, for building JavaScript or TypeScript Web applications
+ project management tools:
	- Working with Jira and Confluence a plus.
		* Jira, for issue tracking.
		* Confluence, wiki-based collaboration platform
+ CI/CD with:
	- ***Docker***, software platform for container orchestration by exploiting OS-level virtualization
	- ***Kubernetes***, for container orchestration
	- AWS
	- continuous integration (CI) systems, deployment tools/platforms, such as:
		* ***Jenkins***
		* TeamCity
		* GitHub Actions
		* GitLab
		* Circle CI
		* Terraform
		* Saltstack/Ansible
		* Semaphore
		* Slurm
	- ***CI/CD pipelines***
	- microservice architecture
	- infrastructure as code
+ configuration management:
	- software configuration management includes:
		* revision/version control
		* build automation
		* system configuration
		* process management
		* environment management
		* defect tracking
	- Experience with configuration management systems (Ansible and/or Puppet, Saltstack)
		* ***Ansible***, software tool suite to enable infrastructure as code, Python-based
		* CFEngine
		* Chef, Ruby-based
		* LCFG
		* ***Nagios Core***, or ***Nagios***, for monitoring systems, networks, and infrastructure
		* NixOS Declarative configuration model
		* OpenMake Software Release Engineer
		* Otter
		* ***Puppet***, for software configuration to specify system configuration, Ruby-based
		* Rex, Perl-based
		* Salt, Python-based
		* ***Saltstack***, for event-driven IT automation, remote task execution, and configuration management
+ tools for agile methods, such as XP and Scrum:
	- Git
	- JIRA
	- Confluence
+ package managers:
	- Mamba
		* reimplementation of *conda* package manager in *C++*.
+ documentation
	- *LaTeX*
	- Markdown
		* MDX markup language, authorable format for writing JSX in Markdown documents
			+ JSX, or JavaScript Extension, or JavaScript XML
				- similar in appearance to HTML
+ skill set:
	- Full ownership including: Designing, Implementing, Testing and Metric Analysis.
	- Production quality coding standards and patterns.
+ Hibernate ORM is an object-relational mapping tool for the Java programming language
	- object-relational mapping allows software developers to convert data between type systems using object-oriented programming languages, OOPL.
+ ***Where possible, exploit [incremental computing](https://en.wikipedia.org/wiki/Incremental_computing), to speed up the performance of EDA tools that I develop.***
	- use "checkpoint"s to save temporary results of computing
		* This allows results from computation performed thus far to be reused.
		* If computation crashes and has to be restarted from the most recent or second last checkpoint, this checkpoint provides intermittent results that the software can use to resume computing.
+ skill set:
	- Strive for high code standards (continuously improving testability and code quality).
	- Disciplined, methodical, minimalist approach to design and construct layered software components that can be embedded within larger frameworks or applications.
	- ***experiment driven development***
+ Proven capability to create maintainable, adaptable software that is non-brittle and capable of change
+ Take pride in the quality of the code you write. Your code is readable, testable, and understandable six months later. You adhere to the Zen of Python.
+ experience with these software testing methodologies:
	- unit tests
	- integration tests
	- regression tests
	- smoke tests
	- load tests
	- chaos tests
+ Software libraries
	- C++ libraries:
		* Boost C++
		* http://doc.hc2.ch/c_cpp/en/cpp/links/libs.html
		* Eigen, C++ library for numerical computing
	- Python libraries:
		* NetworkX, for graph computing
		* NumPy, for numerical linear algebra and tensor algebra
		* SciPy
			+ for scientific computing, or computational science, and computational engineering
			+ for the following:
				- optimization
				- linear algebra
				- integration
				- ODE solvers
				- interpolation
				- FFT
				- signal processing
				- image processing
		* mpmath, for arbitrary-precision floating-point arithmetic
		* Biopython, for computational biology and bioinformatics
		* CuPy, for GPU-accelerated parallel programming
		* Distributed Evolutionary Algorithms in Python, DEAP, framework for evolutionary computing to enable rapid prototyping and experimentation to test ideas
	- for symbolic computing
		* SageMath, computer algebra system, CAS
			+ SageManifolds
			+ has bindings for C++ and Python
	- numerical computing:
		* GNU Octave
		* MATLAB, and Simulink
		* FreeMat
		* Intel oneAPI Math Kernel Library, Intel oneMKL
			+ formerly Intel Math Kernel Library, Intel MKL
		* Eigen, C++ library for numerical computing
+ parallel and distributed computing
	- parallel computing
		* parallel data structures
		* parallel algorithms
		* ***Knowledge of parallelism in shared (Intel TBB, OpenMP) and distributed (Intel MPI, Apache Spark, Dask) memory***
		* Knowledge of parallelism in shared memory:
			+ Intel TBB
			+ OpenMP
			+ Dask, for Python
			+ SCOOP, Scalable Concurrent Operations in Python, for Python
		* Knowledge of OpenCL/SYCL languages
			+ OpenCL, Open Computing Language
				- low-level API, and parallel computing/programming framework and run-time for heterogeneous platforms of:
					* general-purpose processors
					* graphics processors
					* digital signal processors
					* FPGAs
					* domain-specific architectures, including hardware accelerators
				- compile and execute kernel programs (kernels) in parallel in computer systems with heterogeneous system architecture (HSA)
				- enables GPGPU computing
				- speeds up numerical computing, and computation for applied machine learning and data science
				- has two APIs, application programming interfaces:
					* platform layer API
					* runtime API
			+ SYCL:
				- higher-level programming model for improving programming productivity on hardware accelerators
				- single-source embedded domain-specific language (eDSL) based on pure C++17
				- royalty-free, cross-platform abstraction layer for developing software that are executed on heterogeneous platforms
				- single-source C++ programming model for heterogeneous computing
		* GPU programming, GPGPU, using:
			+ NVIDIA CUDA
				- NVIDIA cuDNN
			+ OpenCL
	- distributed computing
		* distributed data structures
		* distributed algorithms
		* Knowledge of parallelism in distributed memory:
			+ Intel MPI
			+ Apache Spark
			+ ***Dask***
+ workflow management:
	- goals/tasks:
		* for data engineering pipelines
	- use ***workflow management systems, WfMS, WFMS*** for specific applications, such as:
		* data science
		* machine learning
		* Avoid the use of workflow management systems, WfMS, WFMS, for business process modeling and other activities not related to my projects.
	- Adobe Workfront, with built-in workflow management systems, WfMS, WFMS
	- ***Apache Airflow***
		* Cloud Composer, for Google Cloud Platform, GCP
	- Apache Flink
	- Apache Taverna ???
	- Azkaban
	- Collective Knowledge, CK
	- Cuneiform programming language
		* based on Erlang, functional programming language
	- Jenkins
	- Jira, with built-in workflow management systems, WfMS, WFMS
	- [***Luigi***](https://github.com/spotify/luigi), for workflow management and managing ML pipelines (machine learning pipelines)
	- Oozie
	- research workflow
		* The Collective Knowledge (CK) project is an open-source framework and repository to enable collaborative, reproducible and sustainable research and development of complex computational systems. CK is a small, portable, customizable and decentralized infrastructure helping researchers and practitioners
	- Salesforce.com Process Workflow
+ program analysis tools:
	- PerfView
		* CPU, memory, garbage collection
+ 














###	EDA: Electronic Design Automation Job Opportunities


####	Integrated Device Manufacturers (IDMs)


IDMs that may have their own EDA software develoment group:
+ [From Wikipedia, list of integrated device manufacturers (IDMs)](https://en.wikipedia.org/wiki/Integrated_device_manufacturer)
+ [From Wikipedia, list of semiconductor IP core vendors](https://en.wikipedia.org/wiki/List_of_semiconductor_IP_core_vendors)
+ [From Wikipedia, list of the top semiconductor companies in terms of the most revenue](https://en.wikipedia.org/wiki/Semiconductor_industry)


####	EDA Companies


List(s) of EDA companies:
+ BLAH






###	Machine Learning for EDA

+ skill set:
	- Required Machine Learning Experts for EDA Products, we need people who are passionate about technology, constantly seeking to learn and improve the skill set with good communication and interpersonal skills.
	- Should be proficient in Python and applying ML Algorithms.
	- Good knowledge of machine learning algorithms like Neural network, CNN, Logistic regression, KNN, Random forest, decision tree, clustering etc.
	- knowledge of C with good programming skills & logical interpretation.
	- Decent depth in understanding ML algorithm concepts like supervised/unsupervised, regression/classification, time series algorithms
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







###	System-Technology Co-Optimization, STCO


####	Notes about STCO & DTCO

By definition, System-Technology Co-Optimization, STCO, includes Design-Technology Co-Optimization (DTCO).


System-Technology Co-Optimization, STCO
+ Design-Technology Co-Optimization, DTCO
+ AutoML for STCO
+ Benchmarking
	- for ***non- von Neumann computing paradigms***
		* hybrid non- von Neumann computing paradigms:
			+ quantum computing + optical computing
				- linear optical quantum computing, linear optics quantum computation, LOQC
					* Boson sampling
					* KLM scheme, KLM protocol
		* ***optical computing, photonic computing***
			+ approaches:
				- computing by xeroxing on transparencies
				- Ising machines
				- masking optical beams
				- optical Fourier co-processors
				- time delays optical computing
				- wavelength-based computing
			+ nanophotonics, or nano-optics
				- metamaterials
				- near-field optics
				- microphotonics
				- biophotonics
					* biofluorescence
					* biolasing
					* bioluminescence
					* biophosphorescence
					* fluorescence resonance energy transfer, Forster resonance energy transfer, FRET
			+ optical rectenna, or optical rectifying antenna
				- ["An optical rectenna is a rectenna (rectifying antenna) that works with visible or infrared light. A rectenna is a circuit containing an antenna and a diode, which turns electromagnetic waves into direct current electricity. While rectennas have long been used for radio waves or microwaves, an optical rectenna would operate the same way but with infrared or visible light, turning it into electricity."](https://en.wikipedia.org/wiki/Optical_rectenna)
			+ optical transistors
				- ["An optical transistor, also known as an optical switch or a light valve, is a device that switches or amplifies optical signals. Light occurring on an optical transistor's input changes the intensity of light emitted from the transistor's output while output power is supplied by an additional optical source. Since the input signal intensity may be weaker than that of the source, an optical transistor amplifies the optical signal. The device is the optical analog of the electronic transistor that forms the basis of modern electronic devices. Optical transistors provide a means to control light using only light and has applications in optical computing and fiber-optic communication networks. Such technology has the potential to exceed the speed of electronics, while conserving more power."](https://en.wikipedia.org/wiki/Optical_transistor)
			+ photonic ICs, PIC, integrated optical circuit
			+ photonic logic
				- require resonators
			+ silicon photonics
			+ related emerging technologies:
				- phased-array optics, optical phased array, OPA
				- screenless displays:
					* categories:
						+ visual image
						+ retinal direct
						+ synaptic interface
					* examples:
						+ virtual retinal display, VRD, retinal scan display, RSD, retinal projector, RP
							- use adaptive optics, AO
						+ bionic contact lens
						+ fog display, fog screen, vapor screen, vapor display
					* applications:
						+ augmented reality
						+ virtual reality
				- volumetric display devices, 3-D displays
					* are autostereoscopic				
		* in-memory computing
		* hyperdimensional computing, HDC
		* quantum computing
			+ based on [***atomtronic circuits***](https://en.wikipedia.org/wiki/Atomtronics)
		* hypercomputation, super-Turing computation
	- for heterogeneous system architectures, HSA
	- for von Neumann computing
	- other applications:
		* memory subsystems
			+ racetrack memory, domain-wall memory, DWM
		* digital scent technology, olfactory technology
			+ sense, transmit, and receive scent-enabled digital media
			+ sensor implementations:
				- olfactometers
					* dynamic dilution olfactometers
					* field olfactometers
					* flow olfactometers
				- electronic noses
				- fluctuation-enhanced sensing, FES
					* based on higher-order statistics, HOS
			+ scentography devices
			+ machine olfaction
		* electronic skin
			+ includes:
				- tactile sensors
			+ conductive electronic skin
			+ flexible and stretchy electronic skin
			+ recyclable electronic skin
		* electronic tongues
			+ artificial taste
		* flexible electronics, flex circuits
			+ flexible display, or rollable display
			+ flexible printed circuits, FPC
		* printed electronics
			+ printed technologies
				- aerosol jet printing
				- evaporation printing
				- inkjet printing
				- screen printing
		+ nanoradios
			+ biomedical applications, for drug delivery




####	Skill sets for STCO and DTCO:



Skill sets for STCO and DTCO:
+ skill set:
	- Development of compilers and peripheral libraries for MN-Core
	- We will develop the compiler and peripheral libraries of MN-Core. Specifically, we are assuming the following themes
		* Utilization of domain-specialized languages such as JAX and Halide
		* Research and development of learning models with low accuracy
		* Implementation of kernels for MN-Core such as FFT
	- Development of peripheral tools such as profilers
	- Communication Language: Japanese
	- Coding ability using Python and C++
	- Experience with low-level optimization
	- Knowledge of compilers for deep learning
+ skill set:
	- Development of framework and library for deploying deep learning models in real world
	- Develop compiler/runtime (PFVM) that optimizes computational graphs of deep learning models to perform inference performantly (in terms of execution time and memory usage), targetting on various backends such as CUDA or edge devices. 
	- Develop an open-source library that uses GPU (CuPy).
	- Communication Language: English/Japanese
	- Programming in C++
	- Programming in Python
	- Basic knowledge of computer science
	- Experience on deep learning model development
	- Experience of running deep learning models on edge devices or smartphones
	- Development experience of multi-pass compilers
+ AI/ML for architectural exploration and bottleneck identification
+ BLAH.









###	VLSI Formal Verification


####	Notes about VLSI Formal Verification


Notes about VLSI formal verification:
+ Focus on mostly equivalence checking and model checking, and less on theorem proving (except in combination with decision procedures and automated reasoning, such as SAT/SMT solvers).
+ This includes:
	- clock domain crossing (CDC) verification, or CDC check
		* for functional static sign-off checks


####	Sets of Skills for about VLSI Formal Verification


The sets of skills for VLSI formal verification are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






###	Embedded Formal Verification, and Formal Verification of Cyber-Physical Systems


Focus on mostly model checking, and less on theorem proving (except in combination with decision procedures and automated reasoning, such as SAT/SMT solvers).

Can include equivalence checking.


The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






###	Software Formal Verification



+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.













###	Circuit Simulation


###	Notes about Circuit Simulation

Skill sets for circuit simulation include those for:
+ analog ICs
+ RFICs
+ digital ICs, especially "FastSPICE"
+ mixed-signal ICs
	- especially mixed-signal ICs.
+ model order reduction, or macromodeling
	- ***nonlinear model order reduction***




###	Skill Sets for Circuit Simulation



The sets of skills for circuit simulation are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










###	Electronic System-Level Design & Verification


####	Notes about Electronic System-Level Design & Verification

The emphasis regarding the design and verification processes for electronic-system level tasks are about embedded computing, or computer systems. See Subsubsection on the design automation of cyber-physical systems, and related systems, to address the interactions with the physical environment, via sensing and actuation.


Includes the following:
+ high-level synthesis, or synthesis of HCL (hardware construction languages) models into logic circuits.
+ hardware/software co-design
+ hardware/software co-verification
+ hardware/software partitioning
+ software synthesis, program synthesis, or automated source code generation
+ transaction-level modeling
	- verification
	- synthesis, via HLS
+ power optimization, energy-efficient designs



####	Skill Sets for Electronic System-Level Design & Verification



The sets of skills are:
+ skill set:
	- Able to solve a wide-range of difficult problems in imaginative and creative ways, exercising judgment within broadly defined practices and policies.
	- MSc in Computer Science, Applied Mathematics or related field with 3+ years of experience, or BSc with 5+ years of experience
	- ***Proficiency in developing and maintaining modern C++ based applications in a Unix/Linux and Windows environment. Proficiency in Qt, Python, and Tcl a plus. Experience with OpenAccess also a plus.***
	- Experience in developing enterprise level software, proficiency with debug and configuration management tools as well as quality and performance metric tools.
	- Strong communication skills and ability to write specifications and reference documentation.
	- Proficiency in English is a must.
	- Interest in high performance data structures and algorithms.
	- Prior experience with or developing CAD/EDA tools and/or hardware design also a plus as is experience with geometric algorithms.
	- ***Excellent organizational, prioritization, time management skills and an unwavering commitment to integrity and professionalism.***
	- ***Self-starter and strong closer with multitasking ability***
	- Any other duties as assigned by the Department head
	- ***Computational Geometry/Topology***
	- ***Graph theory***
	- ***Pattern recognition/machine learning***
	- ***Compilers/parsers (experience with Flex/Bison a plus)***
	- Computer architecture (caching, memory, networking, etc.)
	- ***Boost***
	- Test Driven Development
	- Displays strong analytical abilities both quantitative and qualitative.
	- Excellent communication skills and the ability to interface with all levels of management.
	- Relies on experience and judgment to plan and accomplish goals.
	- Performs a variety of complicated tasks - a certain degree of creativity and latitude is required.
	- ***A key requirement of this role is being the master of all details.***
	- ***Ability to multi-task and handle matters with little supervision and with excellent follow up.***
	- ***A strong entrepreneurial and can-do mindset, undaunted by shifting priorities, uncertainty, and a “figuring it out as we go” environment.***
	- ***Enough courage to say “I don't know”.***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










###	Design Automation of Cyber-Physical Systems and Their Networks



####	Notes about Design Automation of Cyber-Physical Systems and Their Networks


This section covers the design automation of cyber-physical systems, and networks of cyber-physical systems (or, networked cyber-physical systems, including networked embedded systems).
+ [ambient intelligence](https://en.wikipedia.org/wiki/Ambient_intelligence), AmI
+ networks of cyber-physical systems
	- Internet of Things, IoT
		* AIoT, AI-based IoT
			+ global brain
				- From the CACM paper.
				- ***extended intelligence***, EI or XI
					* extend human intelligence with AI
		* ***Supranet***
			+ see Gartner research report
+ Intelligent Environments, IE
	- ["IEs describe physical environments in which information and communication technologies and sensor systems disappear as they become embedded into physical objects, infrastructures, and the surroundings in which we live, travel and work. The goal here is to allow computers to take part in activities never previously involved and allow people to interact with computers via gesture, voice, movement, and context."](https://en.wikipedia.org/wiki/Intelligent_environment)
+ [pervasive computing](https://en.wikipedia.org/wiki/Pervasive_computing)
+ [physical computing](https://en.wikipedia.org/wiki/Physical_computing)
+ [ubiquitous computing](https://en.wikipedia.org/wiki/Ubiquitous_computing)
+ with users
	- [haptic technology](https://en.wikipedia.org/wiki/Haptic_technology), or haptic computing
		* brain-computer interface, BCI; or brain-machine interface, BMCI,
			+ neural dust




####	Skill Sets for Design Automation of Cyber-Physical Systems and Their Networks



Skill set for the design automation of cyber-physical systems, and networks of cyber-physical systems:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.













###	Energy-Efficent EDA, or Low-Power EDA



####	Notes about Energy-Efficent EDA, or Low-Power EDA


circuit-level power optimization: 
+ transistor sizing
+ voltage scaling
+ voltage islands
+ variable VDD
+ multiple threshold voltages
	- Modern processes can build transistors with different thresholds. Power can be saved by using a mixture of CMOS transistors with two or more different threshold voltages. In the simplest form there are two different thresholds available, common called High-Vt and Low-Vt, where Vt stands for threshold voltage. ***High threshold transistors are slower but leak less, and can be used in non-critical circuits.***
+ power gating
	- This technique uses high Vt sleep transistors which cut-off a circuit block when the block is not switching. The sleep transistor sizing is an important design parameter. This technique, also known as MTCMOS, or Multi-Threshold CMOS reduces stand-by or leakage power, and also enables Iddq testing.
+ long-channel transistors
	- Transistors of more than minimum length leak less, but are bigger and slower.
+ stacking and parking states
	- Logic gates may leak differently during logically equivalent input states (say 10 on a NAND gate, as opposed to 01). State machines may have less leakage in certain states.
+ logic styles:
	- dynamic and static logic





logic synthesis for low power
+ clock gating
+ logic factorization
+ path balancing
+ technology mapping
+ state encoding
+ finite-state machine decomposition
+ retiming
+ as part of FPGA logic synthesis


Data organization for low power: https://en.wikipedia.org/wiki/Data_organization_for_low_power
 


Energy harvesting, EH, power harvesting, energy scavenging, or ambient power:
+ Circuits and systems that exploit ambient energy to power low-energy electronic circuits and systems, in applications such as:
	- wearable electronics
	- wireless sensor networks
	- other wireless autonomous devices



####	Skill Sets for Energy-Efficent EDA, or Low-Power EDA

The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Logic Synthesis


Includes information on:
+ FPGA logic synthesis



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Physical Design & Physical Synthesis



####	Notes about Physical Design & Physical Synthesis


Sets of skills for physical design of digital ICs/SoCs:
+ placement
+ routing
+ physical design for 3-D ICs
	- placement for 3-D ICs
	- routing for 3-D ICs
	- account for different types of 3-D ICs that connect dies or wafers, using:
		* TSVs, through-silicon vias
		* Cu-Cu connections
		* classification by level of interconnect hierarchy:
			+ global level, using packages
				- 3DWLP, 3-D wafer-level packaging
			+ intermediate level, using bond pads
			+ local level, using transistors
		* alternate classifications:
			+ 3DWLP, 3-D wafer-level packaging
			+ 2.5D interposer-based integration
			+ 3-D interposer-based integration
			+ 3-D stacked ICs
			+ monolithic 3-D ICs
			+ 3-D heterogeneous integration
			+ 3-D systems integration
	- Notes:
		* 3-D packaging:
			+ 3-D integration schemes that rely on traditional interconnection methods for vertical stacking, such as:
				- wire bonding
				- flip chip
			+ classifications:
				- 3-D SiP, 3-D system-in-package
					* for stacked memory dies interconnected with:
						+ wire bonds
						+ package on package PoP
				- 3-D WLP, 3-D wafer-level package
					+ uses wafer-level processes such as redistribution layers (RDLs) and wafer bumping processes to form interconnects
					* 2.5D interposer, using silicon/glass/organic interposer using through silicon vias (TSVs) and RDL
		* 3-D ICs
			+ 3-D SIC, 3-D stacked ICs
				- stack ICs chips
					* using TSV interconnects
					* stacking approaches:
						+ die-to-die
						+ die-to-wafer
						+ wafer-to-wafer
				- monolithic 3-D ICs
					* use fabrication processes to realize 3-D interconnects at the local levels of the on-chip wiring hierarchy (dictated by IRDS/ITRS)
		* Benefits:
			+ footprint, or volume that the package takes up on a board.
			+ cost
			+ heterogeneous integration
			+ shorter interconnect
			+ power
			+ design
			+ circuit security
			+ bandwidth
		* challenges:
			+ cost
			+ yield
			+ heat, thermal hotspots
				- correlation between electrical proximity and thermal proximity
			+ design complexity
			+ TSV-introduced overhead
			+ testing
			+ lack of standards
			+ heterogeneous integration supply chain
			+ lack of clearly defined ownership
		* design styles
			+ gate-level integration
			+ block-level integration
+ clock network synthesis
	- clock tree synthesis
+ cell library synthesis
	- cell library migration from a given semiconductor manufacturing process technology node to a more advanced node
+ chip-package-board co-design
	- hybrid integrated circuits, HIC, hybrid microcircuits
+ FPGA physical design
+ DFM-aware physical design
+ power supply networks
	- or, power and ground routing




Sets of skills for physical design of analog, RF, and mixed-signal ICs/SoCs:
+ placement
+ routing





Sets of skills for physical synthesis:
+ gate sizing
+ buffer insertion
+ wire sizing
+ yield-aware physical synthesis
	- as part of DFM-aware physical synthesis, to support proactive DFM.












####	Skill Sets for Physical Design & Physical Synthesis



Here are the sets of skills for physical design and physical synthesis.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Memory Compilers



+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















###	Static Timing Analysis (STA), and Statistical Static Timing Analysis (SSTA)



+ skill set:
	- timing convergence issues associated with deep-submicron processes for high-performance design
		* crosstalk delay
		* noise
		* glitch
		* POCV
		* IR-STA
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	VLSI Verification



Focuses on non-formal VLSI verification, other than circuit simulation and physical verification, such as:
+ logic simulation
+ fault simulation
+ RTL simulation
+ intelligent verification, intelligent testbench automation



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.























###	VLSI Design for Manufacturing, DFM


####	Notes about VLSI DFM



Skills sets for DFM, especially reactive DRM (e.g., exploiting computational lithography for pixelization of layout designs), include:
+ RET, resolution enhancement techniques
+ OPC, optical proximity correction
	- rules-based OPC
	- model-based OPC
	- inverse OPC
	- SRAF, sub-resolution assist features
+ PSM, phase-shifting mask
+ parametric DFM for semiconductor manufacturing yield optimization
	- statistical circuit simulation, with statistical SPICE models
		* Monte Carlo analysis
		* response surface modeling
		* mismatch simulation
+ OAI, off-axis illumination
+ STI, shallow-trench isolation




####	Skill Sets for VLSI DFM



Sets of skills for DFM:
+ Direct or indirect experience in OPC (Optical Proximity Correction), including rogorious lithography simulation (Hyperlith, Prolith), RET, and advanced mask technology.
+ Solid understanding of imaging theories (Abbe, Hopkins).
	- Abbe-PCA (Abbe-Hopkins): microlithography aerial image analytical compact kernel generation based on principle component analysis
	- Hybrid Hopkins-Abbe method for modeling oblique angle mask effects in OPC
	- Application of the hybrid Hopkins–Abbe method in full-chip OPC
	- transmission cross coefficients (TCCs)
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









###	Numerical EDA, other than circuit simulation


####	Notes about Numerical EDA, other than circuit simulation

This includes:
+ electromagnetic field solvers
+ layout extraction
+ parasitic extraction
+ signal integrity analysis
+ power integrity analysis
+ voltage drop analysis
+ electromigration lifetime checks
+ noise analysis
	- static noise analysis
	- crosstalk analysis
	- mitigation of noise coupling



####	Skill Sets for Numerical EDA, other than circuit simulation


The sets of skills are:
+ The successful candidate must be an expert in field solver-based parasitic extraction and be able to quickly become an expert in new simulation approaches and to develop robust, maintainable, and efficient code.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








###	IC Testing, including digital and mixed-signal VLSI Testing


####	Notes about IC Testing

This includes:
+ ATPG, automatic test pattern generation
+ BIST, built-in self-test
+ DFT, design for testability
+ high-level test synthesis



####	Skill Sets for IC Testing


The sets of skills are:
+ skill set:
	- Background in 3D computer graphics, including APIs such as OpenGL
	- Proficient in Java, Maven, Python, Jenkins/Groovy, Vagrant/Docker
	- Good knowledge in DFT: OCC insertion (for on-chip clock controllers), ATPG generation
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











###	Post-Silicon Validation


####	Notes about Post-Silicon Validation

Skill set for post-silicon validation and post-silicon debugging.
+ complements VLSI simulation (ESL/TLM simulation, RTL simulation, logic simulation, and circuit simulation), VLSI formal verification and logic emulation
+ use system-boards, logic analyzers, and assertion-based tools with VLSI testing for post-silicon validation and post-silicon debugging
+ use of hardware emulator, which is like hardware acceleration for RTL/logic simulation
+ in-circuit emulation
+ hardware virtualization
	- hardware-assisted virtualization, via platform virtualization
		* also known as accelerated virtualization, hardware virtual machine or HVM, native virtualization


####	Skills about Post-Silicon Validation

The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











###	Technology CAD, TCAD


####	Notes about Technology CAD

+ DTCO
	- Support tools
		* BACPAC, Berkeley Advanced Chip Performance Calculator
			+ Estimate impact of semiconductor manufacturing process technology node, or process node
				- semiconductor manufacturing process technology node is defined by the minimum feature size





####	TCAD for Process Simulation



Notes about TCAD for process simulation:
+ ion implantation
+ diffusion
+ oxidation
+ dry/wet etching
+ deposition
+ ***optical lithography, photolithography, or optical photolithography***, and next-generation lithography (NGL) techniques for optical lithography
	- ***computational lithography***, or computational scaling
		* Mathematical and algorithmic approaches to improve attainable resolution from optical lithography.
		* For 22 nm semiconductor manufacturing process technology nodes and beyond.
		* To fix problems with:
			+ 193 nm deep UV optical lithography.
		* includes DFM techniques, such as:
			+ resolution enhancement technologies, RET
				- scattering bars
				- phase-shoft masks
				- multiple/double patterning
			+ optical proximity correction, OPC
			+ source mask optimization
				- or, complex modeling of the lens system and photoresist
					* aims to improve chip manufacturability and manufacturing yield
					* use signature of scanner to improve:
						+ OPC model
						+ polarization characteristics of the lens pupil
						+ Jones matrix of the stepper lens
						+ optical parameters of the photoresist stack
						+ diffusion through the photoresist
						+ stepper illumination control variables
	- multiple patterning, or multi-patterning
	- classifications:
		* ultraviolet lithography, UV lithography
		* X-ray lithography
		* EUV lithography
	- EUV lithography, extreme ultraviolet lithography, EUVL
		* 13.5 nm extreme ultraviolet lithography
		* assist features
		* source mask optimization
		* phase shift masks
		* EUV photoresist exposure
		* contamination effects
			+ resist outgassing
			+ tin deposition
			+ hydrogen blistering
			+ resist erosion
			+ membrane
		* mask defects
		* throughput scaling issues
			+ EUV stochastic issues
		* used with multile patterning
		* single patterning extension, anamorphic high-NA (NA, numerical aperture)
	- deep UV immersion lithography, immersion lithography
	- X-ray lithography
	- BEUV lithography, or beyond extreme ultraviolet lithography
		* about 6.7 nm wavelength
	- maskless lithography, MPL
		* electronic beam lithography, e-beam lithography, EBL
		* plasmonic direct writing lithography
		* optical maskless lithography
			+ multiphoton lithography, ***direct laser writing***, direct laser lithography
	- quantum optical lithography, QOL
	- other non-mainstream lithography techniques
		* nanoimprint lithography
			+ thermoplastic nanoimprint lithography
			+ photo nanoimprint lithography
			+ resist-free direct thermal nanoimprint lithography
		* molecular self-assembly lithography
		* stencil lithography
		* charged-beam lithography
			+ ion beam lithography, or ion-beam lithography, or ion-projection lithography
				- ion beam proximity lithography, IBL
				- focused ion beam lithography, FIB
				- similar to:
					* electronic beam lithography
				- electron-projection lithography
		* magnetolithography, ML
			+ photoresist-less and photomaskless lithography
			+ backside lithography
		* plasmonic lithography, plasmonic nanolithography, plasmonic photolithography
		* ***soft lithography***
			+ use elastomeric stamps, molds, and conformable photomasks to fabricate or replicate structures
			* PDMS lithography
			* microcontact printing
			* multilayer soft lithography
		* laser printing lithography
			+ laser printing of single nanoparticles
		* nanosphere lithography, NSL
		* direct-write lithography process
			+ proton beam lithography, or p-beam writing, or proton beam writing
		* multiphoton lithography, direct laser lithography, direct laser writing
		* ***scanning probe lithography, SPL***
			+ mechanical/thermo-mechanical SPL, m-SPL
			+ thermal SPL, t-SPL
			+ thermo-chemical SPL, tc-SPL, or thermochemical nanolithography, TCNL
			+ dip-pen SPL, dp-SPL, or dip-pen nanolithography, DPN
				- thermal dip-pen lithography
				- beam pen lithography
			+ local oxidation lithography, o-SPL
			+ bias-induced SPL, b-SPL
			+ current-induced SPL, c-SPL
			+ thermally-assisted magnetic SPL, tam-SPL
		* local oxidation nanolithography, LON
		* interference lithography, or holographic lithography
			+ not maskless lithography
			+ no 1:1 imaging system in between
			+ electron holographic lithography
			+ atom holographic lithography
		* nanofountain darwing, or nanofountain probe
+ silicidation
+ modeling mechanics of semiconductor manufacturing processes
+ CMP, chemical-mechanical polishing, chemical-mechanical planarization
+ annealing (diffusion and dopant activation)
+ epitaxy










Sets of skills for process simulation TCAD are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






####	TCAD for Device Simulation



Notes about TCAD for device simulation includes:
+ device modeling for common transistors
	- MOSFET, metal-oxide-semiconductor FETs 
		* a type of insulated-gate FET
			+ enhancement mode MOSFET
			+ depletion mode MOSFET
		* p-channel MOSFET, PMOS
		* n-channel MOSFET, NMOS
			+ ggNMOS, grounded-gate NMOS
				- electrostatic discharge (ESD) protection device
		* CMOS, complementary MOSFET
			+ advantages:
				- high noise immunity
				- low static power consumption
		* ***FGMOS, floating-gate MOSFETs***
			+ for floating-gate memory cell, digital storage element in:
				- EPROM
				- EEPROM
				- flash memory
		* MOS capacitor
		* intrinsic MOSFET device modeling
			+ inversion-layer mobility modeling
			+ channel charge modeling
			+ threshold voltage modeling
		* substrate effects on MOSFETs
			+ subthreshold current
			+ drain-induced-barrier-lowering current
		* parasitic junction & inhomogeneous substrate effects
		* ***MuGFETs, multi-gate MOSFETs, multi-gate FETs***, or ***multi-gate semiconductor devices***
			+ The multiple gates can be controlled by:
				- a single gate electrode, where the multiple gate surfaces act electrically as a single gate
				- independent gate electrodes
					* ***MIGFETs, multiple-independent-gate FETs***
			+ 3-D transistors, or non-planar transistors
				- ***FinFET, fin FETs***
				- ***GAAFET, gate-all-around FETs***
					* sometimes called, SGT, "surrounding gate transistors"
					* MBCFETs, multi-bridge channel FETs
			+ ***DGMOS, DGMOSFET, dual-gate MOFETs, double-gate MOFETs***
				- planar double-gate MOFETs
				- double-gate TFT, double-gate thin-film transistors
				- with silicon thin film in:
					* strong inversion, volume-inversion MOSFET
					* strong accumulation, volume-accumulation MOSFET
			+ tri-gate MOSFETs
			+ FlexFET: planar, independently double-gated transistor
	- other field-effect transistors, FETs
		* propeties and charracteristics
			+ FETs are 3-terminal devices:
				- source
				- gate
				- drain
			+ FETs are unipolar transistors
			+ source-gated transistors
		* ***MISFET, metal-insulator-semiconductor FETs***, or ***insulator-gate FETs (IGFETs)***
			+ all MOSFETs are MISFETs
			+ but, not all MISFETs are MOSFETs
			+ insulators can be:
				- silicon dioxide
				- organic insulators for organic FETs
		* ***QFET, quantum FET***; or, ***QWFET, quantum well FET***
			- exploit quantum tunneling
		* ***TFT, thin-film transistors***
			+ metal oxide thin-film transistors, metal oxide TFT
				- or, oxide thin-film transistors, oxide TFT
			+ TFT LCDs, TFT liquid-crystal displays
		* ***CNTFET, carbon nanotube FET***
		* MESFET, metal-semiconductor FET
		* JFETs, junction FETs, junction-gate FETs
		* VTFET, vertical-transport FET
		* Fe FET, ferroelectric FET
		* GFET, graphene-based FET
			+ GNRFET, graphene nanoribbon FET
		* NOMFET, nanoparticle organic memory FET
		* SB-FET. Schottky-barrier FET
		* VeSFET, vertical-slit FET
		* HEMT, high-electron-mobility FET, HFET, heterostructure FET
		* TQFET, topological quantum FET
		* TFET, tunnel FET
		* MODFET, modulation-doped FET
		* HIGFET, heterostructure insulated-gate FET
		* DEPFET, FET formed in fully depleted substrate and simultaneously act as a sensor, amplifier, and memory node
		* JLNT, junctionless nanowire transistor
		* Bipolar-MOS transistors:
			+ BiCMOS, Bipolar CMOS
			+ IGBT, see information under "power semiconductor devices"
		* MOS sensors
		* RHBD, radiation-hardened-by-design
			+ use enclosed-layout-transistors, ELTs
			+ H-gate, another RHBD MOSFET
			+ shallow trench isolation designs
		* OFET, organic FET
			+ use semiconductor device architecture of TFT, thin-film transistors
		* ChemFET, chemically-sensitive FET
			+ FET used as a sensor for measuring chemical concentrations in a solution
			+ compare to chemiresistors
		* ISFET, ion-sensitive FET
			+ for measuring ion concentrations in a solution
		* BioFET, Bio-FET, biosensor FET, FET-based biosensor, field-effect biosensor, FEB, biosensor MOSFET
		* DNAFET, DNA FET
			+ biosensor that is based the field effect due to partial charges of DNA molecules
		* VMOS, vertical MOSFET, V-groove MOSFET
			+ MOSFET with V-groove shape vertically cut into the substrate material
		* Additional notes:
			+ flowFET
				- microfluidic analog of FETs
				- a microfluidic component
	- transistors manufactured using:
		* SOI, silicon-on-insulator
			+ types of insulators:
				- silicon dioxide
				- sapphire
					* SOS, silicon on sapphire
			+ ***SOI MOSFETs***
				- ***PDSOI, partially depleted SOI MOSFETs***
				- ***FDSOI, fully depleted SOI MOSFETs***
		* GOI, germanium-on-insulator
	- BJTs, bipolar junction transistors
	- SiGe, silicon germanium
	- GaAs, gallium arsenide
	- SiC, silicon carbide
	- LET, light-emitting transistors
		* organic LETs
			+ for digital displays and on-chip optical interconnects
			+ can be used in an active matrix of OLETs, which OLEDs cannot be used to form
				- OLEDs can only form active matrix of OLEDs, in combination with switching elements (such as TFTs, thin-film transistors)
	- for LEDs:
		* InAs, indium arsenide
		* InSb, indium antimonide
		* InP, indium phosphide
		* organic LEDs:
			+ use organic semiconductors
	- for photovoltaic solar cells:
		* selenium sulfide
	- RF CMOS
		* based on LDMOS
	- power semiconductor devices, for power electronics:
		* power MOSFETs
			+ HexFET, hexagonal type of power MOSFET
			+ UMOS, trench-MOS, trench-gate MOSFET
		* DMOS, double-diffused MOSFET
			+ LDMOS, lateral DMOS, lateral-diffused MOSFET, laterally-diffused MOSFET
				- planar double-diffused MOSFET
				- RF LDMOS
					* for power amplifiers and other applications
			+ VDMOS, vertical DMOS
		* IGBT, insulated-gate bipolar transistor
		* SCR, silicon-controlled rectifiers
		* thyristors
			+ GTO, gate turn-off thyristors
			+ MCT, MOS-controlled thyristors, or MCT, MOSFET-controlled thyristors
				- MOS-gated thyristors
			+ IGCT, integrated gate-commutated thyristors
		* triac
		* diodes
			+ Schottky diodes, or Schottky barrier diodes, or hot-carrier diodes
			+ PiN diodes
				- has wide, undoped instrinsic semiconductor region between p-type semiconductor and n-type semiconductor
				- for RF applications:
					* RF power amplifiers
		* silicon-controlled switches
		* classifications:
			+ based on number of terminals:
				- 2-terminal semiconductor devices
				- 3-terminal semiconductor devices
				- 4-terminal semiconductor devices
			+ based on proportion of carriers:
				- majority carrier devices
				- minority carrier devices
		* parameters:
			+ breakdown voltage
			+ on-resistance
			+ rise & fall times
				- rise times
				- fall times
			+ safe-operating area
			+ thermal resistance
	- point-contact transistors
	- UJT, unijunction transistor
		* programmable UJT, or programmable unijunction transistor
	- *transistor classification based on number of terminals*:
		* tetrode transistors:
			+ 4 active terminals
		* pentode transistors:
			+ 5 active terminals
+ device modeling for other devices:
	- memristors
	- memtransistors
		* 2-terminal devices
		* 7-terminal devices
	- memistors
		* 7-terminal devices
	- molecular electronics
		* molecular scale electronics, or single-molecule electronics
	- [organic electronics](https://en.wikipedia.org/wiki/Organic_electronics)
		* organic electronics
			+ organic solar cells
			+ photovoltaics
		* organic field-effect transistors, organic FETs, OFETs
		* organic light-emitting diodes, OLED
			+ active-matrix OLED, AMOLED
	- supramolecular electronics
	- trancitors, "transfer-capacitors"
+ large-signal nonlinear semiconductor device modeling
	- physical models, based on semiconductor device physics
		* approximate physical phenomena
		* pamateric models for physical properties:
			+ oxide thickness
			+ substrate doping concentration
			+ carrier mobility
		* for simplified estimates of signal-swing limitations
	- empirical models, using curve fitting with experimental data to obtain statistical models
+ small-signal nonlinear semiconductor device modeling
	- to evaluate:
		* stability
		* gain
		* noise
		* bandwidth
		* bias point, or Q-point or quiescent point
	- small-signal parameters, from production-line testing and circuit design
		* for predicting:
			+ circuit gain
			+ input impedance
			+ output impedance
		* from parameter sets of 2-port networks:
			+ T-parameters, transmission parameters
			+ h-parameters, hybrid-parameters
			+ z-parameters, impedance parameters
			+ y-parameters, admittance parameters
			+ S-parameters, scattering parameters
+ methods:
	- explicit solution
	- iterative solution
	- graphical solution
+ types of modeling approximation
	- piecewise linear model
	- mathematically idealized semiconductor device models



Sets of skills for device simulation TCAD are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








###	Mask Data Preparation, MDP


This includes:
+ layout-to-mask preparation
+ mask generation



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






###	Physical Verification


Includes:
+ DRC, design rule check
+ LVS, layout verses schematic check
+ XOR check
+ antenna check
	- check for antenna effects
+ ERC, electrical rule check
	




The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.





###	MEMS CAD, and NEMS CAD



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






















###	Other EDA topics


Other EDA topics:
+ design databases
+ ***radiation hardening***
	- ***radiation-hardened*** ICs, ***rad-hardened*** ICs, ***rad(iation)-hardened*** ICs, or ***rad-hard*** ICs, or ***hardened*** ICs (with context provided to indicate ***radiation-hardened*** ICs)
	- protect against:
		* TID, total ionizing dose
		* ELDRS, enhanced low dose rate effects
		* neutron and proton displacement damage
		* SEEs, single event effects
	- sources of radiation damage:
		* cosmic ray radiation
		* solar particle events
		* Van Allen radiation belts* 
		* secondary particles resulting from the interaction of other kinds of radiation with structures around the electronic devices
		* nuclear reactors, in nuclear power plants
			+ gamma radiation
			+ neutron radiation
		* particle accelerators
		* nuclear explosions
		* chip packaging materials
			+ fixed by using:
				- more pure, or purer, chip packaging materials
				- error correction code, or error correcting code, ECC
	- radiation effects on electronics
		* fundamental mechanisms
			+ lattice displacement
			+ ionizing effects
		* resultant effects
			+ total ionizing dose effects
			+ transient dose effects
			+ systems-generated EMP effects
		* digital damage of SEEs, single event effects
			+ single-event transient
			+ single-event upset
			+ single-event latchup
			+ single-event snapback
			+ single-event induced burnout
			+ single-event gate rupture
		+ SEE testing, or testing for single event effects
	- radiation hardening techniques
		+ physical radiation hardening
		+ logical radiation hardening








###	Hardware Security


####	Notes about Hardware Security

hardware security topics from Trust-Hub website Trust-Hub.org:
+ System-on-chip (SoC) Attacks and Security
+ Intellectual Property (IP) Trust and Assurance
+ Reverse Engineering
+ Invasive and Semi-invasive Physical Attacks
+ Computer-aided design (CAD) for Security
+ Side Channel Attacks and Mitigation
+ Hardware Security Primitives (PUFs, TRNGs, etc.)
+ Hardware Obfuscation
+ Hardware Trojans and Backdoors
+ Counterfeit Electronics
+ FPGA/eFPGA Security
+ IoT and Cyber-physical System Security
+ Emerging and Nanoscale Device Security




####	Skill Sets for Hardware Security



+ skill set:
	- Experience with hypervisor / container development
		* Especially, Xen or OpenXT
	- Experience with Trusted Platform Module (TPM)
	- Experience with firmware-level code
	- FPGA physical design
	- Experience with device characterization or PUF techniques
		* physical unclonable function
	- Experience with ASIC analog and/or digital design











####	Skill Sets for Cyber-Physical System Security


Cyber-physical system security includes:
+ automotive security
+ autonomous underwater glider, AUG
+ robotics security



Skill sets for cyber-physical system security, including robotics security and automotive security:
+ experience with automotive security:
	- CAN
	- OTA
	- Autosar
	- SILS/HILS
		* SILS, software-in-the-loop simulation
		* HILS, hardware-in-the-loop simulation, HWIL, HITL
			+ for automotive anti-lock braking systems
				- vehicle dynamics, such as suspension, wheeels, tires, roll, pitch, and yaw
				- dynamics of brake systems's hydraulic components
				- road characteristics
			+ fixed-wing aircraft flight control systems
				- fly-by-wire control systems
		* EILS, emulation-in-the-loop simulation
	- RTOS
	- QNX
	- AGL
+ programming language security, for RISC processes:
	- function block diagram, FBD
	- ladder diagram, LBD
	- structured text, ST
	- instruction list
	- sequential function chart, SFC
+ skill set for vehicle security architect (middleware), or product security engineer:
	- open software ecosystem
	- collaborate with cross-functional partners
	- lead and influence security architecture, risk analysis, vulnerability testing, and security reviews for vehicle products across Woven Planet cross-functional teams
	- ensure security and privacy by design for embedded or middleware software products
	- assist development teams in architecting and securing the software and hardware ecosystem
	- evaluate the security of middleware, libraries, and protocols for embedded systems
	- lead threat modeling towards components of the hardware abstraction layer, service layer, runtime environment and application layer
		* identify security issues, risks, and associated mitigations
	- audit C++ code to identify and patch security vulnerabilities
	- provide designs to implement the architected solutions
	- evaluate and recommend new and emerging security products and technologies
	- 3 years of experience in software security as an architect, or a developer of security solutions
	- knowledge and experience in the following domains:
		* security engineering
		* system and network security
		* authentication and security protocols
		* cryptography
		* operating systems
		* application security
	- security expertise in at least one of the following:
		* implementation of multilayered independent levels of security (MILS) architecture for high-assurance embedded systems
		* operating system, ARM, and kernel security
		* security of components of compile time and runtime environment (RTE)
		* practical experience of development of libraries (C/C++) for software security
	- knowledge of DevSecOps methodology and components of a secure SDLC
	- experience with secure operating systems architecture, security design, and threat modeling
	- experience with:
		* development of:
			+ software components for automotive systems or robots
			+ hardware abstraction layer (HAL) components
		* security of real-time operating system, RTOS
		* in-depth knowledge of UNIX-like operating systems and their security components, preferable - used in automotive industry, such as automotive grade Linux
		* vehicle software development
		* kernel and hypervisor security
	- understanding of standards:
		* ISO 21434
		* ISO 26262
+ skill set:
	- develop and review requirements, and strategy and road map, for:
		* static security testing
		* dynamic security testing
	- drive investigations into security issues, identify opportunities for further automation to eliminate future issues
	- knowledge of application security tools:
		* SAST
		* DAST
		* SCA
	- experience managing execution and high-quality product delivery
	- ability to handle multiple competing priorities in a fast-paced environment
	- excellent analytical and communication skills, as well as ability to take initiative and build productive relationships
	- experience in the design and implementation of security solutions, systems, and mechanisms:
		* data security
		* application security
		* cryptography
		* systems security
		* network security
		* exploit development
+ skill set:
	- participate in threat modeling and application security reviews
	- audit embedded code to identify security vulnerabilities
	- vehicle penetration testing experience
	- knowledge or experience of advanced smart fuzzing strategies:
		* mutational
		* symbolic
		* American Fuzzy Lop, AFL
	- experience with QNX
	- understanding of vehicle functional safety standards:
		* ISO26262
+ skill set:
	- security-critical code at scale
	- security automation tools and processes
	- information security:
		* threat modeling
		* secure code review
		* security testing
	- build and maintain security tooling and infrastructure
	- lead the design and engineering of static analysis tooling
		* SAST
		* semantic code analysis
		* vulnerability management
	- foster a culture of automation, and build sustainable tooling systems
	- identify application security risks, define requirements, and then build and extend systems to help reduce and track these risks














##	VLSI Deep Learning & Embedded Deep Learning





+ skill set:
	- R&D Director - ML Systems
	- We are a Cambridge-based startup developing a revolutionary B2B SaaS product for automated synthesis of ultra-efficient Intelligent Systems. Our product will empower Edge AI & Robotics companies to achieve supreme efficiency and flexibility, while slash the development time and costs tenfold.
	- To perform advanced R&D critical for the success of our product, we are looking for a passionate and impactful leader to direct our growing activities in designing and optimizing Computer Systems for Machine Learning (ML Systems).
	- Perform critical R&D for a revolutionary neuralware/middleware/hardware co-design product.
	- Lead a team of ninja-class engineers (many at PhD-level) with glorious achievements in performance analysis and optimization.
	- Collaborate with high-profile ML Hardware customers to create highly competitive and fully compliant submissions to MLPerf.
	- Collaborate with Robotics and Edge AI customers to apply the hard-earned optimization knowledge to real-world use cases.
	- Represent KRAI in the most active working groups of MLCommons, including Inference and Power, and contribute to the roadmap.
	- Push the number of automated KRAI submissions in each round from hundreds to hundreds of thousands!
	- A PhD or MSc in Computing or Natural Sciences, with 5+ years of post-graduate experience.
	- Deep understanding of the full software/hardware stack, including algorithms, compilers, libraries, computer architecture.
	- Expertise in domain-specific accelerators (e.g. NPUs, GPUs, DSPs).
	- Hands-on experience with ML frameworks (e.g. Torch, TensorFlow) and inference engines (e.g. OpenVINO, TensorRT, TFLite, ArmNN).
	- Familiarity with ML optimization techniques (e.g. quantization, pruning).
	- If you're a systems person, you can play to your strengths and keep growing your expertise in any of the above areas, or instead jump outside your comfort zone and learn more about Edge AI & Robotics applications. Two things are certain: a) you'll be constantly learning and pushing the boundaries of your skills and knowledge; b) it'll be fun!
	- It is a unique opportunity to advance the state-of-the-art in ML Systems by considering all critical elements of the stack: from hardware to middleware to neuralware, akin to the amazing Nand-to-Tetris approach. (In fact, we hope to write our own book about our learnings one day!)
	- Please send your CV and short covering letter to info@krai.ai - we will be happy to arrange a friendly chat! What we ask for is evidence supporting your abilities and motivation: it's up to you to decide what that evidence might be.
	- Our team at KRAI (formerly, dividiti) has come a long way from co-organizing the ReQuEST tournament at ASPLOS'18 to becoming leading contributors to MLCommons/MLPerf™, the industry-leading forum for benchmarking Computer Systems for Machine Learning (ML). As part of our journey, we have collaborated with Arm, Dell, Intel, VMware, Qualcomm and leading ML hardware startups. In particular, in our public collaboration with Qualcomm we have produced some of the fastest and most energy-efficient Inference results, both in the Datacenter and Edge category. Over the three year history of MLPerf Inference, we submitted over 50% of all results, that is, more than other 40+ submitters combined.
+ skill set:
	- Ultra-efficient Computer Systems for Edge AI and Robotics
	- Krai is a Cambridge-based startup focusing on creating ultra-efficient computer systems for Edge AI and Robotics applications. We are looking for curious and motivated R&D engineers to join us on our exciting journey!
	- Automating software/hardware co-design
	- We are building an automated platform for software/hardware (SW/HW) co-design. We envision our customers will provide their requirements such as training and validation datasets, quality targets, performance and energy efficiency constraints, etc. Then, our platform will design several candidate AI/SW/HW stacks composed of neural networks, libraries, inference engines, etc., for running on one or more HW platforms. The platform will integrate many state-of-the-art and emerging techniques such as network architecture search, network optimisation, graph compilers, etc. - all wrapped up in an intelligent meta-technology for searching through myriads of combinations and configuration options. Our automated platform will produce superior designs and slash development time and cost by 10-100 times, opening up unprecedented opportunities for Edge AI and Robotics applications.
	- Collaborating with a broad computer systems community
	- As part of our strategy, we collaborate with many leading organisations ranging from stealth-mode AI HW startups to global corporations. Our core expertise and responsibilities include compilers, runtime systems, architecture definition, performance modelling, optimization, workload mapping and benchmarking. For example, we implemented, validated and optimized the MLPerf Inference benchmarks for Qualcomm's impressive entrance with their Cloud AI 100 accelerators, achieving up to 6x energy efficiency over the entrenched competition.
	- If you're a systems person, you can play to your strengths and keep growing your expertise in any of the above areas, or instead jump outside your comfort zone and learn more about AI applications. Two things are certain: a) you'll be constantly learning and pushing the boundaries of your skills and knowledge; b) it'll be fun! What we ask for is evidence supporting your abilities and motivation: it's up to you to decide what that evidence might be.
	- If that sounds like your cup of tea, please send us your CV and covering letter to info@krai.ai - we will be happy to arrange a friendly chat.
+ skill set:
	- Microarchitecture study of next-generation MN-Core
	- We will work on various studies to improve the performance, power, and area of the next-generation MN-Core microarchitecture.
	- Communication Language: Japanese
	- Knowledge of computer architecture
	- Advanced Verilog HDL/System Verilog coding skills
	- Experience in verifying RTL by logic simulation
	- Experience in using synthesis/place-and-route tools
	- Knowledge of STA
	- Basic knowledge of deep learning
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.















###	Start-ups related to VLSI Deep Learning & Embedded Deep Learning

+ 
















##	Companies in the Semiconductor Industry



+ FPGA companies
	- AMD/Xilinx
	- Intel Altera
	- [Lattice Semiconductor](https://www.latticesemi.com/)
	- [QuickLogic Corporation](https://www.quicklogic.com/company/careers/)
		* Not friendly to non- U.S. citizens.
	- [Menta S.A.S, Sophia Antipolis](https://www.menta-efpga.com/careers)
	- [Achronix Semiconductor Corporation](https://www.achronix.com/company/careers)
	- [Flex Logix Technologies, Inc.](https://flex-logix.com/)
	- [Microchip Technology](https://www.microchip.com/)
+ machine learning hardware accelerators (including coarse-grained reconfigurable architctures, CGRA), machine learning acceleration via domain-specific computing, heterogeneous computing systems for machine learning, VLSI deep learning, and embedded deep learning
	- [SimpleMachines, Inc.](https://www.simplemachines.ai/company)
	- [Codeplay Software Ltd.](https://www.codeplay.com/company/careers/#career-list)
	- [Thirdwayv](https://www.thirdwayv.com/careers/)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [Qualcomm Technologies, Inc.](https://www.qualcomm.com/research/artificial-intelligence/ai-research): https://www.qualcomm.com/company/careers
	- [Cornami, Inc.](https://cornami.com/)
+ companies selling RISC-V -based products
	- [SiFive, Inc.](https://www.sifive.com/careers)
+ edge computing
	- [EdgeImpulse Inc.](https://edgeimpulse.com/careers)















##	Additional Information about EDA

+ [IP-XACT is an XML format that defines and describes individual, re-usable electronic circuit designs (individual pieces of intellectual property, or IPs) to facilitate their use in creating integrated circuits (i.e. microchips). IP-XACT was created by the SPIRIT Consortium as a standard to enable automated configuration and integration through tools.](https://en.wikipedia.org/wiki/IP-XACT)
+ [The SystemRDL language, supported by the SPIRIT Consortium, was specifically designed to describe and implement a wide variety of control status registers. Using SystemRDL, developers can automatically generate and synchronize register views for specification, hardware design, software development, verification, and documentation.](https://en.wikipedia.org/wiki/SystemRDL)
	- [SystemRDL Compiler](https://github.com/SystemRDL/systemrdl-compilerSystemRDL Compiler)
	- [open-register-design-tool, Ordt](https://github.com/Juniper/open-register-design-tool)
	- https://www.eda.org/images/downloads/standards/systemrdl/SystemRDL_2.0_Jan2018.pdf
+ Automatic Register Verification (ARV)
+ synthesizable RTL, UVM, c-header, RALF eRM, SystemRDL, IP-XACT
+ UVM, Universal Verification Methodology, SystemVerilog based
+ VMM, Verification Methodology Manual
+ OVM, Open Verification Methodology
+ URM, Universal Reuse Methodology
+ eRM, e Reuse Methodology, e Verification Language








##	Application engineers of different EDA products

Skill sets for application engineers of different EDA products:
+ Bus protocols such as AMBA-AXI, AHB, APB, I2C, SPI
+ Expert in coding SV Testbench, drivers, monitors, scoreboards, checkers
+ Experience in C/C++,Shell/Perl scripting.
+ Understanding of AHB, AXI and other bus protocols and system architecture is a plus.
+ Expert in System Verilog and OVM/UVM based verification.
+ Preferred Expertise in MIPI UniPro/UFS Protocol and UVM.
+ To help the team to verify the existing design (UFS/UniPro)
+ Preferable: Experience in one/more of the following areas PCI_Express, USB, SATA, SDIO, MIPI and /or AMBA standards (OCP, AXI, AHB etc.)
+ Experience with verification methodology like OVM/VMM/UVM
+ Experience in constrained-random verification is a strong plus




+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










#	Artificial Intelligence + Machine Learning










##	Machine Learning, ML



###	Notes about Machine Learning, ML

+ ML/AI compiler design
	- Poplar framework for IPU architecture compiler.
+ ML/AI compilers, or deep learning compilers:
	- MLIR
	- TVM
	- Glow
	- XLA
+ ML/AI frameworks
	- ***JAX***:
		* JAX = Autograd + XLA (from TensorFlow)
			+ Google JAX
		* for high-performance machine learning research
		* https://github.com/google/jax
	- ***TensorFlow***
		* TensorFlow Lite, TFLite
		* TFX, TensorFlow Extended
		* TensorFlow Probability
		* ***Keras***
	- ***PyTorch***
		* based on ***Torch*** library
		* PyTorch JIT
		* includes:
			+ Deep Speed
				- deep learning optimization engine
				- Zero Redundancy Optimizer, ZeRO, can optimize deep learning models with >1 trillion parameters
			+ Caffe, Convolutional Architecture for Fast Feature Embedding, and Caffe2, which has been merged into PyTorch.
			+ Torch, which is based on Lua
	- Open Neural Network Exchange, ONNX
		* open-source AI ecosystem
		* Neural Network Exchange Format, NNEF
	- ***scikit-learn***
		* mlpy
		* SpaCy, for NLP
		* Natural Language Tookit, NLTK, for NLP
		* Orange
		* PyTorch
		* TensorFlow
		* Infer.NET, for Bayesian inference in graphical models, for probabilistic programming
		* scikit-multiflow, for multi-output/multi-label and stream data
	- ***pandas***, for data analysis
	- [***Lightning, or Lightning AI***](https://lightning.ai)
		* Or, ***PyTorch Lightning***
	- ***ML.NET***
		* Microsoft Cognitive Toolkit, CNTK, The Microsoft Cognitive Toolkit (deprecated)
	- ***Apache MXNet***, for deep learning
	- ***OpenVINO toolkit***, Open Visual Interference and Neural Network Optimization
		* includes:
			+ nGraph
	- ***BigDL***, distributed deep learning framework for Apche Spark
	- Dlib
	- PaddlePaddle
	- Theano
	- Apache SINGA
	- Flux machine learning framework, Julia based
	- Flax, from Google Brain
	- PlaidML, portable tensor compiler
	- RLax, for reinforcement learning agents, by DeepMind
	- Optax, for gradient processing and optimization, by DeepMind
	- [***Chainer***](https://chainer.org)
	- for gradient boosting:
		* LightGBM
		* XGBoost
	- for Python software:
		* PyMC
			+ formerly PyMC3
			+ for Bayesian statistical modeling and probabilistic machine learning
				- Markov chain Monte Carlo
				- variational fitting algorithms
		* TomoPy, tomographic data processing and image reconstruction
	- for C++ software:
		* mlpack
		* OpenNN
	- for Java software:
		* Massive Online Analysis, MOA
			+ SAMOA, derivative of MOA
		* Eclipse Deeplearning4j
	- for distributed machine learning or deep learning:
		* ***Horovod***, open-source software framework for distributed deep learning training
			+ TensorFlow
			+ Keras
			+ PyTorch
			+ Apache MXNet
			+ managed by Linux Foundation AI, LF AI
	- Orange
	- from sanctioned entities:
		* CatBoost, for gradient boosting
	- ***for MLOps***:
		* MLflow
		* Kubeflow
		* Seldon Core
		* TFServing, TensorFlow Serving
		* MLeap
		* Airflow, but for generic pipelined workflow managemnt
	- ***machine learning and data science pipeline management and version control***
		* goals:
			+ pipeline computation
			+ track data
			+ track machine learning models
				- or track statistical models
			+ track experiments
			+ make machine learning models shareable
			+ make experiments reproducible
			+ track versions of the following:
				- data
				- machine learning models, or statistical models
				- machine learning pipelines, or data science pipelines
		* ***DVC, Data Version Control***
		+ [***MLflow***, An open source platform for the machine learning lifecycle](https://mlflow.org/)
		* Git LFS
		* Dolt
		* lakeFS
+ data science frameworks, including libraries and tools for data visualization and information visualization:
	- Madagascar
	- Statsmodels, for statistical analysis
	- Dataplot, for statistical analysis and data visualization
	- Fityk, curve fitting and data analysis application
		* fit analytical, bell-shaped functions to experimental data
	- for Python software:
		* ***matplotlib***, for data visualization
		* ***Bokeh***, for data visualization
		* ***Plotly***, for data visualization
		* ***wxPython***, for data visualization
		* ***PLplot***, for data visualization
		* Gnuplot-py, for data visualization
		* Biggles, for data visualization
		* Chaco, for data visualization
		* MayaVi, for data visualization
		* SQLAlchemy, open-source SQL toolkit and object-relational mapper, ORM
	- PSPP, open-source equivalent of IBM SPSS Statistics (or SPSS Statistics, or Statistical Package for the Social Sciences)
	- other data analytics tools:
		* Apache Arrow
		* Apache Spark, or Spark
		* Baremetrics
		* Chartmogul
		* ***Dask***
			+ "flexible open-source Python library for parallel computing maintained by OSS contributors across dozens of companies"
			+ Scale the Python tools you love
			+ https://www.dask.org
		* Databricks
		* Gephi
			+ for ***graph visualization***
		* Google Analytics
		* Google Data Studio
		* Heap Analytics
		* Jupyter
		* Looker
		* Mesos
		* Mode Analytics
		* Noteable
		* Numba: A High Performance Python Compiler
			- https://numba.pydata.org/
		* Periscope
		* Tableau
			+ alternatives include:
				- Apache Superset
				- Business Intelligence and Reporting Tools, BIRT
				- Domo
				- GoodData
				- Grafana
				- Knime Analytics Platform
				- Looker
				- Metabase
				- Microsoft Power BI???
				- Noteable
				- Oracle Analytics Cloud
				- Pentaho Community Edition
				- Plotly-Dash
				- Qlik Sense
				- QlikView
				- RAWGraphs
				- Redash
				- Retool
				- Sisense
				- SpagoBI
				- Talend
				- ThoughtSpot
				- TIBCO Software
				- Trevor.io
	- for Elasticsearch dashboards
		* Kibana, for data visualization
			+ substitute is: ***Opensearch Dashboards, for Opensearch***
	- for SQL, NoSQL, and NewSQL databases:
		* Amazon DocumentDB
		* Apache Cassandra
		* Apache Hive
		* IBM Db2
		* large-scale databases
			+ THIN
		* MariaDB
		* massively parallel processing databases, MPP databases
			+ BigQuery
			+ Redshift
			+ Snowflake
			+ Vertica
		* Microsoft Access
		* Microsoft Azure SQL Database
		* Microsoft SQL Server
		* MongoDB
		* MySQL
		* Oracle Database
		* PostgreSQL
		* Snowflake
		* SQLAlchemy
			+ open-source SQL toolkit and object-relational mapper, ORM
		* SQLite
		* Teradata Vantage
		* important principles, and properties, to abide by when designing database systems or hardware accelerators for data management (and computation, such as in-memory computing):
			+ common parallel DBMS architectures
				- shared memory architecture
				- shared disk architecture
				- shared nothing architecture
			+ ***CAP theorem, or Brewer's theorem***, in system design and theoretical CS guarantees:
				- consistency: every read receives the most recent write or an error
					* different from consistency definition in ACID properties
				- availability: each request receives a non-error response, without the guarantee that it contains the most recent write
				- partition tolerance: system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes
				- When a network partition failure occurs, it has to do one of the following:
					* cancel the operation and decrease the availability but ensure consistency
					* proceed with the operation and provide availability but risk inconsistency
				- therefore, when a design choice for network partition is made, it has to choose between consistency and availability.
			+ ***ACID = atomicity, consistency, isolation, durability***
				- consistency/correctness: database transactions must change affected data only in allowable ways (as specified by rules):
					* database constraints
					* cascades
					* triggers
					* combination thereof
					* subsequent starts of transactions will have to deal with the effects of past and current transactions
					* database consistency are about transactions
					* atomic consistency refers to the property of each request/response operation sequence
				- isolation is the goal of concurrency control
				- durability guarantees that committed transactions will be completed, even when there is system failure, such as power outage or crash
					* record completed transactions or their effects in NVRAM (or non-volatile memory)
			+ ***CRUD = create, read, update, and delete***
				- essential operations of ***database engine***, or ***storage engine***
				- basic operations of ***persistent storage***
					* data definition, or definition for data organization
						+ creation
						+ modification
						+ removal
					* update (data)
						+ insert data
						+ modify data
						+ delete data
					* retrieval
						+ access data in directly usable format
						+ access data for further processing
					* administration
						- register users
						- monitor users
						- enforce data security
						- monitoring performance
						- maintain data integrity
						- concurrency control
						- recover information corrupted by events, such as unexpected system failure.
				- design for:
					* ***predominantly "persistent data"***, infrequently accessed and not likely to be modified
					* ***predominantly "dynamic data", or predominantly "transactional data"***, asynchronously/periodically update information as new data becomes available
						+ new data can come at any time
						+ ***transaction data***, category of data describing information that refer to ***master data*** and/or ***reference data***, such as dates, times, time zones, and currencies
							- financial transactions
							- work transactions
							- logistics transactions
							- Note: master data and reference data provide context for transaction data (or transactions)
							- Note: reference data also provides information/data for classification and categorization of other data, such as:
								* units of measurement
								* country codes
								* corporate codes
								* fixed conversion rates for mass/weight, temeperature, and length
								* calendar structure and constraints
					* ***streaming data, event stream processing, data stream processing, or distributed stream processing***, constant flow of information
						+ encompasses:
							- dataflow programming
							- reactive programming
							- distributed data programming
					* ***static data, or unchanging data***, which does not change
					* ***unstructured data, or unstructured information***
						+ has no pre-defined data model
						+ not organized in pre-defined manner
						+ text heavy
							- can include data about:
								* dates
								* numbers
								* facts
							- can result in irregularities and ambiguities when processed by structured databases
						+ can be ***semi-structured data***, by containing data that has some structure associated with relational databases, RDBMS
							- XML, extensible markup language
								* XML databases
									+ native XML databases
								* XML-enabled databases
								* data-centric XML databases
							- JSON, JavaScript Object Notation
								* supported by:
									+ MongoDB
									+ Couchbase, formerly Membase
										- CouchDB???
							- Object Exchange Model, OEM
						+ can be highly structured, such that the structure is unanticipated and unannounced
						+ periods of inactivity between data arrival can exist
						+ for unstructured data database
					* ***semi-structured database model***
						+ no separation between data and database schema
					* types of database applications:
						+ online transaction processing, OLTP
							- facilitate and manage transaction-oriented applications
							- goals:
								* availability
								* speed
								* concurrency
								* recoverability
							- requirements:
								* high throughput
								* insert- or update- intensive database management
						+ online analytical processing, OLAP
							- multidimensional analysis, MDA
								* MDA queries
							- complex queries
							- small volume
							- for data analytics
								* such as business intelligence or business analytics
							- types:
								* multidimensional OLAP, MOLAP
								* relational OLAP, ROLAP
								* hybrid OLAP, HOLAP
						+ hybrid transactional/analytical processing, HTAP
							- for "in business real time" decision making
					* ***database administration tools***
						+ to manage DBMS such as:
							- MySQL
							- PostgreSQL
							- SQLite
					* relational database model, or relational model or RM
						+ ***relational databases, relational database management systems, RDBMS, or RDB***
							- row-oriented DBMS
							- column-oriented DBMS, columnar DBMS
								* store data tables based on columns rather than rows
					* SQL
						+ applications of SQL:
							- relational databases, relational database management systems, RDBMS, or RDB
							- relational data stream management systems, RDSMS
								* for stream processing
						+ NewSQL
							- extend relational databases, RDBMS, with scalability of NoSQL for online transaction processing, OLTP, while maintaining ACID guarantees of traditional RDBMS (or SQL databases)
							- primarily uses SQL interface
							- distributed computing uses a cluster of shared-nothing nodes, such that each node uses a subset of the data
								* components include:
									+ distributed concurrency control
									+ flow control
									+ distributed query processing
							- uses optimized storage engines for SQL
								* scale better than built-in engines
							- transparent sharding
								* split databases across multiple nodes using consensus algorithms:
									+ Raft
									+ Paxos
					* NoSQL
						+ non-SQL database, or non-relational database, or not only SQL database
						+ database management systems, DBMS, for data storage and retrieval beyond tabular relations in relational databases, relational database management systems, RDBMS, or RDB
						+ can support the following:
							- ACID properties
							- join operations
						+ applications:
							- Big Data applications
							- real-time Web applications
						+ see Wikipedia entry for NoSQL for comparing data models based on:
							- performance
							- scalability
							- flexibility
							- complexity
							- functionality
							- list of data models that are compared:
								+ [x] key-value database, key-value store
								+ [x] document-oriented database, document store
								+ [x] graph database, GDB
								+ relational database model, or relational model or RM
									- [x] relational databases, relational database management systems, RDBMS, or RDB
									- [x] column-oriented DBMS, columnar DBMS
						+ examples:
							- HBase
							- Cassandra
							- MongoDB
							- DynamoDB
					* ***multi-model database***
						+ database management system, DBMS, that is designed to support multiple data models with an integrated back-end
						+ other DBMSes are organized around a data model that determines how data is:
							- organized
							- stored
							- manipulated
						+ examples of supported data models:
							- relational database model, or relational model or RM
								* relational databases, relational database management systems, RDBMS, or RDB
							- key-value database, key-value store
							- graph database, GDB
							- document-oriented database, document store
						+ examples of multi-model databases/DBMS:
							- PostgreSQL:
								* [x] relational database model, or relational model or RM
									+ relational databases, relational database management systems, RDBMS, or RDB
								* [x] key-value database, key-value store
								* [x] graph database, GDB
								* [x] document-oriented database, document store
									+ JSON
									+ XML
								* [x] object database model
									+ object-relational database model, ORD, for object-relational database management systems, ORDBMS
					* navigational databases
						+ navigational databases use references from objects to find records or other objects
						+ hierarchical database model
						+ network database model
						+ graph database, GDB
					* entity relational database models, entity-relationship models, ER models
						+ enhanced entity relational database model, enhanced entity relationship model, EER model
					* entity-attribute-value database model, EAV
						+ or, object-attribute-value data model
						+ or, vertical database model
						+ or, open schema
					* physical database model, physical data model, or physical database design
						+ inverted index, postings list, postings file, or inverted file
						+ flat file, for 2-D arrays of data
							- for flat-file databases
							- examples are:
								* CSV, standard comma-separated-values
								* TSV, standard tab-separated-values
								* Awk, flat-file processor
					* star schema database model
					* semantic database model
					* named graph
					* correlational database model
					* dimensional database model
					* triplestore, or RDF store
						+ semantic triples, RDF triples, or triples
							- atomic data entities in resource description framework, RDF, data model
						+ uses resource description framework, RDF
						+ for storage and retrieval of triples, through semantic queries
					* uncertain databases
						+ includes the following types of uncertain databases:
							- probabilistic databases, probabilistic DBMS
								* possible worlds have associated probabilities
								* types of uncertainties:
									+ tuple-level uncertainty
									+ attribute-level uncertainty
					* non-relational database models:
						+ graph database model
							- ***graph database, GDB***
								* graph database processing is different from graph computing
									+ a separate component for graph databases is needed for graph computing to implement graph algorithms
								* examples:
									+ GraphQL
						+ ***key-value database, key-value store***
							- associative array, map, symbol, dictionary
							- hash table, dictionary
						+ document database model
							- ***document-oriented database, document store***
								* based on semi-structured database model
						+ ***wide-column store, extensible record store***
							- name and format of columns can vary between rows within a table
							- can be interpreted as a 2-D key-value database, key-value store
						+ multidimensional database model
							- resource space database model
							- multivalue database model
							- useful for:
								* online analytical processing, OLAP, applications
									+ multidimensional OLAP, MOLAP
									+ relational OLAP
									+ hybrid OLAP
						+ object database model
						+ object-relational database model, ORD, for object-relational database management systems, ORDBMS
							- object-relational impedance mismatch, can occur when object-oriented software interact with RDBMS
							- includes:
								* terminology-oriented database, or terminology-oriented database management system, or terminology-oriented DBMS
							- or, Object Relational Mapping (ORM)
					* blockchain-based database
						+ combines traditional database with distributed database
						+ supported by multiple layers of blockchains
						+ use features from SQL and NoSQL databases, with blockchain properties
							- data integrity
							- integrity assurance
							- decentralized control
							- Byzantine fault tolerance
							- transaction traceability
					* cloud database
						+ database/DBMS that runs on cloud computing platform
						+ deployment models
							- virtual machine image
								* run databases on the cloud independently, using virtual machine image
							- database-as-a-service, DBaaS
								* purchase/paid access to database service, maintained by cloud database providers
						+ can support SQL and NoSQL databases
						+ *join operations perform poorly*
					* spatial database
						+ usually implemented with relational databases, relational database management systems, RDBMS, or RDB
						include spatial data that represent objects
							- 3-D objects
							- topological coverages
							- linear networks
							- triangulated irregular networks, TINs
						+ includes:
							- geographical database, geodatabase, or georeferenced spatial database
								* "georeferenced" from georeferencing or georegistration
								* applications of coverage data
									+ geographical information systems, GIS
									+ geospatial content and services
									+ GIS data processing
									+ data sharing
								* types of data:
									+ 1-D sensor time series
									+ 2-D satellite images
									+ 3-D x/y/t image time series or x/y/z geo tomograms
									+ 4-D x/y/z/t climate and ocean data
					* temporal database
						+ store data about time instances
							- valid time
							- transaction time
							- decision time
					* real-time database
						+ DBMS that uses real-time processing to handle workloads that can be constantly changing
							- most of the data is not persistent data that is unaffected by time
						+ applications
							- accounting
							- banking
							- law
							- medical records
							- multimedia applications
							- process control
							- reservation systems
							- scientific data analysis
					* ***in-memory database, IMDB***, main memory database system, MMDB, memory resident database
						+ Can support ACID properties: atomicity, consistency, isolation, durability
						+ Can be implemented with NVRAM.
					* ***database security***
					* ***database scalability***
					* ***slowly changing dimension, SCD***
			+ Armstrong's axioms, references to infer functional dependencies on a relational database
			+ Codd's 12 rules, or Codd's twelve rules, for relational database management systems, RDBMS
+ ML/AI conferences
	- AAAI, IJCAI, NeurIPS, ACL, SIGIR, WWW, RSS, NAACL, KDD, IROS, ICRA, ICML, ICCV, EMNLP, EC, CVPR, AAMAS, HCOMP, HRI, ICAPS, ICDM, ICLR, ICWSM, IUI, KR, SAT, WSDM, UAI, AISTATS, COLT, CORL, CP, CPAIOR, ECAI, OR ECML







###	Machine Learning Scientist & Deep Learning Scientist Roles







***Machine Learning Scientist*** and ***Deep Learning Scientist*** roles:
+ skill set:
	- Principal Software Engineer - Applied Machine Learning
	- As the Principal Software Engineer for our Machine Learning team you will be responsible for ensuring that the development of ML systems and services meets all technical and quality standards. You will work with Product Management and other technical teams within Splunk, incorporating new requirements and providing technical information related to this sophisticated ML Platform as needed.
	- work with a team of senior ML engineers, applied researchers and security researchers, and experts within their own specialty. You will set an example for this group, as well as set high standards on quality, communication and ability to deliver with deadlines.
	- contribute to architecture and technical decisions while also mentoring junior members within the team.
	- be working in a multi-office, multi-location development environment and prior experience working with local and remote teams or groups will be a plus.
	- While expertise with ML products and their application within enterprise solutions is highly desirable, it is not required, provided you are willing to quickly come up to speed and you have some prior experience of ML technology and its application.
	- 12+ years software development with focus on large scale distributed systems.
	- Some Machine Learning application development experience, this is NOT a data scientist role, but a services/platform development role.
	- Ability to communicate effectively in conversations with researchers and engineers from academia background.
	- Passionate about building and encouraging good engineering practices and processes such as continuous integration and deployment.
	- Experience developing and putting into production test automation and CI/CD systems.
	- Expertise in developing software with container deployment and orchestration technologies at scale, with strong knowledge of the fundamentals including service discovery, deployments, monitoring, scheduling, load balancing.
	- Strong background in building streaming applications or streaming analytics platforms.
	- Expert in one of the streaming platforms, preferably Flink.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
+ ***Capsule Networks***, or capsule neural networks
+ skill set for ***Autodesk AI Lab, Pier.9 at San Francisco***:
	- [BrickBot](https://www.fastcompany.com/90204615/autodesks-lego-model-building-robot-is-the-future-of-manufacturing)
	- [Auto Sketching and Vectorization](https://canvasdrawer.autodeskresearch.com/)
	- [Topology Optimization for Specific Manufacturing Processes](https://www.autodesk.com/customer-stories/general-motors-generative-design)
	- Exploring and developing new Machine Learning models and techniques
	- Constantly reviewing relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduces creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Planning and designing research projects: specifying the problem and defining the project scope
	- Connecting with academics and institutions to build relationships and collaborate
	- Realizing solutions through prototypes
	- Exploring new data sources and discovering techniques for best leveraging data
	- Collecting and performing data analysis to validate and further new theories and discoveries
	- Publishing and talking at conferences
	- Working closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meeting with customers to understand how ML could be applied to their problems
	- Thinking strategically about research directions
	- Mentoring more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- Truly excited by the pace of advancement in AI research and technology
	- Understanding of fundamental CS algorithms and their scaling behaviors
	- ***Solid background in statistical methods for Machine Learning: Bayesian methods, dimensional reduction, SVD, clustering, classification, forms of regression, etc***
	- ***Strong familiarity with Deep Learning techniques: various network architectures (CNNs, GANs, RNNs, Auto-Encoders, etc.); regularization; embeddings; loss-functions; optimization strategies; etc***
	- ***Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, PaddlePaddle, etc***
	- Experience training and debugging networks
	- Strong coding abilities in: Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- ***Reinforcement Learning and other areas of Control Theory***
	- Distributed Systems and High Performance Computing methods
	- ***Geometric Shape Analysis***
	- ***Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc***
	- ***Knowledge Representation (semantic models, graph databases, etc.)***
+ skill set:
	- Our AI Labs focus on research in: ***deep learning, control systems, simulation and knowledge representation applied to diverse areas such as: geometry, robotics, advanced sensing, design exploration and sustainable engineering or construction practices***. The labs also host product engineers resulting in early productization of our research, so you can see your work in action.
	- You will be a senior researcher focusing on problems related to geometry understanding, manipulation and synthesis.
	- The Lab brings together AI Researchers, Software Engineers and specialists in various problem areas to create novel AI solutions in all the areas mentioned above and more. They work closely with experts in: geometric modeling, simulation systems, robotics, knowledge representation, sensing and computer vision, industrial manufacturing and construction techniques.
	- Explore and develop new Machine Learning models and techniques
	- Constantly review relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduce creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Plan and design research projects: specifying the problem and defining the project scope
	- Connect with academics and institutions to build relationships and collaborate
	- Realize solutions through prototypes
	- Explore new data sources and discover techniques for best leveraging data
	- Collect and perform data analysis to validate and further new theories and discoveries
	- Publish and talk at conferences
	- Work closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meet with customers to understand how ML could be applied to their problems
	- Think strategically about research directions
	- Mentor more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- ***Solid theoretical background in geometry and geometric methods (e.g. shape analysis, topology, differential geometry, discrete geometry, functional mapping, etc.)***
	- ***Good background in statistical methods for Machine Learning (e.g. Bayesian methods, HMMs, Graphical Models, dimensional reduction, clustering, classification, regression techniques, etc)***
	- ***Familiarity with Deep Learning techniques (e.g. Network architectures; regularization techniques; learning techniques; loss-functions; optimization strategies etc)***
	- ***Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, Chainer, etc.***
	- Strong coding abilities in: Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- ***Reinforcement Learning and other areas of Control Theory***
	- Distributed Systems and High Performance Computing methods
	- ***Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc.***
	- ***Knowledge Representation (semantic models, graph databases, etc.)***
+ [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
	- [TensorBoard, TensorFlow's visualization toolkit](https://www.tensorflow.org/tensorboard)
	- https://databricks.com/tensorflow/visualisation
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Participate in cutting edge research in machine intelligence and machine learning applications.
	- Develop solutions for real world, large scale problems.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- ***Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.***
	- ***Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).***
		* Chainer: open-source deep learning framework
+ skill set:
	- Solid Machine Learning background and familiarity with standard speech processing and machine learning techniques
	- Experience with one or more deep learning libraries and platforms (e.g., ***TensorFlow, Caffe, Chainer or PyTorch***).
	- Industry or academic experience in deep learning research.
	- ***Strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, KDD, PAMI, JMLR, TACL, IJCV).***
+ skill set:
	- **Salesforce Research and Einstein.AI (formerly MetaMind) are looking for extraordinary deep learning or research engineers.**
	- As a deep learning or research engineer, you will work with research scientists and engineers to develop and productize new cutting edge models and associated artifacts such as data preparation pipeline and model characterization logic. You will ensure these models are developed to support accuracy, performance or other specific customer requirements.
	- You will work with platform team to support deployment of these models. In other words, you are problem solver, a deep learning model designer, and an engineer who makes sure the model is deployed at scale to serve our customers with state-of-the-art speech, vision, and language technologies.
	- You have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, voice, or reinforcement learning. Additionally, applicants should have in-depth experience with problems such as text categorization, information extraction, question answering, text summarization, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Partner with product managers to understand customer requirements
	- Conduct research (including reviewing relevant literature) and collaborate with our research team to identify appropriate solution candidates
	- Develop prototypes, then design and carry out experiments to validate and improve the prototypes
	- Bring the ideas to production
	- Monitor model behaviors in production and iteratively improve quality of services over time
	- Work on cutting-edge research in machine learning
	- MA/MS or PhD degree in computer science, artificial intelligence, machine learning, speech recognition, natural language processing, or related technical field such as operations research, computational mathematics, etc.
	- Research experience or contributions in deep learning, machine learning, NLP, computer vision, reinforcement learning, or optimization.
	- Solid Machine Learning background and familiarity with machine learning techniques
	- Problem solving and ability to reuse, customize, and implement latest research
	- Experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, or PyTorch)
	- Industry experience in deep learning research
	- Can thrive in team environments; using agile methodology and interacting with Product Leaders, Scientists and Engineers to solve technology's greatest challenges
	- In particular, we are looking for experienced engineers with Deep Learning experience and domain expertise around Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Vision to provide the best possible experience for our customers.
	- Experience designing and implementing machine learning pipelines in production environments.
	- Experience in building speech recognition and natural language processing systems (e.g. commercial or government-funded speech products) is a huge plus.
	- We value professional industry experience; advanced degrees alone do not replace real world experience.
	- Excellent communication, leadership, and collaboration skills.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- ***Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.***
	- Experience with one or more general purpose programming languages including but not limited to C/C++ or Python.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, object detection or reinforcement . Our postdoctoral researchers have the ability to give talks, attend conferences and build relationships with academic institutions if desired.
	- Collaborate on research to advance the science and technology of artificial intelligence.
	- Contribute to cutting edge research projects in machine intelligence and machine learning applications that can be infused into our world-class CRM.
	- Develop solutions for real world, large scale problems.
	- Influence progress of relevant research communities by producing publications.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Create a year long project proposal with research managers.
	- ***First-author publications at AI conferences and journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
+ skill set:
	- Salesforce Research Asia is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision,
		* speech recognition, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue systems, language and speech, machine translation, language and vision, image classification, object detection, or image semantic segmentation, etc.
	- ***Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, ECCV, SIGKDD, PAMI, JMLR, TACL, IJCV) are preferred.***
	- Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	- Experience with one or more deep learning libraries and platforms, e.g. PyTorch, TensorFlow, Caffe, or Chainer
	- Strong background in machine learning, natural language processing, speech, computer vision, or reinforcement learning
	- Strong algorithmic problem solving skills
	- Programming experience in Python, Java, C/C++, Lua, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV) are preferred.***
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area
	-  Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	-  Experience with one or more deep learning libraries and platforms, e.g. Torch, TensorFlow, Caffe, or Chainer
	-  Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	-  Strong algorithmic problem solving skills
	-  Programming experience in Python, Lua, Java, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for an outstanding entry level research scientists focused on ethics in AI. It is our belief in the words of our CEO Marc Benioff, “The business of business is improving the state of the world." The way we behave — with integrity, transparency, alignment, and accountability — builds trusted relationships. We believe that companies can do well and do good in the world. We know technology is not inherently good or bad. It's what we do with it that matters. With AI, we believe that we can go even further to advance and support its effectiveness by ensuring equality, transparency, and accountability in the models we create and how we implement them in our products.
	- As a research scientist, you discover new research problems, develop novel models, design careful experiments and generally advance the state of the art in AI. At Salesforce, the research team is committed to collaboration with the wider research community. In this unique role, you will have the opportunity to work directly on advancing technologies that nonprofits use to solve problems in the real world that create positive impact for the world while accomplishing publications at major conferences. We believe that making substantive progress on hard problems can drive and sharpen the research questions we study, and, in turn, scientific breakthroughs can spawn entirely new applications. With this in mind, the team maintains a portfolio of projects, some with an immediate path to production, others that may not find an application for several years. Research scientists have the freedom to set their own research agenda and move between pure and applied research.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area (e.g., Machine Learning, AI, AI ethics, law and policy)
	- Excellent understanding of deep learning models and techniques (i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods)
	- Experience with one or more deep learning libraries and platforms (e.g. PyTorch, TensorFlow)
	- Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	- Programming experience in Python or a similar language
	- Strong algorithmic problem-solving skills
	- Demonstrable experience implementing machine learning models and algorithms, e.g., through open-source implementations, or shareable code
	- Strong presentation and communication skills
	- Experience applying deep learning models to ethical issues in AI or social causes (e.g., racial disparity in facial recognition, explainability of AI for redress and remediation)
	- Experience researching artificial intelligence ethics, including areas such as fairness, safety, privacy and transparency in artificial intelligence
	- ***Published in top-tier conferences or journals (e.g., FAT\*, NIPS, AIES, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV)***
	- Open-source implementations of machine learning research projects.
	- The ideal candidate will have a keen interest in producing new science to understand intelligence and technology and how to apply it safely and fairly in real-world settings.
+ ***Open-source projects that demonstrate relevant skills and/or publications in relevant conferences and journals (e.g. NIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICASSP)***
+ skill set:
	- Machine Learning Researcher
	- Machine learning is a critical pillar of Jane Street's global business, and our ever-changing trading environment serves as a unique, rapid-feedback platform for ML experimentation. 
	- Researchers at Jane Street are responsible for building models, strategies, and systems that price and trade a variety of financial instruments. As a mix of the trading and software engineering roles, this work involves many things: analyzing large datasets, building and testing models, creating new trading strategies, and writing the code that implements them.
	- We’re looking for people to join the research team with deep ML experience in either an applied or academic context. A good candidate should have a deep understanding of a wide variety of ML techniques, and a passion for tinkering with model architectures, feature transformations, and hyperparameters to generate robust inferences. We also want people who are good communicators, with the ability to quickly absorb the context of a new problem, carefully consider tradeoffs, and recommend possible solutions.
	- As an ML researcher, your expertise will also shape the firm's future ML developments including hiring new ML researchers, attending conferences, teaching techniques to teammates, and setting firmwide goals.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
































***Machine Learning Engineer***, and ***Deep Learning Engineer***, roles:
+ You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
+ tech stack:
	- Experience with Deep Learning frameworks, e.g., PyTorch, DeepLearning4J, TensorFlow
	- Experience in SPARK (using Python or Scala). Knowledge of AWS services will be appreciated.
+ skill set:
	- 3+ years of experience in machine learning, data mining, natural language processing, information retrieval, or statistical analysis
	- Experience working with large data sets using open source technologies such as Spark, Hadoop, and NoSQL
	- Experience developing and productizing real-world AI/ML applications such as prediction, personalization, recommendation, content understanding and NLP
	- Experience working with at least 3 of the following popular machine learning frameworks/libraries: sklearn, tensorflow, pytorch, caffe, keras, theano, cntk, mxnet, spark mllib
	- Experience developing and deploying deep learning NLP models is a plus
	- Experience working with a knowledge graph is a plus
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
+ skill set:
	- Individuals in this role should be experts in machine learning and NLP and have experience working on problems such as language models, discourse analysis, question-answering, word-sense disambiguation, automatic summarization etc.
	- Improve our existing NLP and Machine Learning systems using your expertise
	- Identify new opportunities to apply NLP and Machine Learning to different parts of the Quora product
	- Work with other engineers to implement algorithms, abstractions and systems in an efficient way, with strong positive impact on our user-facing products
	- Take end to end ownership of Machine Learning systems - from data pipelines and training to realtime prediction engines
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
+ skill set:
	- At Quora, we use Machine Learning in almost every part of the product - feed ranking, answer ranking, search, topic and user recommendations, spam detection etc.
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building internet applications and large systems
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
	- Passion for learning
+ skill set:
	- We use a variety of algorithms — everything from linear models to decision trees and deep neural networks.
	- To that end, we are looking for engineers to help us build our company-wide ML development platform. In this role, you will be the part of a small team solving very interesting technical problems at the intersection of various exciting domains like Machine Learning, Distributed Systems and High Performance Computing.
	- Build and maintain large scale distributed systems to support the whole pipeline from data collection and training to deployment
	- Write efficient implementations of ML algorithms over CPUs & GPUs
	- Integrate our in-house systems with open source libraries like Spark and Tensorflow
	- Build abstractions to automate various steps in different ML workflows
	- Build tools to debug, visualize and inspect various features and models
	- Work with the engineers who use the platform, and help them be more impactful by improving the platform
	- Experience with designing large-scale distributed systems
	- Experience with building end-to-end machine learning systems
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building end to end Machine Learning systems
+ ***Capsule Networks***, or capsule neural networks
+ skill set:
	- Experience working with relational SQL and NoSQL databases
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Fluency with one or more programing language: Python, Java, Scala, etc
	- Good understanding of CS fundamentals, e.g. algorithms and data structures
	- Experience with data science tools and libraries, e.g. R, pandas, Jupyter, scikit-learn, TensorFlow, etc
	- Familiarity with statistical concepts and analyses, e.g. hypothesis testing, regression, etc
	- Familiarity with machine learning techniques, e.g. classification, clustering, regularization, optimization, dimension reduction, etc
	- Guide the utilization or development of a robust CI/CD capability.
	- Identify key performance & effectiveness metrics, monitor & adjust to goals
	- Work with test automation team to lay the groundwork for automated API testing framework and test cases
	- Prioritize backlog & drive product releases
	- Distill strategic intent into structured product release roadmaps that are compelling and achievable
	- Ability to execute and manage performance and expectations within a cross-functional, matrix management environment
+ A background in machine learning and related sub-areas including ranking, personalization, search, recommendation, explore/exploit, causal learning, reinforcement learning, deep learning and probabilistic modeling.
+ skill set:
	- As a Deep Learning Engineer at Simbe Robotics, you will be part of a talented team designing and training state of the art deep learning algorithms to identify placement, presentation, pricing, and availability of products in retail stores across the globe.  
	- In this role you will lead various initiatives designing, developing, and training in-house character recognition and image caption algorithms powered by deep learning.
	- Participate in planning and prioritizing, write functional specifications and lead design reviews for our character recognition and image caption algorithms.
	- Generate, clean, and curate real world training datasets
	- Create photorealistic synthetic training data for augmentation
	- Develop, test, tune, and deploy character recognition and image caption systems across a wide variety of customers
	- Evaluate existing character recognition and image caption methods for speed and accuracy performance improvements
	- Collaborate with other developers, quality engineers, product managers, and documentation writers
	- Ph.D. or M.S. preferred
	- Strong machine learning background, with 2+ years of hands-on experience in building real systems
	- Deep understanding of state of the art machine learning and deep learning algorithms, techniques and best practices
	- Solid understanding of linear, non-linear, and dynamic programming
	- Experience using or building synthetic image generation systems, data augmentation pipelines, and OCR/image caption systems
	- Proficient in at least one of the following: Tensorflow, Keras, PyTorch. Tensorboard knowledge is a plus
	- Must be fluent in Python, other languages are a plus
	- Should be familiar with training and running deep learning models on GPUs (both commodity and otherwise)
	- A good understanding of recurrent neural networks (including LSTMs and GRUs)
	- Experience in debugging and diagnosing performance problems with ML algorithms
	- Must have excellent written and verbal communication skills
	- Experience with attention models, text localization, Google Cloud Platform, AWS, and serverless is a plus
	- Strong Linux & Command Line background
	- Ability to work hands-on in cross-functional teams with a strong sense of self-direction
+ skill set:
	- You have industry experience with writing code (e.g., Python, Scala, PySpark, Java) and taking ML models/ algorithms to production. Preference for 5+ years of industry experience (without PhD); at least 2-3+ years of industry experience with PhD. This is not an entry level / new college graduate role.
	- Experience with Apache Spark platform (including Datasets, SparkML) and/or experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe or PyTorch).
+ skill set:
	- As a research engineer at Salesforce Research, your role will be at the intersection of software engineering and research, and may range from implementing novel research models to rapid-prototyping demos that show off applications of deep learning on production data. You will work closely with research scientists to develop models, prototypes, and experiments that push the state of the art in AI research, paving the way for innovative products for the Einstein AI Platform. You will have the opportunity to take on real-world problems from Salesforce's enterprise customers with the latest deep learning models.
	- You have strong programming skills and a background in one or more of the following domains: deep learning, machine learning, natural language processing, or computer vision, with applications such as: text categorization, text summarization, sentiment analysis, information extraction, question answering, dialogue learning, language and vision, image classification, image segmentation, and object detection.
	- Knowledge of linear algebra, calculus, statistics, and machine learning.
	- Practical experience in natural language processing, computer vision, crowdsourcing, or information retrieval.
	- Exposure to industry or academic research, particularly in deep learning, neural networks, or related fields.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Experience with Amazon Web Services and Mechanical Turk.
	- Strong computer systems experience in topics such as filesystems, server architectures, and distributed systems.
	- Experience in GPU programming, data visualization, or web development.
+ skill set:
	- Machine learning. You should be able to understand and apply major machine learning methods, such as logistic regression, SVM, Decision Trees, Principal Component Analysis and K-means. Completion of Andrew Ng's Machine Learning course on Coursera is sufficient to meet this criterion.
	- Deep learning. You should be able to understand and apply major deep learning methods, including neural network training, regularization, optimization methods (gradient descent, Adam), and be familiar with major neural network architecture types such as Convolutional Networks, RNN/LSTM. Completion of the deeplearning.ai specialization is sufficient to meet this criterion.
	- Implementation. You should have prior experience taking a dataset, cleaning it if necessary, and applying a learning algorithm to it to get a result. You should be able to implement a learning algorithm “from scratch” using a framework such as NumPy, Tensorflow, Pytorch, Caffe, etc.
	- General coding. You should be able to code non-trivial functions in object-oriented programming, such as popular sorting or search algorithms.
	- Mathematics (including probabilities and statistics.) You should be able to use mathematical notations and linear algebra (matrix/vector operations, dot products, etc.), and understand basic probability theory (distributions, independence, density functions, etc.) as well as statistics (mean, variance, median, quantiles, covariance, etc.)
	- Software Engineering. You should know how to use your terminal, work with version control systems (Git), relational databases, APIs, and build the back-end of web or mobile applications.
	- Mean Stack
		* MEAN is a free and open-source JavaScript software stack for building dynamic web sites and web applications.
		* The MEAN stack is MongoDB, Express.js, AngularJS (or Angular), and Node.js.
+ skill set:
	- The steps of an end-to-end machine learning project. This includes, but is not limited to:
		* Conducting a structured and deep literature review of a specific field.
		* Strategizing your machine learning project end-to-end.
		* Collecting, cleaning, labeling, and augmenting your own dataset.
		* Training a model for a real-world application.
		* Setting-up an efficient and organized experimentation process.
		* Defining task-specific metrics to optimize in your experiments.
		* Performing error analysis to improve your models.
		* Deploying an AI product.
		* Exposure to real-world problems that multiple AI teams in our community work on.
	- Hands-on experience in designing, building, and deploying end-to-end AI solutions through curated content and instructor-led workshops.
	- Career mentorship and connections with teams aligned with your career aspirations.
	- Meet and share experiences with other machine learning engineers and data scientists.
	- Everyone who successfully completes the Bootcamp will be awarded a certificate of completion and join the AI Bootcamp Alumni community.
	- Machine learning Engineers and Data Scientists who have already worked on Machine Learning projects and want to get exposed to different Machine Learning problems.
	- Demonstrated AI, data science and/or data analysis experience from previous work experience or publications.
	- Demonstrated strong coding from previous work experience or publications. This means you're able to write a non-trivial program in Python, Java, or C++.
	- Solid CS foundation (including but not limited to Operating Systems, Computer Networks, Database, etc.)
+ Experience working with modern deep learning software architecture and frameworks including: Tensorflow, MxNet, Caffe, Caffe2, Torch, and/or PyTorch.
+ experience creating machine learning products
	- deploy model
	- MLOps
+ skill set for machine learning architect:
	- optimize workflows and solve problems using a data-driven approach
	- provide machine learning as a service (MLaaS) for users
	- co-develop machine learning as a service (MLaaS) platform, with the following features:
		* data ingestion
		* data indexing
		* data labeling
		* visualization
		* dashboards
		* data viewers
	- collaborate with team on production quality service development with:
		* unit testing, integration testing for MLaaS
		* CI/CD for MLaaS
		* DevOps for MLaaS
	- develop and maintain services with:
		* high production quality standards
		* components for feature data storage
		* ML model training and inferencing
		* ML model storage and management
		* model evaluation metrics
	- demonstrate experience in MLOps and deep learning related infrastructure
	- solid foundation for machine learning, deep learning architecture and service development experience
	- provide capacity to use and fine-tune latest state-of-the-art machine learning and deep learning models, depending on the use cases
		* develop lifecycle management tool for these machine learning and deep learning models to be deployed as a service and made available for inferencing
	- detailing processes and workflows
	- Programming skills in:
		* Java
		* Python
		* Web service development, using REST API
			+ front-end Web development
				- React
				- Ember.js
	- object-oriented design patterns
	- ability to learn quickly and adapt to different platforms as per the need of the project
	- crafted/developed production quality microservices
	- knowledge of:
		* Hadoop
		* Hive
		* Spark
		* Kafka
		* cloud/distributed infrastructure
		* SQL and NoSQL data platforms
+ skill set:
	- deep knowledge of distributed training concepts and frameworks, such as Megatron and Deepspeed
		* Megatron-Turing Natural Language Generation model, MT-NLG,
	- design and develop analysis tools to drive efficient research, such as:
		* deep cleaning
		* analysis of training dynamics
		* grdient quality
	- design and develop software for scaling models and large-scale experimentation
	- design and develop ML workflows and user interfaces for novel algorithms
	- deep knowledge of machine learning framework, such as TensorFlow and PyTorch
+ skill set:
	- wafer scale engine, WSE
	- NLP models:
		* BERT
		* GPT
	- computer vision models:
		* ResNet
		* Vision Transformer
	- sparse and low-precision training algorithms for reduced training time and increased accuracy
	- compute- and memory- efficient training techniques, such as reversibility and low-rank
	- Scaling laws for increasing model size: accuracy/loss, architecture scaling, hyperparameter transfer 
	- Optimizers, initializers, normalizers to improve distributed training on large scale clusters 
	- Develop novel training algorithms and demonstrate on state-of-the-art large DNNs 
	- Develop novel network architectures and layers such as normalization, activation functions, optimizers, and parameter layers 
	- Design and run research experiments to prove novel algorithms are effective and robust 
	- Analyze results to gain research insights, including training dynamics, gradient quality, and dataset cleaning techniques 
	- Publish and present research at leading machine learning conferences 
	- Collaborate with engineers in co-design of the product to bring the research to customers 
	- Strong grasp of machine learning theory, fundamentals, linear algebra, and statistics 
	- Experience with state-of-the-art DNNs models, such as BERT and GPT 
	- Experience with machine learning frameworks, such as TensorFlow and PyTorch
	- Experience with distributed training concepts and frameworks such as Megatron and Deepspeed 
	- Fluency in a programming language, such as Python 
	- Strong track record of relevant research success through relevant publications/patents at top conferences or journals (e.g. ICLR, ICML, NeurIPS)
+ experience with modern compiler frameworks:
	- TVM
	- LLVM
	- MLIR
	- GLOW
	- XLA
+ skill set:
	- deep learning runtimes: ONNX Runtime, TensorRT
	- inference server or model serving frameworks
		* Triton
		* TFServe
		* KubeFlow
	- distributed systems collective
		* NCCL
		* OpenMPI
	- deploy ML workloads on distributed systems, in a multitenancy environments
	- MLOps, from definition to deployment, including training, quantization, sparsity, model preprocessing, deployment
	- training, tuning, and deploying ML models for:
		* computer vision, such as ResNet
		* natural language processing, such as BERT, GPT
		* recommendation systems, DLRM
+ machine learning model compression techniques, such as quantization or pruning
+ skill set:
	- Machine Learning Research Engineer, Generative AI
	- Performant model code, high quality data, and robust evaluation methods form the foundation of an AI system. Scale’s leading end-to-end solutions for the ML lifecycle based on real-world data will continue to set the bar for the data-centric AI movement. Scale’s Generative AI team focuses on building models to accelerate AI adoption for some of the largest companies in the world. 
	- Your focus will be on developing Models as a Service using a variety of Machine Learning techniques. You will be involved end-to-end from coordinating with operations to create high quality datasets to productionizing models for our customers. If you are excited about shaping the future of the data-centric AI movement, we would love to hear from you!
	- Apply state of the art models, developed both internally and from the community, in production to solve problems for our customers and data labelers. 
	- Work with product and research teams to identify opportunities for ongoing and upcoming services.
	- Explore approaches that integrate human feedback and assisted evaluation into existing product lines. 
	- Work closely with customers - some of the most sophisticated ML organizations in the world - to quickly prototype and build new deep learning models targeted at multi-modal content understanding problems.
	- At least 3 to 5 years of model training, deployment and maintenance experience in a production environment.
	- Strong skills in NLP, LLM and deep learning.
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment.
	- Experience in dealing with large scale AI problems, ideally in the generative-AI field. 
	- Demonstrated expertise in large vision-language models for diverse real-world applications, e.g. classification, detection, question-answering, etc. 
	- Published research in areas of machine learning at major conferences (NeurIPS, ICML, EMNLP, CVPR, etc.) and/or journals. 
	- Strong high-level programming skills (e.g., Python), frameworks and tools such as DeepSpeed, Pytorch lightning, kuberflow, TensorFlow, etc. 
	- Strong written and verbal communication skills to operate in a cross functional team environment. 
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $176,000 - $240,960. Compensation packages at Scale include base salary, equity, and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Scale employees are also granted Stock Options that are awarded upon board of director approval. You’ll also receive benefits including, but not limited to: Comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous PTO. Additionally, this role may be eligible for additional benefits such as a commuter stipend.
+ skill set:
	- Scale Spellbook is a developer platform for prompt engineering, evaluation, deployment, knowledge retrieval and more. We are looking for a strong product engineer to join our team and help us scale and grow our product. The ideal candidate will have a strong understanding of software engineering principles and practices, as well as experience with large-scale distributed systems. You will be responsible for owning large new areas within our product, working across backend, frontend, and interacting with LLMs and ML models. You will solve hard engineering problems in scalability and reliability.
	- Own large new areas within our product
	- Work across backend, frontend, and interacting with LLMs and ML models
	- Deliver experiments at a high velocity and level of quality to engage our customers
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- 5+ years of full-time engineering experience, post-graduation
	- Experience scaling products at hyper growth startups
	- Experience tinkering with or productizing LLMs, CV, vector databases, and the other latest AI technologies
	- Proficient in Python or Javascript/Typescript, and Sql database
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $212,800-$258,121.
+ skill set:
	- Experience  in using the following systems in production: AWS, Typescript, Node, MongoDB, MLflow, Python (note that we are mostly language-agnostic and are open to using whatever is the best tech for the problem at hand)
	- Experience working with Docker, Kubernetes, and Infrastructure as code (eg terraform); bonus points for running GPU/ML workloads
	- Experience building systems that process large volumes of data.
	- Experience with core AWS technologies such as VPC, EC2, ALB, ASG, Spot Instances
	- Experience in operating or managing Infrastructure such as Spark, Presto, Hive
+ skill set:
	- Staff Machine Learning Research Engineer, Generative AI
	- Scale's Generative AI Data Engine powers the most advanced LLMs and generative models in the world through world-class RLHF/RLAIF, data generation, model evaluation, safety, and alignment.
	- As the Lead of the Generative AI team, you will be responsible for managing and leading a group of talented researchers and engineers. Your primary focus will be to leverage your expertise in LLMs, generative models, and other foundational models to create and execute an AI roadmap which will help Scale accelerate our customers' Generative AI initiatives forward. This is an exciting opportunity to work on cutting-edge technologies and collaborate with industry-leading professionals.
	- We are building a large hybrid human-machine system in service of ML pipelines for dozens of industry-leading customers. We currently complete millions of tasks a month and will grow to complete billions monthly.
	- Led a team of highly effective researchers and engineers. Provide guidance, mentorship, and technical leadership to a team of researchers and engineers working on Generative AI projects. Develop and evaluate methods for integrating machine learning into human-in-the-loop labeling systems to ensure high-quality and throughput labels for our customers.
	- Implement and improve on state-of-the-art models developed internally and from the community and put them into production to solve problems for our customers and taskers.
	- Work with product and research teams to identify opportunities for improvement in our current product line and for enabling upcoming product lines.
	- Work with massive datasets to develop both generic models as well as fine-tune models for specific products.
	- Work with customers and 3rd party research groups to understand their goals and define how we can enable them.
	- Build a scalable ML platform to automate our ML services, including automated model retraining and evaluation.
	- Be able and willing to multi-task and learn new technologies quickly.
	- Must be able to commute to the San Francisco Office 1-2x weekly. 
	- 7+ years of full time work experience using LLM, deep learning, deep reinforcement learning, or natural language processing in a production environment. Especially training foundational AI models through pre-training, fine-tuning, and RLHF.
	- A vision for where the field should go and what Scale should do to enable it.
	- Strong programming skills in Python, experience in PyTorch or Tensorflow
	- Experience with MLOps and the automation of model training & evaluation
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Solid background in algorithms, data structures, and object-oriented programming
	- Deep appreciation for building high-quality, robust, reusable machine-learning software
	- Degree in computer science or related field
	- Graduate degree in Computer Science, Machine Learning or Artificial Intelligence specialization
	- Publication experience in the field or related topics.
	- Experience with model optimization techniques for both training and inference
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $176,000 - $250,000.
+ Experience working fluently with standard orchestration & deployment technologies like Kubernetes, Temporal, Terraform, Docker, etc in multiple clouds
+ skill set:
	- Senior Software Engineer, AV / CV
	- At Scale AI, we are building tools to across the AI development lifecycle. Data is the new code, and Scale AI helps companies get the data they need, whether it’s for self-driving vehicles, artificial general intelligence, or robotics. The AV-CV Team (Autonomous Vehicles / Computer Vision) builds the infrastructure for labeling Lidar, Mapping, and Camera data. If seeing autonomous vehicles excited you, you’ll be seeing a lot of that on the AV-CV team. The team builds the tools for labeling Lidar pointclouds, annotating image and video feeds, and linking them together to build perception models.
	- Own large new areas within our product
	- Become an expert in working closely with customers across many CV industries
	- Build technologies ranging from frontend and backend to automated ML systems
	- Work deeply with sales and marketing to run demos and increase customer engagement
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- 5+ years of full-time engineering experience
	- 2+ years working with TypeScript/JavaScript, HTML, CSS, and related web technologies (React, Next.js, Webpack)
	- Experience working with distributed systems and cloud environments
	- Experience working with a production database (Postgres, MongoDB, MySQL, MS SQL) and schema migrations
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Excitement to work with AI technologies
	- Strong written and verbal communication skills
	- Strong problem-solving skills, and be able to work independently or as part of a team.
	- The base salary range for this full-time position in our hub locations of San Francisco is $153,000 - $215,000.
+ skill set:
	- Familiar with CUDA ecology, experience in NCCL and RDMA development is preferred;
	- Understand common large-scale distributed training optimization strategies, familiarity with fsdp/deepspeed/accelerate/galvatron is preferred.
+ skill set:
	- Familiarity with distributed training, NCCL communication library, etc. is preferred; familiarity with large model training frameworks such as DeepSpeed, Megatron, etc. is preferred;
	- Proficiency in CUDA and other heterogeneous programming is preferred; familiarity with GPU/GPGPU architecture is preferred;
	- Familiar with CNN, RNN, Transformer and other deep neural networks is preferred;
	- Familiarity with AI compilation technologies such as TVM and MLIR is preferred.
+ skill set:
	- Proficient in Gtest or Pytest framework, and be familiar with Jenkins, K8s and Docker environment.
	- Experience in PyTorch/TensorRT/Tensorflow programming is a big plus.
	- Experience in KMD/UMD/Operator development is a big plus.
+ Familiar with and have used one or more mainstream deep learning frameworks (Tensorflow/PyTorch/PaddlePaddle/MindSpore, etc.);
+ skill set:
	- Research and analyze common algorithm models (CNN/RNN/Transformer, etc.) in scenarios such as computer vision, speech recognition, natural language processing, and advertisement recommendation, and analyze their implementation, performance, and accuracy on mature GPUs;
	- Based on Biren's chip products (GPGPU), participate in the transplantation, acceleration, precision tuning of deep learning algorithm models on Biren GPU, and performance tuning of application pipelines;
	- Closely track industry trends and technology trends, focusing on hot algorithms/models that will be applied in the industry.
	- Master’s degree or above in EECS or related majors, with more than 5 years of relevant work experience;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorflow/PyTorch/PaddlePaddle/MindSpore, etc.);
	- Familiar with the principles of deep learning algorithms, have experience in precision and performance tuning, and be familiar with the application of algorithms in business scenarios;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Have a developer's perspective understanding of one or more mainstream deep learning frameworks, and have certain insights and experience in framework design or tuning;
	- Familiar with large-scale distributed training, understand the principle and implementation of large models such as GPT;
	- Have experience in algorithm and engineering collaborative optimization, and be able to make a reasonable trade-off between accuracy and performance;
	- Experience in GPGPU programming, experience in domestic GPGPU/AI chips;
	- Have experience in AI compiler.
+ skill set:
	- System Architect - HW/SW Architecture - Apple Vision Pro
	- Apple is where individual imaginations gather together, committing to the values that lead to great work. Every new product we build, service we create, or Apple Store experience we deliver is the result of us making each other’s ideas stronger. That happens because every one of us shares a belief that we can make something wonderful and share it with the world, changing lives for the better. It’s the diversity of our people and their thinking that inspires the innovation that runs through everything we do. When we bring everybody in, we can do the best work of our lives. Here, you’ll do more than join something — you’ll add something.
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- 6+ years of working experience
	- Strong system / SoC architecture background with deep understanding of low power processing.
	- Experience in development of SoC Power and Performance simulation models for use case level analysis and optimization.
	- Strong coding skills in Python, C/C++, SystemC.
	- You have background on NN algorithms and Neural HW architecture.
	- You have background on HW acceleration for image / display / audio signal processing workloads.
	- Ability to drive HW / SW partition and co-optimization in a data driven way.
	- Strong verbal communication skills across organizations and at the executive level.
	- Excellent written skills for clear reporting of data analysis and conclusions.
	- We are seeking a highly motivated, system architect to define compute acceleration strategy into our products. You must have excellent understanding of computer architecture (CPU, GPU, Neural Processing), System / SoC level power architecture, emerging compute engines RISC-V, Low power neural processing architecture. You will be mapping system level use cases to the SoC and system and identify HW/SW partitioning opportunities. With this knowledge you are able to apply it to analyzing requirements tradeoffs impacting system performance and product experience. 
	- The job will have a focus on use case level analysis and development of system level models for power, performance and latency to drive overall product architecture.
	- This cross-functional role requires excellent interpersonal skills, working with camera module, architecture, system EE, and product development teams and communicating across teams and to leadership.
	- MS/PhD in relevant field (EE, CS, Computer Eng)
	- At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ ***Experience with metrics systems such as Grafana.***
+ skill set:
	- ML Systems & Performance Engineer - SPG
	- The key to successful Machine Learning Teams is an efficient, robust, flexible and scalable architectural foundation. The Apple Special Projects Group on Autonomous Systems is looking for a talented, dedicated and result oriented software engineer to help build and improve our Machine Learning Systems. This fundamental and highly impactful work enables us to architect, train, evaluate and deploy state of the art machine learning models for Autonomous Systems.
	- You will be part of a world class team with a highly diverse skillsets. From implementing CUDA kernels, optimizing cache coherency of complex algorithms, reducing memory footprints all the way to optimizing distributed training setups and implementing efficient large scale cloud compute schedules, ML Systems & Performance engineers are taking a holistic view at the whole stack to eliminate bottlenecks resulting in a lean and optimally efficient implementation.
	- 3+ years of professional software development experience.
	- Familiarity with modern Machine Learning frameworks like PyTorch.
	- Familiarity with optimization techniques like Quantization, sparsity and/or pruning.
	- Experience in architecting and implementing large-scale cloud pipelines., e.g using spark, kafka, kubeflow, postgres
	- High proficiency in Python, C++ and/or CUDA.
	- Passion for optimizations and efficient implementations.
	- High software engineering standards: desire to write clean, well-tested and well-structured code.
	- Track record of collaborations across teams, including requirement specifications and successful project delivery.
	- Excellent communication and presentation skills.
	- Curiosity to learn new things and push the boundary towards new approaches without fear of changing existing paradigms.
	- Improve ML Training efficiency to models train faster by scaling out and scaling up.
	- Develop strategies for training distributed models to accommodate model growth beyond state-of-the-art.
	- Design and implement a highly scalable inference pipelines for large scale evaluation.
	- Architect a data processing framework training data preparation and auto-labeling.
	- Collaborate with ML engineers to remove bottlenecks and improve turn-around times.
	- Build visualizations and dashboards to monitor cloud resource utilization.
	- At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ ***Good experience with applying Big Data tools (MapReduce, Hadoop, Hive and/or Pig, Spark) to large quantities of textual data***
+ skill set:
	- AIML - Software Engineer, Machine Learning Platform & Technologies
	- The Machine Learning Platform Team at Apple is looking for a Senior Engineer who has extensive experience in CI/CD, orchestration pipelines, build, release, and code management to manage critical parts of our development lifecycle. You will work with a dedicated team of engineers that will deliver the tooling and pipelines that enable consistent development of a high performance search stack, ensuring high quality delivery and enabling fast engineering turnaround.
	- 5+ years of experience in developing developer tooling, pipelines, automations and API
	- Thorough understanding of software development lifecycle, DevOps (build, continuous integration, deployment tools) and best practices.
	- Strong programming skills in Go, Python, or other language
	- Strong experience with Spinnaker and/or other delivery platforms
	- Strong experience with workflow platforms (Argo, Jenkins, or other)
	- Solid Kubernetes, AWS or other cloud experience
	- Strong communications and collaboration skills required
	- We design and build infrastructures to support features that empowers billions of Apple users. Our team processes trillions of links to find the best content to surface to users through search. We also analyze pages to extract critical features for indexing, ranking. We apply statistical analysis to improve link selection, freshness, retrieval rates, extraction quality, and many others. You’ll have the opportunity to work with large scale systems with trillions of rows and many petabytes of data and incredible complexity.
	- Work in a team of engineers to deliver core tooling and pipelines for builds, validation, and deployment
	- Build tooling and SCM integrations that ensures code consistency, test coverage, and reliability
	- Develop pipelines for continuous, incremental delivery to preproduction environments
	- Extend tooling that helps engineers test, maintain, and deploy solutions across multiple repos and languages
	- Collaborate with DevOps and Release Engineering to enhance release speed and reliability
+ skill set:
	- On the GPGPU-based heterogeneous system, research and implement distributed core modules such as tensorflow/pytorch/horovod for scenarios such as voice/image/NLP, and research multiple hybrid parallel strategies such as data parallelism/model parallelism/pipeline parallelism to improve the scalability of large-scale distributed training;
	- On the heterogeneous system based on GPGPU, research large-scale sparse model training framework for scenarios such as search/advertising/recommendation, support TB-level models, and support multi-machine efficient expansion;
	- Participate in MLPerf rankings to enhance the international influence of domestic GPU chips.
	- Computer, electronics, mathematics and related majors, more than 2 years of relevant work experience;
	- Proficiency in C++/Python programming;
	- Experience in developing Tensorflow/pytorch/horovod and other domestic frameworks;
	- Proficiency in NCCL, RDMA and other communication technologies is preferred;
	- Familiar with data parallel/model parallel/pipeline parallel, familiar with Deep Speed, etc. is preferred;
	- Familiar with CNN/ RNN such as Bert/GPT and other networks is preferred;
	- Familiarity with advertising/recommendation scene algorithms such as Wide&Deep/DLRM/DeepFM/DIEN is preferred, and familiarity with large-scale sparse model training frameworks such as AIBox/HugeCTR/XDL is preferred.
+ skill set:
	- Biren-based chip products (GPGPU), responsible for the implementation of customer applications, including the transplantation and acceleration of the customer's algorithm model on the Biren chip, as well as the performance tuning of the application pipeline;
	- Have certain insights and experience in AI models (CNN/RNN/Transformer, etc.) and pipelines corresponding to scenarios such as computer vision, speech recognition, search advertisement recommendation, and natural language processing;
	- Closely track industry trends and technology trends, and participate in the exploration of new technology solutions.
	- Bachelor degree or above in EE, CS or related majors, more than 5 years of work experience;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.);
	- Familiar with heterogeneous computing systems, and have at least participated in the model transplantation and optimization of one or more AI chips;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Bonus points: 1. For one or more mainstream deep learning frameworks (Tensorflow/PyTorch/MXNet, etc.) have developers; understanding of perspective, certain insights and experience in framework design or tuning;
	- Familiar with and like performance optimization, have certain development experience in CUDA/OpenCL/OpenMP, etc. or have certain insights and experience in performance distributed solutions such as MPI/CCL;
	- Familiar with GPGPU/AI ASIC/CPU micro-architecture, have certain insights and experience in joint optimization of software and hardware;
	- Experience in compiler, video codec, GPU virtualization, K8s cluster management and scheduling is preferred.
+ skill set:
	- AI Software Library Senior Development Engineer (C++).
	- Participate in the design, implementation, testing, performance optimization and problem fixing of deep learning acceleration library
	- Participate in the design, implementation, testing, performance optimization and problem fixing of AI compilation module
	- Participate in the design, development and continuous optimization of the basic framework for AI operator development
	- Participate in the interface design of AI operator docking with the upper framework, and the design and development of debugging tools
	- Proficient in C++11/14/17/20, proficient in STL
	- Proficient in commonly used data structures and algorithms, it is best to have your own implementation
	- Proficient in C++ multi-thread development under multi-platform, have a deep understanding of thread safety, anti-deadlock, thread-related design patterns and functional programming and the ability to get started immediately
	- Proficient in Python
	- Those who have studied the source code of the deep learning framework (Caffe/Tensorflow/PyTorch/MXNet) and have done relevant development are preferred
	- Must have excellent programming style
	- Familiar with deep learning framework (Caffe/Tensorflow/PyTorch/MXNet) or experience in using TensorRT is preferred
+ skill set:
	- Intelligent Platform Inference Engine R&D Engineer
	- Responsible for the design and implementation of the programming model of the intelligent processor reasoning optimization engine;
	- Responsible for the function realization and performance optimization of the intelligent processor reasoning optimization engine;
	- Responsible for the implementation of the intelligent processor reasoning optimization engine, including function realization, performance optimization and problem repair;
	- Participate in the design, development and continuous optimization of deep learning-related solutions;
	- Continue to carry out intelligent processor compilation optimization and integration work.
	- Bachelor degree or above, computer, mathematics, software engineering, automation, communication, microelectronics and other related majors, more than three years of relevant work experience;
	- Familiar with object-oriented programming methods, familiar with C++ programming development, compilation and debugging process in Linux environment, familiar with python, have a solid programming foundation, good programming style and working habits;
	- Familiar with deep learning algorithms, familiar with at least one deep learning programming framework (one of caffe/tensorflow/pytorch/mxnet), familiar with or have experience in using TensorRT is preferred;
	- Have good learning and understanding ability, strong logical thinking;
	- Possess a good teamwork spirit, strong sense of responsibility, do things practically and conscientiously, be able to actively complete relevant work, and have certain ability to resist pressure.
+ skill set:
	- Math Library Development Engineer
	- Participate in the hardware implementation and performance optimization of common neural network models, such as ResNet, BERT, EfficientNet, DLRM, etc., and propose architecture and microarchitecture improvements based on various neural network model architectures;
	- Participate in the development and performance optimization of AI operator libraries and other mathematical libraries;
	- Participate in the function and performance verification of the chip, and propose architecture and micro-architecture changes according to the verification results;
	- Participate in the development and improvement of various tools, which will be used to support the development and verification of AI operators and other mathematical libraries;
	- Cooperate with the hardware design department to support hardware verification;
	- Cooperate with the software department to support the development of AI framework/software/compiler.
	- Master degree or above, major in computer science, applied mathematics, etc.;
	- Proficient in using Python, familiar with algorithms and data structures;
	- Understand computer architecture, familiar with CPU/GPU architecture is preferred;
	- Understand the basic principles of neural networks, experience in the development and use of neural networks is preferred;
	- Good communication skills and teamwork skills.
+ Experience in ***PyTorch/TensorRT/Tensorflow*** programming is a big plus.
+ Experience in ***CUDA/cuDNN*** programming is a big plus
+ skill set:
	- Inference Engine Architect
	- On the heterogeneous system based on GPGPU, research and realize the reasoning engine for voice/image/NLP and other scenarios, and be responsible for related architecture design, performance optimization and business implementation;
	- On the heterogeneous system based on GPGPU, research and realize the prediction engine for search/advertising/recommendation and other scenarios, and be responsible for related architecture design, performance optimization and business implementation;
	- Responsible for the integration of Biren's self-developed reasoning/estimation engine and third-party frameworks such as TF or the reasoning/estimation engine of major Internet customers, to strengthen ecological construction and promote business implementation;
	- Participate in MLPerf rankings to enhance the international influence of domestic GPU chips.
	- Computer, electronics, mathematics and related majors, more than 5 years of relevant work experience;
	- Proficiency in C++/Python programming;
	- Experience in open source framework or engine architecture design such as Tensorflow/Pytorch/Paddle/TensorRT/TNN/MNN/OpenPPL/HugeCTR;
	- Familiar with AI compilation technology such as TVM/MLIR/XLA is preferred;
	- Familiar with CNN/ RNN and other network reasoning optimization / landing priority;
	- Familiarity with Wide&Deep/DeepFM/DIN and other advertising recommendation scene models is preferred, and familiarity with sparse model estimation engine architecture design and implementation is preferred.
+ skill set:
	- Deep Learning Framework Engineer
	- On the heterogeneous system based on GPGPU, research and implement the core module of the deep learning programming framework;
	- Algorithm research and framework implementation of large-scale cluster distributed training.
	- Computer, mathematics and related majors, more than 1 year of relevant work experience, doctor is preferred;
	- Master at least one language (C++/Python);
	- Mastering a mainstream deep learning framework (Tensorflow, PyTorch, MXNet, NNVM/TVM, etc.) is preferred;
	- Familiar with CUDA, OpenCL, OpenMP and other heterogeneous programming is preferred;
	- Familiarity with CNN, BERT, RNN and other networks is preferred; 6. Familiarity with compilation principles and LLVM is preferred.
+ skill set:
	- Deep Learning Framework Architect
	- On the GPGPU-based heterogeneous system, research and implement the core module of tensorflow;
	- According to the characteristics of the model and hardware, optimize the calculation graph, as well as optimize the memory and synchronization scheduling.
	- Computer, mathematics and related majors, more than 5 years of relevant work experience, doctorate is preferred (more than 3 years);
	- Familiar with C++/Python;
	- More than 2 years of experience in the development of mainstream deep learning frameworks, including Tensorflow, PyTorch, Mindspore, PaddlePaddle, Oneflow, MegEngine, MXNet, NNVM/TVM, etc.;
	- Proficiency in heterogeneous programming such as CUDA/OpenCL/Vulkan/OGL/DX Compute is preferred;
	- Familiar with GPU/GPGPU architecture;
	- Familiar with CNN, BERT, RNN and other networks is preferred;
	- Familiar with compilation principles and LLVM is preferred.
+ skill set:
	- Deep Learning Framework Engineer
	- Research and implement the core modules of the deep learning programming framework on the heterogeneous system based on GPGPU;
	- Algorithm research and framework implementation of large-scale cluster distributed training.
	- Computer, mathematics and related majors, with more than 1 year of relevant work experience, doctor is preferred;
	- Master at least one language (C++/Python);
	- Mastering a mainstream deep learning framework (Tensorflow, PyTorch, MXNet, NNVM/TVM, etc.) is preferred;
	- Familiar with CUDA, OpenCL, OpenMP and other heterogeneous programming is preferred;
	- Familiarity with CNN, BERT, RNN and other networks is preferred; 6. Familiarity with compilation principles and LLVM is preferred.
+ skill set:
	- Chip System Architecture Engineer
	- Responsible for driving the architectural design of our general computing/AI processors based on the latest general computing and AI accelerator applications, providing new technologies for products in new markets and existing markets
	- Participating in the development of modeling tools and infrastructure needed to facilitate exploratory design and performance studies;
	- Promote the development of new GPGPU/AI technologies to improve the user experience of various products and applications.
	- Bachelor degree or above in computer science/electronic engineering/applied mathematics, master/doctorate preferred;
	- More than 7 years of experience in general computing/AI hardware architecture, microarchitecture and design;
	- Have a deep understanding of modern general-purpose computing application programming interfaces, such as Cuda, OpenCL;
	- Have a deep understanding of deep learning technology, with artificial neural network framework;
	- Familiar with C/C++, Python, excellent software development skills;
	- Excellent teamwork skills, self-motivation and focus on results.
+ skill set:
	- Senior Software Library Engineer/Architect
	- Design and develop general basic mathematics library;
	- Discuss and implement possible optimization methods for specific hardware implementations;
	- Coordinate with other software engineers to provide solutions to problems and performance tuning in various general purpose platforms.
	- Master degree or above, more than 3 years of work experience;
	- Proficient in C/C++ programming, experience in Python programming is preferred;
	- Understand computer architecture;
	- Understand compute shader programming;
	- Experience in one or more common platforms or mathematical libraries is preferred: such as CuFFT, CuBlas, CuDNN, Tensorflow, TensorRT, etc.;
	- Have a strong ability to analyze and solve problems;
	- Possess strong communication skills, independent working ability and team driving ability, and can coordinate all relevant teams to promote the completion of the plan.
+ skill set:
	- AI/GPU Heterogeneous Platform Software Development Engineer
	- Parallel algorithm implementation and high-performance template library development in AI/GPU software framework;
	- Analyze and research open source machine learning and deep learning training engines, optimize and improve the implementation of operators and models;
	- Participate in the architecture design, key technology research and core code development of heterogeneous computing framework;
	- Development of test cases in general computing and heterogeneous computing programming models.
	- Computer, mathematics and related majors, master degree or above or more than 3 years of relevant work experience, doctor is preferred;
	- Proficient in C/C++ and open source project development tools under Linux; familiar with heterogeneous programming models such as CUDA/HIP/cupy;
	- Strong learning ability, familiar with compilation principles and architecture is preferred;
	- Familiarity with deep learning frameworks, accelerator cards, and typical deep learning networks is preferred.
+ skill set:
	- GPU application optimization engineer
	- Biren-based chip products (GPGPU), responsible for the transplantation and acceleration of deep learning algorithm models on Biren GPU, and performance tuning of application pipelines;
	- Have certain insights and experience in AI models (CNN/RNN/Transformer, etc.) corresponding to scenarios such as computer vision, speech recognition, search advertisement recommendation, and natural language processing;
	- Closely track industry trends and technology trends, and participate in the exploration of new technology solutions.
	- Master degree or above in EECS or related majors;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.);
	- Familiar with heterogeneous computing systems, experience in cuda or parallel computing is preferred;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Have a developer's understanding of one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.), and have certain insights and experience in framework design or tuning;
	- Familiar with GPGPU/AI ASIC/CPU micro-architecture, have certain insights and experience in joint optimization of software and hardware;
	- Experience in compiler, video codec, GPU virtualization, K8s cluster management and scheduling is preferred.
+ skill set:
	- Machine Learning Engineer - SIML, ISE
	- Would you like to help shape the next set of ML features of iPhone? Would you like to contribute to the field of generative AI? Want to contribute to transforming how people interact with AI technologies?
	- The System Intelligence and Machine Learning team is in charge of creating datasets that power many of Apple’s intelligent software. Our datasets range from very small targeted sets to Petabyte scale datasets. As a data scientist on our team you will be in charge of selecting the right assets, removing harmful and toxic assets and extracting insights from the datasets, assessing & reducing harmful biases, and maximizing fairness and inclusion of various ML features.
	- We are looking for an experienced Machine Learning Engineer who can help create and improve the datasets used in Generative AI through solid understanding and usage of ML and stats. You will be using Apple technologies to refine our datasets, remove toxicity and select the right images, videos or texts through active selection and model-in-the-loop methodologies. Focus areas range from text processing across many languages (toxic language detection and removal, identification of colloquial vs formal language) to image and video understanding, deduplication and processing.
	- Familiarity with a broad range of Machine Learning techniques and relevant statistical packages to engineer Machine Learning solutions end-to-end.
	- Experience in contributing to production code bases. Ability to rapidly prototype algorithmic ideas in notebook environments and translate them into production code.
	- Proficient in state-of-the-art ML techniques particularly in the field of Generative AI and Large Language Models (Transformer architecture, CLIP and various visual and text embedding models, GPT and BERT style language models).
	- Exceptional communication and presentation skills and the ability to explain difficult technical topics to everyone from data scientists to engineers to business partners.
	- Strong proficiency with Python (Scikit learn, Jupyter), PyTorch, SQL-based languages. Working proficiency with Git.
	- As a Machine Learning Engineer on the Data Team, you will be working to deepen our understanding of how various datasets can improve the quality of Apple’s ML models on a range of products. You will particularly help shape Apple’s Datasets that are used for generative AI by removing irrelevant or toxic assets, selecting the right assets by employing various asset selection algorithms, utilizing Apple proprietary ML models. For this, you will also use your stats and ML background to build models and algorithms that can select the right assets for ML experiences from a large pool of available assets. And you will work with our data engineers to put your models in data pipelines to run on large scale datasets.
	- In our team, you are expected to collaborate with other AIML product stakeholders and partners to understand needs, design Machine Learning models that help us better understand our data and automatically pick the right assets for ML training. Our Data Scientists actively evaluate and present the progress of their work. Your creative problem solving skills will be used daily.
	- Masters or Phd degree in Computer Science, Engineering; or equivalent practical experience. 
	- Strong analytical product intuition: able to understand the user experience and use data to guide the development of products.
	- 2+ years of experience in an Applied Scientist role, preferably in a technology company.
	- Ability to understand a technically complex product, and work with engineering leads and data engineers.
	- Proficiency in data science and analytics, including statistical data analysis and machine learning. Experience crafting, conducting, analyzing, and interpreting experiments and deep-dive investigations.
	- Ability to build relationships across multiple functions and establish strong partnerships.
+ Proficiency ***in building end-to-end ML pipeline from data ingestion to feature engineering to model training to deploying and scaling the model in production***.
+ Experience ***in building end-to-end ML pipeline from data ingestion to feature engineering to model training to deploying and scaling the model in production***.
+ skill set:
	- Bonus if you have prior data analysis experience (SQL, Python/Jupyter Notebooks, Tableau, Superset)
	- Bonus if you have prior experience exploring data formats and tools for big data systems (e.g. Parquet, Avro, Protobuf).
	- Bonus if you have prior experience working with data engineers (e.g. engineers who use tools such as ***Airflow, Luigi, Jenkins, MapReduce, Pig, Spark, Hadoop, Hive, HBase, Greenplum, Vertica***, etc.)
+ skill set:
	- 2+ years of experience in developing end to end machine learning pipelines on distributed systems.
	- You have experience in deploying machine learning solutions on marketplaces. Any previous experience in advertising technology in areas such as conversion modeling, automatic bidding, keyword/bid/budget recommendation, creative optimization, advertiser churn prediction, automatic targeting and auction design will be a plus.
	- You can drive multi-functional collaboration with product, data engineering, experimentation and machine learning operations.
	- You can actively participate in investigations into multiple streams of ads quality data, draw conclusions from data, and recommend actions.
	- You are comfortable with alternative experiment designs such as budget splitting, switch backs, interleaving in addition to traditional a/b testing.
	- You have extensive experience developing in Python.
	- You are familiar with databases, SQL, and scripting languages.
	- You have a practical understanding of some of the modern machine learning applications to rare events modeling, natural language processing, ranking, clustering and embedding generation.
	- You enjoy working closely with operational teams on deployment, monitoring, and management concerns.
+ skill set:
	- Senior Software Engineer, Autonomous Systems - SPG
	- Apple SPG (Special Projects Group) is seeking an experienced software engineer to work on developing and implementing high-quality software for autonomous systems. Our organization is engaged in conducting world-changing research that requires the development of novel algorithms that need to run in real-time.
	- Proficient in C++ and Python. Language-agile.
	- Experience with building production code.
	- Strong understanding of scientific and numerical programming in domains such as robotics, controls, computer graphics.
	- Familiarity with code developer workflows and tooling.
	- Strong interest in engineering problem solving.
	- Help design and build production-grade software to solve historically difficult problems in the field of autonomous systems under strong engineering constraints
	- To be experienced in designing and building production-grade software in C++
	- Have deep understanding of patterns (and anti-patterns) of architecture
	- Familiarity with scientific programming and numerical techniques
	- A minimum of a BS in Computer Science or related fields.
	- 10+ years of experience on working on production software in scientific and numerical domains.
+ skill set:
	- Perception System Engineer - SPG
	- As Perception System Engineer on a revolutionary Apple project, you will be working on an autonomous system built on state of the art sensing technologies and ground breaking machine learning algorithms. The Perception team provides sense capabilities such as detection, classification, tracking, and observed maps in complex environments using a range of sensing modalities. You will play a key role in measuring end-to-end system performance, identifying key issues and provide detailed feedback for performance improvement. You will engage cross functionally with a wider range of experts to build a robust and scalable triage and measurement system. You will use statistical modeling and develop expertise in Perception system performance trends, forecasting methodologies, and synthesize key findings for leadership reviews.
	- 5+ years of experience in testing, QA or algorithm development for Autonomous Perception systems
	- A deep understanding of perception functions its impact on motion planning
	- Knowledge of machine learning models and deep learning fundamentals
	- A background in statistical analysis, system-level triage of complex systems
	- Proficient in data analysis, scripting and automation using python
	- Familiarity with data products from optical sensors like lidar and camera is desired
	- You will be developing and maintaining a Perception performance measurement pipeline that provides continuous feedback to developers for performance improvement and debugging. The work involves significant cross functional interaction with system test engineers, model, and tooling developers.
	- Defining procedure and tooling requirements for a triage and test pipeline that identifies, classifies, and measures perception failure rates.
	- Engaging with relevant partners to ensure timely implementation and delivery. 
	- Synthesize failure rate data to derive meaningful trends and sensitivities, and track measured improvements and regressions over time. 
	- Create and own dashboards for leadership reviews and develop expertise in observed system performance. 
	- Root causing and failure analyses in partnership with deep learning model developers will be essential to be effective in this role. 
	- You will also have opportunities to develop statistical models to forecast full system performance using developer metrics, critical scenario testing, and past performance.
	- Masters degree in engineering, data science, statistics, or mathematics
	- 5+ years of relevant Industry experience in robotics or autonomous systems
+ skill set:
	- Computer Vision Architect
	- Apple is looking for a computer vision architect with exceptional technical and communication skills to contribute on high-impact projects that will enable game-changing future Apple products. The role requires deep knowledge of image processing and ability to define, analyze and optimize the architecture of video pipelines. Successful candidate must possess good understanding of both hardware and software solutions used in battery operated devices, including tradeoffs in optimal performance vs. power consumption.
	- Ability to evaluate existing and emerging imaging systems against use case requirements, including, understanding of hardware engines (CPU, GPU, Neural Network Accelerator, DSP, etc.) and how to choose the right one for a given application.
	- Understanding of difference between classes of algorithms and how to optimize their efficiency on available hardware engines (Computer Vision vs DeepLearning) as well as resource sharing and arbitration.
	- Understanding of the impact processing flow selection will have on overall system performance, power consumption and resource sharing by various algorithms.
	- Deep knowledge of sensors and camera technologies, including, image acquisition (sensor+lensing+ISP), rolling vs global, color vs monochrome, rectilinear vs equidistant and their impacts on algorithms, color management, performance optimization in low light conditions, etc.
	- Understanding of system resources supporting image collection and camera calibration in multi-camera systems and ability to define common system pre-processing blocks for camera streams.
	- Demonstrated proficiency in system architecture and performance analysis, including, input interface definition/optimization of image streams serving multiple algorithms, estimation of system compute requirements based on required performance.
	- Ability to define system-wide policies optimizing resources in multi-sensor streaming systems with memory constraints as well as estimation of system responsiveness based on latency analysis and system margins.
	- Ability to translate product goals to feature level architectures and the ability to derive for each feature the end-to-end requirements (with rationale) and decompose them down to module/component specific requirements.
	- Ability to work cross-functionally (Algorithms, Embedded, ML, Mechanical, Electrical, Optical, Human Factors, Vision science etc.) with stakeholders to define CV features, understand trades, sensitivities, risks and perform budgeting across full system.
	- Ability to effectively communicate and align with peers while navigating complex trade-spaces and the ability to summarize learnings, take-aways and recommendations in a succinct way to executive leadership.
	- Ideal candidate must have proven track record of having participated in the development of multiple consumer electronic products. Should have broad understanding of hardware and software technologies that are employed in low-power hand-held / wearable devices. Must be a self-starter who is highly-motivated and an organized thinker with excellent communication and presentation skills.
	- BE + 15 yrs of relevant industry experience.
	- Masters/PhD + 10yrs of relevant industry experience
+ skill set:
	- Robotics Software Engineer, Autonomous Systems - SPG
	- Apple SPG (Special Projects Group) is looking for talented robotics and software engineers to join our team to push the boundaries of autonomous planning algorithms (behavior, predictions, motion planning, and architecture).
	- Background in any of the following areas: behavior planning/decision-making, predictions, machine learning, motion planning (sampling, search based planning, optimization), estimation, control, and/or high-performance real-time algorithms.
	- Experience programming autonomous robots, modeling multi-agent systems, and developing algorithms for them.
	- Strong C++ and/or Python development skills.
	- Solid understanding of advanced algorithms and data structures.
	- 2+ years of professional or equivalent experience.
	- You must be hands-on, eager, curious and never satisfied with the status quo.
	- You must love learning and being challenged.
	- You will develop cutting edge robotics technology at the intersection of machine learning, AI, and classical robotics. This involves the design and implementation of algorithms that run on a robot in real-time in a safety critical application that involves autonomous interactions with the surrounding world in an uncontrolled environment. You will test and deploy your work in simulation and in the real world on state-of-the-art robotics hardware. You will contribute to the development of an ambitious and innovative projects as part of a dedicated team of world-class engineers.
	- M.Sc., or Ph.D. in computer science, engineering, or equivalent professional experience.
	- Designed one or more machine-learned approaches to solve a robotics problem.
	- Experience with cloud-based tools to automate experiments and analysis at scale.
	- Familiarity with real-time, multi-process, multi-threaded coding.
	- Comfort using the command line in Linux.
	- Experience with 3D geometric math.
	- Comfortable with collaboration tools for programming
+ skill set:
	- Data Collections Lead - SIML, ISE
	- Would you like to help shape the next set of ML features of iPhone? Would you like to contribute to the field of generative AI? Want to contribute to transforming how people interact with AI technologies? 
	- The System Intelligent and Machine Learning team is in charge of creating datasets that power many of Apple’s intelligent software. Our datasets range from very small targeted sets to Petabyte scale datasets. As part of the Data Collection team, you will be managing end-to-end data collection projects for a wide variety of features. Our data team is responsible for designing and building high quality datasets at scale. At the heart of machine learning, data defines how Apple features and products operate and what is the final user experience that will impact millions of our customers. This is an exciting time to join us: grow fast and have an impact on multiple key features on your first day at Apple!
	- Excellent project management, communication, interpersonal, analytical, and organizational skills
	- Passion for creating great products and understanding the challenges associated with building datasets for machine learning features; while addressing the challenges of inclusion, bias removal, and fairness.
	- Ability to define/design/develop data collection efforts that focus on the end-to-end user experience, including anticipating potential failure modes, edge cases, and anomalies
	- Capacity to multitask and manage several projects in parallel while meeting deadlines and providing access to clients and partners
	- Problem solving & critical thinking capacities, with an eye for innovation and continuous optimization (improving the diversity and quality of assets, reducing time to delivery and cost)
	- Ability to understand data needs and define concrete project deliverables
	- Our SIML Data team focuses on data acquisition, data science, annotation, data QA and robustness analysis. We utilize generative AI tools to help generate and evaluate data. Each year, we power dozens of features and work closely with ML teams across the entire company. Apple's commitment to deliver incredible experiences to a global and diverse set of users with a full respect of their privacy leads our team to explore innovative data collection processes.
	- In this position, you will be responsible for leading and managing data collection projects end-to-end and ensuring the quality of the data delivered to R&D. 
	- Connect with R&D teams to understand expectations and define specs of data collection efforts, with a constant focus on fairness and on potential biases
	- Co-define a data collection workflow with the partners, in accordance with Apple values
	- Coordinate the efforts of internal teams (privacy, legal, procurement, security) and own the administrative setup
	- Lead the data collection project, including working with internal and external teams
	- Define and dynamically adapt the data collection methodology to foster efficient quality analysis and annotation
	- Track quantities and quality delivered
	- A strong understanding of applied machine learning concepts is desired
	- Formal or informal experience working with generative AI tools (including personal projects) is desired
	- Experience in data operations supporting R&D work related to generative AI is a strong plus
	- Ability to maintain and develop relationships with multi-functional teams
+ skill set:
	- Generative AI Applied Researcher - SIML, ISE
	- Are you excited about Generative AI? We are looking for experts in this space to join our applied ML R&D team at Apple! You will be inventing and shipping the next generation of these core technologies with a focused team. Our purpose is to surprise and delight users and developers worldwide.
	- The team comprises domain experts in Computer Vision & Natural Language Processing (NLP) who contribute to a variety of shipping workflows you may already regularly use, including: Photos Search, Curation, Memories, Intelligent Auto-crop, Visual Captioning for Accessibility, Federated Learning on visual content, Real-time Classification & Saliency in Camera, Semantic Segmentation in Camera, and several on- device feature extractors across the system. Further, several of our projects are surfaced to third party developers through Vision & CoreML. Shipping APIs include image tagging, image similarity, saliency estimation and prints for transfer learning. The team collaborates extensively with various teams across Apple in bringing experiences to life across our devices, services and 1st/3rd party APIs. 
	- Selected references to our team’s work:
	- https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon
	- https://machinelearning.apple.com/research/on-device-scene-analysis
	- https://machinelearning.apple.com/research/panoptic-segmentation
	- 5+ years of industry experience with strong ML fundamentals
	- Hands-on experience with building Deep Learning applications
	- Proficiency in using ML toolkits, e.g., PyTorch
	- Strong analytical and problem solving skills
	- Strong programming skills in Python, C and C++
	- You're aware of the challenges associated to the transition of a prototype into a final  product
	- You're familiar with the challenges of developing algorithms that run efficiently on  resource constrained platforms
	- You've demonstrated leadership in both applied research and development
	- Excellent written and verbal communications skills, be comfortable presenting  research to large audiences, and have the ability to work hands-on in multi-functional teams
	- We are looking for a candidate with a proven track record in applied ML research. Responsibilities in the role will include training large scale multimodal (vision-language) models on distributed backends, deployment of compact neural architectures such as transformers efficiently on device, and learning adaptive policies that can be personalized to the user in a privacy preserving manner. Ensuring quality in the field, with an emphasis on fairness and model robustness would constitute an important part of the role. You will be interacting very closely with a variety of ML researchers, software engineers, hardware & design teams cross functionally. The primary responsibilities associated with this position range from algorithm design and implementation, ability to integrate research into production frameworks, and collaborating closely with product teams before and after feature launch.
	- M.S. or PhD in Electrical Engineering/Computer Science, or a related field (mathematics, physics or computer engineering), with a focus on computer vision and/or machine learning or comparable professional experience; or equivalent experience.
	- Familiarity with Multi-modal ML, Graph ML and/or Reinforcement Learning (RL) in a distributed large-scale training environment is desirable.
	- Experience in neural network deployment optimizations is desirable.
+ skill set:
	- Software Engineer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- Passionate about Spacial Computing
	- Passionate about working with hardware architectures
	- Great interest or background in Computer Vision and/or Machine Learning
	- Solid fundamentals in Linear Algebra
	- Strong proficiency in C/C++
	- Performance and optimization oriented
	- Self-motivated and great teammate
	- VPG (Vision Product Group) is the group that is responsible for many of the key algorithms for Apple Vision Pro. We are looking for versatile engineers who are passionate about building products for millions of customers around the world. You’ll be working on cutting-edge technology and develop algorithms that enable a high-quality user experience across a range of tentpole use cases and applications. As a part of our team, you will closely collaborate with HW engineers (cameras, silicon, electrical engineering, product design) and other technology development software teams (computer graphics, video engineering, data generation/annotation, drivers/OS). You can make a difference by researching and prototyping novel deformable object tracking algorithms beyond the state of the art and/or by optimizing the performance of real-time algorithms running on Apple silicon.
	- M.Sc. or B.Sc. degree in Computer Science or similar, alternatively a comparable industry career with a consistent track record of successful projects.
+ Familiarities with Knowledge Graph and Traversal Algorithms
+ skill set:
	- Machine Learning Engineer — NLP
	- Superior verbal and written communication and presentation skills, ability to convey rigorous mathematical concepts and considerations to non-experts
	- Thorough understanding of common machine/deep learning algorithms and practical experience in one or more of the following areas: prompt-based learning, reinforcement learning, BERT, GPT, T5, transformers, conversational models, large-scale NLP model training and fine-tuning
	- Working knowledge of distributed training and parallel computing on machine learning tools, such as AWS SageMaker
	- Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and Spark
	- Ability to implement data intensive pipelines and applications in a general programming language such as Python, Scala, Java or C++
	- Ability to comprehend and debug complex systems integrations spanning toolchains and teams
	- Ability to extract meaningful business insights from data and identify the stories behind the patterns
	- Creativity to engineer novel features and signals, and to push beyond current tools and approaches
	- Excellent verbal and written communication skills, in both Mandarin Chinese and English
	- Engage with others to find opportunities, understand requirements, and translate those requirements into technical solutions 
	- Design internal search and conversational system concerning business and sales topics
	- Design machine learning approach, applying tried-and-true techniques or developing custom algorithms as needed by the business problem 
	- Collaborate with data engineers and platform architects to implement robust production real-time and batch decisioning solutions 
	- Ensure operational and business metric health by monitoring production decision points
	- Investigate adversarial trends, identify behavior patterns, and respond with agile logic changes
	- Communicate results of analyses to business partners and executives 
	- Research new technologies and methods across data science, data engineering, and data visualization to improve the technical capabilities of the team
	- Ph.D. in Computer Science, Machine Learning, Statistics, Operations Research or related field; or 
	- Ph.D. in Math, Engineering, Economics, or hard science with data science fellowship; or 
	- M.S. in related field with 3+ years experience applying deep learning to real business problems
+ Deep technical knowledge in classic Computer Vision (pixel processing and geometry) or Machine Learning are a must. Bayesian filtering, Signal processing, Optics, Camera calibration and/or Mathematics expertise is desirable as well.
+ skill set:
	- Algorithm & Performance Engineer - SPG
	- The Apple Special Projects Group, working on autonomous systems, we are implementing highly complex algorithms. We are looking for a talented, dedicated and result oriented C++ Software Engineer to help improve, expand and further optimize our stack.
	- You will be part of a world class team with a highly diverse skillsets. Implementing complex numerical algorithms in a well designed, testable manner is as much part of an Algorithm & Performance Engineer’s day-to-day as optimizing cache coherency of existing implementations, applying SIMD optimization or reducing memory footprint of modules. You will be addressing a vast variety of challenges from implementing GPU kernels over deploying and optimizing machine-learned models all the way to architecting, implementing and testing a complex software stack.
	- 3+ years of professional software development experience.
	- Expert knowledge in Modern C++.
	- Experience in either ComputerVision, High Performance Computing or Numerical Algorithms.
	- Familiarity with SIMD, concurrency and/or GPU kernels.
	- Passion for optimizations and efficient implementations.
	- High software engineering standards: desire to write clean, well-tested and well-structured code.
	- Excellent communication and presentation skills.
	- Track record of collaborating across teams, gathering requirements and delivering results.
	- Efficient, correct, clean C++ implementation of complex numerical algorithms using efficient data structures.
	- Low-level optimization, for example using SIMD, concurrency, cache optimizations, GPU kernels
	- Design, implementation, testing and maintenance of a complex software stack.
	- Implement visualization tooling to enable insights into complex algorithms.
	- Collaborate with testing and verification teams to ensure correctness and reliability of our stack.
+ skill set:
	- AIML - Engineering Manager, Machine Learning Data Platform
	- The Machine Learning Data Platform group builds cloud-native systems that enable customers across all Apple to design, build and innovate with ML-driven product features rapidly and at scale. These systems provide cloud-native solutions for data exploration, data pre-processing, ETL, ML fine-tuning, and large-scale batch inference for ML models including LLMs. We are looking for an experienced leader who wants to bring their passion for infrastructure & distributed systems to build world-class data platforms/products at a very large scale across cloud environments.
	- 5+ years of experience building, influencing, and growing successful infrastructure teams that consist of senior software engineers and first-line managers.
	- Strong experience in strategizing, planning, and delivering very large-scale cloud-native distributed systems and data infrastructure.
	- Experience managing expectations and relationships with multiple collaborators and project teams across various functional areas.
	- Excellent communication and leadership skills. Ability to influence across the organization and customer leadership.
	- Demonstrated ability to partner with recruiting to grow technical teams.
	- Experience with building production Machine Learning inference pipelines is a plus.
	- Experience building GPU cost/performance observability systems and optimizing them for cost and speed is a plus
	- Our platform is built using a variety of systems and services, from bare metal to managed infrastructure services, and everything in between. We use existing and open-source systems when possible, but do not shy away from building new products ourselves. As the Engineering Leader, you will work closely with customers, and cross-functional teams and lead the planning, execution, and success of technical projects. You also take full ownership of the strategic direction of products and solutions and influence customers, executives, and tour teams. This role requires technical expertise, experience in driving strategy, influence across the leadership stack, growing and mentoring senior engineers and first-time managers, relationship management, and multi-functional coordination.
	- Work closely with customers, cross-functional teams and lead the planning, execution, and success of technical projects. 
	- Lead, mentor, and grow a team of senior software engineers and first-line managers
	- Foster a healthy and collaborative culture as well as a high-output group 
	- Strategize, plan, and execute technical and cross-functional projects and provide leadership in an innovative and fast-paced environment 
	- Measure and improve the reliability and security of data infrastructure systems
+ skill set:
	- State Estimation Engineer — World Representation / Localization
	- Apple is looking for passionate, talented, and results-oriented robotics / state-estimation engineers to join our team and work on exciting technologies for autonomous systems. In this position, you will have the opportunity to work with a cross-functional team on innovative hardware/software technologies, enabled by the systems you build.
	- Joining our team, you will get to work with a fantastic group of talented and dedicated engineers and researchers. We hope you’re excited about the values that drive us: 
		* Passion for the mission:  we’re here to make something great. We take on whatever work is right for the mission and strive for the best possible results. 
		* Humility:  the right answer is more important than being right. We search for solutions as a team and value clear-eyed feedback. 
		* Lean habits:  you can’t grow without limits. Time constraints and big goals encourage us to sharpen our focus and learn to make great decisions.
	- Experience developing and integrating state estimation algorithms for localization, calibration, mapping, or SLAM applications in robotic systems.
	- Experience developing algorithms which fuse many sensor modalities is a plus.
	- Very strong background in C++ development for Linux. Ability to understand and prototype in a python codebase is a plus.
	- Excellent communication: strong interpersonal, verbal and written skills.
	- Experience with machine learning and recent literature is a plus.
	- Develop, deploy, and scale both online and offline state-estimation algorithms for real-world robotic systems.
	- Build software, tools, and processes that accelerate your rate of experimentation, monitor performance, and inform what to try next.
	- Work with large quantities of sensor data on challenging real-world problems.
	- Collaborate with the team to deploy your work in a mission critical environment.
	- Masters or PhD in Computer Science, Robotics, Machine Learning, Engineering or equivalent professional experience.
	- The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ skill set:
	- AIML - Machine Learning Engineer - Machine Learning Platform Technology (MLPT)
	- Want to build the training platform that engineers rely on to develop next-generation Apple products and services?  As a machine learning engineer on our team, you will create software systems and algorithms to enable performant, scalable training for Apple's AI-driven experiences.  Join our team of highly skilled, impact-focused engineers!  This role also includes opportunities to open source your work and publish at top ML conferences.
	- Strong Python programming skills
	- Understanding of data structures, software design principles, and algorithms
	- Experience with deep learning frameworks, such as PyTorch, JAX, or TensorFlow
	- Experience developing model parallel and data parallel training solutions and other training optimizations
	- Experience building large-scale deep learning infrastructure or platforms for distributed model training
	- Experience with parallel training libraries such as torch.distributed, DeepSpeed, etc.
	- We're looking for strong machine learning engineers to help build next-generation tools for training deep learning models at scale. You'll be part of a small team of training technology experts, focusing on training speed and scalability. We're looking for candidates with polished coding skills as well as passion for machine learning and computational science. 
	- Design and develop components for our centralized, scalable ML platform. Help push the limits of existing solutions for large-scale training. Develop novel techniques to circumvent the limitations of these solutions. Deploy your techniques on high-impact tasks from our partners across the company building new Apple products and services. 
	- We encourage publishing novel work at top ML conferences such as MLSys or NeurIPS and releasing contributions as open source. 
	- In exchange, we offer a respectful work environment, flexible responsibilities, and access to world-class experts and growth opportunities—all at one of the best companies in the world.
	- PhD or Masters in the area of Computer Science or equivalent years of industry experience
+ skill set:
	- AIML - Machine Learning Engineer, Siri and Information Intelligence
	- The Siri information intelligence team is creating groundbreaking technology for artificial intelligence, machine learning and natural language processing! The features we create are redefining how hundreds of millions of people use their computers and mobile devices to search and find what they are looking for. Siri’s universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup. As part of this group, you will be doing large scale machine learning and deep learning to improve Query Understanding and Ranking of Siri Search and developing fundamental building blocks needed for Artificial Intelligence. This involves developing sophisticated machine learning models, using word embeddings and deep learning to understand the quality of matches, online learning to react quickly to change, natural language processing to understand queries, taking advantage of petabytes of data and signals from millions of users and combining information from multiple sources to provide the user with results that best satisfies their intent and information seeking needs.
	- 5+ years of experience in machine learning, information retrieval, indexing and retrieval
	- Mastery of two of following languages: Python, Go, Java, C++
	- Excellent knowledge and good practical skills in major machine learning algorithms
	- Excellent data analytical skills
	- Experience with large scale search and machine learning systems is highly desired
	- ***Experience with Hadoop, Hive, and/or Impala is a plus***
		* https://en.wikipedia.org/wiki/Apache_Impala
	- Good interpersonal skills
	- Good team player
	- Analyzing search suggestion ranking and relevance requirements, issues and opportunities
	- Understanding product requirements, translate them into modeling tasks and engineering tasks
	- Building machine learned models for search relevance, ranking and query understanding problems
	- Integrating search functions into Apple products, such as Siri, Spotlight, Safari, Messages, Lookup, etc.
	- Building end-to-end production system including query understanding, ranking and recommendation to power search
	- Utilizing Spark, Hadoop MapReduce, Hive, Impala to perform distributed data processing
+ skill set:
	- AIML - Software Engineer, Machine Learning Platform & Technologies
	- Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there’s no telling what you could accomplish.
	- The Machine Learning Platform Team at Apple is looking for a Senior Engineer who has extensive experience in CI/CD, orchestration pipelines, build, release, and code management to manage critical parts of our development lifecycle. You will work with a dedicated team of engineers that will deliver the tooling and pipelines that enable consistent development of a high performance search stack, ensuring high quality delivery and enabling fast engineering turnaround.
	- 5+ years of experience in developing developer tooling, pipelines, automations and API
	- Thorough understanding of software development lifecycle, DevOps (build, continuous integration, deployment tools) and best practices.
	- Strong programming skills in Go, Python, or other language
	- Strong experience with Spinnaker and/or other delivery platforms
	- Strong experience with workflow platforms (Argo, Jenkins, or other)
	- Solid Kubernetes, AWS or other cloud experience
	- Strong communications and collaboration skills required
	- We design and build infrastructures to support features that empowers billions of Apple users. Our team processes trillions of links to find the best content to surface to users through search. We also analyze pages to extract critical features for indexing, ranking. We apply statistical analysis to improve link selection, freshness, retrieval rates, extraction quality, and many others. You’ll have the opportunity to work with large scale systems with trillions of rows and many petabytes of data and incredible complexity.
	- Work in a team of engineers to deliver core tooling and pipelines for builds, validation, and deployment
	- Build tooling and SCM integrations that ensures code consistency, test coverage, and reliability
	- Develop pipelines for continuous, incremental delivery to preproduction environments
	- Extend tooling that helps engineers test, maintain, and deploy solutions across multiple repos and languages
	- Collaborate with DevOps and Release Engineering to enhance release speed and reliability
+ skill set:
	- AIML - Senior Data Infrastructure Software Engineer, Machine Learning Platform and Technology
	- The Data Infrastructure group within the AI/ML organization powers the analytics, experimentation and ML feature engineering that powers the Machine Learning technologies we all love in our Apple devices. Our mission is to provide cutting edge, reliable and easy to use infrastructure for ingesting, storing, processing and interacting with data while keeping Apple’s users’ data private and secure.
	- Are you a passionate about building scalable, reliable, maintainable infrastructure and solving data problems at scale? Come join us and be part of the Data Infrastructure journey.
	- 5 years of experience in software engineering with deep knowledge in computer science fundamentals.
	- Strong in data structures and algorithms. Must write good quality code with test cases and review PR's in fast faced environment.
	- Expert in one or more functional or object-oriented programming languages (Scala, Java)
	- Fluent in at least one scripting or systems programming language (Python, Bash and Go etc.)
	- ***Experience or knowledge in distributed data systems like Hadoop, Spark, Kafka or Flink.***
	- Experience or knowledge in public cloud is a big plus, preferably AWS.
	- Strong collaboration and communication (verbal and written) skills to work with diff
	- The role involves managing petabytes of data for machine learning applications and designing and implementing new frameworks to build scalable and efficient data processing workflows and machine learning pipelines. The successful candidate will be responsible for ensuring complete data lineage and legal workflow integration while optimizing performance and scalability. You will also be responsible for monitoring the performance of the system, optimizing it for cost and efficiency, and solving any issues that arise. This is an exciting opportunity to work on cutting-edge technology and collaborate with cross-functional teams to deliver high-quality software solutions. The ideal candidate should have a strong background in software development, experience with public cloud platforms, and familiarity with distributed databases.
	- ***Familiarity with distributed databases, such as DynamoDB, MongoDB, or Cassandra.***
	- ***Experience with containerization and orchestration technologies, such as Docker and Kubernetes.***
+ skill set:
	- AIML - Sr Product Manager, Siri Knowledge
	- Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Do you love taking on challenges that create a positive impact? Here, you could play a leading part in revolutionizing how people use their computers and mobile devices by empowering many ground-breaking intelligent experiences.
	- The Siri team is crafting innovative technology for virtual assistants, knowledge graph, and algorithmic search using machine learning, natural language processing, and artificial intelligence. We’re looking for someone to lead product management efforts for Search and Knowledge in Siri, Spotlight, and Safari. You would work with top tier engineering and product teams to serve over 1 billion customers, crafting the type of magnificent user experiences that Apple customers expect and love. As a PM for Siri Knowledge and Search, you would drive search relevance, machine learning, and user experience improvement projects to build and improve end-to-end knowledge products on all major Apple platforms including iPhone, CarPlay, macOS, AppleTV and HomePod.
	- 5+ years experience as a Product Manager, Software Engineer, Engineering Manager, or similar role leading cross-functional product or software engineering teams.
	- 2+ years experience bringing a product from concept to completion, ideally related to joining together sophisticated backend and front-end systems with the user experience in mind
	- Experience driving large cross-functional projects with multiple workstreams
	- Self-motivated and proactive, with demonstrated creative and critical thinking capabilities
	- Self-sufficient in analyzing and drawing conclusions about the quality and product opportunity from raw and refined product data
	- Ability to, directly and indirectly, lead large teams for success
	- Experience shipping machine-learned products
	- Lead roadmap planning and detail execution of Siri Knowledge and Search in Siri, Spotlight, and Safari
	- Lead cross-functional teams from the product ideation/creation to full-stack delivery to our customers
	- Keep teams focused on the right priorities to meet aggressive deadlines; clearly communicate project progress to leads and executive team.
	- Collaborate with engineering, product, and executive leadership to drive outstanding year over year product delivery across large product areas at Apple.
	- Analyze product metrics to measure performance and plan the evolution of search and knowledge for immediate impact and longer-term (5+ years)
	- Be the voice for our customers; do whatever it takes to deliver the highest quality experience to our customers
	- Lead machine learning best practices in Siri team
	- Collaborate with other Apple teams to promote cross-team technology sharing
+ skill set:
	- Machine Learning Engineer
	- Machine learning is a critical pillar of Jane Street's global business, and our ever-changing trading environment serves as a unique, rapid-feedback platform for ML experimentation. 
	- Our ML Engineers develop the infrastructure that makes all of that possible. The work is wide-ranging, including things like:
		* Developing libraries for automating ML workflows and experiment evaluation, and helping us evaluate and onboard existing open-source tools
		* Digging in to the internals of open-source ML tools to extend their capabilities and fix fundamental bugs
		* Optimizing our systems to match the needs of our trading systems, whether that be for efficient training or low-latency inference
		* Building tools to train and evaluate models in parallel over large datasets
	- And much more besides. As an ML Engineer, you’ll help drive the direction of an ML platform that is used daily by traders and researchers across every corner of the firm.
	- We’re looking for accomplished and disciplined software engineers with deep experience in machine learning techniques and the software systems that power them. A good candidate will be curious, with a wide-ranging understanding of the open-source ML ecosystem, and an interest in the latest advances in ML, in both academic and industrial contexts.
+ skill set:
	- Senior Video Processing Algorithm Engineer
	- As a Senior Video Processing Algorithm Engineer, you will explore opportunities to research and develop video process algorithms in order to both improve real-time video quality & performance, and add new features on Zoom video products. 
	- Work across our stack, developing software ranging from Web Server to business application layer for our distributed, cloud-hosted backend. You will work alongside fellow experts in the field, you will deliver happiness to our users, and grow your knowledge base each and every day.  
	- Conduct performance research evaluations on image/video processing algorithms 
	- Perform feasibility analysis and validation, develop corresponding demos, and cooperate with team members for feature deployment on various platforms
	- Develop and prototype innovative algorithms in Zoom’s video processing pipeline
	- Design new video features to tackle new and existing problems on Windows, macOS, IOS, Android and Linux systems
	- Collaborate with internal stakeholders across the business to drive the delivery of features, processes and happiness
	- PhD degree or Master degree with 4 years of working experiences in Computer Science, Electrical Engineering, or a related STEM field
	- Excellent C++/C and Python programming skills 
	- Strong experiences with libraries for deep learning, such as TensorFlow, PyTorch, Keras, Caffe, etc.
	- Solid knowledge of math, including linear algebra, numerical optimization, calculus, etc.
	- Hands-on experiences with video processing techniques (traditional method and deep learning method), such as image/video synthesis and generation, image enhancement, etc.
	- Hands-on experiences in ***deep learning (neural network, neural rendering, generative model, discriminative model, transfer learning, one-shot or few shot learning, Neural Architecture Search, etc.)***
	- Experience on deep learning structure (Transformer, CNN, RNN, etc.) optimization and acceleration
	- Ability to crystallize vague concepts into concise plans with clear documentation
	- Detail oriented, organized, ethical, responsible, and self-motivated
	- Strong communication skills and a desire to learn something new 
	- Basic understanding of Mandarin
+ skill set:
	- Video Processing Algorithm Engineer
	- Conduct research on image/video processing algorithms. Conduct performance evaluations on image/video processing algorithms. Leverage signal processing, machine learning and deep learning techniques to solve computer vision problems. Possess strong skills in the areas of development and real-time implementation of video processing system. Analyze factors that impact algorithm runtime on various platforms and come up with solutions for real-time implementation without sacrificing algorithm performance. Analyses at both algorithm level and coding level (x86/x64/Arm neon assembly optimization, data structure optimization, multiple thread, GPU acceleration, etc.) are conducted to achieve the goal. Develop and prototype innovative algorithms in Zoom’s video processing pipeline. Develop a deep understanding of Zoom’s video processing architecture and develop new features on top. Design new video features to tackle new and existing problems on Windows, macOS, IOS, Android and Linux systems. Perform feasibility analysis and validation, develop corresponding demos and cooperate with team members for feature deployment on various platforms. 
	- Master’s degree in Electrical/Computer Engineering, Computer Science, a related field, or foreign equivalent and 1 year of post-baccalaureate experience in job offered or related. Applicants must have 1 year of experience with the following (1) C++/C and Python programming skills to develop computer vision/video processing features; (2) deep learning frameworks including TensorFlow, PyTorch, Keras, and Caffe; (3) video/image processing including semantic segmentation, object detection, object tracking, and image enhancement (traditional method or deep learning method); (4) Convolutional Neural Networks or (CNN) structure optimization and acceleration; and (5) project development skills including build and release, quality assessment, third party SDKs integration. Telecommuting Permitted.
+ skill set:
	- Machine Learning Engineer
	- You will be part of a team whose focus is to solve cutting edge AI problems and deploying models that constantly advance the state-of-the-art. You will be working across various Natural Language Processing (NLP) areas like summarization, topic segmentation, language modeling, coreference resolution and other interesting challenges that are challenging at Zoom's scale.
	- A person in this role is expected to able to carry out independent research without much supervision, collaborate with other researchers on larger scale projects, and provide directions to junior engineers on their research/engineering tasks.
	- Build and scale Machine Learning (ML) services which enable Zoom’s products.
	- Research, build and deploy state-of-the-art Machine Learning models for Natural Language.  
	- Processing use cases – mostly in the conversational domain.
	- Take ML models from research all the way to production.
	- Integrate and support the ML services with product and operations teams.
	- PhD degree in Computer Science, Machine Learning, or related degree, or Masters with 3+ years of relevant experience
	- Strong expertise in NLP
	- Experience with one or more of the following: text summarization, natural language understanding, natural language generation, conversational AI, and multimodal AI modeling
	- Experience with deep learning modeling
	- Experience in machine learning toolkits (TensorFlow, PyTorch, Scikit, etc.)
	- Experience with large-scale data processing.
	- Strong coding skills in Python
	- Experience in distributed training and performance optimization on GPU’s
	- Experience working on multi-modal ML
	- Familiarity with large-scale data processing and distributed systems.
	- Microservice, Docker, Kubernetes, REST API, AWS
+ skill set:
	- Research Scientist / Research Engineer
	- MosaicML is a deep learning startup with a mission to make ML training more efficient for everyone through fundamental innovations in algorithms, systems, and platforms. We believe that large scale training should be available beyond the well-resourced companies, and bridging the gap between research and industry is core to our success. Our products will enable our customers to train the best neural networks efficiently as possible within given time, cost, or other resource constraints – and to do so with a great user experience.
	- Develop methods for efficient neural network training. You will survey ideas in the literature and develop ideas of your own to change the way neural networks are trained to improve efficiency. This involves exacting scientific inquiry, rigorous empirical analysis of large-scale experiments, and building high-quality research artifacts.
	- Systematize knowledge and adjudicate scientific truth. At MosaicML, we are focused on transforming scientific knowledge into practical efficiency improvements. To do so, we navigate the messy machine learning literature, determine what holds up under scrutiny, and use that knowledge to build better models.
	- Advance the frontier of deep learning. You will drive ambitious research projects that push the limits of existing technology and explore new approaches that go beyond the state of the art.
	- Love our customers. Our goal is to make our customers successful when they train large deep learning models. We seek to encode our scientific expertise in our tools for the benefit of our customers. We love our customers, and we expect you to love them too!
	- Experience training large models. We're looking to hire researcher scientists and research engineers who have experience training modern neural networks for computer vision, natural language processing, and multimodal settings. Ideally, you will have experience training at large scales (100M+ parameters, and ideally 1B+ parameters) and conducting multi-node training. 
	- Extensive experience with NLP for deep learning. We are looking to hire research scientists who specialize in natural language processing.
	- Experience with data preparation and data quality for deep learning. We are looking to hire research scientists who have experience cleaning and curating large-scale data corpora (1B-100B examples) and using those corpora for deep learning.
	- Experience with deep generative model evaluation and improvement. We are looking to hire research scientists with experience evaluating generative models and using those insights to improve the models. 
	- A PhD is NOT required for this role. We are open to hiring candidates with bachelor's and master's degrees and to new graduates. We are open to hiring candidates who are currently in "research engineer" roles at other companies.
	- Keeping up to date with the research literature and thinking beyond the current state of the art.
	- Developing and implementing methods that improve the efficiency and efficacy of deep learning.
	- Rigorously evaluating these methods and communicating the results of your findings.
	- Proficiency with the fundamentals of deep learning.
	- Proficient software engineering skills and proficiency with PyTorch.
	- Knowledge of the systems aspects of how neural networks train and the resources used in the process of doing so.
	- Research experience in deep learning.
	- US work authorization required
	- Salary Range: $145K - $295K
	- Also includes equity (stock options) and benefits
+ skill set:
	- Principal Software Engineer
	- MosaicML is hiring a Principal Software Engineer to play a key role in building our product and in developing our team of software engineers.
	- Serve as a technical lead, overseeing and supervising projects and engineers on the team.
	- Play a leading hands-on role in the design and implementation of ML infrastructure and cloud platform software technologies
	- Drive our technology vision and roadmap
	- Establish software development best practices, and lead by example in applying them
	- Develop our engineering organization and culture through hiring, mentoring, and feedback.
	- ***8+ years of hands-on programming experience with at least one modern language such as Python, Go, or C++***
	- 8+ years of experience contributing to the architecture and design of large scale distributed systems and/or ML systems and tools
	- Strong sense of software design and usability of ML systems
	- Experience applying software engineering methodologies and best practices including coding standards, code reviews, build processes, testing, and security.
	- Prior experience in developing public cloud services or open source ML software is an advantage.
	- We value candidates who are curious about all parts of the company's success and are willing to learn new skills and technologies along the way.
	- Salary Range: $200K - $300K
+ skill set:
	- (Senior) Julia machine learning specialist
	- You love applying mathematical models to real data? You are expert about how to automatically learn from data?
	- What we offer
		* you will be part of an inspiring team of Julia professionals
		* every Friday is time for knowledge sharing, SDGs or generally trying out new product ideas
		* an open, reflective, and caring environment where you can feel like among friends
		* fair salary
	- What you will do
		* understand the kind of machine learning the customer needs
		* communicate with the customer
		* implement the models
		* connect the machine learning part with given data sources and sinks
		* build dashboard solutions
		* present results to the customer
		* give workshops and trainings about Julia, data science and machine learning
		* use your Fridays to empower SDGs (Sustainable Development Goals) with Julia
	- What you bring
		* completed Bachelor or Master or PhD in data science, statistics, computer science, mathematics, physics or a comparable education
		* programming experience in Julia
		* knowledge about Databases and SQL
		* expertise in two fields of machine learning
		* generic strategies for problem solving
		* good soft skills
		* enthusiasm about Julia
+ skill set:
	- (Senior) Julia machine learning engineer
	- You love to see machine learning models in production? You care more about the infrastructure than about the actual model itself?
	- What you will do
		* understand the production demands of the customer
		* communicate with the customer
		* setup appropriate architectures (e.g. for DevOps & MLOps)
		* setup monitoring and alerting
		* automatize training and evaluation of machine learning models
		* present results to the customer
		* give workshops and trainings about Julia and MLOps
		* use your Fridays to empower SDGs (Sustainable Development Goals) with Julia
	- What you bring
		* completed Bachelor or Master or PhD in data science, statistics, computer science, mathematics, physics or a comparable education
		* experience with DevOps & MLOps
		* expertise with docker and kubernetes
		* cloud knowledge, including infrastructure-as-code
		* generic strategies for problem solving
		* good soft skills
		* enthusiasm about Julia
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.























Sets of skills for embedded machine learning engineers, VLSI machine learning engineers, embedded deep learning engineers, and VLSI deep learning engineers:
+ skill set:
	- 5+ years of experience building production software systems within large engineering projects for consumer products on mobile SoCs, specially iOS devices
	- Hands-on experience with at least one compiled language (C/C++/Objective-C, Swift, Go, Java, Rust, etc.), and multi-threaded applications
	- Familiarity with modern mobile development frameworks (e.g., Flutter, Xamarin, Swiftic) and tools (e.g., IoC/DI, analytics, A/B testing, CI-CD and build systems like Bit, Buck, Bazel)
		* https://en.wikipedia.org/wiki/List_of_build_automation_software
		* https://en.wikipedia.org/wiki/Buck_(software)
		* https://en.wikipedia.org/wiki/BitBake
		* https://bit.dev/
	- Experience with profiling and tracing tools
	- Experience with Model Compression techniques (Quantization, Pruning, Distillation)
	- Experience with PyTorch Mobile, Tensorflow Lite or other similar Edge Inference frameworks
	- Experience with techniques to offload compute to GPU, DSP etc.
	- Experience developing Machine Learning models, especially for resource constrained computing environments
+ skill set:
	- Responsible for or assist in the definition of next-generation GPGPU chips and the planning of related software and hardware products. Collect key appeals through architecture research, market research, competitive product analysis, customer interviews, etc., plan chips and hardware products and be responsible for product competitiveness;
	- Lead or participate in the definition of chip specifications, participate in the whole process management of chip architecture design, chip mass production, chip enablement and product optimization; and be responsible for defining various hardware products to meet customer scenario needs;
	- Familiar with various GPGPU and AI chip architectures, and have a deep understanding of deep learning applications (such as CV/NLP/ASR/RecSys, etc.);
	- Familiar with mainstream artificial intelligence frameworks in the industry, and understand the requirements for high-performance GPGPU deployment in data centers;
+ skill set:
	- Perception System Engineer - SPG
	- As Perception System Engineer on a revolutionary Apple project, you will be working on an autonomous system built on state of the art sensing technologies and ground breaking machine learning algorithms. The Perception team provides sense capabilities such as detection, classification, tracking, and observed maps in complex environments using a range of sensing modalities. You will play a key role in measuring end-to-end system performance, identifying key issues and provide detailed feedback for performance improvement. You will engage cross functionally with a wider range of experts to build a robust and scalable triage and measurement system. You will use statistical modeling and develop expertise in Perception system performance trends, forecasting methodologies, and synthesize key findings for leadership reviews.
	- 5+ years of experience in testing, QA or algorithm development for Autonomous Perception systems
	- A deep understanding of perception functions its impact on motion planning
	- Knowledge of machine learning models and deep learning fundamentals
	- A background in statistical analysis, system-level triage of complex systems
	- Proficient in data analysis, scripting and automation using python
	- Familiarity with data products from optical sensors like lidar and camera is desired
	- You will be developing and maintaining a Perception performance measurement pipeline that provides continuous feedback to developers for performance improvement and debugging. The work involves significant cross functional interaction with system test engineers, model, and tooling developers.
	- Defining procedure and tooling requirements for a triage and test pipeline that identifies, classifies, and measures perception failure rates.
	- Engaging with relevant partners to ensure timely implementation and delivery. 
	- Synthesize failure rate data to derive meaningful trends and sensitivities, and track measured improvements and regressions over time. 
	- Create and own dashboards for leadership reviews and develop expertise in observed system performance. 
	- Root causing and failure analyses in partnership with deep learning model developers will be essential to be effective in this role. 
	- You will also have opportunities to develop statistical models to forecast full system performance using developer metrics, critical scenario testing, and past performance.
	- Masters degree in engineering, data science, statistics, or mathematics
	- 5+ years of relevant Industry experience in robotics or autonomous systems
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.














##	Applied Machine Learning, Applied ML



For applications of machine learning, or ML, in the following fields, see the *Markdown* document for [](bio-biochem-biotech-pharma.md)
+ bioinformatics
+ bio design automation, BDA
+ bio manufacturing automation
+ biology
+ biochemistry
+ biotechnology
+ medicinal chemistry
+ pharmacy
+ pharmaceutical science



For applications of machine learning, or ML, in finance, see the *Markdown* document for [Financial Engineering, Computational Finance, and Financial Analytics](financial-engr-n-finance-x.md)



Skill sets for generic applied machine learning, applied ML
+ counter AI engineering
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Knowledge of Deep Learning (CNN, RNN, ANN) using TensorFlow.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 1 year of building Business Data Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies (e.g  Alation, Informatica or custom portals).
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of experience designing and building data models to support large scale BI, Analytics and AI solutions for Big Data.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
+ skill set:
	- Director of Applied Machine Learning Experiences
	- The Machine Learning team is building a new platform called Sparta and this platform will be used to build assistive intelligence experiences (AIEs).  These AIEs enable Splunk customers to accomplish their tasks while using real-time user feedback to build self-tuning cloud services. The focus of the selected candidate will be to manage the AIE portfolio and automate simple tasks and processes by harnessing the power of Big Data, cloud, and data science to aid in actionable decision-making and system remediation.
	- Create the capability to make intelligent recommendations, detect anomalies, and help users prioritize events and alerts.
	- Create libraries, services, and SDK to accelerate AIE development.
	- Lead a team of engineers and researchers that collaborate with product and applied research teams to build ML features into a wide variety of products across Splunk. 
	- Recruit, mentor, and grow world class engineers and managers
	- Partner with cross functional team members to develop and maintain a well-defined roadmap, while balancing technological excellence.
	- Manage the AIE portfolio (the assistive intelligence experiences)
	- Help define technical direction and architecture with engineering team members.
	- Facilitate coordination of multiple scrum teams to successfully deliver committed feature sets.
	- Drive practices in training and development, and drive efficiencies across multiple feature teams.
	- Build a culture of continuous learning, growth, and sharing of technological insights.
	- 10+ years of hands-on technical experience.
	- 4+ years of direct management experience of highly technical managers and engineers.
	- Bachelor’s degree in Computer Science or another quantitative field. We will consider equivalent practical experience
	- Experience in delivering multiple complex technical projects within an Agile environment.
	- Familiar with machine learning and data science workflows.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
	- Proficiency with backend systems built using microservices, containerized infrastructure, and modern continuous delivery practices.
	- Demonstrated ability to build a culture of team building and people management.
	- Demonstrated ability to reach stretch goals in a fast-paced environment
	- Outstanding written and verbal communication skills.
+ skill set:
	- Director of Engineering - Machine Learning
	- Recruit, mentor, and grow world class engineers and managers.
	- Partner with cross functional team members to develop and maintain a well-defined roadmap, while balancing technological excellence.
	- Help define technical direction and architecture with engineering team members.
	- Facilitate coordination of multiple scrum teams to successfully deliver committed feature sets.
	- Drive practices in training and development, and drive efficiencies across multiple feature teams.
	- Build a culture of continuous learning, growth, and sharing of technological insights.
	- 10+ years of hands-on technical experience.
	- 4+ years of direct management experience of highly technical managers and engineers.
	- Bachelor’s degree in Computer Science or another quantitative field. We will consider equivalent practical experience.
	- Experience in delivering multiple complex technical projects within an Agile environment.
	- Familiar with machine learning and data science workflows.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
	- Proficiency with backend systems built using microservices, containerized infrastructure, and modern continuous delivery practices.
	- Demonstrated ability to build a culture of team building and people management.
	- Demonstrated ability to reach stretch goals in a fast-paced environment.
	- Outstanding written and verbal communication skills.
+ skill set:
	- Imagimob is a fast-growing, high-tech startup with an exciting future ahead. We are currently developing our next generation hybrid AI platform that allows for advanced motion detection for smaller Internet-of-things-articles, of virtually every kind. For example, the technology is today being used in projects ranging from the automotive and manufacturing industry to the health sector.
	- We are looking for a Machine Learning / AI Application Engineer to join our development team in Stockholm. Do you have excellent programming skills and are interested in working in the frontline of artificial intelligence? Then this position might be something for you.
	- Working with us you will get the opportunity to become part of our cross functional team with creative and innovative software engineers and AI researchers building the next generation AI beyond Deep Learning. Since we are still in a startup phase, you will also be able to develop in the areas you find the most interesting.
	- Has excellent programming skills in one or several languages, preferably in C, C# or Python
	- Experience with a deep learning framework (e.g. tensorflow, keras, Torch, caffe)
	- University degree or equivalent experience in computer science, electrical engineering, engineering physics or similar
	- A passion for Artificial Intelligence and Machine Learning technologies
	- Extreme ownership and go-get attitude
	- Has experience from programming on embedded platforms
	- Good knowledge in signal processing, statistics and its practical applications
	- Experience from Artificial Intelligence and Machine Learning technologies
	- And if this is not enough, you will get the chance to change history and shape the future of humanity...
	- The opportunity to be part of building the next generation AI beyond deep learning
	- Being part of an excellent international engineering team with highly motivated individuals striving for a common goal
	- A chance to get to solve real world problems using AI
	- A prestigeless and an open minded company culture
	- Short decision paths, we love getting things done
+ skill set:
	- 2+ years of work experience developing and deploying production-quality code
	- Foundational knowledge of commonly used machine learning techniques, such as cluster analysis, classification methods, and linear and nonlinear regression modeling
	- Experience developing applications using Natural Language Processing techniques.
	- Experience working with cross-functional teams in a dynamic environment
	- Hands-on experience building deep learning models on text corpora, preferably using PyTorch and Tensor Flow
	- Experience building machine learning models in the healthcare domain
	- Experience using AWS infrastructure and tools for machine learning
	- Experience with other back-end software engineering frameworks
	- [Talkspace](https://www.talkspace.com/)
+ skill set:
	- Data/Model Validation Engineer
	- We are looking for someone passionate about learning how machine learning systems are developed to assist with validating and processing training data to evolve our state of the art systems.
	- Engage with software engineers on the Perception team to identify and collect training data to evolve our machine learning systems.
	- Working with engineers on the Perception team, train new machine learning models and perform analysis to quantify how they perform based on changes to the training data sets.
	- Provide feedback on tools and processes for efficient workflow of training data creation and validation.
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ skill set:
	- You've got a Master's degree in statistics, econometrics, mathematics, or deep learning architectures including convolutional, recurrent, autoencoders, GAN's, and ResNets
	- You're a coding wizard with Python, C# (.NET), Scala, MxNet, CNTK, R, H2O, TensorFlow, PyTorch, cuDNN, NumPy, and SciPy
+ skill set:
	- At least 4 years' experience in deep learning, machine learning or artificial intelligence applications like virtual agent, robotic process automation and video/image/text analytics
	- Minimum of 2 years' experience in AI/ML/RPA functional expertise with developing use cases and building/leading Proofs of Concept
	- At least 2 years architecting AI Pipelines orchestrating multiple analytics engines
+ skill set:
	- Minimum 5 years of developing machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other.
	- Strong quantitative and analytical skills with minimum 3 years of experience with data science tools, including Python, R, Scala, Julia, or SAS
	- Ability to technically lead data science projects
	- Deadline-driven, organized and able to multi-task
	- Familiarity with using cloud services (AWS, Google, Azure) or Big Data tools (Hadoop, Hive, Spark) in data science solutions
+ skill set:
	- Proven experience with caching, queuing, RPC frameworks and other building blocks of a large scale distributed systems.
	- Experience with NoSQL AWS data stores like DynamoDB, CloudSearch or their open source equivalents like Cassandra, HBase, Solr or ElasticSearch
	- Experience with React or other modern javascript frameworks.
	- Experience with MySQL, Redis, Memcache and related web-backend technologies.
	- Experience with data pipelines (Kafka, AWS Kinesis, AWS Data Pipeline)
	- Experience building web applications, widgets, or interactive experiences.
+ skill set:
	- Cortex is a team of software engineers and machine learning scientist to developing state-of-the-art machine learning capabilities to refine and transform our products.
+ skill set:
	- solid understanding and experience using fundamental data fusion and tracking techniques, such as:
		* Kalman filtering
		* batch processing
		* multiple-hypothesis data association
		* multiple dynamic models
	- experience fusing across multiple real-world sources whose data may exhibit characterstics, such as:
		* sub-dimensioned data
		* time-late data
		* biased data
		* negative data
+ skill set:
	- scikit-learn
	- TensorFlow
	- PyTorch
	- MXNet
	- neural network architectures
		* CNNs
		* RNNs
		* autoencoders
		* generative models
	- other machine learning frameworks
	- experience deploying decision policy algorithms:
		* MDPs
		* RL algorithms
		* tree search
		* rules engines
		* path planning
+ knowledge of state estimation and data fusion algorithms
+ skill set:
	- ***LightGBM***, light gradient-boosting machine
	- ***compare bagging versus boosting***
	- usage of ***KDD data set***
	- ***MLP***
	- ***stacking***
+ skill set:
	- deep learning frameworks:
		* TensorFlow
		* PyTorch
		* PaddlePaddle
+ Expert in prototyping traditional ML (GBMs, scikit, etc.) and AI frameworks (keras, tensorflow, mxnet, pytorch, etc.) for a variety of applications
+ ***Experience with one of the ML platforms: Python / scikit-learn, Spark, vowpal wabbit, etc***
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
	- BS, MS, or PhD in Computer Science, Engineering, Statistics or a related technical field
	- Love of the Quora product
+ skill set:
	- BS, MS or PhD in Computer Science, Machine Learning, NLP or a related technical field
	- 5+ years of industry experience preferred
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
	- Love of the Quora product
+ ***Experience building shallow or deep learning models (GBDT, CNN, RNN, LSTM), toolkits e.g. OpenCV, Matlab, RStudio, Weka, MLLib and frameworks PyTorch, TensorFlow, CNTK***
+ ***Expertise in multivariate analysis, graphical models, Bayesian hierarchical modelling, Markov chain Monte Carlo (MCMC), mixture models, stochastic processes, generalized linear models (GLMs), dimensionality reduction (PCA/CCA/MDS/tSNE) and other machine learning techniques***
+ ***Familiar with one or more machine learning, statistical modeling tools such as R, Matlab, scikit learn and deep learning frameworks, such as tensorflow, keras, caffe, torch.***
+ Knowledge in machine learning framework - Tensorflow, Caffe, Torch or Theano
+ Spark, Kafka
	- Spark, for large-scale data science and applied machine learning
	- Kafka, distributed stream-processing platform
+ skill set:
	- Apache Beam or Google Dataflow
		* https://beam.apache.org/
		* "Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream (continuous) processing.[2] Beam Pipelines are defined using one of the provided SDKs and executed in one of the Beam’s supported runners (distributed processing back-ends) including Apache Flink, Apache Samza, Apache Spark, and Google Cloud Dataflow."
			+ https://en.wikipedia.org/wiki/Apache_Beam
	- MapReduce frameworks
		* Hadoop
		* Spark, Apache Spark
			+ https://en.wikipedia.org/wiki/Apache_Spark
			+ Spark MLlib, MLlib Machine Learning Library
		* BigQuery, BigQuery SQL, Google BigQuery
	- Bayesian methods, MCMC, Gibbs sampling
	- significance testing
	- active learning methods
		* multiarm-bandit
		* active Thompson sampling
		* causal inference using instrumental variables and other forms of multi-factor attribution methods
		* sequential testing methods
		* PyMC3
	- real-time metrics tracking systems
+ skill set:
	- NVIDIA TensorRT
		* https://developer.nvidia.com/tensorrt
			+ NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.
		* https://github.com/NVIDIA/TensorRT
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




























###	Computer Vision


####	Notes about Computer Vision

Topics in computer vision:
+ 3-D computer vision
	- 3D pose estimation
	- 3D scene modeling
+ based on:
	- machine learning
		* deep learning
+ event detection
+ feature extraction
+ ***frameworks, tools, and libraries***
	- ***OpenCV***
+ geometric computer vision
+ image restoration
+ image segmentation
+ image understanding systems, IUS
+ indexing
+ learning
+ motion estimation, motion analysis
	- egomotion, visual odometry
	- optical flow, optic flow
	- tracking, video tracking
+ object categorization
+ object recognition, or object classification
	- object detection
	- object identification
	- specialized tasks:
		* 2-D code reading
		* content-based image retrieval
		* facial recognition
		* object character recognition, OCR
		* pose estimation
		* shape recognition technology, SRT
+ scene reconstruction
+ system methods:
	- image acquistion
	- pre-processing
	- feature extraction
	- detection / segmentation
	- high-level processing
		* verification of model-based and application-specific assumptions
		* estimation of application-specific parameters
			+ object pose
			+ object size
		* image recognition
		* image registration
	- decision making
+ template finding
+ video tracking
+ visual prototyping
+ visual servoing
	- vision-based robot control, VS







Applications of machine learning -based computer vision:
+ automatic inspection in post-manufacturing quality control/assurance
+ autonomous vehicles
+ assisting humans in identification tasks, such as species identification system of plants and animals
+ event detection
+ human-computer interaction, HCI
+ medicine
+ modeling objects or environments
+ navigation
+ organization of information
+ process control
+ tactile feedback
+ tracking surfaces or planes in 3-D coordinates
	- enables augmented reality experiences




Machine learning -based frameworks for computer vision:
+ HALO AI










####	Skill Sets about Computer Vision


Skill sets for computer vision:
+ knowledge of CUDA / OpenCL / OpenCV
+ skill set:
	- Design and implementation of state of the art monocular computer vision algorithms
	- Solve problems involving odometry, landmark detection, structure from motion and segmentation in large scale outdoor environments
	- Integrate vision based algorithms into our probabilistic fusion framework
	- Help in identifying core requirements for camera sensors
	- Code development in C++/Python
	- Work with real data on our self driving car
	- Masters or PhD Computer Science, Electrical Engineering or both.
	- Deep Experience in SfM, VO, and classical computer vision algorithms
	- Expert knowledge in computational geometry
	- Experience in machine learning, feature detection and classification
	- Experience with open source computer vision and linear algebra frameworks
	- A solid background in statistics, probability and linear algebra
	- Experience with real world datasets
	- Experience with real time algorithm implementation
	- Ability to work independently without direct supervision
	- Experience with CV algorithm packages (Eigen, OpenCV, etc.)
	- Knowledge of Deep learning techniques applied to CV
	- Experience in Linux based environments
	- Experience in SLAM and/or motion planning
	- Experience with CUDA, OpenCL or other GPU frameworks
	- Experience with automotive systems or UAV systems
	- Ability to lead a small technical team that balances research and application
+ skill set:
	- Strong knowledge of the state-of-the-art in computer vision and machine learning algorithms with a solid understanding of OpenCV
	- Experience working with point cloud processing and Point Cloud Library (PCL)
+ skill set:
	- Develop pipeline for data tagging, labelling and munging to be consumed for training of ML models for vision based tasks.
	- Architect and train machine learning models for object detection and tracking
	- Build testing environment to test the model and simulate edge-case performance scenarios
	- Experience with at-least one of Tensorflow/Caffe/Theano/Torch
+ skill set:
	- skills and knowledge of topics, such as:
		* object recognition
		* semantic segmentation
		* human recognition
		* SLAM, simultaneous localization and mapping.
			+ 3-D SLAM, or localization
		* 6-D object pose estimation algorithm
			+ human pose estimation
		* ray tracing
		* physically-based rendering
		* efficient 3-D map representation
	- performance tuning for robots in real environments
	- performance improvement of machine learning model using simulation
	- robot system development, using ROS 1, ROS 2
	- cloud-based distributed learning experience
	- transfer learning from simulation to real machine
	- accelerating algorithms with OpenCL and CUDA
	- sensor simulation:
		* camera
		* LIDAR
		* RBG-D sensors
+ skill set:
	- generative AI models
	- image/layout generation space
	- develop untried approaches
	- analyze data sources
	- generate, collect, and prepare data for commercial licensing in mind
	- experiment with large-scale cloud infrastructure with high-end hardware
	- set up data augmentation pipelines and perform hyperparameter searches to build production-ready AI solutions
	- CNNs, transforms, diffusion models (stable diffusion)
	- deep learning frameworks
		* TensorFlow
		* Keras
		* Theano
		* MXNet
	- Python 3
	- NumPy
	- OpenCV
	- Docker, Kubernetes
+ skill set:
	- OpenCV
	- Point Cloud Library, PCL
		* open-source library of algorithms for point cloud processing tasks, 3D geometry processing, 3-D computer vision
		* https://pointclouds.org
	- TensorRT, MLIR, TVM, XLA
	- C++1x
	- ML SW stack, machine learning software stack, cuDNN, cuBLAS
	- SIMD programming 
		* avx2
		* neon














###	Natural Language Processing, NLP


####	Notes about Natural Language Processing, NLP



Topics for natural language processing, NLP:
+ word embeddings/vectors:
	- GLoVe
	- fastText
	- word2vec
+ contextualized embeddings
	- ELMo
	- CoVe
+ encoder-decoder models
	- seq2seq
	- vanilla transformers
+ tranformer language models
	- GPT-3
	- GPT-2
	- BERT
	- XLnet
+ named entity recognition
+ POS tagging
+ parsing
+ sentiment analysis
+ clustering
+ text prediction





Libraries for natural language processing, NLP:
+ spaCy, text processing
+ Kaldi, speech recognition/processing and signal processing




Companies involved in natural language processing, NLP:
+ https://emerj.com/ai-sector-overviews/machine-translation-14-current-applications-and-services/
+ https://localizejs.com/articles/types-of-machine-translation/





####	Skills for Natural Language Processing, NLP



Skills for natural language processing, NLP:
+ Experience with Natural Language Processing topics, such as:
	- Topic Modeling
	- Document Classification
	- Document Summarization
	- Sentiment Analysis
+ skill set:
	- experience with ML deployment frameworks
	- work with highly imbalanced data sets to improve detection performance of our existing NLP models and algorithms
	- work closely with product and platform teams, participate in design reviews, and demo your work
	- apply the latest cutting-edge advancements in AI/ML research to our current solutions in a scalable manner for online detection
	- take full ownership of ML models, including:
		* collecting training data
		* evaluating and deploying models to production with ongoing quality monitoring
+ skill set:
	- build and operate high volume distributed systems
	- design and build systems in a microservice-based architecture
	- experience with ML deployment frameworks
	- design, implement, and deploy NLP models
	- experience working with high growth venture-backed start-ups
+ hybrid approach to NLU combines symbolic human-like comprehension and machine learning to transform language-intensive processes into practical knowledge, providing the insight required to improve decision making throughout organizations
+ skill set:
	- Computational Linguist
	- As a Computational Linguist you will join project teams to build top-notch NLP/NLU solutions for our Customers and Partners worldwide, ensuring that all issues are dealt with in the most effective and satisfactory way. Not only you have a firm grasp of general linguistics (semantics, syntax, morphology) but you have also experience with artificial intelligence, machine learning, natural language processing and computational linguistics.
	- Identify logical patterns underlying the language, write semantic rules on Expert.ai’s proprietary IDE to meet Customers’ expectations
	- Perform tests and validations to verify the effectiveness of the implemented solution
	- Contribute to the continuous improvement of the core technology
	- Identify suitable solutions for clients in the field of Natural Language Processing
	- Work with Project Managers, Software Engineers and other Knowledge Engineers on Customer’s projects
	- Graduated in Computational Linguist or Languages
	- Experienced in coding with Python and/or C++: write data processing scripts to transform or extract knowledge from unstructured data, including hands-on experience with regular expressions
	- Analytical person with excellent written and verbal communication skills in a technical context
	- Independent worker with the ability to effectively operate with flexibility in a fast paced, constantly evolving team environment
	- Team player with exceptional interpersonal and solution-oriented attitude
	- Fluent English, both oral and written (the knowledge of other languages will be considered a plus)
+ skill set:
	- Knowledge Engineer Intern
	- If you are completing your studies in Computational Linguistics or Languages and you would like to be part of a growing company that works in an international environment, you can send us your application: we are looking for a Knowledge Engineer Intern for our Paris office.
	- As knowledge Engineer intern you will support the Professional Services team that works on Artificial Intelligence projects aimed at helping big Companies and Public Institutions in solving the daily practical issues linked to the digital transformation and the processes’ automation. During your internship you will be trained in COGITO technology and you will develop your skills in the field of Natural Language Processing / Natural Language Understanding; you will learn the use cases of NLP for Companies and carry out a first experience as a consultant. Finally, you will contribute to the continuous development of our core technology from the linguistic point of view and you will test and verify the effectiveness of the platform.
	- Degree in Languages, Literature, Linguistics or Computational Linguistics;
	- Excellent knowledge of English and another language;
	- Good knowledge of linguistics will be necessary for the rapid handling of our Cogito technology;
	- Basic knowledge of programming languages like Java and Python will be considered a plus;
	- Good general culture knowledge;
	- Attention to the quality of work and results;
	- Team work skills;
	- Ability to analyze and structure information;
	- The knowledge of the Arabic language will be considered as a plus.
+ skill set:
	- NLP/ML Lead at SciSpace (Formerly Typeset)
	- As one of the first engineers in Content Intelligence at Typeset, you will be required to design infrastructure and models for helping our users discover their research papers better. At Typeset, We are on a mission to help researchers connect the dots faster and accelerate scientific innovation. We want to use Machine Learning to provide an assisted discovery experience to our users.
	- Your job responsibilities would include:
		* 1. Choosing a technology stack for machine learning operations
		* 2. Identify problems in our domain that can benefit from ML
		* 3. Create a data collection/curation system
		* 4. Craft models to help solve problems
		* 5. Train/Tune your models
		* 6. And most importantly, deliver an amazing discovery experience to our users
	- This is an end-to-end gig. Typeset believes in full-stack teams who contribute to all parts of the product delivery and take total ownership of the product/features they work on.
	- Ph.D. holder and 2+ years of prior experience working in a team that centered on NLP/ML
	- Prior experience working with NLTK, Spacy, or similar NLP libraries
	- Strong knowledge of Python, Data Structures and Algorithms
	- Prior experience with any/both of the following will be given preferences a. Statistical modeling techniques, such as conditional random fields b. Deep learning fundamentals, such as RNN, LSTM, CNN, Language Models
	- Published papers in top-tier NLP/ML conferences or journals
	- Experience in Scholarly Document Processing (SDP) would be highly-valued
	- Hands-on experience with AWS cloud infrastructure is a bonus.
+ linguistic quality assurance process, LQA process
+ skill set for speech processing/recognition in NLP (natural language processing):
	- ASR
	- MT
	- NLP
	- NLU
	- TTS
	- DM
	- ASP
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Experience with NLP libraries such as SpaCy, Stanford CoreNLP, OpenNLP, or NLTK
+ NLP library: spaCy, NLTK, GATE, CoreNLP, gensim
+ Deep Learning applied to NLP, for example through distributed representations (e.g. Word2Vec, fastText, etc)
+ Publish papers in:
	- ACL
	- EMNLP
	- SIGIR
	- NeurIPS
	- Interspeech
	- ICASSP
	- ICML
	- WSDM
	- WWW
	- RecSys
	- KDD
	- AAAI
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.





###	Recommender Systems



Sets of skills for recommender systems, recommendation systems (or, recommendation platforms/engines):
+ skill set:
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
+ skill set:
	- Recommender Systems and Personalization. Almost every aspect of the Netflix experience is personalized, and much of that personalization is driven by our various flavors of recommendation algorithms. You'll apply a number of techniques, from the latest in deep learning, reinforcement learning, to causal inference.
	- Search Ranking and Query Understanding. You'll work on the algorithms that allow our members to interactively query and explore our catalog. Using the latest in NLP techniques, you'll solve problems including: query understanding, knowledge graph discovery, and learning to rank across our global catalog of titles.
	- Large Scale Machine Learning. Netflix is available in over 190 countries, with over 148+ million members. This gives us a unique dataset to work with, but also unique challenges in how we scale our models. You'll work on cutting edge techniques to scale your models for use in our production systems.
	- Strong background in machine learning with a broad understanding of unsupervised and supervised learning methods
	- Strong software development experience
	- Successful track record of delivering results in complex cross-functional projects
	- Strong mathematical skills with knowledge of statistical methods







###	Legal Informatics & Computational Law



This subsubsection includes skill sets for applied machine learning roles in legal services, including:
+ legal informatics
+ computational law











###	MLOps, or ML Ops, ModelOps, & AIOps 





####	MLOps, or ML Ops



MLOps is the set of practices at the intersection of Machine Learning, DevOps and Data Engineering.

MLOps or ML Ops is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently.

Machine learning models are tested and developed in isolated experimental systems. When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems.

Similar to DevOps or DataOps approaches, MLOps seeks to increase automation and improve the quality of production models, while also focusing on business and regulatory requirements. While MLOps started as a set of best practices, it is slowly evolving into an independent approach to ML lifecycle management.

MLOps applies to the entire lifecycle:
+ integrating with model generation
	- software development lifecycle
	- continuous integration/continuous delivery, CI/CD
+ orchestration
+ deployment
+ health
+ diagnostics
+ governance
+ business metrics

MLOps is a subset of ModelOps
+ MLOps is focused on the operationalization of ML models, while ModelOps covers the operationalization of all types of AI models.
+ MLOps is the intersection of:
	- machine learning engineering
	- DevOps
	- data engineering



Steps in a machine learning lifecycle:
+ data collection
+ data processing
+ feature engineering
+ data labelling
+ model design
+ model training
+ optimization
+ endpoint deployment
+ endpoint monitoring




Goals of enterprise machine learning that can be achieved through MLOps systems:
+ Deployment and automation
+ Reproducibility of models and predictions
+ Diagnostics
+ Governance and regulatory compliance
+ Scalability
+ Collaboration
+ Business uses
+ Monitoring and management


A common architecture of an MLOps system would include data science platforms where models are constructed and the analytical engines where computations are performed, with the MLOps tool orchestrating the movement of machine learning models, data and outcomes between the systems.



Need to address:
+ model effectiveness
+ model compliance
+ model life cycle (MLC) management as a cross-functional process




***Skill sets for MLOps***:
+ skill set:
	- develop distributed traning infrastructure for faster training of ML models
	- efficiently deploy ML models into production
	- create automation pipelines for continuous training, evaluation, and deployment of models
	- improve tracking of models, data, and experiments
	- create the interfaces, infrastructure, and clusters to process data efficiently
	- advance monitoring to identify model drift and active learning opportunities
	- establish scalable, efficient, automated processes for data analyses, model deployment, model validation, and model implementation
	- create and deploy new product features via collaboration with data scientists and software developers
	- ***champion engineering excellence and culture, establish metrics for regular improvement***
	- automate ML pipelines and deploy ML models in production environments
	- design and build systems in a microservices-based architecture
	- experience with ML deployment frameworks
	- experience with containerized applications, databases, and distributed computing
	- experience with deploying ML models as a Web service, and building scalable machine learning systems spanning multiple teams and organizations
+ skill set:
	- Principal Software Engineer - Cloud Service and Machine Learning
	- As the Principal Software Engineer for our Machine Learning team you will be responsible for ensuring that the development of ML systems and services meets all technical and quality standards. You will work with Product Management and other technical teams within Splunk, incorporating new requirements and providing technical information related to this sophisticated ML Platform as needed.
	- work with a team of senior ML engineers, applied researchers and security researchers, and experts within their own specialty. You will set an example for this group, as well as set high standards on quality, communication and ability to deliver with deadlines.
	- contribute to architecture and technical decisions while also mentoring junior members within the team.
	- be working in a multi-office, multi-location development environment and prior experience working with local and remote teams or groups will be a plus.
	- While expertise with ML products and their application within enterprise solutions is highly desirable, it is not required, provided you are willing to quickly come up to speed and you have some prior experience of ML technology and its application.
	- 12+ years software development with focus on large scale distributed systems.
	- Some Machine Learning application development experience, this is NOT a data scientist role, but a services/platform development role.
	- Ability to communicate effectively in conversations with researchers and engineers from academia background.
	- Passionate about building and encouraging good engineering practices and processes such as continuous integration and deployment.
	- Experience developing and putting into production test automation and CI/CD systems.
	- Expertise in developing software with container deployment and orchestration technologies at scale, with strong knowledge of the fundamentals including service discovery, deployments, monitoring, scheduling, load balancing.
	- Strong background in building streaming applications or streaming analytics platforms.
	- Expert in one of the streaming platforms, preferably Flink.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
+ skill set:
	- You will also work closely with our data scientists to make sure our customers have the necessary tools to perform high quality data integrations by building out the Machine Learning and AI infrastructure for entity resolution, automated data mapping, predictive analytics, and risk analysis.
	- As a Software Engineer Intern you will work with a mentor to improve storage, compute, privacy, security, and compliance features necessary to support the operational workflows that help people get the assistance they need.
+ skill set:
	- We are seeking a strategic technical leader who will be responsible for delivering the core infrastructure for machine learning on Databricks. This includes the ML runtime (a packaged environment containing Spark, Tensorflow, and other frameworks), our own machine learning algorithms, storage and IO optimizations, as well as higher level abstractions such as hyper parameter tuning and feature registries.
	- Grow a team of application developers responsible for the Databricks ML Runtime.
	- Grow Databricks' machine learning capabilities - increase YoY product revenue and adoption at > 100%
	- Manage technical debt, including long term technical architecture decisions and balance product roadmap
+ [***MLflow***, An open source platform for the machine learning lifecycle](https://mlflow.org/)
+ skill set:
	- The Machine Learning Platform team is hiring strong engineers to help us design MLflow, an open source tool for managing the Machine Learning lifecycle. In this role you will help define the APIs creating the standard that organizations use to manage their Machine Learning, from tracking offline experimentation through deployment to production systems. You will also build the services supporting the APIs in the open source and their integration into the Databricks product, a unified analytics platform that helps manage data processing and machine learning workloads in a collaborative, enterprise grade product.
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Be responsible for full software development lifecycle - design, development, testing, operating in production
	- Architect solutions to achieve a high level of reliability, scalability and security
	- Communicate effectively with other engineers in the same team, with other teams and with various other stakeholders such as product managers
	- Mentor junior engineers or other engineers on the team to help level up their skillset
	- 7+ years of production experience developing services in: Java, Scala, C++, Go, or Python
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- Preferred Experience working on a SaaS platform or with Service Oriented Architectures
	- Preferred Experience with software security and systems that handle sensitive data
+ skill set:
	- Development and optimization of storage systems for deep learning and simulation
	- Our goal is to develop the technology to deliver the training data in storage media (either HDD or SSD), to the memory of GPU or MN-Core, as well as the technology to store the data obtained by simulation into our storage media.
	- Communication Language: English/Japanese
	- System programming in Linux (TCP/IP, Ethernet, system calls, file I/O, FUSE)
	- Kernel programming in Linux (VFS, kernel modules, etc.)
+ skill set:
	- Research and development of large-scale computing infrastructure (infrastructure technology) for machine learning
	- We will work on research and development of large-scale computers (clusters) using GPUs and MN-Core. We plan to adjust the content of the work from themes such as performance improvement of calculation infrastructure, verification of elemental technologies, and better operation technology (visualization and automation).
	- Communication Language: Japanese
	- Ability to work on research projects voluntarily and ambitiously
	- Basic computer science knowledge and hardware skills
+ skill set:
	- distributed machine learning
	- query assistance
		* autocomplete, and popular and related searches
	- user experience, with universal search systems and diversity-aware ranking
	- indexing, visual content representation and content understanding
	- retrieval, query understanding and language understanding
	- ranking
		* topical, contextual, personalized, and business objective feature modelling and ranking systems
	- metrics and experimentation
		* development of sensitive offline and online metrics, and more efficient and predictive experimentation systems
	- information retrieval technologies
		* OpenSearch
		* ElasticSearch
		* Solr
		* Learning to Rank algorithms and toolkits
		* build ML solutions that meet SLA guidelines, beyond ML model training
	- skills in:
		* Learning to Rank algorithms and toolkits
		* machine learning
		* information retrieval
		* search-specific experimentation and metrics
- skills to develop:
	- Deep understanding of at least one popular server side MVC Framework (e.g Django, Rails, AngularJS etc).
	- Knowledge of backend storage systems like MySQL, HBase, Memcached, Redis, Kafka etc.
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
+ tech stack:
	- Experience with NoSQL databases. MongoDB is a plus
	- Experience with real-time and streaming data processing
	- Experience with queuing platforms like Kafka
	- Knowledge of BigQuery
	- Familiarity with GCP/AWS cloud services
	- Familiarity with TensorFlow
	- Comfortable with CI/CD Pipelines
	- Experience with Git version control
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






















####	ModelOps



MLOps is a subset of ModelOps
+ MLOps is focused on the operationalization of ML models, while ModelOps covers the operationalization of all types of AI models.


ModelOps (model operations), as defined by Gartner, "is focused primarily on the governance and life cycle management of a wide range of operationalized"
+ artificial intelligence (AI) models
+ decision models
	- machine learning models
	- knowledge graphs models
	- rules
	- optimization
	- linguistic models
	- agent-based models
	- decision optimization models
+ optimization models
+ transformational models



ModelOps has overlaps wothin:
+ DataOps
+ DevOps


ModelOps is a programming model for reusable, platform-independent, and composable AI workflows.
+ Mitigate the accumulation of AI and machine learning models that are:
	- undeployed
	- unused
	- unrefreshed
	- manually deployed
+ Support model management of AI and machine learning models
+ address the gap between model deployment and model governance
+ ensure that all models are running in production with strong governance, and aligned with technical and business KPIs while managing the risk
+ programmatic solution for AI-aware staged deployment and reusable components that would enable model versions to match business apps, and which would include AI model concepts such as:
	- model monitoring
	- drift detection
	- active learning
+ cloud-based framework and platform for end-to-end development and lifecycle management of artificial intelligence (AI) applications
+ extend the principles of software lifecycle management to enable the following for AI model pipelines:
	- automation
	- trust
	- reliability
	- traceability
	- quality control
	- reproducibility
+ includes:
	- routine deployment of machine learning models
	- continuous retraining
	- automated updating
	- synchronized development
	- deployment of more complex machine learning models




References:
+ Hummer, Waldemar; Muthusamy, Vinod. ModelOps: Cloud-based Lifecycle Management for Reliable and Trusted AI. IEEE International Conference on Cloud Engineering. Parijat Dube, Kaoutar El Maghraoui. p. 1.


The ModelOps process focuses on:
+ automating the governance, management and monitoring of models in production across the enterprise
+ enabling AI and application developers to easily plug in life cycle capabilities
	- bias-detection
	- robustness and reliability
	- drift detection
	- technical, business and compliance KPIs
	+ regulatory constraints and approval flows for putting AI models into production as business applications



The ModelOps process starts with a standard representation of candidate models for production that includes a metamodel (the model specification) with all of the component and dependent pieces that go into building the model such as:
+ data
+ hardware and software environments
+ classifiers
+ code plug-ins
+ business and compliance/risk KPIs




Skill set for ModelOps:
+ Spring Boot, Spring Data Rest, and Microservice Development experience
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












####	AIOps

AIOps, a similarly named, but different concept - using AI (ML) in IT and Operations.



Skill set for AIOps:
+ skill set:
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Cloud computing platforms and large web-scale distributed systems



















##	Logical AI + Other AI




















##	Differential Machine Learning











##	Differential Privacy





+ Hestia - Differential Privacy - Data Anonymization Challenge, https://www.topcoder.com/challenges/30082341
	- https://github.com/uber/sql-differential-privacy
	- https://github.com/arx-deidentifier/arx










##	Information Retrieval




+ Experience with search and information retrieval systems, either custom or commercial (Elasticsearch, Solr).
+ You have used GraphQL in production environments



























##	Data Science + Data Engineering + DataOps


###	Notes about Data Science + Data Engineering + DataOps

This section provides information about data science roles and skills set regarding:
+ [Generic data science positions]()
+ [Business analytics]()
+ [Sports Analytics]()
+ [Data Science for public health]()
+ [Data Science for Advocacy, Lobbying, Think Tanks]()
+ []()
+ []()
+ []()
+ []()
+ []()




For skill sets in data science roles regarding finance, see the *Markdown* document [](financial-engr-n-finance-x.md).





For skill sets in data science roles regarding the following fields, see the *Markdown* document [](bio-biochem-biotech-pharma.md).
+ bioinformatics
+ bio design automation, BDA
+ bio manufacturing automation
+ biology
+ biochemistry
+ biotechnology
+ medicinal chemistry
+ pharmacy
+ pharmaceutical science






Notes:
+ In database normalization, unnormalized form (UNF), also known as an unnormalized relation or non first normal form (N1NF or NF^2),[1] is a database data model (organization of data in a database) which does not meet any of the conditions of database normalization defined by the relational model. Database systems which support unnormalized data are sometimes called non-relational or NoSQL databases. In the relational model, unnormalized relations can be considered the starting point for a process of normalization. It should not be confused with denormalization, where normalization is deliberately compromised for selected tables in a relational database.







###	Generic Data Science Roles



Sets of skills for generic data science roles, or data scientist positions:
+ ***Experience with metrics systems such as Grafana.***
+ ***Experience working with analytics tools such as Google Analytics, Heap Analytics, Chartmogul, Baremetrics, Periscope, Tableau, Mode Analytics, Looker, or similar***
+ skill set:
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data analyst experience with 4+ years of proven industry experience in a large scale environment(PBs scale, globally distributed teams).
	- Strong experience in Python, R, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ skill set:
	- Partner and align with business leaders, stakeholders, product managers and internal teams to understand the business and product challenges and goals and address them using predictive analytics in a globally distributed environment.
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Understand business/product strategy and high-level roadmap and align analysis efforts to enable them with data insights and help achieve their strategic goals.
	- Strong audience focused presentation and storytelling skills focused on key takeaways in a crisp and concise manner.
	- Define hypothesis driven models and best practices to derive and publish model scores/insights/learnings at scale within the company.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data scientist experience with 4+ years of proven industry experience in a large scale environment (PBs scale, globally distributed teams).
	- Proven lead in driving multi-million dollar revenue generator models for the company and setting up data science related best practices at a company.
	- 2+ years experience with a fast-growing SaaS business based company is preferred.
	- Strong experience in Python, R, Spark, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ Experience with data analytics platforms, such as Semantic Pro, Semantic Cortex, IBM i2
+ tech stack:
	- 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Experience  communicating effectively with non-technical audiences
	- Strong ability to devise data-driven solutions to business problems
	- Strong competency with SQL
	- Experience with or exposure to a scripting language (Python preferred)
+ tech stack:
	- 5+ years of relevant analytical experience working with data or MS in a relevant technical field and 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Comfort and expertise communicating effectively with a wide-range of audiences (including product managers, engineers, business development managers, occasionally executives)
	- Strong ability to devise data-driven solutions to business problems
	- Ability to drive impact by thoughtfully tackling open-ended problems
	- Strong data intuition and deep understanding of and experience with statistical and/or ML modeling techniques
	- Strong competency with SQL
	- Fluency in a scripting language (Python preferred)
	- A plus: Experience with large scale data processing tools (Apache Spark) or implementing systems in production at scale
+ skill set:
	- Strong proficiency in Python a necessity, especially the Python data science stack
	- Experience developing data science models, workflows, and software for real world applications and working with imperfect data
	- Exploratory analysis, modeling, and visualization in Jupyter notebooks
	- Integrating data sources, creating subsets (ex. train/test) for modeling, and assessing potential datasets, tools, and approaches
	- Translating the results of analysis into implications for people and problems
	- Developing well-organized code that can be collaboratively reviewed, reproduced, and integrated into applications
	- Quickly assessing and becoming productive in relevant new technologies and methods
	- Experience with core data science tools (ex. pandas, scikit-learn, numpy, jupyter)
	- Experience working with messy data and real-world applications
	- Experience using IaaS like Amazon AWS
	- Working on a small team means doing a little bit of a lot of things. We're looking for somebody who can ask the right questions to figure out what is important, iterate between brainstorming together, working independently, and managing other data scientists, scope new data science projects, and exercise sound judgment to make reasonable decisions under conditions of ambiguity.
	- Communication is a core data science skill at DrivenData. Doing client-facing work involves articulating concepts, interpreting results, and selecting the method that satisfies the constraints of the project.
	- Working familiarity with the tools and practices used in software engineering and deployment (including test-driven deployment, containerization (ex. Docker), platform as a service (ex. Heroku), and infrastructure as a service (ex. AWS, Azure)
+ skill set:
	- Use Python and SQL to draw insights from data at scale
	- Extract actionable insights from broad, open-ended questions
	- Create dashboards and develop metrics to track the success and growth of the product
	- Design and evaluate experiments to measure the impact of product changes
	- Analyze data from across the product to uncover the root causes of metric movements
	- Communicate results to cross-functional stakeholders to inform product decisions
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Improve the work of other data scientists through mentorship and by bringing industry best practices to the team
	- Experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ skill set:
	- Identify new methods to test product changes where traditional A/B testing is not possible
	- Drive adoption of good experimental and statistical practices across the company
	- Apply statistical techniques to increase the efficiency and rigor of our experimental analyses
	- Proactively identify ways to optimize and scale up the way we run experiments, and to increase data scientists' impact in general, and create processes and tools to meet these needs
	- Mentor other data scientists in experimental design and causal inference techniques
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ skill set:
	- Experience in data store design (data lakes; relational, columnar, NoSQL databases, analytics/OLAP cubes)
	- AWS and DevOps experience with AI/ML pipelines
	- Create the vision to build and evolve the team’s data infrastructure and tools. Technically lead for the design, building, and launching of new data models and data pipelines
	- Ensure production quality methods to retrieve, condition, validate, synthesize, and manipulate data
	- Full-stack build custom integrations between cloud-based systems using APIs
	- Continuously refine and improve the data architecture and delivery as new requirements surface
	- Experience ensuring data integrity, security, and encryption
+ skill set:
	- Associate Data Scientist (Internship):
	- VMware Tanzu Labs partners with organizations worldwide to accelerate innovation, while reducing operating costs and risk.  The data science team at VMware Tanzu Labs is primarily a consulting practice; we work with our customers to solve real world problems.  Our customers, like us, are cross-disciplinary. We service engagements with use cases running from customer churn to optimization to detecting fraud and misconduct.  We are not just doers; we are also educators and enablers. programs.
	- You have a passion for exploring data and connecting the value hidden in data with business outcomes and user needs.  You’re agile and retrospective, and not afraid to identify what we’re doing wrong so we can fix it, and what we're doing right so we can improve on it.  You’ll be working on a wide variety of data problems for a diverse range of clients. You will often be asked to learn new technologies and domains on the fly.   Above all, you judge your success by the success of your team and the happiness of your customers.
	- Be given an opportunity to attend fun and educational events to kickstart your career, meanwhile, you’ll get a better feel for our culture. 
	- Get hands-on exposure with cutting edge technology from managers, mentors, and fellow team members.
	- Acquire the tools and knowledge to contribute on a large scale.
	- Have the ability to advance your career in the direction you choose in the future. 
	- While there is no such thing as a “typical day”, these are activities we frequently find ourselves doing the following:
	- Partnering with clients to uncover and frame new opportunities for data science. Clients often come to us without a clear understanding of what we can do, so this is our chance to open their eyes to new possibilities for their businesses.
	- Pairing and writing code together with clients around engineering features, training models, tuning hyperparameters and evaluating results.  We emphasize rigor, because data science done right at this stage leads to models that shine in production.
	- Taking the models we build into production. This is an exciting stage for anyone who likes collaborating with product teams and seeing their model become real when users interact with it.
	- Helping our clients develop their internal data science practices, through mentoring and pair programming, so that they can be successful when we hand off the project.
	- Continuous learning by building demos and prototypes on new technologies, methodologies, and frameworks.  Presentation of learnings and findings for internal audiences, external conferences, and blog posts.
	- Bachelor’s degree in an analytical or quantitative field, or currently pursuing a master’s or doctorate degree in an analytical or technical field (e.g. applied mathematics, statistics, computer science, operations research, economics, data science, etc.). 
	- Clear and empathetic communicator. You’ll be the one sharing your insights with clients and stakeholders. As such, frequent communication and tireless empathy are essential to succeed in this role.
	- Fluent speaking and writing ability in Chinese and English.
	- Advanced knowledge of statistical modeling and/or machine learning methods. These are the technical skills we need to iterate and improve data science pipelines.
	- Strong exploratory data analysis skills. Every engagement starts with an investigation of the data, and thorough EDA saves us a lot of headaches in the long run.
	- Strong programming skills in SQL and Python/R
	- Hands-on experience working in a distributed computing environment
+ ***A project around the areas of big data, searching and statistical modelling***
+ skill set:
	- ***Familiarity with analytics notebooks (Jupyter, RStudio, DataBricks)***
	- ***Strong programming skills and ability to utilize a variety of data/analytic software/languages/tools; e.g., Spark (ML, Mllib, Spark SQL), R (caret, ggplot2), Python (pandas, numpy, scipy, scikit-learn), Scala, Hive, SQL, SAS, Tableau, etc.***
	- Familiarity with cloud computing (in AWS or Azure)
	- Deep knowledge of a variety of statistical and data mining concepts and procedures including: generalized regression, machine learning algorithms, deep learning, media mix algorithms, and statistical graphics
	- Predictive Analytics experience desired
	- Experience with big data- Spark, Hive, Hadoop desired
	- Designing and overseeing implementation of solutions for non-routine problems utilizing a large array of know-how areas within analytics e.g. generalized regression, decision tree, non-parametric; and machine learning, e.g., gradient boosting, random forest, neural networks, clustering, pattern recognition
	- Developing best practices and repeatable processes for routine problems arising in business problems business cases including driving targeted marketing campaigns for tune-in and product adoption, creating an enhanced consumer experiences, and developing digital/social advertising audience segments
	- Assisting with strategic decisions about processes, frameworks and standards
+ skill set:
	- Proficient in SQL/Hive
	- Proven ability to apply scientific methods to solve real-world problems
	- Knowledgeable about the machine learning trade-offs and model evaluation
	- Over 4 years of industry experience with proven ability to apply scientific methods to solve real-world problems on web scale data
	- Ability to lead initiatives across multiple product areas and communicate findings with leadership and product teams
+ Hands-on experience with open source big data platforms (Hadoop, Hive, Presto) and familiarity with data visualization (Tableau, D3) technologies
+ Familiarity with implementing metric logging and interpreting metrics to make decisions
+ Must possess knowledge in SW to Enterprise, SaaS and on Premise, and cloud technologies Elasticsearch for text indexing, MongoDB for structured data storage, Mysql/postgres for SQL storage, JanusGraph for Properties’ based graph storage, ArangoDB for Documents/Key Value/Graph storage, and Amazon AWS.
+ skill set:
	- Deep understanding of machine learning, statistical modeling and data mining techniques such as gradient boosting, neural networks, natural language processing and clustering
	- Understanding of experimental design and adaptive sampling
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Experience working with relational SQL and NoSQL databases
	- Familiar with ML and statistical modeling tools such as R, SparkML, TensorFlow, SciKit
	- Proven track record overseeing multiple data science projects at all stages, from initial conception to implementation and optimization
	- Good programming skills using analytics-oriented languages such as Python, R and Scala
+ skill set:
	- Hands-on experience with AWS (Lambda, SAM, S3, DynamoDB, CloudFormation, EC2, IAM)
	- Experience building and interpreting data models and analytics dashboards
+ skill set:
	- Experience with data quality processes, data quality checks, validations, data quality metrics, definition, and measurement.
	- Ability to operate with cross-functional teams (for example, customer support, data science, engineering, and sales), including a willingness to balance the changing needs of a client-facing team with a demand for data engineering best practices and the ability to communicate the tradeoffs.
+ skill set:
	- Experience with presentation or data visualization software, such as Microsoft PPT, Tableau, Shiny, etc.
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ skill set:
	- Proficiency with statistical programming languages (R, Python, etc.) and proven ability to work pragmatically with statistical concepts
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ Proficiency with machine learning and statistical modeling (e.g., scikit-learn, TensorFlow, Stan)
+ Experience identifying data quality and developing automated QC checks and/or reports
+ A senior analytics professional with a proven track record of data analysis, reporting and visualization (e.g. Tableau, D3)
+ skill set:
	- Use Databricks to build internal data warehouse and integrate it with BI and CRM services used internally
	- Use Databricks to analyze usage data, and create dashboards and reports
	- Build self-serving internal data products to make data simple within the company
	- Work closely with Product Management and other stakeholders to understand product usage patterns and trends and to make data-driven decisions and forecasts
	- Provide product feedback to PM and Engineering teams
	- Strong desire to work at a rapidly growing startup
	- Knowledge of data processing and applied statistics
	- Proficient in data analysis and visualization using R or PyData
	- Familiar with SQL and databases like MySQL or PostgreSQL
	- Experience with distributed data processing systems like Spark and Hadoop
	- General-purpose languages such as Python and Scala
	- Desire to explore lots of data to find unexpected insights
	- Strong communication and presentation skills
	- [Plus] Advanced degrees in statistics, computer science, math, or similar fields
	- [Plus] Familiarity with interactive data visualization using tools like D3.js
+ Research, evaluate, and present statistical or Machine Learning methods to provide actionable insights.
+ Enforce SOX & GDPR compliance across the analytics database and reporting tools
+ Good grasp of statistical concepts (e.g. hypothesis testing, regression)
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ **Experience and knowledge of programming languages, data analysis packages (e.g., Python, R, SAS, MatLab, Stata, GAMS, SPSS, Hadoop, BigML, Pandas). Experience and knowledge of visualization tools (e.g., Tableau, Sigma JS) is preferred.**
+ Working knowledge of data analysis packages (e.g., SAS, MatLab, Stata, GAMS) is strongly preferred.
+ skill set:
	- analytical mindset and strong experience with data-driven product decisions using analytics tools, such as:
		- Amplitude
		- Retool
	- AI-driven insights and recommendation engines to help creators build better businesses
	- category defining analytics platform to help creators better understand their audience
+ Expertise in statistical inference including experimentation and observational methods to causal inference
+ Strong coding experience. Experience with open-source ML packages (specifically sklearn, TensorFlow/Keras/PyTorch).
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
	- Experience in optimization algorithms and numerical computation
	- Experience with Spark, TensorFlow, or Keras
	- Experience with cloud computing platforms and large web-scale distributed systems
	- Open source contributions
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Optimization algorithms and numerical computation
	- Spark, TensorFlow, or Keras
	- Cloud computing platforms and large web-scale distributed systems
+ Expertise in additional statistical methods (e.g., Bayesian approaches, dyadic analysis, causal inference approaches, factor analysis, SEM)
+ Experience with Databricks or Spark, EMR
	- Databricks: data lakehouse architecture, cluster management for computer clusters (in the context of cluster computing), and associated machine learning technology
	- Adobe Spark, or Spark: open-source unified analytics engine for large-scale data processing
		* large-scale, multi-language engine extreme data engineering, data science, and machine learning on single-node machines or clusters
		* for the following tasks:
			+ machine learning
			+ data science, or data analytics, at scale
			+ batch/streaming data
			+ SQL analytics
		* for programming languages:
			+ Python
			+ SQL
			+ Scala
			+ Java
			+ R
	- Amazon Elastic MapReduce, Amazon EMR
		* for applications in:
			+ machine learning
			+ extract, transform, load (ETL)
			+ real-time streaming
			+ genomics
			+ clickstream analysis
		* accessed using:
			+ AWS Command Line Interface, AWS CLI
			+ AWS Console
		* Not EMR, electronic medical record = electronic health record
+ build multivariate simulation data pipelines on a cloud for integration tests
+ skill set:
	- experience with defining, implementing, and analyzing A/B and multivariate experiments
	- capacity as data analysts to work with data engineers and data scientsts as appropriate to design, scope, and work through new projects
	- statistical modeling and analysis in R and/or Python
	- reporting and analysis using major Web analytics tools, such as:
		* Google Analytics
		* Adobe Analytics
		* Parse.ly
		* comScore
		* SimilarWeb
	- analyze data and build dashboards using business intelligence, BI, tools, such as:
		* Data Studio
		* Looker
		* Tableau
	- use SQL-based database applications for handling large data sets:
		* Snowflake
		* BigQuery
		* Athena
		* RedShift
+ A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
+ skill set:
	- B.S., M.S., or Ph.D. in a quantitative field
	- 4+ years work experience in an analytical or quantitative role as a Data Scientist
	- 2+ years experience working on product analytics in a two-sided marketplace
	- Extensive experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
	- Active Quora user with curiosity about the product
	- Deep experience with MySQL, NoSQL data stores like HBase or similar
	- Strong grasp of Configuration Management (Chef, Puppet, Ansible, Salt Stack)
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ ***Experience with big data techniques (such as Hadoop, MapReduce, Hive, Pig, Spark)***
+ skill set for data science:
	- ***Technical mastery in one or more of the following languages/tools to wrangle and understand data: Python (NumPy, SciPy, scikit-learn), Spotfire, Tableau.***
	- ***Experience with Spark (MapReduce, PIG, HIVE)***
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
+ data science:
	- Knowledge of ElasticSearch/Solr/Lucene is a big plus.
	- Understanding in Java server platform and system tuning is a plus.
	- Knowledge with vector space models, text classification and categorization.
	- Implement high-quality code in an agile software development environment.
+ data science skill set:
	- Implement scalable algorithms and services using technologies such as Scala, Akka, elasticsearch, Kafka, Cassandra and Hadoop technologies such as Hive, Spark or MapReduce
	- Hands-on experience in analyzing large datasets (e.g. with SQL, Python, R, Hive, etc.)
	- Some knowledge and experience in working with technologies such as Kafka, Cassandra, Elasticsearch, Akka, Kubernetes, etc.
	- Experience in Scala or Java is a plus
	- You are fluent in English; German skills are a plus
+ skill set:
	- Python
	- R
	- SQL
	- Jupyter Notebooks
	- Tableau
	- Looker Studio
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.



















Sets of skills for more senior data science roles, such as management of data science teams:
+ skill set:
	- At the heart of Netflix Product Innovations is an experimentation driven culture led by Science & Analytics (S&A).  In this role, you will lead teams of data scientists and analysts responsible for shaping UI and Content Innovations decisions through experimentation (A/B, quasi) and empirical studies to guide product strategy.
	- Set an impact-focused, strategic science roadmap to guide product innovations.
	- Recruit and inspire exceptional data scientists focused on the span of causal inference, behavioral research, and analytical activities.
	- Uphold the culture of rigor in product decision-making through active participation in product debates.
	- Lead and contribute to cross functional initiatives between product development (product management, design, engineering), content, and marketing.
	- Define a team culture that balances supporting high impact business needs with forward looking research.
	- Serve as thought partner to product development executives across product management, engineering, and design.
	- 5+ years experience in building and inspiring a high-performing data science and analytics team.
	- Capacity and passion to translate business objectives into actionable analyses, and analytic results into business and product recommendations
+ skill set:
	- Starsky Robotics is looking for a full-time Senior Data Scientist. Your job will tackle a wide variety of problems in autonomous vehicles. From finding every time a car cut in front of our truck, to figuring out how to report on the quality of autonomous driving, to creating new tools and statistical methods for robotics engineers to characterize the behavior of their systems, we're looking for someone motivated to attack self-driving problems with mounds of data. Tackling these problems will require learning about the whole suite of robotics fields applied to make autonomous vehicles: motion planning, controls, perception, and behavior planning.
	- You'll own high-level decisions such as “How do we determine if a route fits our current driving capabilities”. Day-to-day projects may have mission statements as technical as “Help us solve this spike in cross-track error on curves”, or as business-focused as “Can we get a heatmap of all the places our trucks have driven over the last year”.
	- Additionally, you can bring best-practices for data-science to the company, including helping build up the base platform and infrastructure necessary to speed up data-centric work. Starsky has a solid base of tooling around our data, but it is ripe for improvement.
	- Demonstrated expertise in the data scientists modern toolkit: Pandas, R, SQL, etc, and don't mind sharing your experience with the team
	- Deep quantitative thinker: Masters or PhD in a quantitative field, or multiple years of experience in a quantitative-focused position
	- Relish delivering answers and metrics and seeing change affected by your work
	- Can take high level directives and take them through from research project, proof of concept, to applied & implemented feature.
	- Are constantly looking for problems that could be solved with liberal application of data
+ skill set:
	- AIML - Head of Data Engineering, Data & Machine Learning Innovation
	- Are you passionate about data, data quality, efficiency, and scale?  Would you like to play a critical part in accelerating data-informed product development to drive innovations and amazing user experiences?
	- As the Head of Data Engineering, you and your team will be responsible for the crucial role of designing, operating, and supporting the increasingly complex environments that power modern data analytics and machine learning use cases for Apple products such as Siri, Dictation, Translations, Apple Search and others.
	- You will partner closely with cross-functional teams and be responsible for the planning, execution, and success of technical projects as well as help drive scalable practices to accelerate the evolution of our products. You will lead existing data resources, build new datasets and data pipelines, and implement new technologies and tools to enable rigorous scalable science and analytics, and data-informed product development. Your team and vision will enable data to inform product engineering teams at scale, with the ultimate purpose of improving the product experience for Apple users across hardware, software, and services, while preserving Apple’s privacy values. You will drive a strong culture of data excellence!
	- 10 years in a managerial role, growing and scaling organizations
	- Experience in multiple large-scale data processing systems, data telemetry, data-driven performance and reliability improvement across platforms and products, the data foundation for advanced machine learning problems, and data/analytics.
	- Demonstrated passion and leadership in data engineering, data-informed product development, building and shaping data culture
	- Proven ability to proactively drive a data engineering agenda and prioritize work based on impact and strategic investment
	- Experience with modern data engineering stack, processing technologies, data, and analytic pipelines, including batch (Spark, others), streaming systems (Flink, etc.), and scalable query engines (e.g. Trino, Druid, Pinot, etc.)
	- Solid understanding of both relational and NoSQL database technologies and experience in engineering datasets and metrics out of massive and complex data
	- Expertise in data engineering partnering with analytics and ML teams (comprised of data scientists and statisticians) to enable horizontal and vertical use cases
	- Demonstrated ability to establish strong partnerships and drive processes across organizations, e.g. to manage the creation, processing, and use of instrumentation to enable using data impactfully in developing products
	- Outstanding communication and presentation skills, written and verbal, to all levels of an organization
	- Proactively drive the vision for Data Warehouse, Data Platform, Analytics, and ML analytics to accelerate the building and improvement of our products, and define and execute a plan to achieve that vision.
	- Grow and scale a large, established data engineering team and build strong relationships with partner teams
	- Drive the design, building, and launching of new data models and data pipelines in production
	- Stay focused and prioritize a heavy workload while achieving exceptional quality and driving long-term vision
	- Bachelors or Master in CS, Engineering, Math, Statistics, or a related field, or equivalent practical experience in engineering
+ skill set:
	- The FSQ BI app manager will lead a small team of Backend and Frontend engineers to build and develop the FSQ BI application. The candidate should have a relevant background in building analytic dashboard products, with an understanding of basic SQL, charts, and maps.
	- Manage, lead, and coach a team of backend and frontend engineers to develop the FSQ business intelligence application
	- Own the team's technical roadmap and direction, working closely with Product Management, Program Management, Client Success, Engineering, and stakeholders to achieve company goals
	- Lead the development of a scalable and robust application stack for delivering business intelligence dashboards
	- Contribute hands-on engineering solutions as needed
	- Partner with engineering, product teams, and department stakeholders to drive forward broader engineering and company initiatives
	- Recruit talent to grow the team in line with Foursquare company growth and priorities
	- Proven experience with business intelligence application development, and building data analytic products
	- In-depth knowledge of data analytic concepts, tools, and methodologies
	- Hands-on experience with data analytics tools (e.g., Tableau, Power BI, SQL, Python, etc.)
	- Familiarity with data modeling, ETL processes, and data warehousing techniques
	- Knowledge of data visualization, charts, and maps
	- Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and data warehouses (e.g., BigQuery, Snowflake)
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












####	Data Visualization


+ data visualization with the following tools and/or libraries:
	- Plotly
	- Tableau
	- PowerBI
	- Qlik
	- Google Charts
	- d3.js
	- plotly.js
	- Angular.js
	- PowerBI
+ Producing effective and interactive data visualizations (Tableau, Shiny, D3)
+ Bonus points for experience building interactive data visualizations using libraries like D3, Highcharts, and Leaflet, and for experience working with big data systems like Hive, Hadoop, Scalding and Spark.
+ illustrate visualization ideas using storyboards, process flows, wireframes, and prototypes
+ [Plus] Familiarity with interactive data visualization using tools like D3.js
+ metrics systems:
	- ***Grafana.***



















####	Data Science for Social Media Companies



+ skill set:
	- The Consumer Data Science organization works closely with our cross-functional partners, and Twitter's leadership to understand user behavior, inform product decisions, safeguard the health and integrity of our services, and to influence company strategy. We are currently hiring for the following subteams on Consumer Data Science. These high-impact teams value creativity, critical thinking, and teamwork.
	- The Consumer Data Science organization is hiring Senior Data Scientists in the following areas:
		* Health (SF, Boulder) - The goal of this team is to improve the health of the public conversation, ensuring that users feel safe while using our platform. As part of our team, you'll help the Health Organization make strategic decisions that ensure that Twitter is a safe and informative experience for our customers. You will do this by performing and mentoring others through analyses, metrics, experimentation, research, and more.
		* Growth (SF) - Their mission is to increase Twitter's daily utility for new and returning users through impactful and creative applications of experimentation and data analysis. As a key member of Growth Data Science, your work will directly influence exciting new product areas and help grow Twitter usage around the globe.
		* Metrics (SF) - This team works to support company strategy by helping to define, maintain, and understand key success metrics to ensure that we continue to meet the demands of our customers.
		* Video (NY) - This team works with the Live Video, Video on Demand, Publishers, and Camera products. The team is involved in opportunity sizing, experiment setup and analysis, and metric design in order to influence video and media strategy at Twitter.
	- Support the entire product development lifecycle from product ideation to opportunity sizing to measurement design to experimentation and causal analysis to post-launch learning and iteration into the next development cycle.
	- "Design and implement experiments or other econometric methods to understand how changes to the platform affect user behavior."
	- Build novel metrics, identify the impact of product and policies, and study causal impact of our Product launches and Health initiatives.
	- Work in tandem with team members, applying advanced statistical methods; writing complex data flows using multiple languages/frameworks such as SQL, Scala (Scalding, Spark), Python; and using data visualization tools.
	- Communicate findings to executives and cross-functional stakeholders.
	- You are a self-starter who is capable of learning on the job, taking initiative, and thriving within a large team.
	- You are excited to learn and apply new data analysis techniques and tools. You are passionate about insights, not just data and methods. You are a strategic thinker and are able to synthesize technical concepts into actionable recommendations for a diverse audience.
	- You communicate your findings clearly and effectively to a wide audience of relevant partners and are capable of building meaningful presentations and analyses that tell a story.
	- You are rigorous, care about data quality, and strive to understand surprising results and underlying mechanisms in your analyses. You combine business insight with detailed data knowledge and statistical expertise to ensure an accurate interpretation of results.
	- You are a capable mentor. You enjoy knowledge sharing and working with junior teammates to up-level their skills and take the time to learn from them.
	- You value teamwork and teammates. You contribute positively and meaningfully to cultivate an inclusive team culture. You are personable, empathetic, and able to connect with each and every person on the team and throughout the company.
	- Experience using SQL, R, or Python for analysis, modeling, and data visualization.
	- 5+ years experience working with and analyzing large datasets to understand behavior, solve problems, and answer business questions.












###	Business Analytics



This subsubsection includes the following topics of business analytics, except financial analytics which is found in the following *Markdown* document for [Financial Engineering, Computational Finance, and Financial Analytics]().
+ [Marketing Analytics]()
+ [Human Resource Analytics]()
+ [Data Science for Logistics, Supply Chain Management, Industrial Distribution, & Retail Sales]()




Skill sets for business analytics:
+ skill set:
	- Minimum of 3 years' delivery experience in advanced modeling environment: strong understanding of statistical concepts and predictive modeling. (e.g., AI neural networks, multi-scalar dimensional models, logistic regression techniques, machine-based learning, big data platforms, SQL, etc.).
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
	- Possess a blend of marketing acumen, consulting expertise and analytical capabilities that can create value and insights for our clients.
+ You're familiar with business intelligence reporting platforms like OBIEE, Tableau, MicroStrategy, and Business Objects
+ skill set:
	- 2+ years SQL working experience (Redshift/PostgreSQL/MySQL)
	- Experience with BI & reporting dashboards (Periscope, Tableau, etc)
+ skill set:
	- enterprise resource planning, ERP
		* ***business intelligence***
			+ customer relationship management, CRM, customer services
			+ sales:
				- invoicing
				- order placement
				- order scheduling
				- shipping
		* e-commerce, electronic commerce
			+ product lifecycle management, PLM
				- planning
				- optimizing manufacturing capacity and material resources
				- manufacturing resource planning, MRP
					* material requirements planning, MRP
			+ supplier relationship management, SRM
				- maximize cost savings with support for the end-to-end procurement and logistic processes
		* enterprise asset management
			+ corporate performance and governance
			+ human resource
		* industrial distribution, logistics, supply chain management, SCM
		* accounting
			+ financial operations
			+ regulatory compliance
+ skill set:
	- SQL
	- Snowflake, Redshift, BigQuery
	- marketing platforms:
		* Facebook Ads
		* Google Ads
		* Appsflyer
		* SA360
		* CM360
		* TikTok ads
	- GeoXExperience
	- Looker, Mode, Tableau
+ PowerBI, Tableau, Qlikview






















####	Marketing Analytics




+ Use marketing technology such as Hubspot, Salesforce, WordPress, Sendoso, and Go-To-Webinar
	- While this list is provided for a marketing position, it indicates technologies people in marketing analytics can build for people on these platforms.







####	Human Resource Analytics









####	Data Science for Logistics, Supply Chain Management, Industrial Distribution, & Retail Sales


+ skill set:
	- At least 5 years demonstrated results in areas of Operations Research and/or Supply Chain Projects (inventory optimization, network design, and S&OP) in sophisticated and complex environments including the use of simulation and modeling tools (Llamasoft, CPLEX, Gurobi, or other similar)
	- At least 5 years performing data analytics and modeling with advanced languages (e.g. Python or R)






####	Data Science for Economics



+ skill set:
	- B.S. or M.S. in Economics, Statistics, or a similar field and 1+ year work experience in data science or analytics, or Ph.D. in a quantitative social/behavioral science (e.g. Economics, Sociology, Psychology, Statistics, or a similar field)
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Familiarity with Python or similar scripting language
	- Experience communicating technical statistical concepts clearly, for example, teaching or consulting
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework










###	Sports Analytics or Data Science for Sports









###	Data Science for Public Health










###	Data Science for Health Companies & Other Organizations


+ skill set:
	- At least 2 years designing and building healthcare data analysis solutions for the business payer or provider industry
	- At least 2 years using new developments in AI, machine learning, cognitive systems, and robotics to build amazing analytical tools
	- At least of 2 years working with tools like SAS, Python, SPSS, R, or SQL
	- At least 2 years working with data integration tools to streamline processes in platforms like Cerner EMR, Apache Spark, MapReduce, MongoDB and Couchbase
	- You can use data mining techniques to solve real world business problems
+ tech stack:
	- Hadoop ecosystem and its components.
	- Hadoop, Hive, HBase, and Pig
	- Working experience in HQL
	- Pig Latin Scripts and MapReduce jobs
	- Hands-on experience in backend programming, particularly Java, and Node.js
	- Analytical and problem-solving skills
+ skill set:
	- knowledge of:
		* clincial trials
		* digital health tools
		* employee's health management
		* health care system in other countries
	- management of health care real-world data, RWD
	- experience in defining/creating requirement documents (e.g., PRD) of health care product/service, launch them, and improve in an agile way 
















###	Data Science for Advocacy, Lobbying, Think Tanks












###	Data Science for Legal Informatics & Computational Law


This subsubsection includes skill sets for data science roles in legal services, including:
+ legal informatics
+ computational law











###	Data Science for Semiconductor Manufacturing





###	Data Science for Computational Science and Computational Engineering (except EDA)




Skill sets for data science roles in computational science (or scientific computing) and computational engineering (except EDA):
+ skill set:
	- Imagine a super-resolution microscope so easy to use that anyone on earth can take high-resolution images of bacteria, proteins, cells, and even genes. Imagine the same microscope accelerating cancer research, pharmaceutical development, and virus identification by empowering scientists with real-time, nanoscale imagery of cells and proteins. We’re building this technology at ONI!
	- Our aim is to make super-resolution imaging so easy and the insights so impactful that it becomes widely used by scientists and leads to radical discoveries and innovations. To achieve this goal, our platform will automate every stage of the workflow, integrating the Nanoimager, a microfluidics device (Roboflow) and an online analysis package (CODI) with next-gen super-resolution assays and application-specific microfluidic consumables. 
	- We will soon be launching our first kit which is designed for extracellular vesicles. With one click, the kit captures, images and analyses these tiny particles allowing researchers to characterise their biomarkers for the first time. We are also developing a revolutionary new super-resolution technology called Every Molecule Counts (EMC) that will give our customers a new level of confidence about the biology underlying their images. EMC technology will be integrated into all of our future consumables.
	- To drive all this innovation we have built a world-class R&D team with colleagues from disciplines including biology, computer science, mathematics and physics. A core principle behind our work is to be detailed in our thinking, but to make products that are simple and intuitive so they can be put in the hands of anyone irrespective of experience, background, or training. We are excited to welcome new team members who share these values and who are excited by our vision.
	- Currently employing a diverse team of 120+ people, representing over 40 nationalities, ONI is in a period of rapid growth. We closed our $75m Series B round at the start of 2022, led by ARCH Ventures and Casdin Capital, putting our post money valuation at c. $225M, to drive the development of our next generation of products. What we have achieved so far is just the beginning and we are always looking for passionate people to join us on this journey.
	- We are looking for a Senior Software Engineer who will support the team that develops and deploys leading edge data analysis tools and solutions to the users of our microscope. You will develop creative methods that can extract information from these results and use software engineering skills to consolidate these methods into usable tools. This role is a fantastic opportunity to work on ground breaking applications for super-resolution and single-molecule microscopy as well as to help shape this new diverse team.
	- As an experienced member of the team, you will be at the heart of developing new tools and techniques for super-resolution data analysis. You will take a leading role working closely with Data Scientists, Software Engineers, Application Development Scientists, and Hardware R&D, to create a bridge between the back-end and the data science team, mentoring, coaching and empowering data scientists, and helping provide infrastructure and system support to help supercharge the work of the data science team, and increase the delivery of code into the cloud platform.
	- Drive communications between data scientists and backend cloud developers to deliver fast and effective research tools
	- Be confident in applying domain specific knowledge to the development of algorithms.
	- Have a strong background in mathematics, probability, and statistics.
	- Be able to work independently or as part of  a team. 
	- Be comfortable with ambiguity and complexity, thrive in group discussions and be an able communicator with colleagues with diverse technical backgrounds.
	- Have a passion for learning, working in a team, teaching others & making an impact on the world.
	- Greater than 4 years experience as a software developer in a professional environment.
	- Have an academic background (BSc, MSc, or PhD) in Computer Science, or STEM related fields. 
	- Strong familiarity with the python ecosystem, and available tools, and best practices of python in the context of both Data Science and delivery of production code. 
	- Strength in packaging using pypi, and optionally conda, or alternatives, including compiled C++ accelerated bindings (Pybind11) across platforms.
	- Strength and familiarity using Cmake to build C++ projects across Linux and Windows platforms. 
	- Strong understanding of C++ ecosystem, standards, and comfortability navigating and learning unknown code-bases. 
	- Strength in maintenance and establishment of CI/CD workflows for python and C++, including integration testing and deployment, using common solutions such as Github Actions or CircleCI.
	- Experience utilizing Docker to enhance development, testing, and deployment.
	- Sound knowledge of basic mathematics and statistics concepts
	- Confidence communicating between scientists and backend cloud developers
	- GPU acceleration using CUDA.
	- MLOps; with deployment and or training in cloud infrastructure, or deployment of C++ compiled models in real-time.
	- Image processing experience. 
	- Strength in development and implementation of computationally intensive numerical algorithms. 
	- Experience with cloud and web-based solutions and toolings, including tools such as Django, Kubernetes, Celery, RabbitMQ, on platforms such as GCP, or AWS
+ skill set:
	- ONI is looking for a highly motivated and creative scientist to join our Applications Development team as a Data Scientist. The Applications Development team invents new, cutting-edge molecular biology techniques based on single-molecule microscopy, and converts them into integrated bioware products that enable our users to access the full potential of super-resolution imaging. This work spans a diverse range of biological and technological fields, firmly founded in advanced fluorescence microscopy.
	- The work of the Applications Development team demands creative, innovative ways to process and analyse imaging data of many formats - including localisation-based point clouds, single-particle tracking information, and pixel-based data. As the Data Scientist within the team, you will be responsible for identifying and developing the most effective and efficient ways to analyse data generated in the course of our developmental work. In the process you will bring new analysis tools into the company, and design bespoke tools that are unique to ONI. The insights that we need to extract from our data are highly dynamic and change according to the state of each project, and so this role is well-suited to a talented Data Scientist who likes solving problems creatively and rapidly. 
	- This role is open to candidates able to work in our Oxford, UK office and will involve interaction with scientists at both of our research centres. You will work across several research projects involving multiple development teams, and fulfil a vital function within the company by pushing the boundaries of the insights we can derive from our imaging technology.
	- Take a leading role in defining how data are analysed within Applications Development’s product- and technology-development process. This is through the development of new analytical methods to unlock previously unobtainable information within our data; advising on the most appropriate analytical approaches to take; and transferring the most cutting-edge analyses into the team
	- Directly handle the processing of some data collected by the wider team as part of our ongoing developmental work
	- Assist and support other members of the team in their data analysis 
	- Bridge ONI’s Applications Development and Software teams to ensure that requirements and strategies are fully understood by scientists with diverse biology and computing backgrounds
	- Contribute to ONI’s ongoing research projects as an integral and intellectually-invested team member
	- Contribute positively to ONI’s mission beyond your immediate work - e.g. providing technical advice/support to the wider company, promoting an exciting work culture etc.
	- Significant expertise in image analysis methods, particularly single-molecule methods
	- Experience of data analysis using Python
	- Understanding of super-resolution or single-particle imaging methods
	- Some laboratory experience in cell biology, molecular biology, or biochemistry
	- A passion and hunger for improvement, and inability to settle for average
	- Ability to be motivated by the success of a team, and derive fulfilment from enabling others
	- Strong cultural alignment to ONI and excellent people skills
	- Excellent record-keeping and data-handling skills
	- Ability to work to a high standard as part of a dynamic team
	- Ability to think independently and take initiative
	- Practical experience in sample preparation for fluorescence microscopy
	- Domain knowledge in a specific area of biology or biochemistry; preferably immunology, extracellular vesicle biology, oncology, virology, neurology, developmental biology, genetics/epigenetics, or pathology
	- Experience of other relevant data-handling resources; e.g. MatLab, R etc. 
	- Obtained a higher qualification (BSc, MSc, PhD, or equivalent) in a relevant area of science
	- Worked in an environment for with significant focus on imaging analysis methods (in either academia or industry)
	- Solved challenging data science problems in creative and innovative ways
	- Worked as part of a close team to deliver ambitious results together
	- Contributed creatively and intellectually to the success of your projects












###	Data Engineering



####	Notes about Data Engineering


Data engineering roles involve creating *Big Data* extract, transform, load (ETL) pipelines, and provide infrastructure support to help data scientists obtain insights from processing huge amounts of data.

They address:
+ readiness of data (sets)
+ format of data sets
+ resilience of infrastructure support for information systems
+ scaling of infrastructure support for information systems
+ security of infrastructure support for information systems

They support databases for:
+ operational data stores
+ data marts
+ data lakes
+ (enterprise) data warehouses, DW, DWH, or EDW




####	Skill Sets for Data Engineering

***Skill sets for data engineering***:
+ Experience with Scala, Scalding, Luigi, Hive, machine learning pipelines and model training is a plus
	- Luigi:
		* [The Enterprise-Ready Micro Frontend Framework](https://luigi-project.io/)
			+ "Luigi helps you to build modularizable, extensible, scalable and consistent UIs and Web Apps."
			+ "Create a unified user experience around your complex functionality in a distributed development environment."
			+ "Build administration and business User Interfaces using Luigi and discover its benefits."
		* [***Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.***](https://github.com/spotify/luigi)
			+ "Luigi is a Python (3.6, 3.7, 3.8, 3.9 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more."
			+ "The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else."
			+ "There are other software packages that focus on lower level aspects of data processing, like Hive, Pig, or Cascading. Luigi is not a framework to replace these. Instead it helps you stitch many tasks together, where each task can be a Hive query, a Hadoop job in Java, a Spark job in Scala or Python, a Python snippet, dumping a table from a database, or anything else. It's easy to build up long-running pipelines that comprise thousands of tasks and take days or weeks to complete. Luigi takes care of a lot of the workflow management so that you can focus on the tasks themselves and their dependencies."
			+ "You can build pretty much any task you want, but Luigi also comes with a toolbox of several common task templates that you use. It includes support for running Python mapreduce jobs in Hadoop, as well as Hive, and Pig, jobs. It also comes with file system abstractions for HDFS, and local files that ensures all file system operations are atomic. This is important because it means your data pipeline will not crash in a state containing partial data."
				- Hive:
					* ["The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive."](https://hive.apache.org/)
						+ "The Hive DDL operations are documented in Hive Data Definition Language."
						+ "The Hive DML operations are documented in Hive Data Manipulation Language."
						+ ["The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax."](https://cwiki.apache.org/confluence/display/Hive//Home)
							- "Tools to enable easy access to data via SQL, thus enabling data warehousing tasks such as extract/transform/load (ETL), reporting, and data analysis."
							- "Hive is not designed for online transaction processing (OLTP) workloads. It is best used for traditional data warehousing tasks."
							- "Hive is designed to maximize scalability (scale out with more machines added dynamically to the Hadoop cluster), performance, extensibility, fault-tolerance, and loose-coupling with its input formats."
				- Pig:
					* ["Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets."](https://pig.apache.org/)
						+ At the present time, Pig's infrastructure layer consists of a compiler that produces sequences of Map-Reduce programs, for which large-scale parallel implementations already exist (e.g., the Hadoop subproject).
						+ Pig's language layer currently consists of a textual language called Pig Latin, which has the following key properties:
							- Ease of programming. It is trivial to achieve parallel execution of simple, "embarrassingly parallel" data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.
							- Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.
							- Extensibility. Users can create their own functions to do special-purpose processing.
				- ["The ***Cascading Ecosystem*** is a collection of applications, languages, and APIs for developing data-intensive applications."](https://www.cascading.org/)
			+ ["Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more."](https://luigi.readthedocs.io/en/stable/index.html)
				- "The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else."
			+ From https://qconnewyork.com/ny2015/system/files/presentation-slides/MattWilliams%20-%20qcon2015%20-%20luigi.pdf:
				- Python module to help build complex pipelines
					* dependency resolution
					* workflow management
					* visualization
					* hadoop support built in
				- Doesn’t help you with the code, that’s what Scalding (scala), Pig, or anything else is good at.
				- It helps you with the plumbing of connecting lots of tasks into complicated pipelines, especially if those tasks run on Hadoop.
				- Luigi doesn’t replace Hadoop, Scalding, Pig, Hive, Redshift. It orchestrates them.
				- Core beliefs of Luigi:
					* Should remove all boilerplate code.
					* Be as general as possible.
					* Be easy to go from test to production code.
	- Scalding:
		* ["Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs."](https://twitter.github.io/scalding/)
			+ https://github.com/twitter/scalding
			+ "Scalding is built on top of Cascading, a Java library that abstracts away low-level Hadoop details."
			+ "Scalding is comparable to Pig, but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs."
+ skill set:
	- Investigate the feasibility of applying scientific principles and concepts to business problems.
	- Understand the ***Goodreads/Amazon data structures (MySQL/Data Lake/Redshift)***.
	- Acquire data by building the necessary ***SQL ETL queries***.
	- Import processes through various company specific interfaces for RedShift and Data Lake storage systems.
	- Analyze data for trends and input validity by inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies.
	- Build models using statistical modeling, mathematical modeling, econometric modeling, network modeling, social network modeling, natural language processing, machine learning algorithms, genetic algorithms, and neural networks.
	- Validate models against alternative approaches, expected and observed outcome, and other business defined key performance indicators.
	- Develop metrics to quantify the benefits of a solution and influence project resources. Partner with Engineering/Data Engineering to improve the quality of existing data and bring additional data sources in line. Audit metric data and measure project progress and success. Build/automate reports/dashboards (in Tableau) that allow the business leaders to get a clear snapshot of their operations. Design and analyze A/B tests to quantify impact of customer-facing changes. Develop innovative experimental design and measurement methodologies to understand customer growth and business efficacy. Participate in discussions, team planning, office hours, and metric reviews. Design and implement scalable and reliable approaches to support or automate decision-making throughout the business. Communicate insights to the business partners, Goodreads leadership, and Amazon stakeholders, with an emphasis on clarity, completeness, and actionability.
+ skill set:
	- Senior Data Engineer (US Remote Available)
	- The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.
	- The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible to build and scale the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineer and infrastructure teams). Strong technical experience within enterprise software is essential.
	- Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
	- Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
	- Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
	- Identify and document standard methodologies, standards, and architecture guidelines
	- Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
	- 7+ years of data architecture related experience such as data analysis, data modeling, and data integration.
	- Experience with GTM and customer success business processes and applications
	- ***Experience in custom ETL design, implementation, and maintenance***
	- Knowledge of programming languages (e.g. Python and Object Oriented Programming)
	- Hands-on experience with SQL database design
	- Experience working on CI/CD processes and source control tools such as Github and GitLab
	- Experience working Snowflake and relational databases
	- ***Extensive hands on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake***
	- Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
	- Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
	- Passionate about data solutions, technologies, and frameworks
+ skill set:
	- Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
	- 3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
	- Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
	- Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
	- Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
+ skill set:
	- You know how to work with data engineering technologies like Spark, no SQL DB or Lambda
	- You know everything there is to know about Robotic Process Automation
	- A minimum of 7 years experience in deep learning, machine learning or artificial intelligence applications like virtual agent, RPA, or video/image/text analytics
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift) is a plus
+ skill set:
	- Our data infrastructure team is responsible for all things data — our data warehouse, Hadoop, Redshift, Spark, Kafka, Airflow and so on.
	- Deep experience with MySQL, NoSQL data stores like HBase or similar.
	- Strong understanding of Unix/Linux variants, web network protocols, persistence solutions
+ skill set:
	- Python/Java Developer (Datagens)
	- LAB  SYSTEM & DATA ENGINEER
	- Are you interested in being part of a small, diverse team passionate about building technologies that change the way people learn?  Splunk Education is looking for a Lab and Data Engineer to join our Education Technologies Team. Our team is focused on building systems that help users learn to use our products in innovative ways.
	- Do you love tackling interesting problems and coming up with clean, stable solutions that delight users? We need to talk.
	- Opportunity to grow as an engineer. This role will provide a constant stream of new things to learn, providing the opportunity to expand your current knowledge and deep dive into new technologies.
	- Growth. We strongly believe in growing team members through ownership and leadership opportunities.
	- We pride ourselves on a collaborative, open and supported work environment.
	- The ability to work from home, or one of our many offices across the globe.
	- Participate in design and development of projects, either independently or in a team.
	- Be self-sufficient and take ownership of seeing projects through to successful conclusions.
	- Work with a diverse team of experts in education, video production, security and IT infrastructure.
	- Tasks include creating Terraform and Ansible playbooks to provision lab instances, along with building data generators that mock machine data across many technologies. 
	- B.S. degree in Computer Science or related field.
	- 8 years of experience
	- Deep knowledge of Python, Ansible, Terraform, Java, and AWS technologies.
	- Ability to read, understand and reproduce machine logs.
	- Experience with Docker, Kubernetes, GIT, and Splunk.
+ skill set:
	- Senior Software Engineer
	- We are seeking a passionate engineer to join our group, Data Platform. Our team designs distributed systems to collect and analyze high volumes of machine-generated data at scale. We are proud of owning what we build even after it's deployed to production. We ensure code hygiene, use open source libraries, employ continuous integration and delivery, and have a strong belief in automated testing at multiple levels (unit, integration, system). We are uniquely positioned as a globally distributed team with team members in a variety of locations. 
	- Develop and debug client-server system software written in C++ and/or Golang
	- Experience in distributed systems and large scale environments deployed at scale, both "on-premise" and in "cloud".
	- Experience with Linux deployments hosted by cloud service providers such as AWS and GCP.
	- Excellent problem solving, collaboration and communication skills, both verbal and written.
	- Mentored junior engineers in their development skills via code reviews and design discussions.
	- Owned features or sub-systems end-to-end from design to deployment and continuous improvement..
	- Develop server-side applications for data collection, indexing, clustering and other distributed systems.
	- Build robust, fault-tolerant distributed systems in a multi-threaded, multi-process environment.
	- Analyze, identify and resolve the bottlenecks of distributed systems, data pipeline, multi-threaded coherency and other complicated scenarios.
	- Analyze and improve the scalability of data collection, storage and retrieval.
	- Interact cross-functionally with other partners such as PMs, SREs, Devops, and support engineers.
	- Participate in rotating on-call duties to diagnose and fix customer issues.
+ skill set:
	- Principal Technical Program Manager - GDI (Remote - US)
	- We are looking for a Principal Technical Program Manager, who will be responsible for leading large-scale, complex programs from start to end within our GDI (getting data in) area. You will be working cross-functionally with a broad set of technical and business partners to drive programs that will further Splunk’s long range goals. We want a TPM who can thrive with a nebulous problem set, distill complex problems into concrete work, and guide multiple teams with their strong, critical thinking skills.
	- Strategically leading technical programs in the Platform Engineering organization, with a clear and constant understanding of priorities to drive expected outcomes, including New Product Introduction (NPI) and Go To Market (GTM) processes
	- Breaking down problems and operationalizing initiatives into coherent workstreams; defining program timelines and ensuring accountability of program goals; operating as the single source of truth in execution of highly complex programs
	- Proactively anticipating risks, developing mitigation plans, and driving problems to resolution
	- Aligning with organizational leadership and key business customers as a program lead across multiple programs
	- Understanding system interdependencies and facilitating technical discussions; communicating solutions and decisions with both engineers and non-technical audiences
	- Creating and maintaining detailed and easy-to-digest program documentation, managing dependencies, and tracking status across multiple teams and workstreams
	- 10+ years experience in software program management or closely related roles
	- Experience with enterprise software and cloud technologies, along with a deep understanding of the software development lifecycle
	- Understanding and experience with creating and consuming APIs
	- Experience working with or knowledge of Data Ingestion messaging or stream processing technologies, such as Flink, Kafka, Kinesis, Pulsar, Spark or similar
	- Extensive experience with large, cross-functional programs that have broad organizational impact
	- Demonstrated ability at juggling multiple, concurrent programs and prioritizing tasks based on criticality
	- Communication and risk management are your forte. You can communicate effectively with stakeholders of all levels and develop plans to act upon risks appropriately
	- Phenomenal at decision-making, consensus building, and problem solving; you don't shy away from challenges
	- Strong technical proficiency required to understand development tasks and identify and resolve issues as they arise
	- Understanding of key development tools and technologies such as Gitlab, AWS, Kubernetes, Terraform, etc. Experience with program management tools in the Atlassian stack (Jira and Confluence) are great
+ skill set:
	- Data Engineer, DevOps (US Remote Available)
	- The Analytics & Data Platform team (ADP) is responsible for Splunk’s data platform from ingestion to visualization. This platform enables our business partners to make the best data-driven decisions possible. We’re looking for a Data Engineer to join the team and contribute to the development and adoption of a true, self-serve, Data Mesh platform, with a focus on automation and observability. This position is responsible for the development, maintenance, and support of the data platform (including on-call rotations), partnering with all levels of customers in the course of support/adoption/migration activities, and actively participating in the growth and development of the ADP team and its capabilities and processes.
	- Operate: Perform the day to day updates, changes, and scheduled activities on the data platform. All the while, being attentive to opportunities for automation.
	- Support: Ensure SLAs are met by monitoring the data platform and responding to issues as they arise. Partner with other engineers or dependent teams as needed to resolve issues.
	- Develop: SQL and Python will be your go to tools, but by no means is that all. You’ll have a broad range of technologies that you’ll need to wrestle into submission to be successful: Terraform, Gitlab, Airflow, K8, Docker, DBT, and Linux, just to name a few.
	- Collaborate: Members of the ADP team are expected to work together to continuously improve the team’s processes, infrastructure, codebase, etc. Asking questions, challenging ideas, and recommending alternatives are all necessary for the team to continue to grow and improve. Passionate opinions are welcome.
	- Innovate: Develop creative solutions to edge cases that are causing unnecessary complications, with a focus on maintainability and scalability.
	- 4+ years of work experience in a relevant field (Data Engineer, DevOps Engineer, Backend Engineer, etc.)
	- Experience with data warehousing technologies (Redshift, Snowflake, BigQuery, or similar)
	- Proficient SQL skills and strong experience working with relational databases
	- Proficiency in a major programming language (ideally Python)
	- Excellent code and repo hygiene
	- Past experience as a user of Splunk
	- Experience with or a strong interest in containerization and/or Kubernetes
	- Experience with declarative configuration tools such as Terraform
	- Past experience contributing to an open source project
	- Passionate about technology with an insatiable curiosity for learning new things
	- B.S. degree in computer science, mathematics, statistics or a similar quantitative field, or sufficient relevant experience
+ skill set:
	- Director, Engineering
	- This Director for Engineering will lead critical services within our Data Processing umbrella. Success will be measured by your ability to build and lead your team as part of our overall engineering strategy. You will seamlessly integrate within our deeply innovative culture and contribute to our excellent record for delivering market leading solutions. You are a seasoned leader with a demonstrated history building and leading both established and emerging engineering talent, while encouraging an environment of inclusion, creativity, and innovation.
	- Our Data Processing team is a dynamic technology group with a mission to be the primary data processing path for any type of data transformation and routing activity in near real-time. If you possess a passion for extraordinary technology leadership and embrace the challenge of working at the frontier of what is possible in the industry today, then this position is for you. We are building state-of-the art capabilities, real-time messaging and streaming systems, support tools, and automation instrumentation that will greatly impact how our customers successfully use data to improve their businesses performance, scalability, profitability, and market strategies.
	- We are looking for a proven, seasoned engineering leader to lead streaming capabilities for the Data Processing team. You will build and lead an elite team of engineers who will be working on our streaming platform that will power the next generation of Splunk and enable ever deeper customer insights. You are a force multiplier who looks for ways to gain efficiencies for greater impact. You will influence the technical strategy and the roadmap to best serve our customers.
	- Hire and grow a new team within the Data Processing Engineering organization, focused on making our industry leading technology even more valuable to our customers.
	- Be an advocate of scalable and extensible, recoverable, manageable architecture for Core products and services.
	- Work with senior leadership on business goals, resource requirements and influence technical strategy
	- Diagnose and resolve systemic obstacles that prevent your team from delivering high-quality software.
	- Champion an atmosphere of continuous improvement by serving as a coach, mentor, and technical advisor for senior managers, managers and engineers.
	- Plan and support career development.
	- Recruit and retain top talent.
	- Masters or PhD in Computer Science or Engineering with 15+ years of industry experience, of which 10+ years is in Engineering Management.
	- 3+ years of experience managing multiple managers/teams on working on solutions to deeply challenging problems in processing data at massive scale.
	- Experience hiring and cultivating teams.
	- Experience developing new products, either in small companies or within the context of larger organizations. A mix of start-up and bigger company experience is a plus.
	- Experience working within geographically distributed organizations.
	- Experience building and cultivating strong engineering practices and processes.
	- Consistent track record of delivering scalable, high performance, and high quality software systems.
	- Broad understanding of various cloud development technologies and trends for enterprise-scale, distributed systems.
	- Strong technical acumen, creativity, interpersonal skills, and emotional intelligence.
+ ***[Apache Airflow, Luigi](https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7), workflow management system (WMS), Azkaban, [Open Source Data Pipeline – Luigi vs Azkaban vs Oozie vs Airflow](https://www.bizety.com/2017/06/05/open-source-data-pipeline-luigi-vs-azkaban-vs-oozie-vs-airflow/), [Pinball](https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8), Airbnb Airflow vs Apache Nifi***
	- ***Jenkins vs Airflow. Jenkins is an open source continuous integration tool written in Java.***
+ Exposure to big data systems like Hadoop, Spark, Kafka, etc.
+ skill set:
	- Understanding and experience with NoSQL such as MongoDB or Neo4j
	- Experience with the Hadoop ecosystem (HBase, MapReduce, Hive/Pig) or Spark
+ skill set:
	- Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Python, Elastic Search, Storm, Kafka, Oozie WFs etc. in both an on premise and cloud deployment model to solve large scale processing problems
	- Design, build and maintain Big Data workflows/pipelines to process billions of records into and out of our data lake
	- Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security
	- Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
	- Keen understanding of big data and parallelization accompanied with a stellar record of delivery
	- Experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred), AWS Glue
	- Experience with on-premises to cloud migrations including re-hosting, re-platforming and re-factoring
	- Experience with orchestration template technologies such as AWS CloudFormation
+ skill set:
	- Architect and operate high quality, large scale, multi-geo data pipelines that drive business decisions.
	- Redesigned data pipelines using the applicable DBR features, and incorporating external tools where necessary to have better reliability and tighter SLAs.
	- Established conventions or new APIs for logging feature usage for PM use-cases.
	- Understandable SLAs for each of the production data pipelines.
	- Improved test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests.
	- CI and deployment processes and best practices for the production data pipelines.
	- Reduction in overall alert noise and increase responsiveness by rethinking the current alert categories and priorities.
	- Design schemas for financial, sales and support data in the data warehouse.
	- Experience building, shipping and operating multi-geo data pipelines at scale.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Excellent communication (writing, conversation, presentation) skills, consensus builder
	- Strong analytical and problem solving skills
	- Passion for data engineering and for enabling others by making their data easier to access.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- If you are looking for an unparalleled opportunity to build the next generation big data processing platform, and learn how to launch hundreds of thousands of VMs a day at scale while running thousands of Kubernetes clusters, you have come to the right place. The platform team builds and manages the core systems powering Databricks, allowing it to seamlessly scale and run across various geographic regions/clouds, and making Databricks the go-to product for big data processing in the cloud.
	- You will be a senior software engineer responsible for architecting scalable systems to power Databricks, making it the de-facto platform for running Big Data and AI workloads. You will build and extend the Databricks cloud platform, which is based on a micro service architecture and includes systems for managing thousands of Kubernetes clusters at scale, systems for streaming and consuming gigabytes of log data per minute, onboarding and managing thousands of data scientists on Databricks, scalable API gateway, rate limiting framework, network security and encryption, build infrastructure (we use Bazel), and scalable CI/CD framework among many others.
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python or Javascript and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with internal APIs.
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- To achieve this, we build data reporting pipelines that support the underlying pricing infrastructure supporting tens to hundreds of millions of DBUs (Databricks Units) across multiple clouds and regions, UIs that allow Databricks administrators to view and manage their bill, and APIs and integrations to downstream processors to handle payments for all customers.
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
	- Unified Analytics Platform
+ skill set:
	- Our team drives state-of-the-art, open source Delta Lake project bringing reliable, scalable, ACID transactions to Apache Spark and other Big Data engines. Our mission is to deliver a robust and performant engine that enables users to build reliable data pipelines that ingest massive data volumes, optimize data layout, generate metadata and evolve data schemas all while guaranteeing transactional correctness and high query performance.
	- Build the core features that make Delta Lake the world's best Big Data storage abstraction in terms of performance, stability, security and scalability.
+ [***Delta Lake Community***; Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.](https://delta.io/)
+ skill set:
	- You will build tools and features to make Databricks the best place for large-scale enterprise R workloads.
	- Improve state of distributed R computing through Apache Spark and R integration on Databricks
	- Implement new features on Databricks platform for R users (e.g., ACL)
	- Improve and extend Databricks R notebooks to satisfy R users' use cases and requirements
	- Implement new R-based APIs on Databricks platform (e.g., secret management API)
	- Expand Databricks workspace through integration with third-party tools such as RStudio and Shiny.
	- Integrate critical packages from the R ecosystem into Databricks Runtime
	- Provide engineering support and thought leadership to Databricks field engineering teams on R
	- Give talks and write blog posts about R on Databricks
+ skill set:
	- You revel in building features quickly and iterating in a data-driven fashion
	- You lay awake thinking about improving the design, implementation and maintenance of large software systems with millions of users
	- Passion to hack social commerce
	- Data & Relevancy engineers work on our massive semi-structured datasets. They have domain experience in data mining, information retrieval, or machine learning, and a strong system orientation. Key product initiatives include product feed relevance, ad targeting, information extraction, and recommendations.
	- Infrastructure engineers scale a massive, highly-available platform end-to-end. They design distributed systems, validate performance, factor in security, and proactively monitor every corner of our stack. When things do go wrong, they are on-hand to fight the fires.
+ skill set:
	- Experience with data processing frameworks and data warehouses such as Hadoop, Spark, Redshift
	- Experience with designing, implementing, and optimizing ETL in Pentaho
+ skill set:
	- Technical fluency in one language and tool such as Python, Java or Scala, AWS (S3/EMR/Athena/Glue) and SQL.
	- Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow.
	- Experience working with either a MapReduce system of any size/scale.
+ skill set:
	- Minimum 3 years of designing, building and operationalizing large scale enterprise data solutions and applications using one or more of Azure / AWS / GCP data and analytics services in combination with custom solutions -  Spark, Azure Data Lake, HDInsights, SQL DW, DocumentDB, Search, Elastic Pool etc.  
	- Minimum 3 years experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on AZURE.
+ skill set:
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
+ skill set:
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- Demonstrates knowledge of the data engineering domain with experience in building and supporting non-interactive (batch, distributed) or real-time, highly available data, data pipelines.
	- Able to build fault tolerant, self-healing, adaptive computational pipelines
	- Contribute to the decision-making process related to the selection of software solutions that make up the architecture
+ skill set:
	- Minimum 3+ years of architecting, implementing and successfully operationalizing large scale data solutions in production environments using Hadoop and NoSQL ecosystem on premise or on Cloud (AWS, Google or Azure) using  many of the relevant technologies such as  Nifi, Spark, Kafka, HBase, Hive, Cassandra, EMR, Kinesis, BigQuery, DataProc, Azure Data Lake etc.  
	- Minimum 2+ years of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale and others
	- Minimum 3+ years of architecting data and building performant data models at scale for Hadoop/NoSQL ecosystem of data stores to support different business consumption patterns off a centralized data platform  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
	- Minimum 3++ years of architecting and industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scale
	- Minimum 2+ years of experience implementing large scale BI/Visualization solutions on Big Data platforms
	- Minimum 3+ years of experience implementing large scale secure cloud data solutions using AWS data and analytics services e.g. S3, EMR, Redshift
	- Minimum 2+ years of experience implementing large scale secure cloud data solutions using Google data and analytics services e.g. BigQuery, DataProc
	- Minimum 2+ years of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for modern data platforms that use Hadoop and NoSQL on premise or on AWS, Google and Azure cloud
	- Minimum 2+ years of experience securing Hadoop/NoSQL based modern data platforms on-premise or on AWS, Google, Azure cloud
	- Minimum 2+ years of Re-architecting and rationalizing traditional data warehouses with Hadoop or NoSQL technologies on premise or transition to AWS, Google clouds
	- Experience implementing data wrangling and data blending solutions for enabling self-service solutions using tools such as Trifacta, Paxata
	- 4 years industry systems development and implementation experience OR Minimum of 3 years of data loading, acquisition, storage, transformation, and analysis
	- Minimum 2+ years of using Talend, Informatica like ETL tools within a Big Data environment to perform large scale metadata integrated data transformation
	- Minimum 1+ years of building Business Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies
	- Architect modern data solutions in a hybrid environment of traditional and modern data technologies such as Hadoop, NoSQL
	- Create technical and operational architectures for these solutions incorporating Hadoop, NoSQL and other modern data technologies
	- Implement and deploy custom solutions/applications using Hadoop/NoSQL
	- Lead and guide implementation teams and provide technical subject matter expertise in support of the following:
	- Designing, implementing and deploying ETL to load data into Hadoop/NoSQL
	- Security implementation of a Hadoop/NoSQL solutions
	- Managing data in Hadoop/NoSQL co-existing with traditional data technologies in a hybrid environment
	- Troubleshooting production issues with Hadoop/NoSQL  
	- Performance tuning of a Hadoop/NoSQL environment
	- Architecting and implementing metadata management solutions around Hadoop and NoSQL in a hybrid environment
+ skill set:
	- Experience with Apache Big Data technologies such as Hadoop, Spark, Hive, Flink, Kafka, Beam etc
		* Apache Hadoop: for distributed computing
		* Apache Spark:
			+ open-source unified analytics engine for large-scale data processing
			+ large-scale data analytics
		* Apache Hive:
			+ [data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage](https://hive.apache.org/)
				- [The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.](https://cwiki.apache.org/confluence/display/HIVE)
			+ [Apache Hive is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale.](https://aws.amazon.com/big-data/what-is-hive/)
			+ data warehouse software project built on top of Apache Hadoop for providing data query and analysis
		* Apache Flink:
			+ open-source unified stream-processing and batch-processing framework
			+ distributed streaming data-flow engine
			+ for dataflow programs, executed in a data-parallel and pipelined manner (or task parallel manner)
			+ pipelined runtime system that enables the execution of bulk/batch and stream processing programs
		* Apache Kafka:
			+ distributed event store and stream-processing platform
			+ provide a unified, high-throughput, low-latency platform for handling real-time data feeds
			+ provides the Kafka Streams libraries for stream processing applications
			+ [open-source distributed event streaming platform used by thousands of companies for](https://kafka.apache.org/):
				- high-performance data pipelines
				- streaming analytics
				- data integration
				- mission-critical applications
				- ["event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time."](https://kafka.apache.org/intro)
		* Apache Beam: for defining and executing (via distributed processing back-ends) data processing pipelines for:
			+ ETL
			+ batch processing
			+ stream processing = continuous processing
	- Experience with messaging systems such as ActiveMQ
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Backend development experience with a strong interest in work involving data pipelines, distributed systems, performance analysis, and/or large-scale data processing Experience with software engineering practices (e.g. unit testing, code reviews, design documentation)
	- Able to take on complex problems, learn quickly, and persist towards a good solution
	- Experience designing fault-tolerant distributed systems
	- Experience with data pipelines
	- Experience with Hadoop or other MapReduce-based architectures
	- Experience with Kafka, Druid or other Streaming Compute based technologies is a plus
	- Experience with ad tech is a plus
+ tech stack:
	- Netflix culture resonates with you.
	- You can communicate effectively with experts of all backgrounds.
	- You are an expert analyst and can pick up any tool (e.g. Tableau, D3) to get the job done.
	- You dream in SQL and Python (or other similar languages).
	- You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Expertise in SQL, programming (e.g. Python, Scala), ETL and data warehousing concepts at scale (TBs of data)
+ Expertise in broad technical skills spanning data access, data storage, data processing, and data visualization.  Skills include: SQL, logical / semantic data modeling, ETL and data warehousing concepts, programming languages (Python)
+ Build data tooling to enable data lake, data warehouse, and analytics workflows within the AWS cloud (S3, Redshift, DynamoDB, Spark, Kinesis, Kubernetes, etc.)
+ experience with API design
	- REST
	- OpenAPI Specifications
	- GraphQL
+ Spark and/or other big data architectures (Hadoop, MapReduce) in high-volume environments
+ ***Experience with large-scale, distributed data processing frameworks (e.g., Spark, Kafka, YARN, Tachyon, Mesos, etc.) is a plus***
	- Apache Spark:
		* open-source unified analytics engine for large-scale data processing
		* open-source unified engine for large-scale data analytics
	- Apache Kafka:
		* distributed event store and stream processing platform
		* open-source distributed event streaming platform, for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications
	- YARN, Apache Hadoop YARN:
		* split up functionalities of resource management and job scheduling/monitoring into separate daemons, via global resource manager (RM) and per-application application master (AM)
		* not yarn package manager
	- Tachyon
		* from Alluxio Inc., previously Tachyon Nexus
			+ improve Apache Spark performance
			+ reliable memory-centric distributed storage system
			+ Alluxio: open-source virtual distributed file system, VDFS
				- in the big data analytics stack, Alluxio is the data asbtraction layer in between the computation frameworks and (multiple) storage systems, which is accessible via a common interface
				- APIs for:
					* Hadoop HDFS
					* Amazon S3
					* FUSE
			+ open-source, distributed, fault-tolerant, in-memory file system to enable data sharing across frameworks and perform operations at memory speed
		* not open-source component library
		* not the parallel/multiprocessor ray tracing software
	- Mesos
+ Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
+ tech stack:
	- Well-versed in one or more of the following languages and functional programming in general: Scala, Java, Python, JavaScript
	- Expert in SQL and comfortable designing, writing and maintaining complex SQL based ETL.
	- Experience with building large-scale batch and real-time data pipelines; ETL design, implementation, and maintenance.
	- Experience with schema design and data modeling, and the analytical skills to QA data and identify gaps and inconsistencies.
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift)
+ ***Programming skills sufficient to extract, transform, and clean large (multi-TB) data sets in a Unix/Linux environment.***
+ big data tools and stream-processing systems: Hadoop, Spark, Storm, Spark-Streaming
+ Extensive experience manipulating and analyzing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
+ skill set:
	- As the Principal Data Engineer on the Platform team at Zignal Labs, you will get to use your Scala and Java experience to build a best-in-class distributed data and analytics infrastructure by leveraging open source technologies such as Apache Spark, Apache Storm, and Elasticsearch.  We use social media, news, blogs and other media sources to empower our users with key insights based on real-time analysis.
	- Solve complex real-time data collection & analysis problems with cutting edge technical solutions
	- Iterate on our high performance and scalable platform for massive data collection, real-time analytics, NLP, machine learning, and backend data services
	- Build high performance, scalable, real-time, server-side technologies
	- Write scalable code with extensive test coverage, working in a professional software engineering environment with source control, dev/stage/production release cycles, continuous integration, and deployment
	- Work closely with product management, design, quality assurance and operations teams to understand our customers’ needs and effectively translate them to technical specifications
	- Lead projects from translating product requirements into architecture to production
	- Tech Stack:
		* Scala, Java, Python
		* Apache Spark, Spark Streaming, Databricks/Delta Lake, Apache Storm, Elasticsearch, Apache Nifi
		* Kafka, MongoDB, Redis
		* AWS
	- Bachelor's degree (or higher) in Computer Science, Engineering, or similar and/or relevant work experience
	- Experience providing technical leadership at the enterprise level for the design of information technology systems
	- Crafted and implemented operational data stores, as well as data lakes in production environments
	- Ability to analyze, diagnose and resolve complex architectural problems using industry standard engineering principles
	- Design and build data ingestion pipelines and ETL processing, including stream processing, while factoring in performance and cost
	- Identify and solve issues concerning data management to improve data quality
	- Clean, prepare and optimize data for ingestion and consumption
	- Experience solving performance problems with Lucene based search solutions like Elasticsearch or Solr
	- 9+ years experience in server-side/back-end full cycle product development in a production environment
	- 4+ years developing with Apache Spark, including Structured Streaming.   Experience with Databricks is a big plus
	- Knowledge of Scala or Java with exposure to or interest in Scala
	- Leads and mentors other team members
	- Provides partners with coaching and feedback in order to build effective teams
	- Provides effective support to cross-functional teams
+ skill set:
	- OLAP datastores:
		* Druid
		* ClickHouse
		* Scio
		* Flink
		* Spark
		* BigQuery
		* Parquet
		* Databricks
+ skill set:
	- As Scale's Analytics Engineer, you will spearhead building Scale's analytical and business-intelligence infrastructure. Scale's customers process millions of tasks through our APIs, and we're looking for a talented Analytics Engineer to build scalable solutions to support this growth. You will have widespread purview, with responsibility for understanding, mining, aggregating, and exposing data across the entire business to support timely and efficient decision-making and data exploration. You will also implement Scale's data warehouse, data mart, and business intelligence reporting environments, and help users transition their workflows to these systems.
	- Work with operations, finance, and engineering to drive the development of pipelines that provide single-source-of-truth foundational accuracy
	- Continually improve ongoing data pipelines and simplify self-service support for business stakeholders
	- Perform regular system audits, and create data quality tests to ensure complete and accurate reporting of data/metrics
	- 3+ years of relevant work experience in a role requiring application of data modeling and analytic skills
	- Ability to create extensible and scalable data schema and pipelines that lay the foundation for downstream analysis using SQL and Python
	- Experience with ETL tools and building / maintaining a data warehouse and data pipelines using tools such as DBT
	- Partner with operations and sales teams to automate manual workflows
	- Experience in using highly scalable data engineering technologies such as DBT, Airflow 
	- Experience in best practices in table partitioning/data sharding strategies and query optimization
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $148,800 – $178,560. Compensation packages at Scale include base salary, equity, and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Scale employees are also granted Stock Options that are awarded upon board of director approval. You’ll also receive benefits including, but not limited to: Comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous PTO. Additionally, this role may be eligible for additional benefits such as a commuter stipend.
+ skill set:
	- Foursquare’s flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK); data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Studio).
	- Foursquare’s Marketers Engineering team writes and operates the software which produces core data sets for our Marketers suite of products. These petabyte-scale pipelines process geospatial data for the purposes of marketing use cases, such as ad targeting and attribution. It’s critical to this team’s success that we have rich data sets to build our applications on top of, and this data is kept fresh, easy to explore, and simple to make changes. The engineers on this team work closely with application engineers to prove out variant approaches and introduce new functionality.
	- In the Data Software Engineer role, you will ship products with high visibility and strategic importance to Foursquare and contribute directly to the revenue. Our pipelines are written in a variety of programming languages and deployed to multiple orchestration platforms. The main technologies we work with are Spark, Amazon EMR, Ruby, Java, and Apache Airflow.
	- Write and operate the data pipelines which produce Foursquare’s core data sets for our Marketers suite of products – Attribution and Targeting.
	- Document the expected and actual behavior of these pipelines, along with expectations for inputs and outputs.
	- Monitor data quality and freshness, with a focus on proactively evaluating the business impact of changes. Report regularly on the state of the data sets and the software which produces them.
	- Maintain a prioritized list of data questions and bugs which require further investigation. Escalate as needed to call attention to problems with the input data.
	- Evaluate new sources of data, and build new pipelines that combine our data in creative ways that drive customer value.
	- Participate in on-call rotation duties to ensure that data is correct and produced on-time, and to restore service when the pipelines are experiencing an outage.
	- 2-4 years of software development experience.
	- Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines.
	- Strong algorithms and data structures knowledge.
	- Professional experience scripting with the Unix/Linux command line or Python.
	- Experience with cloud computing service providers, such as AWS.
	- Experience with ***containerization technologies, such as Docker, Mesos or Kubernetes***.
	- Excellent written communication skills.
	- ***Your own unique talents! If you don’t meet 100% of the qualifications outlined above, we encourage and welcome you to still apply!***
	- Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI.
	- Experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform.
	- Experience with geospatial data processing.
+ skill set:
	- Foursquare’s flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK);  data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Unfolded Studio).
	- Our Data Platform team provides the infrastructure, tools, libraries, and APIs that power our data processing infrastructure. The goal of the team is to make engineers at Foursquare more effective and happier by offloading common problems from the product and content experts, enabling them to apply their skills and knowledge to their specific domain, while the Data Platform team works on problems that affect the company as a whole.
	- In this role, you will ship products with high visibility and strategic importance to Foursquare and contribute directly to revenue. Help us build and collaborate with Product, Engineering, and Data Science teams to create tools and processes to bring research and machine learning models to production.
	- Build tools and APIs that would be used by other FSQ Engineers
	- Build and maintain Foursquare's event streaming platform, Framework, and applications for data ingestion
	- Build resilient services and tooling which drive all of our offline processing of petabytes of data
	- Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production
	- Participate in on-call rotation duties
	- What you’ll need: If you have more or less experience than listed, please apply anyways and we will see if another role aligns better with your experience.
	- BS/BA in a technical field such as computer science or equivalent experience
	- 3+ year of experience in software development working with production-level code
	- Experience in one or more of the programming languages we use
	- Excellent communication skills, including the ability to identify and communicate data-driven insights
	- Experience with working in the cloud, preferably AWS
	- Strong algorithms and data structures knowledge
	- Comfort with Unix/Linux and the command line
	- Experience with Hadoop, Kafka, MapReduce, and/or Spark
	- Experience with AWS data processing services (EMR, Glue, Athena, …)
	- Experience with relational or document-oriented database systems
	- Prior software internship experience
	- ***Languages: Java, Scala, Python, Clojure, Ruby***
	- ***Tools for pipeline orchestration: Airflow, Luigi***
	- ***Frameworks: Spark, MapReduce, Scalding, Spring Boot***
	- ***Infrastructure: AWS, Hadoop, Kafka, Kubernetes, Docker***
	- ***Other technologies: Postgres, Hive, HBase, MongoDB***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















###	DataOps


####	Notes about DataOps

DataOps is a set of practices, processes and technologies that combines an integrated and process-oriented perspective on data with automation and methods from agile software engineering to improve quality, speed, and collaboration and promote a culture of continuous improvement in the area of data analytics.

DataOps incorporates the Agile methodology to shorten the cycle time of analytics development in alignment with business goals.

DevOps focuses on continuous delivery by leveraging on-demand IT resources and by automating test and deployment of software. This merging of software development and IT operations has improved velocity, quality, predictability and scale of software engineering and deployment. Borrowing methods from DevOps, DataOps seeks to bring these same improvements to data analytics.

DataOps utilizes statistical process control (SPC) to monitor and control the data analytics pipeline. With SPC in place, the data flowing through an operational system is constantly monitored and verified to be working. If an anomaly occurs, the data analytics team can be notified through an automated alert.

DataOps is not tied to a particular technology, architecture, tool, language or framework.

Tools that support DataOps promote:
+ collaboration
+ orchestration
+ quality
+ security
+ access
+ ease of use



The volume of data is forecast to grow at a rate of 32% CAGR to 180 Zettabytes by the year 2025. 

DataOps seeks to provide the ***tools, processes, and organizational structures*** to cope with this significant increase in data. ***Automation streamlines the daily demands of managing large integrated databases,*** freeing the data team to develop new analytics in a more efficient and effective way. ***DataOps seeks to increase velocity, reliability, and quality of data analytics. It emphasizes communication, collaboration, integration, automation, measurement and cooperation between data scientists, analysts, data/ETL (extract, transform, load) engineers, information technology (IT), and quality assurance/governance.***




DataOps leadership principles:
+ Establish progress and performance measurements at every stage of the data flow. Where possible, benchmark data-flow cycle times.
+ Define rules for an abstracted semantic layer. Ensure everyone is "speaking the same language" and agrees upon what the data (and metadata) is and is not.
+ Validate with the "eyeball test":
	- Include continuous-improvement -oriented human feedback loops.
	- Consumers must be able to trust the data, and that can only come with incremental validation.
+ Automate as many stages of the data flow as possible, including:
	- BI
	- data science
	- data analytics
+ Using benchmarked performance information, identify bottlenecks and then optimize for them. This may require investment in commodity hardware, or automation of a formerly-human-delivered data-science step in the process.
+ Establish governance discipline, with a particular focus on two-way data control, data ownership, transparency, and comprehensive data lineage tracking through the entire workflow.
+ ***Design process for growth and extensibility. The data flow model must be designed to accommodate volume and variety of data.*** Ensure enabling technologies are priced affordably to scale with that enterprise data growth.”









####	Skill Sets about DataOps

Skill sets for DataOps:
+ Demonstrated track record working with data warehouse concepts.
+ skill set:
	- Experience with automation tools and configuration-as-code (CloudFormation, Ansible, Puppet, Chef, Vagrant, etc.)
	- Experience working with either AWS or GCP services such as compute, databases, VPCs, networking, permissioning and storage
+ skill set:
	- Big Data platforms e.g. Cloudera, Hortonworks MapR
	- Big Data Analytic frameworks and query tools such as: HDINsight, Spark, Storm, Hive, Impala
	- IoT protocols, gateways, queues, messaging hubs such as IoT Hub, MQTT, XMPP, CoAP, etc.
	- IoT development experience on at least one of the industry leading platforms (Azure IoT, AWS IoT, GE Predix, Siemens Mindsphere, PTC Thingworx, SAP Leonardo, GCP)
	- Streaming data tools and techniques such as Apache Kafka, Azure Streaming Analytics, AWS Kinesis
+ skill set:
	- Minimum 1 year of building and coding applications using at least two Hadoop components – MapReduce, HDFS, Hbase, Pig, Hive, Spark, Scoop, Flume, etc
	- Minimum 1 year coding one of the following: Python, Pig programming, Hadoop Streaming, HiveQL
	- Minimum 1 year understanding of data modelling & data pipeline design: iterative data pipeline development from raw, curated, integrated to published data, with fit for use data modelling on Hadoop and NoSQL platforms
	- Minimum 1 year of experience implementing large scale cloud data solutions using Cloud Service Providers:  AWS data services (e.g. EMR, Redshift, GLUE) or Azure (Data Lake Store/Analytics, SQL Data Warehouse) or Google Cloud Platform Google Cloud (Big Data:  Big Query, Big Insights)
	- Minimum 1 year of experience delivering an operational Big Data solution using one or more of the following technologies: Hadoop, HortonWorks, Cloudera, Cassandra
	- Minimum 1 year of experience throughout the SDLC of a Hadoop implementation technologies including HortonWorks, Cloudera, Hive, Pig, MapReduce 
	- Minimum 1 year of experience throughout the SDLC of a HortonWorks, Cloudera, Cassandra / Hbase implementation 
	- Minimum of a Bachelor's Degree or 3 years IT/Programming experience
	- Minimum 1 year of experience developing REST web services
	- Industry experience (financial services, resources, healthcare, government, products, communications, high tech)  
	- Experience leading teams
	- Machine Learning tools, interfaces & Libraries: R, R-Studio, Spark R, sparklyr, MLlib, H2O etc.  
	- Experience with other tools, databases and Apache projects: Google BigQuery, Presto, Drill, Kylin, OpenTSDB, Spark Streaming
	- Enterprise data integration, BI and analytics platforms: Informatica, Talend, InfoSphere, SAS, RevoR, QlikView, Qlik Sense, Tableau, Spotfire, D3.js
	- Processing frameworks & programming tools: Spark (Scala/Python/Java), Kafka, Flink
	- Client facing skills: ability to build trusted relationships with client stakeholders and act as a trusted adviser
+ skill set:
	- Partner with engineering leadership to buildout data driven roadmap items to address performance in critical areas
	- Established performance test environments and frameworks
	- Experience evangelizing performance engineering techniques within a data driven engineering culture
	- Deep hands on experience with JVM tuning techniques
	- Supported efforts in performance testing and improvement in common JavaScript frameworks (Angular, React, JQuery)
	- Experience with Ruby (JRuby) and JavaScript
	- Extremely well versed in solving data access performance challenges across SQL data stores
	- Experience in AWS and other cloud providers when exploring different approaches to performance engineering
	- Experience with distributed architectures
	- Passionate about driving a performance engineering culture
+ skill set:
	- Our leaders have had significant involvement in the creation and maintenance of NumPy, SciPy, Jupyter, Spyder, Dask, Conda, Numba, Anaconda and PyData NumFOCUS.
	- We are seeking a fully remote, experienced Open Source Infrastructure Engineer to join our team at Quansight. In this role, you will support Quansight’s growing cloud and on-premises infrastructure and help make them more reliable, scalable, and efficient. You will also  address support issues from our clients and collaborators, explore emerging technologies in the Cloud and DevOps spaces, and design and implement cloud computing systems with the rest of our infrastructure team.
	- Contribute to nebari (https://nebari.dev), an open source Data Science platform built on JupyterHub, Dask, and other tools from the PyData ecosystem.
	- Participate in upstream open source communities we rely on (such as JupyterHub, BinderHub, Dask, etc.) in partnership with the established leaders of those communities.
	- Deploy and ensure the reliable operation of Quansight’s and clients’ infrastructure.
	- Collaborate with a fully distributed team - team members are expected to communicate and collaborate proactively to allocate effort and maximize the team’s impact.
	- ***Experience using some form of infrastructure-as-code tooling (i.e., Ansible, Salt, Puppet, Terraform, etc.)***
	- Experience with at least one major cloud platform (AWS, Azure, GCP)
	- Experience developing tools in a general purpose programming language (eg. Python)..
	- ***Experience with Continuous Integration and Continuous Delivery services (e.g., Circle CI, GitHub Actions)***
	- Familiarity with software engineering best practices – including unit tests, code review, version control, production monitoring, etc.
	- ***Experience deploying and developing with Linux container-based technologies, such as Docker and Kubernetes.***
	- Comfortable working independently and reaching out for feedback and support as needed.
	- Experience collaborating and coordinating work via online platforms, such as GitHub, GitLab, or BitBucket, and distributed revision control.
	- Ability to provide and constructively receive feedback.
	- While this is a remote position, we are looking for candidates with significant time overlap with US Central and Eastern time zones due to the location of many of our infrastructure team members and collaborators
	- Experience working on geographically distributed open-source projects.
	- ***Exposure to the Python Data Science stack - Pandas, Numpy, Dask, etc.***
	- Experience with the Jupyter ecosystem and other tools for interactive computing.
	- Experience building and maintaining continuous deployment pipelines.
	- ***Experience with common data science methods, platforms, workflows, and infrastructures; with data management systems, practices, and standards; and the capacity to gain familiarity with new related topics.***













































##	Applied Machine Learning & Data Science Roles in Other Domains




For the ***transportation industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***management consulting market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











For the ***K-12 (kindergarten to grade 12) education market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.














For the ***higher education market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- Experience using native APIs from higher ed core systems (SIS, ERP, LMS) a plus
		* MIS, such as: ***student information systems, SIS***, student management systems, school administration software, school administration system
		* enterprise resource planning, ERP
			+ business intelligence
				- customer relationship management, CRM, customer services
				- sales:
					* invoicing
					* order placement
					* order scheduling
					* shipping
			+ e-commerce, electronic commerce
				- product lifecycle management, PLM
					* planning
					* optimizing manufacturing capacity and material resources
					* manufacturing resource planning, MRP
						+ material requirements planning, MRP
				- supplier relationship management, SRM
					* maximize cost savings with support for the end-to-end procurement and logistic processes
			+ enterprise asset management
				- corporate performance and governance
				- human resource
			+ industrial distribution, logistics, supply chain management, SCM
			+ accounting
				- financial operations
				- regulatory compliance
		* learning management systems, LMS
	- 3+ years' experience with enterprise level data integration working with multiple systems simultaneously; Including extracting data utilizing API integration from a variety of platforms, performing data mapping, data transformation, and loading data to the target system
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








For the ***real estate market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***waste management market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











For the ***health care industry***, including health care informatics, health care analytics, health care management and hospital management, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







For the ***media industry***, including mass media and social media, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








For the ***hospitality industry***, including the tourism market, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






For the ***telecommunication industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***electric power industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
















For the ***construction industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







For the ***fashion industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***entertainment industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***music industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***manufacturing industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***regulatory compliance market***, and regulatory enforcement and inspection/auditing market, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




























#	Corporate Research Labs



+ Toyota Research Institute, TRI





##	EDA Research







###	EDA + Related Research with Alibaba Group


+ Using HW/SW Mechanisms to Improve Performance of remote Heterogeneous Systems
	- Alibaba is an e-commerce and AI company. We generate enormous data and consume huge amount of computation and storage resources every day. It is critical for Alibaba to keep on improving data center design given the emerging of powerful accelerator computation clusters.
	- We would focus on:
		* 1. Analyze different AI workloads in distributed GPU clusters, study their computation and network requirement
		* 2. Based on current remote accelerator technique, improve its efficiency via hardware/software solutions
		* 3. Apply the technique to real workload
	- Requirement:
		* 1. PHD candidate, experienced with distributed heterogeneous systems
		* 2. It's a plus if candidate worked with deep learning algorithms
		* 3. it's a plus if candidate has top conference publications
+ Last mile of datacenter as a computer -- local protocol and semantics based ASIC/FPGA cloud
	- Developers and customers prefer to use heterogeneous compute resources with a set of local server access protocol and semantics. We need to find talents to do research and prototyping with a specific local API on an ASIC or FPGA chip.
+ Emerging Accelerator Architecture, Programming Model, and Optimizations
	- The emerging hardware accelerator architectures, such as process-in-memory (PIM) and neuromorphic computing,  have shown great potential to speed up AI/ML and data-heavy applications. This research aims to investigate these non-traditional architecture designs and their performance implications for domain-specific applications in Alibaba datacenters and ecosystem. It will study the emerging architecture's programming model for usability and explore the software-hardware co-design strategies (e.g. reinforcement learning based architecture space exploration, architecture-aware compression and sparsity exploitation) and optimization trade-offs to maximize the performance.
+ Execution engine optimization based on GPUs and other modern hardware
	- Targeting Maxcompute SQL engine, we'd like to import modern hardware technology (such as GPU, FPGA etc) to model and improve the core operators of the distributed execution engine, optimize the system performance on specific scenes at last.
+ Performance/Power/Area (PPA) Modeling & Analysis
	- The ***Computing Technology Lab of Alibaba Damo Academy*** focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Domain Specific Architecture
	- As the end of Dennard's scaling and Moore's Law running out of steam, the traditional architecture for general-purpose processors can no longer meet the requirements of high performance and low energy consumption for various emerging applications. To allow the computing to have higher performance/energy efficiency, Domain Specific Architecture (DSA) has become a popular solution. However, there are many challenges in the DSA design. For example, the definition of the scope of Domain, trade-off between specialization and general-purpose, instruction set design, compiler design and optimization, memory wall, ultra-low-power design, micro-structure design and optimization, etc. This internship Project is a thorough and detailed study of the DSA to address these challenges.
	- The Computing Technology Lab focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Cloud Server Architecture
	- Perform profiling/modeling and evaluation of workloads for our cloud server, design and optimize server architecture including but not limited to: CPU, cache/memory, storage and accelerators.
+ Research on algorithms/architectures of the next-generation AliNPU for training
	- AliNPU targeting for neural network training is a key component of Alibaba's AI Chip strategy. To design an architecture surpassing the best of the AI training chips, such as GPU and TPU, we must look into all aspects from algorithms to HW architecture, for the potential computational efficiency improvements.
	- The works may focus on one or a few of the following directions：
		* 1. Algorithm innovations that may improve the system efficiency, and the experiments.
		* 2. The analysis of the theoretic bounds and/or the proof with regards to the algorithm innovations.
		* 3. Experimental HW architecture designs, simulations and their PPA analysis.
+ Hyper-scale cloud datacenter's compute resource pool and management platform prototype
	- Compute pools will be widely deployed in hyper-scale cloud datacenter. Alibaba Infrastructure AI Ops Platform (TIANJI) team is now actively seeking talents to work on research in this area.
+ Research on optimization of AI accelerator
	- Nowadays high performance computing has become one of the hot topics of AI research. The research aims on optimizing power dissipation and energy efficiency of AI accelerators targeting various of AI applications, providing high quality computation support for AI applications.
	- The research topics include:
		* 1. Research on computation pattern of various AI applications to look for bottleneck
		* 2. Research on AI accelerator architectures, implementations to improve performance and energy efficiency
		* 3. Codesign AI accelerator (SW/HW) and application to maximize the performance of accelerator)
+ Accelerating Machine Learning Applications on Heterogeneous Computing Architectures
	- This research aims to optimize ML applications on heterogeneous accelerators such as GPUs, FPGAs, and/or ASICs. S/he will conduct analysis and exploration on various performance bottlenecks in the full software/hardware stack, including ML algorithm improvement, model level transformation (e.g. compression, sparsity, data parallelization), and domain-specific architecture innovations, in order to dramatically boost the ML application's performance.






##	Computer Architecture Research




***Resources***
+ [HiPEAC](https://www.hipeac.net/)
	- For computer architecture, compiler design, and related areas.
	- [HiPEAC Jobs](https://www.hipeac.net/jobs/#/)





































##	Hardware Security Research














##	Formal Verification Research & Logical AI Research


***Resources***:
+ [SAT Live!](https://www.satlive.org/)















##	Machine Learning Research & Deep Learning Research







###	Machine Learning Research with Alibaba Group

+ Building an innovative and systematic AI benchmarks platform
	- Currently in Alibaba Group, deep learning and related applications have been employed in various business departments. Tmall, Alitrip, Taobao, Ant Financial and other departments are making extensive use of emerging deep learning technologies to continuously improve application and algorithms and enhance the consumer experience. On the one hand, Alibaba's engineering teams design, experiment and deploy different deep learning algorithms and applications every day. On the other hand, deep learning requires a lot of computational power, which also puts higher requirements on the computational power of the hardware and their adaptability to the application. How to balance the demand and supply relationship between these two and integrate the solution into a systematic platform product? How to automatically and systematically evaluate the computional power of an AI hardware? How to evaluate the advantages and disadvantages of a hardware for usage in an application and give customer recommendations through a systematic platform? These are the challenges we are currently dealing with and we need to solve. Recently we have launched the AI Matrix product (through aimatrix.ai website), but it is still in the early stage of the product. In the future, we need more people who have the same understanding as us and are willing to involve in solving these problems. Let's contribute our own strength and make the AI Matrix as an effective systematic platform and an impactful technical brand.
	- [AI Matrix](https://aimatrix.ai/en-us/)
	- https://github.com/alibaba/ai-matrix









##	Computer Vision Research









##	Compiler Design & Program Analysis (or Software Analysis) Research














##	Quantum Computing Research





###	Quantum Computing Research with Alibaba Group

+ Quantum Algorithm for Near-term Quantum Devices
	- A general-purpose fault-tolerant quantum computer will require millions of physical qubits and millions of quantum gate operations. With quantum computers of significant size now on the horizon, we should understand how to best exploit the initially limited abilities and how to develop and run useful quantum algorithms within the limited circuit depth of intermediate size quantum devices with limited error correction.
+ Research in Practical Applications of Quantum-Safe Communication
	- Quantum communication may refer to quantum cryptography, quantum teleportation, and quantum entanglement. Among those, quantum key distribution (QKD) is one of the most practical applications in recent years. Quantum cryptography takes the advantage of the laws of quantum physics to protect data,
	- Currently, the most significant problems in practical quantum cryptography systems include: high-speed quantum random number generation, long-distance fiber quantum key distribution with high key generation speed, co-fiber transmission of classical and quantum optical signals, as well as practical commercialization and stabilization.
	- Our project aims to study these critical issues in quantum cryptography system for practical applications. Due to the transmission loss and dark count, the bottleneck for its practical application lies in the trade-off between high speed key generation rate and long transmission distance. In order to solve these problems, one potential solution is to design more efficient telecommunication protocol to exceed the theoretical up bound of the generation rate. Meanwhile, the project will also focus on the study and practical solutions for quantum random number generation, post-quantum cryptography algorithm, the migration of classical and quantum networks, etc












#	Academic/University Research Labs


##	U.S. Academic/University Research Labs




+ Johns Hopkins University
	- Johns Hopkins University Applied Physics Laboratory, Johns Hopkins APL












