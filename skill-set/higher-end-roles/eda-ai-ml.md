#	EDA, AI, (Applied) Machine Learning, MLOps, ModelOps, Data Science, Data Engineering, DataOps, MIS, & Corporate Research Labs



##	Important Information about Innovation Management


Important [technology roadmaps](https://en.wikipedia.org/wiki/Technology_roadmap) to pay attention to, so that I can create my own:
+ Semiconductor pathfinding for research and development (R&D) activities
	- References:
		* [Semiconductor Pathfinding and Development](https://www.thermofisher.com/us/en/home/semiconductors/pathfinding.html)
			+ Directed from Thermo Fisher Scientific -> Applications & Techniques -> Industrial & Applied Sciences -> ***Semiconductor Analysis*** -> Applications -> Semiconductor Pathfinding and Development
				- https://www.thermofisher.com/us/en/home/semiconductors.html#applications
			+ Use cases:
				- [NEXS Software, CAD navigation and design debug solutions](https://www.thermofisher.com/us/en/home/electron-microscopy/products/software-em-3d-vis/nexs-software.html) 
				- Circuit edit and nanoprobing
					* [use of high-resolution focused ion beams (FiBs) and advanced chemistry to perform "nanosurgery" on semiconductor devices](https://www.thermofisher.com/us/en/home/semiconductors/circuit-edit.html)
				- defect localization and analysis
				- physical and chemical characterization
+ additional resources for:
	- environmental scanning
		* horizon scanning, horizon scan
			+ trend analysis
			+ issue tree, logic tree
			+ scenario planning, scenario thinking, scenario analysis, scenario prediction, scenario method
			+ morphological analysis, or general morphological analysis, in the context of problem solving
		* analyze the [***market environment*** and ***business environment***](https://en.wikipedia.org/wiki/Market_environment)
	- determine scientific lacuna, or knowledge gaps in science and ***engineering***
	- technological readiness assessment, TRA
		* determine the [technological readiness level, TRL](https://en.wikipedia.org/wiki/Technology_readiness_level)
			+ basic technology research, TRLs 1-2
			+ research to prove feasibility, TRLs 2-4
			+ technology development, TRLs 3-5
			+ technology demonstration, TRLs 5-7
			+ system/subsystem development, TRLs 6-9
				- system test, launch, and operations, TRLs 8-9
	- [technological innovation system](https://en.wikipedia.org/wiki/Technological_innovation_system)
		* Entrepreneurial activities
		* Knowledge development
		* Knowledge diffusion / knowledge exchange through networks
		* Guidance of the search
		* Market formation
		* Resource mobilization
		* Support from advocacy coalitions
	- ethics in EDA and machine learning research
		* [differential technological development](https://en.wikipedia.org/wiki/Differential_technological_development)
		* [proactionary principle](https://en.wikipedia.org/wiki/Proactionary_principle)
		* [precautionary principle, or precautionary approach](https://en.wikipedia.org/wiki/Precautionary_principle)
		* [postcautionary principle](https://en.wikipedia.org/wiki/Postcautionary_principle)
	- list of research labs, such as corporate research labs, research universities, and non-profit research institutes, that perform [***exploratory engineering***](https://en.wikipedia.org/wiki/Exploratory_engineering)
	- [technology forecasting](https://en.wikipedia.org/wiki/Technology_forecasting)
		* [technology scouting](https://en.wikipedia.org/wiki/Technology_scouting)
			+ part of [corporate foresight](https://en.wikipedia.org/wiki/Corporate_foresight)
			+ improves [***competitive intelligence***, CI](https://en.wikipedia.org/wiki/Competitive_intelligence) to improve competitive strategy
		* part of [Technology management](https://en.wikipedia.org/wiki/Technology_management)
			+ part of [engineering management](https://en.wikipedia.org/wiki/Engineering_management)
			+ technology strategy
			+ technology forecasting
				- technology scouting
			+ technology roadmap
				- map technologies to business and market needs
			+ technology project portfolio
			+ [innovation management](https://en.wikipedia.org/wiki/Innovation_management)
				- Knowledge management tools
				- Market intelligence techniques
				- Cooperative and networking tools
				- Human resources management techniques
				- Interface management approaches
				- Creativity development techniques
				- Process improvement techniques
				- Innovation project management techniques
				- Design and product development management tools
				- Business creation tools
	- technology assessment
	- ***cross-impact analysis***
	- [frugal innovation/engineering](https://en.wikipedia.org/wiki/Frugal_innovation)
		* jugaad, or jugaar, Hindi for a stop-gap solution
		* inclusive innovation
		* catalytic innovation
		* reverse innovation
		* bottom of the pyramid (BOP) innovation
			+ bottom of the income pyramid
			+ bottom of the wealth pyramid





##	EDA, Electronic Design Automation


Skills for EDA software development, and other high-end software development:
+ Production quality coding standards and patterns.
+ Build system experience, like:
	- Apache Buildr, historic open-source build system, Rake-based, gives the full power of scripting in Ruby with integral support for most abilities wanted in a build system
	- ***Bazel***
		* For automating the building and testing of software.
		* Derived from the Google internal tool, Blaze.
		* for multiple programming languages
		* Build systems most similar to Bazel are:
			+ Pants
			+ Buck
			+ Please
	- Blaze, predecessor to Bazel
	- boost.build, for C++ projects, cross-platform, based on Perforce Jam
	- Buildout, a Python-based build system for creating, assembling and deploying applications from multiple parts
	- ***CMake***
	- Gradle, for JVM software, and C and C++
	- ***Jenkins***, an extensible continuous-integration engine, forked from Hudson
	- language-specific build systems:
		* Ant and Maven for Java
			+ ***Apache Ant***, popular for Java platform development and uses an XML file format
			+ ***Apache Maven***, a Java platform tool for dependency management and automated software build
		* ***Leiningen*** for Clojure
		* ***sbt*** for Scala
	- ***Maven*** (for JVM software, and other languages, such as C\# and Ruby)
	- ***Meson*** is a software tool for automating the building (compiling) of software. 
	- ***SCons***, Python-based, with integrated functionality similar to autoconf/automake
	- Stack, a tool to ***build Haskell projects***, manage their dependencies (compilers and libraries), and for testing and benchmarking
	- tinyrick, a ***Rust build tool***
	- ***Travis CI***, a hosted continuous-integration service
	- ***Waf***, a Python-based tool for configuring, compiling and installing applications. It is a replacement for other tools such as Autotools, Scons, CMake or Ant
	- Turbo, or Turborepo, for building JavaScript or TypeScript Web applications
+ project management tools:
	- Working with Jira and Confluence a plus.
		* Jira, for issue tracking.
		* Confluence, wiki-based collaboration platform
+ CI/CD with:
	- ***Docker***, software platform for container orchestration by exploiting OS-level virtualization
	- ***Kubernetes***, for container orchestration
	- AWS
	- continuous integration (CI) systems, deployment tools/platforms, such as:
		* ***Jenkins***
		* TeamCity
		* GitHub Actions
		* GitLab
		* Circle CI
		* Terraform
		* Saltstack/Ansible
		* Semaphore
		* Slurm
	- ***CI/CD pipelines***
	- microservice architecture
	- infrastructure as code
+ configuration management:
	- software configuration management includes:
		* revision/version control
		* build automation
		* system configuration
		* process management
		* environment management
		* defect tracking
	- Experience with configuration management systems (Ansible and/or Puppet, Saltstack)
		* ***Ansible***, software tool suite to enable infrastructure as code, Python-based
		* CFEngine
		* Chef, Ruby-based
		* LCFG
		* ***Nagios Core***, or ***Nagios***, for monitoring systems, networks, and infrastructure
		* NixOS Declarative configuration model
		* OpenMake Software Release Engineer
		* Otter
		* ***Puppet***, for software configuration to specify system configuration, Ruby-based
		* Rex, Perl-based
		* Salt, Python-based
		* ***Saltstack***, for event-driven IT automation, remote task execution, and configuration management
+ tools for agile methods, such as XP and Scrum:
	- Git
	- JIRA
	- Confluence
+ package managers:
	- Mamba
		* reimplementation of *conda* package manager in *C++*.
+ documentation
	- *LaTeX*
	- Markdown
		* MDX markup language, authorable format for writing JSX in Markdown documents
			+ JSX, or JavaScript Extension, or JavaScript XML
				- similar in appearance to HTML
+ skill set:
	- Full ownership including: Designing, Implementing, Testing and Metric Analysis.
	- Production quality coding standards and patterns.
+ Hibernate ORM is an object-relational mapping tool for the Java programming language
	- object-relational mapping allows software developers to convert data between type systems using object-oriented programming languages, OOPL.
+ ***Where possible, exploit [incremental computing](https://en.wikipedia.org/wiki/Incremental_computing), to speed up the performance of EDA tools that I develop.***
	- use "checkpoint"s to save temporary results of computing
		* This allows results from computation performed thus far to be reused.
		* If computation crashes and has to be restarted from the most recent or second last checkpoint, this checkpoint provides intermittent results that the software can use to resume computing.
+ skill set:
	- Strive for high code standards (continuously improving testability and code quality).
	- Disciplined, methodical, minimalist approach to design and construct layered software components that can be embedded within larger frameworks or applications.
	- ***experiment driven development***
+ Proven capability to create maintainable, adaptable software that is non-brittle and capable of change
+ Take pride in the quality of the code you write. Your code is readable, testable, and understandable six months later. You adhere to the Zen of Python.
+ experience with these software testing methodologies:
	- unit tests
	- integration tests
	- regression tests
	- smoke tests
	- load tests
	- chaos tests
+ Software libraries
	- C++ libraries:
		* Boost C++
		* http://doc.hc2.ch/c_cpp/en/cpp/links/libs.html
		* Eigen, C++ library for numerical computing
	- ***Python libraries***:
		* NetworkX, for graph computing
		* NumPy, for numerical linear algebra and tensor algebra
		* SciPy
			+ for scientific computing, or computational science, and computational engineering
			+ for the following:
				- optimization
				- linear algebra
				- integration
				- ODE solvers
				- interpolation
				- FFT
				- signal processing
				- image processing
		* mpmath, for arbitrary-precision floating-point arithmetic
		* Biopython, for computational biology and bioinformatics
		* CuPy, for GPU-accelerated parallel programming
		* Distributed Evolutionary Algorithms in Python, DEAP, framework for evolutionary computing to enable rapid prototyping and experimentation to test ideas
		* MuJoCo
			+ advanced simulator for multi-body dynamics with contact
			+ https://mujoco.org/
			+ https://github.com/deepmind/mujoco
	- for symbolic computing
		* SageMath, computer algebra system, CAS
			+ SageManifolds
			+ has bindings for C++ and Python
	- numerical computing:
		* GNU Octave
		* MATLAB, and Simulink
		* FreeMat
		* Intel oneAPI Math Kernel Library, Intel oneMKL
			+ formerly Intel Math Kernel Library, Intel MKL
		* Eigen, C++ library for numerical computing
+ parallel and distributed computing
	- parallel computing
		* parallel data structures
		* parallel algorithms
		* ***Knowledge of parallelism in shared (Intel TBB, OpenMP) and distributed (Intel MPI, Apache Spark, Dask) memory***
		* Knowledge of parallelism in shared memory:
			+ Intel TBB
			+ OpenMP
			+ Dask, for Python
			+ SCOOP, Scalable Concurrent Operations in Python, for Python
		* Knowledge of OpenCL/SYCL languages
			+ OpenCL, Open Computing Language
				- low-level API, and parallel computing/programming framework and run-time for heterogeneous platforms of:
					* general-purpose processors
					* graphics processors
					* digital signal processors
					* FPGAs
					* domain-specific architectures, including hardware accelerators
				- compile and execute kernel programs (kernels) in parallel in computer systems with heterogeneous system architecture (HSA)
				- enables GPGPU computing
				- speeds up numerical computing, and computation for applied machine learning and data science
				- has two APIs, application programming interfaces:
					* platform layer API
					* runtime API
			+ SYCL:
				- higher-level programming model for improving programming productivity on hardware accelerators
				- single-source embedded domain-specific language (eDSL) based on pure C++17
				- royalty-free, cross-platform abstraction layer for developing software that are executed on heterogeneous platforms
				- single-source C++ programming model for heterogeneous computing
		* GPU programming, GPGPU, using:
			+ NVIDIA CUDA
				- NVIDIA cuDNN
			+ OpenCL
	- distributed computing
		* distributed data structures
		* distributed algorithms
		* Knowledge of parallelism in distributed memory:
			+ Intel MPI
			+ Apache Spark
			+ ***Dask***
+ workflow management:
	- goals/tasks:
		* for data engineering pipelines
	- use ***workflow management systems, WfMS, WFMS*** for specific applications, such as:
		* data science
		* machine learning
		* Avoid the use of workflow management systems, WfMS, WFMS, for business process modeling and other activities not related to my projects.
	- Adobe Workfront, with built-in workflow management systems, WfMS, WFMS
	- ***Apache Airflow***
		* Cloud Composer, for Google Cloud Platform, GCP
	- Apache Flink
	- Apache Taverna ???
	- Azkaban
	- Collective Knowledge, CK
	- Cuneiform programming language
		* based on Erlang, functional programming language
	- Jenkins
	- Jira, with built-in workflow management systems, WfMS, WFMS
	- [***Luigi***](https://github.com/spotify/luigi), for workflow management and managing ML pipelines (machine learning pipelines)
	- Oozie
	- research workflow
		* The Collective Knowledge (CK) project is an open-source framework and repository to enable collaborative, reproducible and sustainable research and development of complex computational systems. CK is a small, portable, customizable and decentralized infrastructure helping researchers and practitioners
	- Salesforce.com Process Workflow
+ program analysis tools:
	- PerfView
		* CPU, memory, garbage collection
+ 














###	EDA: Electronic Design Automation Job Opportunities







####	Integrated Device Manufacturers (IDMs)


IDMs that may have their own EDA software develoment group:
+ [From Wikipedia, list of integrated device manufacturers (IDMs)](https://en.wikipedia.org/wiki/Integrated_device_manufacturer)
+ [From Wikipedia, list of semiconductor IP core vendors](https://en.wikipedia.org/wiki/List_of_semiconductor_IP_core_vendors)
+ [From Wikipedia, list of the top semiconductor companies in terms of the most revenue](https://en.wikipedia.org/wiki/Semiconductor_industry)









####	EDA Companies


List(s) of EDA companies:
+ Electronic System Design Alliance (ESD Alliance) members: https://www.semi.org/en/communities/esda/membership-directory
	- SEMI: http://www.semi.org/en/Membership/MemberDirectory
+ Silicon Integration Initiative, Inc.: https://si2.org/member-directory/
+ Accellera Systems Initiative: https://www.accellera.org/about/members
+ Global Semiconductor Alliance: https://www.gsaglobal.org/membership/
	- Their membership directory cannot be accessed, unless you are an employee of their member companies.
	- This is a list of fabless IC design companies.
+ Semiconductor Research Corporation: https://www.src.org/src/member/roster/
+ Semiconductor Industry Association: https://www.semiconductors.org/about/members/
+ MIPI Allliance: https://www.mipi.org/membership/all-member-directory
+ SNIA: https://www.snia.org/member_com/member_directory
+ TechJobsCafe: https://techjobscafe.com/employer_top.php
+ defunct resources:
	- list from SEMATECH.




Additional companies that hire EDA software developers:
+ [Achronix Semiconductor Corporation](https://www.achronix.com/company/careers)
+ BLAH






##	Companies in the Semiconductor Industry


List of semiconductor companies:
+ [Global Semiconductor Alliance](https://www.gsaglobal.org/)
+ FPGA companies
	- AMD/Xilinx
	- Intel Altera
	- [Lattice Semiconductor](https://www.latticesemi.com/)
	- [QuickLogic Corporation](https://www.quicklogic.com/company/careers/)
		* Not friendly to non- U.S. citizens.
	- [Menta S.A.S, Sophia Antipolis](https://www.menta-efpga.com/careers)
	- [Achronix Semiconductor Corporation](https://www.achronix.com/company/careers)
	- [Flex Logix Technologies, Inc.](https://flex-logix.com/)
	- [Microchip Technology](https://www.microchip.com/)
+ machine learning hardware accelerators (including coarse-grained reconfigurable architctures, CGRA), machine learning acceleration via domain-specific computing, heterogeneous computing systems for machine learning, VLSI deep learning, and embedded deep learning
	- [SimpleMachines, Inc.](https://www.simplemachines.ai/company)
	- [Codeplay Software Ltd.](https://www.codeplay.com/company/careers/#career-list)
	- [Thirdwayv](https://www.thirdwayv.com/careers/)
	- [Lightmatter](https://lightmatter.co/people/join-us/)
	- [Habana Labs](https://habana.ai/about-us/)
	- [Achronix Semiconductor Corporation](https://www.achronix.com/company/careers)
	- [AlphaICs Corp](https://alphaics.ai/company/careers/)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [NAME](URL)
	- [Qualcomm Technologies, Inc.](https://www.qualcomm.com/research/artificial-intelligence/ai-research): https://www.qualcomm.com/company/careers
	- [Cornami, Inc.](https://cornami.com/)
+ companies selling RISC-V -based products
	- [SiFive, Inc.](https://www.sifive.com/careers)
+ edge computing
	- [EdgeImpulse Inc.](https://edgeimpulse.com/careers)
+ Open Compute Project (OCP): https://www.opencompute.org/membership/membership-directory














###	Machine Learning for EDA

+ skill set:
	- Required Machine Learning Experts for EDA Products, we need people who are passionate about technology, constantly seeking to learn and improve the skill set with good communication and interpersonal skills.
	- Should be proficient in Python and applying ML Algorithms.
	- Good knowledge of machine learning algorithms like Neural network, CNN, Logistic regression, KNN, Random forest, decision tree, clustering etc.
	- knowledge of C with good programming skills & logical interpretation.
	- Decent depth in understanding ML algorithm concepts like supervised/unsupervised, regression/classification, time series algorithms
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







###	System-Technology Co-Optimization, STCO


####	Notes about STCO & DTCO

By definition, System-Technology Co-Optimization, STCO, includes Design-Technology Co-Optimization (DTCO).


System-Technology Co-Optimization, STCO
+ Design-Technology Co-Optimization, DTCO
+ AutoML for STCO
+ Benchmarking
	- for ***non- von Neumann computing paradigms***
		* hybrid non- von Neumann computing paradigms:
			+ quantum computing + optical computing
				- linear optical quantum computing, linear optics quantum computation, LOQC
					* Boson sampling
					* KLM scheme, KLM protocol
		* ***optical computing, photonic computing***
			+ approaches:
				- computing by xeroxing on transparencies
				- Ising machines
				- masking optical beams
				- optical Fourier co-processors
				- time delays optical computing
				- wavelength-based computing
			+ nanophotonics, or nano-optics
				- metamaterials
				- near-field optics
				- microphotonics
				- biophotonics
					* biofluorescence
					* biolasing
					* bioluminescence
					* biophosphorescence
					* fluorescence resonance energy transfer, Forster resonance energy transfer, FRET
			+ optical rectenna, or optical rectifying antenna
				- ["An optical rectenna is a rectenna (rectifying antenna) that works with visible or infrared light. A rectenna is a circuit containing an antenna and a diode, which turns electromagnetic waves into direct current electricity. While rectennas have long been used for radio waves or microwaves, an optical rectenna would operate the same way but with infrared or visible light, turning it into electricity."](https://en.wikipedia.org/wiki/Optical_rectenna)
			+ optical transistors
				- ["An optical transistor, also known as an optical switch or a light valve, is a device that switches or amplifies optical signals. Light occurring on an optical transistor's input changes the intensity of light emitted from the transistor's output while output power is supplied by an additional optical source. Since the input signal intensity may be weaker than that of the source, an optical transistor amplifies the optical signal. The device is the optical analog of the electronic transistor that forms the basis of modern electronic devices. Optical transistors provide a means to control light using only light and has applications in optical computing and fiber-optic communication networks. Such technology has the potential to exceed the speed of electronics, while conserving more power."](https://en.wikipedia.org/wiki/Optical_transistor)
			+ photonic ICs, PIC, integrated optical circuit
			+ photonic logic
				- require resonators
			+ silicon photonics
			+ related emerging technologies:
				- phased-array optics, optical phased array, OPA
				- screenless displays:
					* categories:
						+ visual image
						+ retinal direct
						+ synaptic interface
					* examples:
						+ virtual retinal display, VRD, retinal scan display, RSD, retinal projector, RP
							- use adaptive optics, AO
						+ bionic contact lens
						+ fog display, fog screen, vapor screen, vapor display
					* applications:
						+ augmented reality
						+ virtual reality
				- volumetric display devices, 3-D displays
					* are autostereoscopic				
		* in-memory computing
		* hyperdimensional computing, HDC
		* quantum computing
			+ based on [***atomtronic circuits***](https://en.wikipedia.org/wiki/Atomtronics)
		* hypercomputation, super-Turing computation
	- for heterogeneous system architectures, HSA
	- for von Neumann computing
	- other applications:
		* memory subsystems
			+ racetrack memory, domain-wall memory, DWM
		* digital scent technology, olfactory technology
			+ sense, transmit, and receive scent-enabled digital media
			+ sensor implementations:
				- olfactometers
					* dynamic dilution olfactometers
					* field olfactometers
					* flow olfactometers
				- electronic noses
				- fluctuation-enhanced sensing, FES
					* based on higher-order statistics, HOS
			+ scentography devices
			+ machine olfaction
		* electronic skin
			+ includes:
				- tactile sensors
			+ conductive electronic skin
			+ flexible and stretchy electronic skin
			+ recyclable electronic skin
		* electronic tongues
			+ artificial taste
		* flexible electronics, flex circuits
			+ flexible display, or rollable display
			+ flexible printed circuits, FPC
		* printed electronics
			+ printed technologies
				- aerosol jet printing
				- evaporation printing
				- inkjet printing
				- screen printing
		+ nanoradios
			+ biomedical applications, for drug delivery




####	Skill sets for STCO and DTCO:



Skill sets for STCO and DTCO:
+ skill set:
	- Development of compilers and peripheral libraries for MN-Core
	- We will develop the compiler and peripheral libraries of MN-Core. Specifically, we are assuming the following themes
		* Utilization of domain-specialized languages such as JAX and Halide
		* Research and development of learning models with low accuracy
		* Implementation of kernels for MN-Core such as FFT
	- Development of peripheral tools such as profilers
	- Communication Language: Japanese
	- Coding ability using Python and C++
	- Experience with low-level optimization
	- Knowledge of compilers for deep learning
+ skill set:
	- Development of framework and library for deploying deep learning models in real world
	- Develop compiler/runtime (PFVM) that optimizes computational graphs of deep learning models to perform inference performantly (in terms of execution time and memory usage), targetting on various backends such as CUDA or edge devices. 
	- Develop an open-source library that uses GPU (CuPy).
	- Communication Language: English/Japanese
	- Programming in C++
	- Programming in Python
	- Basic knowledge of computer science
	- Experience on deep learning model development
	- Experience of running deep learning models on edge devices or smartphones
	- Development experience of multi-pass compilers
+ AI/ML for architectural exploration and bottleneck identification
+ AIML - Sr. Software Engineer, Large Language Models (MIND)
	- The Machine Intelligence, Neural Design (MIND) team employs HW/SW co-design to achieve best-in-class performance and energy efficiency for numerous use cases that deploy neural networks. We seek a Sr. Software Engineer to help define and implement features that accelerate and compress large language models (LLMs) in our next-gen inference engine. Our team is comprised of Efficient ML experts with skillsets in HW, SW, and ML. Our charter is to push the frontiers of perf and power for DNNs with minimal memory footprint.
	- As a SWE, you will be responsible for writing high-quality, well-tested code. Our ideal team member is courageous when it comes to trying new things, is adept at reasoning about systems performance, and is willing to iterate on ideas. We value team members with strong communication skills with experience working cross-functionally with HW, SW, and ML teams.
	- Proficient in C++, working knowledge of Python
	- Implement features that compress and accelerate LLMs in our on-device inference engine
	- Convert models from a high-level framework to a target device for correctness and performance issues
	- Write unit and system integration tests to ensure functional correctness and to reduce performance regressions
	- Diagnose performance bottlenecks and minimize memory footprint of large language models
	- Work with HW Arch teams to co-design solutions that further improve perf,  power, and memory footprint of neural workloads
	- Work with a variety of partners from all parts of the stack — from Apps to Compilation, HW Arch, and Silicon Validation
	- Strong communicator with ability to analyze complex and ambiguous problems
	- Familiarity with ML model compression techniques (e.g., quantization, pruning/sparsity) and their mapping to a target backend
	- Disciplined programming skillset with a strong attention to detail
	- Experience with backend compilation, HW/SW co-design, and/or performance optimization
	- The base pay range for this role is between $189,800 and $346,300, and your base pay will depend on your skills, qualifications, experience, and location.
+ Deep understanding of computer systems and the interactions between HW and SW
+ Familiarity with at least one deep learning framework (e.g., PyTorch, Keras, TensorFlow)
+ skill set:
	- Post-Moore Microelectronics/VLSI Co-Design Postdoctoral Scholar
	- Berkeley Lab’s Applied math and Computational Sciences Division has an opening for a Postdoctoral Scholar to evaluate and develop devices to hardware/circuit co-design flow for architectural specializations for high performance computing and edge computing applications.
	- In the absence of Moore’s Law Scaling, the Department of Energy (DOE) must investigate alternative paths to continuing computing performance improvements for scientific applications through architectural specialization. The successful candidate will contribute to the development and evaluation of novel heterogeneous device-based circuit design for extreme heterogeneous SoC (System on Chip) designs, and evaluate their merit for emerging computational workloads for the purpose of maximizing performance and energy efficiency. This work will have a broad impact on high performance and other larger-scale computing for critical applications for society and science. 
	- The successful applicant will need to have expertise with computer architecture and processor design and from the ground up, and have skills in Spice analog/digital circuit design, Verilog and use of CAD/EDA tools. It is desirable if the applicant has familiarity with higher-level hardware design languages such as CHISEL, PyMTL or other HDLs that target an FIRRTL intermediate representation. It is also beneficial if the candidate has experience with full tape-out experience of ASICs. Using those skills, the successful candidate will design post-Moore devices-based compute, memory, or data transfer blocks for key application kernels to demonstrate the merit of this approach. This position will also make key intellectual contributions and consequently publish papers to the emerging field of extreme heterogeneous computing and domain-specific specializations.
	- Design circuits, hardware accelerators and processor architectures using post-Moore devices to accelerate key HPC applications and application kernels.
	- Develop compact models and methodologies to use these circuits for performance and energy characterizations which can be used in architectural simulation framework for tightly integrating these accelerators into heterogeneous systems and SoCs that may contain multiple different kinds of accelerator devices.
	- Identify opportunities and challenges for devices to architectural design space exploration for several post-Moore devices to address those bottlenecks and develop circuit design models to determine the performance potential for those solutions.
	- Develop metrics and benchmark tests in order to compare conventional CMOS based processors/accelerators and enhanced post-Moore devices based computational accelerators for key HPC applications and algorithms. 
	- PhD or equivalent in a Computing Science or Computer Engineering related scientific discipline.
	- Past experience in either Machine learning accelerators or SRAM array design or basic blocks of processor at transistor level.
	- Courses or experience in CAD for VLSI algorithms and C++ Programming.
	- Proficient in Spice Circuit Simulations, Verilog and hardware design in CMOS, FeFET, NCFET etc.
	- Familiarity with hardware EDA/CAD tools and evaluation/modeling tools in order to extend existing infrastructure to rapidly evaluate CMOS designs.
	- Demonstrated creativity, initiative and ability to design, develop and implement complex solutions in consultation with designated technical expert(s) and/or supervisor.
	- Experience and track-record writing technical papers and reports.
	- Experience with the use of script languages and system utilities such as configure, Perl, UNIX shell scripts, and “make.”
	- Proven record of working effectively in a team, seeing projects through to completion, meeting deadlines, interacting with users, and thorough documentation of contributions.
	- Willingness to learn and develop skills in new topics.
	- Previous experience and publications in Processing-In-Memory and Logic-in-Memory architectures is highly desirable.
	- Experience with coding in C++/python for CAD tool development for ASIC design.
	- Experience with higher-level hardware design languages (HDLs) such as CHISEL, PyMTL, or others.
	- Experience with FPGA design flows would also be beneficial.
	- Demonstrated ability to lead technical efforts with teams of people will also be beneficial.
	- The monthly salary range for this position is $6431-$8642 and is expected to start at $7843 or above. 
	- This is a full-time 3 year, postdoctoral appointment with the possibility of renewal based upon satisfactory job performance, continuing availability of funds and ongoing operational needs. You must have less than 2 years of paid postdoctoral experience. Salary for Postdoctoral positions depends on years of experience post-degree.
+ skill set:
	- ML Hardware Accelerator Modeling (Boston or Bay Area)
	- Lightmatter builds chips for artificial intelligence computing. Our architecture leverages unique properties of light to enable fast and efficient inference and training engines. If you're a collaborative engineer or scientist who has a passion for innovation, solving challenging technical problems and doing impactful work like building the world's first optical computers, consider joining the team at Lightmatter!
	- We are looking for talented software engineers to help us build the next generation of AI processors and systems. In this role, you will be responsible for developing tools that enable the design of high performance ML accelerators and an optimized software stack. You'll be working with multiple software teams, as well as digital, analog, and photonic designers, to ensure that both our software and hardware can achieve a generational leap in ML performance.
	- Design, development, and maintenance of functional and performance models of upcoming hardware
	- Participate in the co-design process by working closely with hardware teams to iterate on early performance models, adding fidelity as designs progress
	- Collaborate with machine learning teams to construct and evaluate the performance of machine learning workloads
	- Build cycle approximate performance and bit-accurate functional models that further the co-design process and allow the software teams to build and optimize our next generation software stack
	- Collaborate with Design Verification teams to validate hardware against the simulators
	- Contribute to a system-level simulator that integrates the accelerator simulator into a larger system simulator
	- Support the development and optimization of our software stack by diagnosing and fixing issues across the simulator and software stack
	- BS or higher in computer/software engineering, electrical engineering, or related field.
	- 5+ years of industry experience in mapping algorithms to hardware accelerators, hardware modeling, computer architecture, or related fields
	- Proficient with C++ and Python
	- Strong understanding of computer architecture
	- Familiarity with machine learning workloads
	- Experience in mapping software algorithms to hardware accelerators
	- Experience with modeling and analysis of machine learning accelerator architectures
	- Proficient developing in a Linux-based environment
	- Experience working with or modeling distributed hardware and workloads
+ skill set:
	- Software Engineer – AI/Machine Learning software product development
	- To further sustain and accelerate its growth, Cadence Belgium is looking for a Software Engineer – AI/Machine learning software product development to join the international team based in Brussels. 
	- Cadence Belgium is part of Cadence, a pivotal leader in electronic design and computational expertise, using its Intelligent System Design strategy to turn design concepts into reality. Cadence customers are the world’s most creative and innovative companies, delivering extraordinary electronic products from chips to boards to systems for the most dynamic market applications. 
	- Cadence Belgium develops simulation software for fluid flows, multiphysics and optimization, widely used by engineers and designers in a large range of fluid engineering applications, from aerospace, power generation and energy to race cars and ships. Cadence Belgium is extending its work force and is looking for qualified new colleagues. 
	- By joining Cadence Belgium you will build the next generation AI/ML based software product and collaborate with experts in the world of simulation and Artificial Intelligence/Machine Learning. Our teams are fully committed to develop and implement creative solutions. We believe that quality, rigor, and innovation are key to success. With a team of highly skilled and motivated co-workers, we offer a stimulating, young and multicultural environment with career growth opportunities and internal mobility. 
	- Join our technical team and contribute to the development of our CFD simulations. 
	- As a Software Engineer AI/Machine learning software product development, your main responsibilities will be: 
	- Building the next generation AI/ML software product 
	- Contributing to front-end developments in C++, QML, and Python 
	- Developing and maintaining the tool chain and Graphical User Interface of our novel Machine-Learning product 
	- Working with the Machine-Learning core team to identify and translate their requests into state-of-the-art interactive software 
	- Working with the Product Engineering teams identify and translate end user requirements into state-of-the-art interactive software 
	- Understand and follow best software practices through design and code review, testing and validation. 
	- Keeping abreast of the latest trends and technology in the field of human-computer interaction design pushing forward innovation and creativity in this domain 
	- A bachelor’s or master’s degree in computer science, Mathematics, Physics or Engineering, with experience in the development of 3D visualization applications 
	- In-depth knowledge of C++, Python and JavaScript (or QML) languages 
	- Knowledge of Linux and Windows (basics) 
	- Passionate about UI/UX development and keeping up to date with the latest trends 
	- Write clean, structured, and maintainable code 
	- Fluent English language is a prerequisite for the role. 
	- Experience using the Qt framework is an asset
	- A prior experience with development of GUI or tool chain for industrial CAE/CAD/CFD packages is an asset 
	- French or Dutch language skill is a plus 
	- You should be highly motivated and dynamic, have good communication and analytical skills, be a stress-resistant problem solver, be a team player able to meet the highest quality standards, and ideally have a passion for programming. 
+ skill set:
	- Staff Machine Learning Engineer
	- Synopsys Central Engineering (SCE) team is looking for Software Engineering Experts with an excellent focus on solving complex problems, and capable of leading & building an excellent team.
	- As a part of the Synopsys SCE team, you will be working with a world-class team of software engineers and architects on the mission to build our state of art tools, and you would be expected to conceptualize and develop new tools & applications with cutting-edge technology. You will work together from requirement elicitation, design, implementation, and testing phases to production deployment in the cloud environment and on-prem.
	- Leading and driving the team technically,
	- To design and implement  AI/ML based solutions
	- To incorporate AI/ML technology into existing tools
	- To re-design and develop existing applications for key R&D productivity
	- BSc / MSc in the domain of Computer/Electronics Engineering or Computer Science
	- 7+ years of relevant industry experience in developing and productizing software solutions
	- Exemplary leadership and team-working skills
	- Solid background in Data Structures & Algorithms
	- Knowledge of Machine Learning & Data Science
	- Excellent programming skills, preferably in Python
	- Knowledge of cloud technologies
	- Exposure to distributed computing will be an added advantage
	- Quick learning and adaption to new technologies























###	VLSI Formal Verification


####	Notes about VLSI Formal Verification


Notes about VLSI formal verification:
+ Focus on mostly equivalence checking and model checking, and less on theorem proving (except in combination with decision procedures and automated reasoning, such as SAT/SMT solvers).
+ This includes:
	- clock domain crossing (CDC) verification, or CDC check
		* for functional static sign-off checks



VLSI formal verification companies:
+ Axiomise: https://www.axiomise.com/careers/









####	Sets of Skills for about VLSI Formal Verification


The sets of skills for VLSI formal verification are:
+ skill set:
	- Principal Formal Verification Engineer
	- Axiomise is the world’s only formal verification training, consulting, services and custom solutions company. In its sixth year, we have delivered training to over a hundred engineers globally and provided consulting & services to some of the best names in the semiconductor industry. We designed the industry’s first vendor-neutral, fully automated RISC-V formal verification app that has been used to find bugs in pre-existing processors and exhaustively prove bug absence. We love formal methods and use them day and night to sign off designs so our customers do not leave bugs in the silicon. We are headquartered in the UK in London.
	- We are looking to hire top-notch experienced engineering talent for the UK. Your typical day job would involve building cutting-edge formal verification testbench environments to find bugs and build proofs of bug absence in SoCs containing processors, video/GPUs, networking, AI/ML designs. Formal verification is the only way to generate proofs of correctness and build proofs of bug absence. For this position we expect you to have prior industry experience in design verification including formal verification.
	- We are looking for bright spirited individuals with a positive can-do attitude. We often work on very challenging problems that are not always solvable within a 9-to-5 framework, so we expect our team to put in extra hours if needed. The outcome will be very rewarding, we can promise that. We do not have a hierarchical structure so you will learn fast. You will get to build cool new solutions, publish papers, file patents, and work live with customers. We are an equal opportunity employer. Everyone is welcome.
	- Remember you will be working in a small organization that is growing rapidly with lots of opportunities for you to grow and learn. We welcome our engineering talent to also get involved in other areas of our business and we take pride that we are agile and can respond swiftly to our customer and employee needs.
	- You must have an engineering degree in EEE/ECE/CS/Maths and must have six to eight years of experience in the industry in design verification.
	- You will be required to carry out hands-on work on formal verification of processors, GPU blocks, networking designs or AI/ML. The work will include building strategy, verification plans, testbenches and sign-off using the Axiomise six-dimensional coverage methodology. You are expected to be confident in Verilog/VHDL as well as fluent in SVA and Tcl/Perl/Python and Unix/Linux scripting.
	- Bachelor/Masters/Doctorate in EEE/ECE/CS/Maths
	- Linux/Unix experience
	- Verilog/VHDL design experience
	- SV/UVM experience
	- Open-source projects in design/verification
	- RISC-V/Arm/x86/MIPS
	- SVA/PSL/Theorem proving
	- Tcl/Python/Bash
+ skill set:
	- Staff Formal Verification Engineer
	- Axiomise is the world’s only formal verification training, consulting, services and custom solutions company. In its sixth year, we have delivered training to over a hundred engineers globally and provided consulting & services to some of the best names in the semiconductor industry. We designed the industry’s first vendor-neutral, fully automated RISC-V formal verification app that has been used to find bugs in pre-existing processors and exhaustively prove bug absence. We love formal methods and use them day and night to sign off designs so our customers do not leave bugs in the silicon. We are headquartered in the UK in London.
	- We are looking to hire top-notch experienced engineering talent for the UK. Your typical day job would involve building cutting-edge formal verification testbench environments to find bugs and build proofs of bug absence in SoCs containing processors, video/GPUs, networking, AI/ML designs. Formal verification is the only way to generate proofs of correctness and build proofs of bug absence. For this position we expect you to have prior industry experience in design verification including formal verification.
	- We are looking for bright spirited individuals with a positive can-do attitude. We often work on very challenging problems that are not always solvable within a 9-to-5 framework, so we expect our team to put in extra hours if needed. The outcome will be very rewarding, we can promise that. We do not have a hierarchical structure so you will learn fast. You will get to build cool new solutions, publish papers, file patents, and work live with customers. We are an equal opportunity employer. Everyone is welcome.
	- Remember you will be working in a small organization that is growing rapidly with lots of opportunities for you to grow and learn. We welcome our engineering talent to also get involved in other areas of our business and we take pride that we are agile and can respond swiftly to our customer and employee needs.
	- You must have an engineering degree in EEE/ECE/CS/Maths and must have six to eight years of experience in the industry in design verification.
	- You will be required to carry out hands-on work on formal verification of processors, GPU blocks, networking designs or AI/ML. The work will include building strategy, verification plans, testbenches and sign-off using the Axiomise six-dimensional coverage methodology. You are expected to be confident in Verilog/VHDL as well as fluent in SVA and Tcl/Perl/Python and Unix/Linux scripting.
	- Bachelor/Masters/Doctorate in EEE/ECE/CS/Maths
	- Experience in design verification using SVA 
	- Evidence of sign-off capable work done previously
	- Have a can-do attitude, can show self-learning attributes
	- Individual contributor & excellent team player
	- Strong reasoning skills
	- Excellent attention to detail
	- Excellent communication details in English
	- Linux/Unix experience
	- Verilog/VHDL design experience
	- SV/UVM experience
	- Open-source projects in design/verification
	- RISC-V/Arm/x86/MIPS
	- SVA/PSL/Theorem proving
	- Tcl/Python/Bash
+ skill set:
	- Silicon Formal Verification Engineer
	- The Silicon Design group is a diverse team of world class silicon design, verification and validation experts. We have 100+ years of cumulative hands-on experience in architecture, logic design, verification, physical design, emulation and firmware. We use the latest silicon technologies and processes to help our clients create well-designed solutions to highly complex challenges. We are designing and developing next-generation, high performance SoCs, supporting our clients in their drive to deliver their product vision to their users. We are involved in all aspects of chip design from definition and architecture through to verification and signoff. Accenture Consultants are true “Silicon to SW” Partners, allowing a new breed of companies in the semiconductor ecosystem to innovate in an unparalleled time to market.
	- An experienced Formal Verification Engineer able to provide formal verification services for multiple blocks and IP’s.
	- Developing formal verification test plan
	- Drive automation of formal testbenches and ensure they are a part of regressions
	- Develop assertions, cover properties and connectivity checks and debug any failures in RTL regressions
	- Work with cross functional teams (DV/Arch/Design/FW)
	- Engage with the team to drive continuous improvement to the verification environment to find more bugs and improve coverage
	- Work as a team to grow together.
	- Mentor and coach junior team members
	- A minimum of three years of experience with Formal Verification
	- Bachelor’s Degree or equivalent (12 years) work experience (If an, Associate Degree with 6 years of work experience).
	- Experience working with one or more formal verification tools such as Jasper gold, VC-Formal, Incisive Formal Verifier (IFV), Questa Formal, etc.
	- Experience in interpreting design specifications and using temporal logic assertion-based languages such as SVA
	- Experience in formal property verification (FPV), Sequential Logic Equivalence Checking (SEC/SEQ/SLEC), and/or academic formal methods
	- Experience with hardware description languages (SystemVerilog, Verilog, VHDL) and simulators (VCS, NC, Questa).
	- Proficiency in programing/scripting languages
	- In-depth knowledge of digital logic design, chip architecture and microarchitecture
	- Problem solving and debug skills for complex logic and digital designs
	- Team player with excellent communication skills and be able to work independently on the verification efforts for a block/area of the design
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






###	Embedded Formal Verification, and Formal Verification of Cyber-Physical Systems


Focus on mostly model checking, and less on theorem proving (except in combination with decision procedures and automated reasoning, such as SAT/SMT solvers).

Can include equivalence checking.


List of formal verification companies for cyber-physical systems:
+ [Prover](https://www.prover.com/about-us/career/#available-positions)
+ [Galois](https://galois.com/careers/)






The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






###	Software Formal Verification



List of software formal verification companies:
+ CertiK: https://www.certik.com/company/careers
+ Elasticsearch B.V.: https://www.elastic.co/careers
+ Formal Vindication: https://formalv.com/
+ Trellix, which is part of Musarubra US LLC: https://careers.trellix.com/





+ skill set:
	- Tezos is a leading crypto-currency featuring the unique capability to evolve thanks to a voting system. In addition, formal verification is a strong community focus to make Tezos the safest platform, usable by individuals and large corporates.
	- In this job, you will participate in the verification effort project of the crypto-currency Tezos. We publish all our results in open-source on https://nomadic-labs.gitlab.io/coq-tezos-of-ocaml/ Explore this website beforehand to have more details. We use the automated system coq-ofocaml to convert the OCaml code of Tezos to Coq. Your job will be to write and automate Coq proofs on various sub-systems of the blockchain.
	- We are looking for profiles who are interested in functional programming and proof assistants. Prior knowledge of OCaml or Haskell is a plus. Prior knowledge of a proof assistant is mandatory. Prior knowledge of crypto-currencies is optional.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.













###	Circuit Simulation


###	Notes about Circuit Simulation

Skill sets for circuit simulation include those for:
+ analog ICs
+ RFICs
+ digital ICs, especially "FastSPICE"
+ mixed-signal ICs
	- especially mixed-signal ICs.
+ model order reduction, or macromodeling
	- ***nonlinear model order reduction***




###	Skill Sets for Circuit Simulation



The sets of skills for circuit simulation are:
+ skill set:
	- Software Developer- Circuit Simulation- Remote · Siemens EDA · Remote Position
	- At Siemens, we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: [1] www.siemens.com/careers/digitalminds
	- Siemens EDA is part of Siemens Digital Industries Software, a Siemens Group company and world leader in the global digitalization of companies. Siemens EDA, a world leader of Electronic Design Automation (EDA), develops and markets software products and hardware solutions (emulators) covering the entire design & verification flow.
	- Siemens EDA counts among its customers the majority of the world's Top 50 in the fields of Semiconductors, Computing, Consumer, Automotive, Telecommunications and the Defense and Space sector.
	- Siemens EDA R&D teams, located across the globe, develop disruptive semiconductor chip design CAD software, including the Analog FastSPICE™ (AFS), the world’s fastest nanometer accurate circuit verification platform. AFS is used by over 200 semiconductor companies worldwide for their toughest circuit verification challenges while designing high-speed I/Os, PLLs, ADCs/DACs, CMOS image sensors, RFICs, and embedded memory.
	- This position will be part of the AFS R&D team, focused on circuit simulation. As a key member of a highly proficient, productive, and motivated R&D team, developing industry’s leading circuit simulator, using cutting edge software development techniques, you will be offered:
		* Technical challenges to solve toughest nanometer scale circuit verification problems for the leading semiconductor companies in the world.
		* A motivating, stimulating, and rewarding work environment
		* Excellent training and growth opportunities throughout your career
		* Attractive compensation and benefits
		* A motivating, stimulating, and rewarding work environment
		* Excellent training and growth opportunities throughout your career
		* Attractive compensation and benefits
	- As a member of AFS R&D team you will participate in the design and implementation of efficient algorithms using state of the art software engineering processes and development tools, with a strong emphasis on software quality; thus, adding to your industry level experience in solving tough computational complexity problems while participating in entire software development lifecycle. We are looking for a self-motivated and inspiring team player with outstanding problem-solving skills to maintain and grow the technical dominance of the AFS product.
	- Develop new and compatibility features for the core circuit simulator.
	- Participate in the specification, architecture, design, and development of features
	- Enhance core circuit simulator, e.g., performance, accuracy, capacity, convergence.
	- Maintain and enhance compatibility with other simulators.
	- Profile and identify bottlenecks in performance of various analyses for very large circuits.
	- Improve numerical algorithms used in the core engine.
	- Debug difficult testcases with accuracy, performance, capacity, or functional issues.
	- Extend and maintain the capabilities of the AFS circuit simulator.
	- Be a force for improving development processes and product quality.
	- Work effectively with globally distributed engineering teams and the Product Validation team
	- Minimum 7 years of proven strong background in developing efficient, high-quality software for engineering applications using numerical methods and sparse matrix techniques
	- Deep understanding of numerical methods and sparse matrix techniques
	- Working knowledge of analog electrical circuit analysis
	- Outstanding programming skills in C and C++, preferably on Linux platform
	- Proficiency in memory optimization, high-performance data structures and algorithms
	- Advanced multithreading programming experience.
	- Understanding of advanced computer architectures
	- Solid background in object-oriented design and software engineering processes.
	- Self-motivated individual with excellent problem-solving skills.
	- Strong interpersonal and excellent oral and written communication skills.
	- Highly motivated to work in globally distributed engineering environment
	- M.S or PhD in Computer Science, Electrical Engineering, Applied Mathematics, or relevant area
	- Understanding of the internal workings of a circuit simulator
	- Knowledge of Verilog-A and modeling in Verilog-A.
	- Background in semiconductor devices and their modeling
	- Python programming experience
	- EDA industry level work experience
+ skill set:
	- Software Developer, RF Circuit Simulation-Remote · Siemens EDA · Remote Position
	- At Siemens, we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: [1] www.siemens.com/careers/digitalminds
	- Siemens EDA R&Dteams, located across the globe, develop disruptive semiconductor chip design CAD software, including the Analog FastSPICE™ (AFS), the world’s fastest nanometer accurate circuit verification platform. AFS is used by over 200 semiconductor companies worldwide for their toughest circuit verification challenges while designing high-speed I/Os, PLLs, ADCs/DACs, CMOS image sensors, RFICs, and embedded memory.
	- This position will be part of the AFS R&D team, focused on RF (Radio frequency) circuit simulation. As a key member of a highly proficient, productive, and motivated R&D team, developing industry’s leading circuit simulator, using cutting edge software development techniques, you will be offered:
	- Technical challenges to solve toughest nanometer scale circuit verification problems for the leading semiconductor companies in the world.
	- A motivating, stimulating, and rewarding work environment
	- Excellent training and growth opportunities throughout your career
	- Attractive compensation and benefits
	- As a member of AFS R&D team you will participate in the design and implementation of efficient algorithms using state of the art software engineering processes and development tools, with a strong emphasis on software quality; thus, adding to your industry level experience in solving tough computational complexity problems while participating in entire software development lifecycle. We are looking for a self-motivated and inspiring team player with outstanding problem-solving skills to maintain and grow the technical dominance of the AFS product.
	- Develop new and compatibility RF features for AFS circuit simulator
	- Participate in the specification, architecture, design, and development of RF features
	- Enhance performance, accuracy, capacity, and convergence for RF analyses
	- Profile and identify bottlenecks in performance of RF analyses for very large circuits.
	- Improve numerical algorithms used in the core RF engine.
	- Debug difficult testcases with accuracy, performance, capacity, or functional issues.
	- Extend and maintain the RF capabilities of the AFS circuit simulator.
	- Work effectively with globally distributed engineering teams and the Product Validation team
	- Experience in large-scale periodic and quasi-periodic solution of differential-algebraic circuit equations, including distributed elements
	- expertise in large-scale sparse matrix, krylov, and eigen solvers
	- expertise in high-performance multi-threaded numerical software development in C++
	- Outstanding programming skills in C and C++, preferably on Linux platform
	- Proficiency in memory optimization, high-performance data structures and algorithms
	- Advanced multithreading programming experience.
	- Solid background in object-oriented design and software engineering processes.
	- Self-motivated individual with excellent problem-solving skills.
	- Strong interpersonal and excellent oral and written communication skills.
	- Highly motivated to work in globally distributed engineering environment
	- M.S or PhD (preferred) in Computer Science, Electrical Engineering or Applied Mathematics
	- 7+ years of relevant industry experience
	- The salary range for this position is $141,100 to $225,800 and this role is eligible to earn incentive compensation.
+ Consider different quality attributes (FURPS) while validating the various products with special emphasis on Performance attributes.
+ skill set:
	- Software Engineer - Circuit Simulation (EMIR Analysis)
	- Circuit simulation developer with an emphasis on EMIR analysis
	- The Circuit Simulation Developer is responsible for designing, implementing and maintaining software designed to perform transistor-level VLSI circuit simulation, with an emphasis on EMIR analysis.  The ideal candidate would have expertise in device modeling and numerical techniques for VLSI circuit simulation. Understanding or experience of parasitic extraction is a strong plus.   Candidate should have an advanced degree in electrical engineering, computer science, applied mathematics, or similar.  Candidates with experience in related fields will be considered, particularly:  
	- 1. Transistor-level SPICE analysis algorithms
	- 2. Parasitic linear network reduction and analysis algorithms
	- 3. Statistical analysis, EMIR analysis, electro-thermal analysis algorithms
	- 4. Device physics, compact device modeling, behavioral modeling, macro modelling, statistical modeling, reliability modeling
	- 5. Numerical analysis, especially numerical linear algebra, sparse matrix techniques, or numerical methods for solution of ordinary and partial differential equations
	- 6. High performance computing / large scale scientific computing and deployment of parallel numerical algorithms
	- Candidate should be proficient in C/C++ development.  Experience with scripting languages like Python and GUI frame works like QT is a plus. Knowledge or Demonstrated software engineering skills, with a good understanding of efficient implementation of high-performance numerical algorithms and associated data structure design, and experience  in relevant software frameworks is a plus. Exposure to high-performance numerical computing, CPU/GPU systems. The candidate should have ability to work with an engineering and cross-functional team to deliver innovative technologies in a production environment. 
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










###	Electronic System-Level Design & Verification


####	Notes about Electronic System-Level Design & Verification

The emphasis regarding the design and verification processes for electronic-system level tasks are about embedded computing, or computer systems. See Subsubsection on the design automation of cyber-physical systems, and related systems, to address the interactions with the physical environment, via sensing and actuation.


Includes the following:
+ high-level synthesis, or synthesis of HCL (hardware construction languages) models into logic circuits.
+ hardware/software co-design
+ hardware/software co-verification
+ hardware/software partitioning
+ software synthesis, program synthesis, or automated source code generation
+ transaction-level modeling
	- verification
	- synthesis, via HLS
+ power optimization, energy-efficient designs



####	Skill Sets for Electronic System-Level Design & Verification



The sets of skills are:
+ skill set:
	- Able to solve a wide-range of difficult problems in imaginative and creative ways, exercising judgment within broadly defined practices and policies.
	- MSc in Computer Science, Applied Mathematics or related field with 3+ years of experience, or BSc with 5+ years of experience
	- ***Proficiency in developing and maintaining modern C++ based applications in a Unix/Linux and Windows environment. Proficiency in Qt, Python, and Tcl a plus. Experience with OpenAccess also a plus.***
	- Experience in developing enterprise level software, proficiency with debug and configuration management tools as well as quality and performance metric tools.
	- Strong communication skills and ability to write specifications and reference documentation.
	- Proficiency in English is a must.
	- Interest in high performance data structures and algorithms.
	- Prior experience with or developing CAD/EDA tools and/or hardware design also a plus as is experience with geometric algorithms.
	- ***Excellent organizational, prioritization, time management skills and an unwavering commitment to integrity and professionalism.***
	- ***Self-starter and strong closer with multitasking ability***
	- Any other duties as assigned by the Department head
	- ***Computational Geometry/Topology***
	- ***Graph theory***
	- ***Pattern recognition/machine learning***
	- ***Compilers/parsers (experience with Flex/Bison a plus)***
	- Computer architecture (caching, memory, networking, etc.)
	- ***Boost***
	- Test Driven Development
	- Displays strong analytical abilities both quantitative and qualitative.
	- Excellent communication skills and the ability to interface with all levels of management.
	- Relies on experience and judgment to plan and accomplish goals.
	- Performs a variety of complicated tasks - a certain degree of creativity and latitude is required.
	- ***A key requirement of this role is being the master of all details.***
	- ***Ability to multi-task and handle matters with little supervision and with excellent follow up.***
	- ***A strong entrepreneurial and can-do mindset, undaunted by shifting priorities, uncertainty, and a “figuring it out as we go” environment.***
	- ***Enough courage to say “I don't know”.***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










###	Design Automation of Cyber-Physical Systems and Their Networks



####	Notes about Design Automation of Cyber-Physical Systems and Their Networks


This section covers the design automation of cyber-physical systems, and networks of cyber-physical systems (or, networked cyber-physical systems, including networked embedded systems).
+ [ambient intelligence](https://en.wikipedia.org/wiki/Ambient_intelligence), AmI
+ networks of cyber-physical systems
	- Internet of Things, IoT
		* AIoT, AI-based IoT
			+ global brain
				- From the CACM paper.
				- ***extended intelligence***, EI or XI
					* extend human intelligence with AI
		* ***Supranet***
			+ see Gartner research report
+ Intelligent Environments, IE
	- ["IEs describe physical environments in which information and communication technologies and sensor systems disappear as they become embedded into physical objects, infrastructures, and the surroundings in which we live, travel and work. The goal here is to allow computers to take part in activities never previously involved and allow people to interact with computers via gesture, voice, movement, and context."](https://en.wikipedia.org/wiki/Intelligent_environment)
+ [pervasive computing](https://en.wikipedia.org/wiki/Pervasive_computing)
+ [physical computing](https://en.wikipedia.org/wiki/Physical_computing)
+ [ubiquitous computing](https://en.wikipedia.org/wiki/Ubiquitous_computing)
+ with users
	- [haptic technology](https://en.wikipedia.org/wiki/Haptic_technology), or haptic computing
		* brain-computer interface, BCI; or brain-machine interface, BMCI,
			+ neural dust




####	Skill Sets for Design Automation of Cyber-Physical Systems and Their Networks



Skill set for the design automation of cyber-physical systems, and networks of cyber-physical systems:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.













###	Energy-Efficent EDA, or Low-Power EDA



####	Notes about Energy-Efficent EDA, or Low-Power EDA


circuit-level power optimization: 
+ transistor sizing
+ voltage scaling
+ voltage islands
+ variable VDD
+ multiple threshold voltages
	- Modern processes can build transistors with different thresholds. Power can be saved by using a mixture of CMOS transistors with two or more different threshold voltages. In the simplest form there are two different thresholds available, common called High-Vt and Low-Vt, where Vt stands for threshold voltage. ***High threshold transistors are slower but leak less, and can be used in non-critical circuits.***
+ power gating
	- This technique uses high Vt sleep transistors which cut-off a circuit block when the block is not switching. The sleep transistor sizing is an important design parameter. This technique, also known as MTCMOS, or Multi-Threshold CMOS reduces stand-by or leakage power, and also enables Iddq testing.
+ long-channel transistors
	- Transistors of more than minimum length leak less, but are bigger and slower.
+ stacking and parking states
	- Logic gates may leak differently during logically equivalent input states (say 10 on a NAND gate, as opposed to 01). State machines may have less leakage in certain states.
+ logic styles:
	- dynamic and static logic





logic synthesis for low power
+ clock gating
+ logic factorization
+ path balancing
+ technology mapping
+ state encoding
+ finite-state machine decomposition
+ retiming
+ as part of FPGA logic synthesis


Data organization for low power: https://en.wikipedia.org/wiki/Data_organization_for_low_power
 


Energy harvesting, EH, power harvesting, energy scavenging, or ambient power:
+ Circuits and systems that exploit ambient energy to power low-energy electronic circuits and systems, in applications such as:
	- wearable electronics
	- wireless sensor networks
	- other wireless autonomous devices



####	Skill Sets for Energy-Efficent EDA, or Low-Power EDA

The sets of skills are:
+ skill set:
	- We are looking for a highly motivated, creative, and energetic Principal Software Engineer to work on a new award-winning product for three-dimensional Integrated Circuit power sign-off verification. In this role, you will report to the Senior Engineering Director of Software Development with the following objectives:
	- Develop accurate and efficient models for IC behavior at advanced process technology nodes to analyze reliability, electrical, and thermal effects in digital and analog circuits.
	- Design, implement, and enhance scalable software solutions for large linear systems modeling on-chip power grids and system-level power delivery networks.
	- Effectively collaborate with the Product Engineering team in defining the product enhancement roadmap with solutions addressing customer requirements and ensuring competitive leadership of the product.
	- Work on simulation/extraction / static-timing / thermal analysis code.
	- Lead complex projects with minimal supervision and guide junior engineers.
	- Research, propose, and prototype solutions for advanced 3D IC power analysis. Maintain technical expertise by following technical advances in industry and academia.
	- Manage assigned projects, including defining scope, plans, schedules, and deliverables.
	- BS Degree in Electrical Engineering, Physics, Mathematics, or Computer Science/Engineering from an accredited institution.
	- Minimum 10+ years of experience in designing and developing EDA software products, preferably for ICdesign, analysis, and verification.
	- Strong knowledge of data structures and algorithms , and their complexities.
	- Strong C++ / STL programming skills, and good knowledge of OOD.
	- Ability to understand and support complex software products.
	- Experience in reliable planning and software effort estimation.
	- Ability to provide comprehensive testing of the developed solutions.
	- Experience with modern software development lifecycle flows and tools, including issue tracking, builds, and static and dynamic code analysis tools.
	- Ability to design high-quality software with on-time delivery.
	- Knowledge of Linux .
	- Excellent communication skills: ability to effectively interact with cross-functional teams (R&D, QA, Product Engineering), ability to work in a team. Ability to communicate complex technical concepts clearly and effectively.
	- MS or Ph.D. in Electrical Engineering, Physics, Mathematics, or Computer Science/Engineering from an accredited institution.
	- Knowledge of TCL .
	- Knowledge of AI/ML and their applications for IC verification.
	- Familiarity with Computational Geometry , Graph Theory.
	- Familiarity with HPC (high-performance computing).
	- Strong mathematic knowledge, of matrix and numerical algorithms.
	- Previous experience designing efficient algorithms and tools for IR/EM analysis, parasitic extraction, circuit simulation, or static timing analysis.
	- Familiarity with design patterns and code refactoring.
	- The salary range for this position is $136,700 to $246,100 and this role is eligible to earn incentive compensation.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Logic Synthesis


Includes information on:
+ FPGA logic synthesis



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Physical Design & Physical Synthesis



####	Notes about Physical Design & Physical Synthesis


Sets of skills for physical design of digital ICs/SoCs:
+ placement
+ routing
+ physical design for 3-D ICs
	- placement for 3-D ICs
	- routing for 3-D ICs
	- account for different types of 3-D ICs that connect dies or wafers, using:
		* TSVs, through-silicon vias
		* Cu-Cu connections
		* classification by level of interconnect hierarchy:
			+ global level, using packages
				- 3DWLP, 3-D wafer-level packaging
			+ intermediate level, using bond pads
			+ local level, using transistors
		* alternate classifications:
			+ 3DWLP, 3-D wafer-level packaging
			+ 2.5D interposer-based integration
			+ 3-D interposer-based integration
			+ 3-D stacked ICs
			+ monolithic 3-D ICs
			+ 3-D heterogeneous integration
			+ 3-D systems integration
	- Notes:
		* 3-D packaging:
			+ 3-D integration schemes that rely on traditional interconnection methods for vertical stacking, such as:
				- wire bonding
				- flip chip
			+ classifications:
				- 3-D SiP, 3-D system-in-package
					* for stacked memory dies interconnected with:
						+ wire bonds
						+ package on package PoP
				- 3-D WLP, 3-D wafer-level package
					+ uses wafer-level processes such as redistribution layers (RDLs) and wafer bumping processes to form interconnects
					* 2.5D interposer, using silicon/glass/organic interposer using through silicon vias (TSVs) and RDL
		* 3-D ICs
			+ 3-D SIC, 3-D stacked ICs
				- stack ICs chips
					* using TSV interconnects
					* stacking approaches:
						+ die-to-die
						+ die-to-wafer
						+ wafer-to-wafer
				- monolithic 3-D ICs
					* use fabrication processes to realize 3-D interconnects at the local levels of the on-chip wiring hierarchy (dictated by IRDS/ITRS)
		* Benefits:
			+ footprint, or volume that the package takes up on a board.
			+ cost
			+ heterogeneous integration
			+ shorter interconnect
			+ power
			+ design
			+ circuit security
			+ bandwidth
		* challenges:
			+ cost
			+ yield
			+ heat, thermal hotspots
				- correlation between electrical proximity and thermal proximity
			+ design complexity
			+ TSV-introduced overhead
			+ testing
			+ lack of standards
			+ heterogeneous integration supply chain
			+ lack of clearly defined ownership
		* design styles
			+ gate-level integration
			+ block-level integration
+ clock network synthesis
	- clock tree synthesis
+ cell library synthesis
	- cell library migration from a given semiconductor manufacturing process technology node to a more advanced node
+ chip-package-board co-design
	- hybrid integrated circuits, HIC, hybrid microcircuits
+ FPGA physical design
+ DFM-aware physical design
+ power supply networks
	- or, power and ground routing




Sets of skills for physical design of analog, RF, and mixed-signal ICs/SoCs:
+ placement
+ routing





Sets of skills for physical synthesis:
+ gate sizing
+ buffer insertion
+ wire sizing
+ yield-aware physical synthesis
	- as part of DFM-aware physical synthesis, to support proactive DFM.












####	Skill Sets for Physical Design & Physical Synthesis



Here are the sets of skills for physical design and physical synthesis.
+ skill set:
	- Senior Physical Design Engineer
	- Candidate must have 7-10 years of experiences in place & route. Tasks including but not limited to the following:
	- Develop the physical development flow
	- Floor Plan, Power Plan, Place & Route, Clock Tree synthesis, timing/SI closure of blocks & full chip
	- Padring, RDL development
	- DRC/LVS
	- IR/EM analys
	- DFT knowledge is a plus
	- Excellent communication with internal team and customers
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	Memory Compilers



+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















###	Static Timing Analysis (STA), and Statistical Static Timing Analysis (SSTA)



+ skill set:
	- timing convergence issues associated with deep-submicron processes for high-performance design
		* crosstalk delay
		* noise
		* glitch
		* POCV
		* IR-STA
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












###	VLSI Verification



Focuses on non-formal VLSI verification, other than circuit simulation and physical verification, such as:
+ logic simulation
+ fault simulation
+ RTL simulation
+ intelligent verification, intelligent testbench automation



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.























###	VLSI Design for Manufacturing, DFM


####	Notes about VLSI DFM



Skills sets for DFM, especially reactive DRM (e.g., exploiting computational lithography for pixelization of layout designs), include:
+ RET, resolution enhancement techniques
+ OPC, optical proximity correction
	- rules-based OPC
	- model-based OPC
	- inverse OPC
	- SRAF, sub-resolution assist features
+ PSM, phase-shifting mask
+ parametric DFM for semiconductor manufacturing yield optimization
	- statistical circuit simulation, with statistical SPICE models
		* Monte Carlo analysis
		* response surface modeling
		* mismatch simulation
+ OAI, off-axis illumination
+ STI, shallow-trench isolation




####	Skill Sets for VLSI DFM



Sets of skills for DFM:
+ Direct or indirect experience in OPC (Optical Proximity Correction), including rogorious lithography simulation (Hyperlith, Prolith), RET, and advanced mask technology.
+ Solid understanding of imaging theories (Abbe, Hopkins).
	- Abbe-PCA (Abbe-Hopkins): microlithography aerial image analytical compact kernel generation based on principle component analysis
	- Hybrid Hopkins-Abbe method for modeling oblique angle mask effects in OPC
	- Application of the hybrid Hopkins–Abbe method in full-chip OPC
	- transmission cross coefficients (TCCs)
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









###	Numerical EDA, other than circuit simulation


####	Notes about Numerical EDA, other than circuit simulation

This includes:
+ electromagnetic field solvers
+ layout extraction
+ parasitic extraction
+ signal integrity analysis
+ power integrity analysis
+ voltage drop analysis
+ electromigration lifetime checks
+ noise analysis
	- static noise analysis
	- crosstalk analysis
	- mitigation of noise coupling



####	Skill Sets for Numerical EDA, other than circuit simulation


The sets of skills are:
+ The successful candidate must be an expert in field solver-based parasitic extraction and be able to quickly become an expert in new simulation approaches and to develop robust, maintainable, and efficient code.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








###	IC Testing, including digital and mixed-signal VLSI Testing


####	Notes about IC Testing

This includes:
+ ATPG, automatic test pattern generation
+ BIST, built-in self-test
+ DFT, design for testability
+ high-level test synthesis



####	Skill Sets for IC Testing


The sets of skills are:
+ skill set:
	- Background in 3D computer graphics, including APIs such as OpenGL
	- Proficient in Java, Maven, Python, Jenkins/Groovy, Vagrant/Docker
	- Good knowledge in DFT: OCC insertion (for on-chip clock controllers), ATPG generation
+ skill set:
	- This role will support the Siemens EDA DFT products which will involve building relationships with our customers, helping them embrace and deploy the Tessent product lines. It is a frontend team which works on top of the analyzer and elaborator.
	- We are not looking for hard workers, just super minds
	- You’re a Graduate / Postgraduate (Bachelors/Masters) in EEE) / ECE/Computer Science (CS) from top reputed Engineering colleges with significant experience in software development.
	- Working Experience should be 8 + Years of experience.
	- Experience in EDA will be a phenomenal plus.
	- Your sound understanding of C/C++ languages, design patterns along with data structure and algorithms will be key to development of software.
	- Your understanding of HDL languages – Verilog/VHDL/System Verilog - low power aware synthesis and power formats - UPF/CPF – will supplemental.
		* ***Si2 Common Power Format, or CPF***
		* ***Unified Power Format, or UPF***
	- Knowledge or experience on flex/bison will be advantage.
	- Understanding of gate level digital logic design.
	- Experience in Analyzer/Compiler development.
	- Your good analytical, abstraction and communication skills will help in creating bigger and sustainable solutions for complex systems.
	- Your ability to work with multi-functional teams will help in good crafting solutions that resolve actual customer issues.
+ Experience in usage of debugging targets JTAG, BDI
	- ***Joint Test Action Group, JTAG***
	- ***BDI (Background Debug Interface)***
	- ***BDM (Background Debug Mode)***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











###	Post-Silicon Validation


####	Notes about Post-Silicon Validation

Skill set for post-silicon validation and post-silicon debugging.
+ complements VLSI simulation (ESL/TLM simulation, RTL simulation, logic simulation, and circuit simulation), VLSI formal verification and logic emulation
+ use system-boards, logic analyzers, and assertion-based tools with VLSI testing for post-silicon validation and post-silicon debugging
+ use of hardware emulator, which is like hardware acceleration for RTL/logic simulation
+ in-circuit emulation
+ hardware virtualization
	- hardware-assisted virtualization, via platform virtualization
		* also known as accelerated virtualization, hardware virtual machine or HVM, native virtualization


####	Skills about Post-Silicon Validation

The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











###	Technology CAD, TCAD


####	Notes about Technology CAD

+ DTCO
	- Support tools
		* BACPAC, Berkeley Advanced Chip Performance Calculator
			+ Estimate impact of semiconductor manufacturing process technology node, or process node
				- semiconductor manufacturing process technology node is defined by the minimum feature size





####	TCAD for Process Simulation



Notes about TCAD for process simulation:
+ ion implantation
+ diffusion
+ oxidation
+ dry/wet etching
+ deposition
+ ***optical lithography, photolithography, or optical photolithography***, and next-generation lithography (NGL) techniques for optical lithography
	- ***computational lithography***, or computational scaling
		* Mathematical and algorithmic approaches to improve attainable resolution from optical lithography.
		* For 22 nm semiconductor manufacturing process technology nodes and beyond.
		* To fix problems with:
			+ 193 nm deep UV optical lithography.
		* includes DFM techniques, such as:
			+ resolution enhancement technologies, RET
				- scattering bars
				- phase-shoft masks
				- multiple/double patterning
			+ optical proximity correction, OPC
			+ source mask optimization
				- or, complex modeling of the lens system and photoresist
					* aims to improve chip manufacturability and manufacturing yield
					* use signature of scanner to improve:
						+ OPC model
						+ polarization characteristics of the lens pupil
						+ Jones matrix of the stepper lens
						+ optical parameters of the photoresist stack
						+ diffusion through the photoresist
						+ stepper illumination control variables
	- multiple patterning, or multi-patterning
	- classifications:
		* ultraviolet lithography, UV lithography
		* X-ray lithography
		* EUV lithography
	- EUV lithography, extreme ultraviolet lithography, EUVL
		* 13.5 nm extreme ultraviolet lithography
		* assist features
		* source mask optimization
		* phase shift masks
		* EUV photoresist exposure
		* contamination effects
			+ resist outgassing
			+ tin deposition
			+ hydrogen blistering
			+ resist erosion
			+ membrane
		* mask defects
		* throughput scaling issues
			+ EUV stochastic issues
		* used with multile patterning
		* single patterning extension, anamorphic high-NA (NA, numerical aperture)
	- deep UV immersion lithography, immersion lithography
	- X-ray lithography
	- BEUV lithography, or beyond extreme ultraviolet lithography
		* about 6.7 nm wavelength
	- maskless lithography, MPL
		* electronic beam lithography, e-beam lithography, EBL
		* plasmonic direct writing lithography
		* optical maskless lithography
			+ multiphoton lithography, ***direct laser writing***, direct laser lithography
	- quantum optical lithography, QOL
	- other non-mainstream lithography techniques
		* nanoimprint lithography
			+ thermoplastic nanoimprint lithography
			+ photo nanoimprint lithography
			+ resist-free direct thermal nanoimprint lithography
		* molecular self-assembly lithography
		* stencil lithography
		* charged-beam lithography
			+ ion beam lithography, or ion-beam lithography, or ion-projection lithography
				- ion beam proximity lithography, IBL
				- focused ion beam lithography, FIB
				- similar to:
					* electronic beam lithography
				- electron-projection lithography
		* magnetolithography, ML
			+ photoresist-less and photomaskless lithography
			+ backside lithography
		* plasmonic lithography, plasmonic nanolithography, plasmonic photolithography
		* ***soft lithography***
			+ use elastomeric stamps, molds, and conformable photomasks to fabricate or replicate structures
			* PDMS lithography
			* microcontact printing
			* multilayer soft lithography
		* laser printing lithography
			+ laser printing of single nanoparticles
		* nanosphere lithography, NSL
		* direct-write lithography process
			+ proton beam lithography, or p-beam writing, or proton beam writing
		* multiphoton lithography, direct laser lithography, direct laser writing
		* ***scanning probe lithography, SPL***
			+ mechanical/thermo-mechanical SPL, m-SPL
			+ thermal SPL, t-SPL
			+ thermo-chemical SPL, tc-SPL, or thermochemical nanolithography, TCNL
			+ dip-pen SPL, dp-SPL, or dip-pen nanolithography, DPN
				- thermal dip-pen lithography
				- beam pen lithography
			+ local oxidation lithography, o-SPL
			+ bias-induced SPL, b-SPL
			+ current-induced SPL, c-SPL
			+ thermally-assisted magnetic SPL, tam-SPL
		* local oxidation nanolithography, LON
		* interference lithography, or holographic lithography
			+ not maskless lithography
			+ no 1:1 imaging system in between
			+ electron holographic lithography
			+ atom holographic lithography
		* nanofountain darwing, or nanofountain probe
+ silicidation
+ modeling mechanics of semiconductor manufacturing processes
+ CMP, chemical-mechanical polishing, chemical-mechanical planarization
+ annealing (diffusion and dopant activation)
+ epitaxy










Sets of skills for process simulation TCAD are:
+ skill set:
	- Senior Software Engineer · ASML · United States
	- ASML is an innovation leader in the semiconductor industry. We provide chipmakers with everything they need – hardware, software, and services – to mass-produce patterns on silicon through lithography. Our lithography machines are a hybrid of high-tech hardware and advanced software
	- Senior Software Engineer at ASML play a critical role in owning and driving the technical details of the projects that would ultimately drive customer satisfaction. We are looking for an experienced engineer and technical leader with strong analytical, design and development skills, technical depth, curiosity, enthusiasm, integrity and being results oriented. You will be a key member of our highly integrated multi-disciplinary matrixed team of software, systems, applications and algorithm engineers. If you have a desire to work collaboratively while solving tough problems across real full software stacks including hardware, firmware, OS, desktop applications and web applications, we want to hear from you.
	- Working at the cutting edge of tech, you’ll always have new challenges and new problems to solve – and working together is the only way to do that. You won’t work in a silo. Instead, you’ll be part of a creative, dynamic work environment where you’ll collaborate with supportive colleagues. There is always space for creative and unique points of view. You’ll have the flexibility and trust to choose how best to tackle tasks and solve problems.
	- The California base annual salary/hourly range for this role is currently $154,125 - $256,875.
+ skill set:
	- computational lithography
	- If you're a motivated individual with a strong understanding of software development, and you'd like to collaborate and continue learning with other engineers who are passionate about and experienced in developing our computational lithography models (optics, photoresist, etching, machine learning, etc.), come and join our computational lithography modeling team!
	- To learn more about what we do: https://eda.sw.siemens.com/en-US/ic/calibre-manufacturing/computational-lithography/
	- You bring your programming and scientific experience to learn how to model semiconductor manufacturing processes and implement these models in a computational framework with C++.
	- You'll be responsible for developing new features, debugging and supporting existing models, and you'll be able to suggest and provide new directions for improving existing algorithms.
	- We would love to hear from you if you have B.S. or M.S. in Computer Science, Electrical Engineering, Physics, Mathematics, or similar and you have a good Understanding of physics, mathematics, optimization, or engineering and Experience in C++ on UNIX and/or LINUX platforms.
	- Your collaborative spirit is essential to develop critical components consistently and on time with other team members.
	- You are curious and enjoy discovering new technologies.
	- You have initial project experience as well as good analytical skills and an appetite for problem-solving.
	- Ideally, you will have some initial experience with Python, MATLAB and Tcl/Tk, and some knowledge of semiconductor manufacturing and computational lithography is a plus.
	- As you work in an international team, English (spoken and written) is a must to have
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






####	TCAD for Device Simulation



Notes about TCAD for device simulation includes:
+ device modeling for common transistors
	- MOSFET, metal-oxide-semiconductor FETs 
		* a type of insulated-gate FET
			+ enhancement mode MOSFET
			+ depletion mode MOSFET
		* p-channel MOSFET, PMOS
		* n-channel MOSFET, NMOS
			+ ggNMOS, grounded-gate NMOS
				- electrostatic discharge (ESD) protection device
		* CMOS, complementary MOSFET
			+ advantages:
				- high noise immunity
				- low static power consumption
		* ***FGMOS, floating-gate MOSFETs***
			+ for floating-gate memory cell, digital storage element in:
				- EPROM
				- EEPROM
				- flash memory
		* MOS capacitor
		* intrinsic MOSFET device modeling
			+ inversion-layer mobility modeling
			+ channel charge modeling
			+ threshold voltage modeling
		* substrate effects on MOSFETs
			+ subthreshold current
			+ drain-induced-barrier-lowering current
		* parasitic junction & inhomogeneous substrate effects
		* ***MuGFETs, multi-gate MOSFETs, multi-gate FETs***, or ***multi-gate semiconductor devices***
			+ The multiple gates can be controlled by:
				- a single gate electrode, where the multiple gate surfaces act electrically as a single gate
				- independent gate electrodes
					* ***MIGFETs, multiple-independent-gate FETs***
			+ 3-D transistors, or non-planar transistors
				- ***FinFET, fin FETs***
				- ***GAAFET, gate-all-around FETs***
					* sometimes called, SGT, "surrounding gate transistors"
					* MBCFETs, multi-bridge channel FETs
			+ ***DGMOS, DGMOSFET, dual-gate MOFETs, double-gate MOFETs***
				- planar double-gate MOFETs
				- double-gate TFT, double-gate thin-film transistors
				- with silicon thin film in:
					* strong inversion, volume-inversion MOSFET
					* strong accumulation, volume-accumulation MOSFET
			+ tri-gate MOSFETs
			+ FlexFET: planar, independently double-gated transistor
	- other field-effect transistors, FETs
		* propeties and charracteristics
			+ FETs are 3-terminal devices:
				- source
				- gate
				- drain
			+ FETs are unipolar transistors
			+ source-gated transistors
		* ***MISFET, metal-insulator-semiconductor FETs***, or ***insulator-gate FETs (IGFETs)***
			+ all MOSFETs are MISFETs
			+ but, not all MISFETs are MOSFETs
			+ insulators can be:
				- silicon dioxide
				- organic insulators for organic FETs
		* ***QFET, quantum FET***; or, ***QWFET, quantum well FET***
			- exploit quantum tunneling
		* ***TFT, thin-film transistors***
			+ metal oxide thin-film transistors, metal oxide TFT
				- or, oxide thin-film transistors, oxide TFT
			+ TFT LCDs, TFT liquid-crystal displays
		* ***CNTFET, carbon nanotube FET***
		* MESFET, metal-semiconductor FET
		* JFETs, junction FETs, junction-gate FETs
		* VTFET, vertical-transport FET
		* Fe FET, ferroelectric FET
		* GFET, graphene-based FET
			+ GNRFET, graphene nanoribbon FET
		* NOMFET, nanoparticle organic memory FET
		* SB-FET. Schottky-barrier FET
		* VeSFET, vertical-slit FET
		* HEMT, high-electron-mobility FET, HFET, heterostructure FET
		* TQFET, topological quantum FET
		* TFET, tunnel FET
		* MODFET, modulation-doped FET
		* HIGFET, heterostructure insulated-gate FET
		* DEPFET, FET formed in fully depleted substrate and simultaneously act as a sensor, amplifier, and memory node
		* JLNT, junctionless nanowire transistor
		* Bipolar-MOS transistors:
			+ BiCMOS, Bipolar CMOS
			+ IGBT, see information under "power semiconductor devices"
		* MOS sensors
		* RHBD, radiation-hardened-by-design
			+ use enclosed-layout-transistors, ELTs
			+ H-gate, another RHBD MOSFET
			+ shallow trench isolation designs
		* OFET, organic FET
			+ use semiconductor device architecture of TFT, thin-film transistors
		* ChemFET, chemically-sensitive FET
			+ FET used as a sensor for measuring chemical concentrations in a solution
			+ compare to chemiresistors
		* ISFET, ion-sensitive FET
			+ for measuring ion concentrations in a solution
		* BioFET, Bio-FET, biosensor FET, FET-based biosensor, field-effect biosensor, FEB, biosensor MOSFET
		* DNAFET, DNA FET
			+ biosensor that is based the field effect due to partial charges of DNA molecules
		* VMOS, vertical MOSFET, V-groove MOSFET
			+ MOSFET with V-groove shape vertically cut into the substrate material
		* Additional notes:
			+ flowFET
				- microfluidic analog of FETs
				- a microfluidic component
	- transistors manufactured using:
		* SOI, silicon-on-insulator
			+ types of insulators:
				- silicon dioxide
				- sapphire
					* SOS, silicon on sapphire
			+ ***SOI MOSFETs***
				- ***PDSOI, partially depleted SOI MOSFETs***
				- ***FDSOI, fully depleted SOI MOSFETs***
		* GOI, germanium-on-insulator
	- BJTs, bipolar junction transistors
	- SiGe, silicon germanium
	- GaAs, gallium arsenide
	- SiC, silicon carbide
	- LET, light-emitting transistors
		* organic LETs
			+ for digital displays and on-chip optical interconnects
			+ can be used in an active matrix of OLETs, which OLEDs cannot be used to form
				- OLEDs can only form active matrix of OLEDs, in combination with switching elements (such as TFTs, thin-film transistors)
	- for LEDs:
		* InAs, indium arsenide
		* InSb, indium antimonide
		* InP, indium phosphide
		* organic LEDs:
			+ use organic semiconductors
	- for photovoltaic solar cells:
		* selenium sulfide
	- RF CMOS
		* based on LDMOS
	- power semiconductor devices, for power electronics:
		* power MOSFETs
			+ HexFET, hexagonal type of power MOSFET
			+ UMOS, trench-MOS, trench-gate MOSFET
		* DMOS, double-diffused MOSFET
			+ LDMOS, lateral DMOS, lateral-diffused MOSFET, laterally-diffused MOSFET
				- planar double-diffused MOSFET
				- RF LDMOS
					* for power amplifiers and other applications
			+ VDMOS, vertical DMOS
		* IGBT, insulated-gate bipolar transistor
		* SCR, silicon-controlled rectifiers
		* thyristors
			+ GTO, gate turn-off thyristors
			+ MCT, MOS-controlled thyristors, or MCT, MOSFET-controlled thyristors
				- MOS-gated thyristors
			+ IGCT, integrated gate-commutated thyristors
		* triac
		* diodes
			+ Schottky diodes, or Schottky barrier diodes, or hot-carrier diodes
			+ PiN diodes
				- has wide, undoped instrinsic semiconductor region between p-type semiconductor and n-type semiconductor
				- for RF applications:
					* RF power amplifiers
		* silicon-controlled switches
		* classifications:
			+ based on number of terminals:
				- 2-terminal semiconductor devices
				- 3-terminal semiconductor devices
				- 4-terminal semiconductor devices
			+ based on proportion of carriers:
				- majority carrier devices
				- minority carrier devices
		* parameters:
			+ breakdown voltage
			+ on-resistance
			+ rise & fall times
				- rise times
				- fall times
			+ safe-operating area
			+ thermal resistance
	- point-contact transistors
	- UJT, unijunction transistor
		* programmable UJT, or programmable unijunction transistor
	- *transistor classification based on number of terminals*:
		* tetrode transistors:
			+ 4 active terminals
		* pentode transistors:
			+ 5 active terminals
+ device modeling for other devices:
	- memristors
	- memtransistors
		* 2-terminal devices
		* 7-terminal devices
	- memistors
		* 7-terminal devices
	- molecular electronics
		* molecular scale electronics, or single-molecule electronics
	- [organic electronics](https://en.wikipedia.org/wiki/Organic_electronics)
		* organic electronics
			+ organic solar cells
			+ photovoltaics
		* organic field-effect transistors, organic FETs, OFETs
		* organic light-emitting diodes, OLED
			+ active-matrix OLED, AMOLED
	- supramolecular electronics
	- trancitors, "transfer-capacitors"
+ large-signal nonlinear semiconductor device modeling
	- physical models, based on semiconductor device physics
		* approximate physical phenomena
		* pamateric models for physical properties:
			+ oxide thickness
			+ substrate doping concentration
			+ carrier mobility
		* for simplified estimates of signal-swing limitations
	- empirical models, using curve fitting with experimental data to obtain statistical models
+ small-signal nonlinear semiconductor device modeling
	- to evaluate:
		* stability
		* gain
		* noise
		* bandwidth
		* bias point, or Q-point or quiescent point
	- small-signal parameters, from production-line testing and circuit design
		* for predicting:
			+ circuit gain
			+ input impedance
			+ output impedance
		* from parameter sets of 2-port networks:
			+ T-parameters, transmission parameters
			+ h-parameters, hybrid-parameters
			+ z-parameters, impedance parameters
			+ y-parameters, admittance parameters
			+ S-parameters, scattering parameters
+ methods:
	- explicit solution
	- iterative solution
	- graphical solution
+ types of modeling approximation
	- piecewise linear model
	- mathematically idealized semiconductor device models



Sets of skills for device simulation TCAD are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








###	Mask Data Preparation, MDP


This includes:
+ layout-to-mask preparation
+ mask generation



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







###	Process Design Kit (PDK) Development



The sets of skills for Process Design Kit (PDK) are:
+ skill set:
	- Design Kit Development: Developing Siemens EDA Design Kits for worldwide foundries, including OpenAccess PDK, iPDK, and TDK development to keep competence and leading the innovation.
	- Technical Contribution: Growing the development of design kits in Taiwan. Demonstrate the ability to drive and encourage a highly productive development team.
	- Communication: Being passionate to communicate technical issues within the team and division efficiency and optimally. Demonstrating the ability to communicate within a globally distributed division.
	- Technological Comprehension: Understanding the several methods used to develop Design Kits, identifying weaknesses and advantages of each. The candidate will chip in technical expertise to a team, develop the kits, the software, and the processes used to build each.
	- The Design Kit Development Engineer will be part of a team of design kit developers responsible for the development, QA, and delivery of high quality Process Design Kits (PDK) to Siemens EDA foundry partners. The appropriate candidate will have experience in the development and QA of process design kits including Pcell, symbol, and device callbacks. We are looking for an engineer who can adapt to different methodologies used in the development, QA and use of Design Kits from different EDA companies and foundries.
	- BS in CS/EE with 5+ years or MS with 3+ years of practicing in EDA/CAD engineering is needed.
	- Showing strong programming skill in languages like Python, TCL, Perl, C or C+ - Deep understanding of scripting, CAD engineering and layout design methodologies.
	- Proven experience in any of design kits development cycle, devices layout and physics is a must.
	- Self motivated, quick learner and able to work independently in a large amount of automatically generated code.
	- Ability to lead multi-functional projects, working closely with marketing and sales on the entire development process from requirements through end-customer delivery.
+ skill set:
	- With Bachelor or Master’s degree in EE, CE, or CS with above 24-year validated experiences on large-scale enterprise software projects is extraordinary but not required. Although, of course, that if you know how to develop new algorithms, handle external tool integration and optimization in large-scale software projects, is much appreciated.
	- You have a good understanding of topics around C/C++ software development and debugging techniques.
	- Bachelor of Science or Master of Science. in Electrical Engineering, Computer Science, Computer Engineering with 24-year experiences in ASIC multiple die graphic presentation in OpenGL & design multiple die data.
	- C/C++ proficiency
	- Knowledge of algorithm development, graph theory and geometry manipulation and debugging skill
	- Knowledge of Lex, Yacc, Qt / OpenGL, Linux, Shell script and Tcl language would be a plus
	- Knowledge of multithreaded implementation and Linux/UNIX is a plus
	- Prior experience with EDA design file formats is highly preferred
	- Excellent English & Mandarine languages communication skill
+ skill set:
	- Principal Software Engineer
	- Cadence is seeking candidates to fill open positions in the EMX Product Engineering team as more customers utilize EMX’s world-class capabilities to design and develop their new products.
	- Cadence is a pivotal leader in electronic design, building upon more than 30 years of computational software expertise. The company applies its underlying Intelligent System Design strategy to deliver software, hardware and IP that turn design concepts into reality.
	- Cadence customers are the world’s most innovative companies, delivering extraordinary electronic products from chips to boards to systems for the most dynamic market applications including consumer, hyperscale computing, 5G communications, automotive, aerospace industrial and health.
	- As an EDA Development Engineer for the EMX family of products, you will be responsible for capturing the requirements of, developing, testing and delivering software integrations between EMX / EMX Designer and other EDA tools within the Cadence EDA portfolio. You will work closely with other members of the EMX Product Engineering and R&D teams to respond to development and enhancement requests, focusing primarily upon the usability and user experience of EMX and EMX Designer and their interactions with other Cadence tools. Some travel may be required.
	- Develop, enhance and maintain the integrations of the EMX product family and their interactions with the Cadence Virtuoso Studio and Virtuoso ADE product families.
 	- Work in cooperation with local/international Field Applications team members to capture feedback and usability improvement requests for the EMX family of products.
 	- Work in cooperation with the EMX Product Engineering and R&D teams to transform feedback and improvement requests into detailed requirement specifications.
 	- Translate the requirement specifications into working software implementations.
 	- Develop in SKILL / SKILL++ / Python / Perl / shell scripting.
 	- Perform detailed testing of the software implementations in preparation for release to customers.
 	- Author documentation and work with Technical Publications to ensure its inclusion into correct and easy to understand documents.
 	- Bachelor or master’s degree (or equivalent) in Computer Science/Engineering
 	- A programming background with at least 8 years of professional experience
 	- Experience writing clean, structured and maintainable code
 	- In-depth knowledge of SKILL, SKILL++, Python, Perl and shell scripting and their usage in EDA tool and flow development
 	- Prior development experience with integrations into Cadence EDA tools such as Cadence Virtuoso Studio and Virtuoso ADE product families
 	- Passionate about usability and delivery of right-the-first-time integrations
 	- Creativity and out-of-the-box thinking
 	- Capable of giving and accepting constructive feedback
 	- Collaboration and customer interaction skills
 	- Oral and written English proficiency
 	- Experience working in an agile and fast-paced product development environment
 	- Experience with GUI development is a plus
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











###	Physical Verification


Includes:
+ DRC, design rule check
+ LVS, layout verses schematic check
+ XOR check
+ antenna check
	- check for antenna effects
+ ERC, electrical rule check
	




The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.





###	MEMS CAD, and NEMS CAD



The sets of skills are:
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






















###	Other EDA topics


Other EDA topics:
+ design databases
+ ***radiation hardening***
	- ***radiation-hardened*** ICs, ***rad-hardened*** ICs, ***rad(iation)-hardened*** ICs, or ***rad-hard*** ICs, or ***hardened*** ICs (with context provided to indicate ***radiation-hardened*** ICs)
	- protect against:
		* TID, total ionizing dose
		* ELDRS, enhanced low dose rate effects
		* neutron and proton displacement damage
		* SEEs, single event effects
	- sources of radiation damage:
		* cosmic ray radiation
		* solar particle events
		* Van Allen radiation belts* 
		* secondary particles resulting from the interaction of other kinds of radiation with structures around the electronic devices
		* nuclear reactors, in nuclear power plants
			+ gamma radiation
			+ neutron radiation
		* particle accelerators
		* nuclear explosions
		* chip packaging materials
			+ fixed by using:
				- more pure, or purer, chip packaging materials
				- error correction code, or error correcting code, ECC
	- radiation effects on electronics
		* fundamental mechanisms
			+ lattice displacement
			+ ionizing effects
		* resultant effects
			+ total ionizing dose effects
			+ transient dose effects
			+ systems-generated EMP effects
		* digital damage of SEEs, single event effects
			+ single-event transient
			+ single-event upset
			+ single-event latchup
			+ single-event snapback
			+ single-event induced burnout
			+ single-event gate rupture
		+ SEE testing, or testing for single event effects
	- radiation hardening techniques
		+ physical radiation hardening
		+ logical radiation hardening








###	Hardware Security


####	Notes about Hardware Security

hardware security topics from Trust-Hub website Trust-Hub.org:
+ System-on-chip (SoC) Attacks and Security
+ Intellectual Property (IP) Trust and Assurance
+ Reverse Engineering
+ Invasive and Semi-invasive Physical Attacks
+ Computer-aided design (CAD) for Security
+ Side Channel Attacks and Mitigation
+ Hardware Security Primitives (PUFs, TRNGs, etc.)
+ Hardware Obfuscation
+ Hardware Trojans and Backdoors
+ Counterfeit Electronics
+ FPGA/eFPGA Security
+ IoT and Cyber-physical System Security
+ Emerging and Nanoscale Device Security




####	Skill Sets for Hardware Security



+ skill set:
	- Experience with hypervisor / container development
		* Especially, Xen or OpenXT
	- Experience with Trusted Platform Module (TPM)
	- Experience with firmware-level code
	- FPGA physical design
	- Experience with device characterization or PUF techniques
		* physical unclonable function
	- Experience with ASIC analog and/or digital design
+ skill set:
	- Experience with RoT techniques including TPM
	- Silicon Root of Trust (RoT)
	- trusted platform module (TPM)























####	Skill Sets for Cyber-Physical System Security


Cyber-physical system security includes:
+ automotive security
+ autonomous underwater glider, AUG
+ robotics security



Skill sets for cyber-physical system security, including robotics security and automotive security:
+ experience with automotive security:
	- CAN
	- OTA
	- Autosar
	- SILS/HILS
		* SILS, software-in-the-loop simulation
		* HILS, hardware-in-the-loop simulation, HWIL, HITL
			+ for automotive anti-lock braking systems
				- vehicle dynamics, such as suspension, wheeels, tires, roll, pitch, and yaw
				- dynamics of brake systems's hydraulic components
				- road characteristics
			+ fixed-wing aircraft flight control systems
				- fly-by-wire control systems
		* EILS, emulation-in-the-loop simulation
	- RTOS
	- QNX
	- AGL
+ programming language security, for RISC processes:
	- function block diagram, FBD
	- ladder diagram, LBD
	- structured text, ST
	- instruction list
	- sequential function chart, SFC
+ skill set for vehicle security architect (middleware), or product security engineer:
	- open software ecosystem
	- collaborate with cross-functional partners
	- lead and influence security architecture, risk analysis, vulnerability testing, and security reviews for vehicle products across Woven Planet cross-functional teams
	- ensure security and privacy by design for embedded or middleware software products
	- assist development teams in architecting and securing the software and hardware ecosystem
	- evaluate the security of middleware, libraries, and protocols for embedded systems
	- lead threat modeling towards components of the hardware abstraction layer, service layer, runtime environment and application layer
		* identify security issues, risks, and associated mitigations
	- audit C++ code to identify and patch security vulnerabilities
	- provide designs to implement the architected solutions
	- evaluate and recommend new and emerging security products and technologies
	- 3 years of experience in software security as an architect, or a developer of security solutions
	- knowledge and experience in the following domains:
		* security engineering
		* system and network security
		* authentication and security protocols
		* cryptography
		* operating systems
		* application security
	- security expertise in at least one of the following:
		* implementation of multilayered independent levels of security (MILS) architecture for high-assurance embedded systems
		* operating system, ARM, and kernel security
		* security of components of compile time and runtime environment (RTE)
		* practical experience of development of libraries (C/C++) for software security
	- knowledge of DevSecOps methodology and components of a secure SDLC
	- experience with secure operating systems architecture, security design, and threat modeling
	- experience with:
		* development of:
			+ software components for automotive systems or robots
			+ hardware abstraction layer (HAL) components
		* security of real-time operating system, RTOS
		* in-depth knowledge of UNIX-like operating systems and their security components, preferable - used in automotive industry, such as automotive grade Linux
		* vehicle software development
		* kernel and hypervisor security
	- understanding of standards:
		* ISO 21434
		* ISO 26262
+ skill set:
	- develop and review requirements, and strategy and road map, for:
		* static security testing
		* dynamic security testing
	- drive investigations into security issues, identify opportunities for further automation to eliminate future issues
	- knowledge of application security tools:
		* SAST
		* DAST
		* SCA
	- experience managing execution and high-quality product delivery
	- ability to handle multiple competing priorities in a fast-paced environment
	- excellent analytical and communication skills, as well as ability to take initiative and build productive relationships
	- experience in the design and implementation of security solutions, systems, and mechanisms:
		* data security
		* application security
		* cryptography
		* systems security
		* network security
		* exploit development
+ skill set:
	- participate in threat modeling and application security reviews
	- audit embedded code to identify security vulnerabilities
	- vehicle penetration testing experience
	- knowledge or experience of advanced smart fuzzing strategies:
		* mutational
		* symbolic
		* American Fuzzy Lop, AFL
	- experience with QNX
	- understanding of vehicle functional safety standards:
		* ISO26262
+ skill set:
	- security-critical code at scale
	- security automation tools and processes
	- information security:
		* threat modeling
		* secure code review
		* security testing
	- build and maintain security tooling and infrastructure
	- lead the design and engineering of static analysis tooling
		* SAST
		* semantic code analysis
		* vulnerability management
	- foster a culture of automation, and build sustainable tooling systems
	- identify application security risks, define requirements, and then build and extend systems to help reduce and track these risks






##	Additional Information about EDA

+ [IP-XACT is an XML format that defines and describes individual, re-usable electronic circuit designs (individual pieces of intellectual property, or IPs) to facilitate their use in creating integrated circuits (i.e. microchips). IP-XACT was created by the SPIRIT Consortium as a standard to enable automated configuration and integration through tools.](https://en.wikipedia.org/wiki/IP-XACT)
+ [The SystemRDL language, supported by the SPIRIT Consortium, was specifically designed to describe and implement a wide variety of control status registers. Using SystemRDL, developers can automatically generate and synchronize register views for specification, hardware design, software development, verification, and documentation.](https://en.wikipedia.org/wiki/SystemRDL)
	- [SystemRDL Compiler](https://github.com/SystemRDL/systemrdl-compilerSystemRDL Compiler)
	- [open-register-design-tool, Ordt](https://github.com/Juniper/open-register-design-tool)
	- https://www.eda.org/images/downloads/standards/systemrdl/SystemRDL_2.0_Jan2018.pdf
+ Automatic Register Verification (ARV)
+ synthesizable RTL, UVM, c-header, RALF eRM, SystemRDL, IP-XACT
+ UVM, Universal Verification Methodology, SystemVerilog based
+ VMM, Verification Methodology Manual
+ OVM, Open Verification Methodology
+ URM, Universal Reuse Methodology
+ eRM, e Reuse Methodology, e Verification Language











##	Generic Skill Sets for Lower-Level EDA Software Development




+ skill set:
	- Python Experts for Electronic Design Automation (EDA) tool development  
	- EDA tool development for SoC design, integration & verification
	- Model hardware design problems in software and provide automation
	- Create UML models for design data and generate code from it
	- Create data structure to hold design data, fill them with formalized data (e.g. XML, HDL-Models), and generate code out of it.
	- Educational Qualification:  Bachelor's / Master Degree in  Electronics & Communication Engineering / VLSI 
	- Experience: 1 - 4 years related experience.
	- Strong Expertise in Python / C++ , Data structures, Algorithms, OOPS concepts
	- Exposure to Artificial Intelligence / Machine Learning / Deep Learning concepts
	- Experienced in designing/ coding in software/EDA tools and flows
	- Exposure to Verilog / VHDL / System Verilog / SystemC
	- Understanding of UML modelling language preferred
	- Experienced in using test automation framework like pytest or Google Test.
	- C++, SoC Design, FPGA
	- Willing to dive into new things and pick them up fast
	- Good analytical skills required to evaluate complex situations
	- Focus on exchange of complex technical information
	- Good reporting and presentation skills
	- Good understanding of internal interfaces













##	VLSI Deep Learning & Embedded Deep Learning





Sets of skills for embedded machine learning engineers, VLSI machine learning engineers, embedded deep learning engineers, and VLSI deep learning engineers:
+ skill set:
	- 5+ years of experience building production software systems within large engineering projects for consumer products on mobile SoCs, specially iOS devices
	- Hands-on experience with at least one compiled language (C/C++/Objective-C, Swift, Go, Java, Rust, etc.), and multi-threaded applications
	- Familiarity with modern mobile development frameworks (e.g., Flutter, Xamarin, Swiftic) and tools (e.g., IoC/DI, analytics, A/B testing, CI-CD and build systems like Bit, Buck, Bazel)
		* https://en.wikipedia.org/wiki/List_of_build_automation_software
		* https://en.wikipedia.org/wiki/Buck_(software)
		* https://en.wikipedia.org/wiki/BitBake
		* https://bit.dev/
	- Experience with profiling and tracing tools
	- Experience with Model Compression techniques (Quantization, Pruning, Distillation)
	- Experience with ***PyTorch Mobile, Tensorflow Lite*** or other similar Edge Inference frameworks
	- Experience with techniques to offload compute to GPU, DSP etc.
	- Experience developing Machine Learning models, especially for resource constrained computing environments
+ skill set:
	- Responsible for or assist in the definition of next-generation GPGPU chips and the planning of related software and hardware products. Collect key appeals through architecture research, market research, competitive product analysis, customer interviews, etc., plan chips and hardware products and be responsible for product competitiveness;
	- Lead or participate in the definition of chip specifications, participate in the whole process management of chip architecture design, chip mass production, chip enablement and product optimization; and be responsible for defining various hardware products to meet customer scenario needs;
	- Familiar with various GPGPU and AI chip architectures, and have a deep understanding of deep learning applications (such as CV/NLP/ASR/RecSys, etc.);
	- Familiar with mainstream artificial intelligence frameworks in the industry, and understand the requirements for high-performance GPGPU deployment in data centers;
+ skill set:
	- Perception System Engineer - SPG
	- As Perception System Engineer on a revolutionary Apple project, you will be working on an autonomous system built on state of the art sensing technologies and ground breaking machine learning algorithms. The Perception team provides sense capabilities such as detection, classification, tracking, and observed maps in complex environments using a range of sensing modalities. You will play a key role in measuring end-to-end system performance, identifying key issues and provide detailed feedback for performance improvement. You will engage cross functionally with a wider range of experts to build a robust and scalable triage and measurement system. You will use statistical modeling and develop expertise in Perception system performance trends, forecasting methodologies, and synthesize key findings for leadership reviews.
	- 5+ years of experience in testing, QA or algorithm development for Autonomous Perception systems
	- A deep understanding of perception functions its impact on motion planning
	- Knowledge of machine learning models and deep learning fundamentals
	- A background in statistical analysis, system-level triage of complex systems
	- Proficient in data analysis, scripting and automation using python
	- Familiarity with data products from optical sensors like lidar and camera is desired
	- You will be developing and maintaining a Perception performance measurement pipeline that provides continuous feedback to developers for performance improvement and debugging. The work involves significant cross functional interaction with system test engineers, model, and tooling developers.
	- Defining procedure and tooling requirements for a triage and test pipeline that identifies, classifies, and measures perception failure rates.
	- Engaging with relevant partners to ensure timely implementation and delivery. 
	- Synthesize failure rate data to derive meaningful trends and sensitivities, and track measured improvements and regressions over time. 
	- Create and own dashboards for leadership reviews and develop expertise in observed system performance. 
	- Root causing and failure analyses in partnership with deep learning model developers will be essential to be effective in this role. 
	- You will also have opportunities to develop statistical models to forecast full system performance using developer metrics, critical scenario testing, and past performance.
	- Masters degree in engineering, data science, statistics, or mathematics
	- 5+ years of relevant Industry experience in robotics or autonomous systems
+ skill set:
	- R&D Director - ML Systems
	- We are a Cambridge-based startup developing a revolutionary B2B SaaS product for automated synthesis of ultra-efficient Intelligent Systems. Our product will empower Edge AI & Robotics companies to achieve supreme efficiency and flexibility, while slash the development time and costs tenfold.
	- To perform advanced R&D critical for the success of our product, we are looking for a passionate and impactful leader to direct our growing activities in designing and optimizing Computer Systems for Machine Learning (ML Systems).
	- Perform critical R&D for a revolutionary neuralware/middleware/hardware co-design product.
	- Lead a team of ninja-class engineers (many at PhD-level) with glorious achievements in performance analysis and optimization.
	- Collaborate with high-profile ML Hardware customers to create highly competitive and fully compliant submissions to MLPerf.
	- Collaborate with Robotics and Edge AI customers to apply the hard-earned optimization knowledge to real-world use cases.
	- Represent KRAI in the most active working groups of MLCommons, including Inference and Power, and contribute to the roadmap.
	- Push the number of automated KRAI submissions in each round from hundreds to hundreds of thousands!
	- A PhD or MSc in Computing or Natural Sciences, with 5+ years of post-graduate experience.
	- Deep understanding of the full software/hardware stack, including algorithms, compilers, libraries, computer architecture.
	- Expertise in domain-specific accelerators (e.g. NPUs, GPUs, DSPs).
	- Hands-on experience with ML frameworks (e.g. Torch, TensorFlow) and ***inference engines (e.g. OpenVINO, TensorRT, TFLite, ArmNN)***.
	- Familiarity with ML optimization techniques (e.g. quantization, pruning).
	- If you're a systems person, you can play to your strengths and keep growing your expertise in any of the above areas, or instead jump outside your comfort zone and learn more about Edge AI & Robotics applications. Two things are certain: a) you'll be constantly learning and pushing the boundaries of your skills and knowledge; b) it'll be fun!
	- It is a unique opportunity to advance the state-of-the-art in ML Systems by considering all critical elements of the stack: from hardware to middleware to neuralware, akin to the amazing Nand-to-Tetris approach. (In fact, we hope to write our own book about our learnings one day!)
	- Please send your CV and short covering letter to info@krai.ai - we will be happy to arrange a friendly chat! What we ask for is evidence supporting your abilities and motivation: it's up to you to decide what that evidence might be.
	- Our team at KRAI (formerly, dividiti) has come a long way from co-organizing the ReQuEST tournament at ASPLOS'18 to becoming leading contributors to MLCommons/MLPerf™, the industry-leading forum for benchmarking Computer Systems for Machine Learning (ML). As part of our journey, we have collaborated with Arm, Dell, Intel, VMware, Qualcomm and leading ML hardware startups. In particular, in our public collaboration with Qualcomm we have produced some of the fastest and most energy-efficient Inference results, both in the Datacenter and Edge category. Over the three year history of MLPerf Inference, we submitted over 50% of all results, that is, more than other 40+ submitters combined.
+ skill set:
	- Ultra-efficient Computer Systems for Edge AI and Robotics
	- Krai is a Cambridge-based startup focusing on creating ultra-efficient computer systems for Edge AI and Robotics applications. We are looking for curious and motivated R&D engineers to join us on our exciting journey!
	- Automating software/hardware co-design
	- We are building an automated platform for software/hardware (SW/HW) co-design. We envision our customers will provide their requirements such as training and validation datasets, quality targets, performance and energy efficiency constraints, etc. Then, our platform will design several candidate AI/SW/HW stacks composed of neural networks, libraries, inference engines, etc., for running on one or more HW platforms. The platform will integrate many state-of-the-art and emerging techniques such as network architecture search, network optimisation, graph compilers, etc. - all wrapped up in an intelligent meta-technology for searching through myriads of combinations and configuration options. Our automated platform will produce superior designs and slash development time and cost by 10-100 times, opening up unprecedented opportunities for Edge AI and Robotics applications.
	- Collaborating with a broad computer systems community
	- As part of our strategy, we collaborate with many leading organisations ranging from stealth-mode AI HW startups to global corporations. Our core expertise and responsibilities include compilers, runtime systems, architecture definition, performance modelling, optimization, workload mapping and benchmarking. For example, we implemented, validated and optimized the MLPerf Inference benchmarks for Qualcomm's impressive entrance with their Cloud AI 100 accelerators, achieving up to 6x energy efficiency over the entrenched competition.
	- If you're a systems person, you can play to your strengths and keep growing your expertise in any of the above areas, or instead jump outside your comfort zone and learn more about AI applications. Two things are certain: a) you'll be constantly learning and pushing the boundaries of your skills and knowledge; b) it'll be fun! What we ask for is evidence supporting your abilities and motivation: it's up to you to decide what that evidence might be.
	- If that sounds like your cup of tea, please send us your CV and covering letter to info@krai.ai - we will be happy to arrange a friendly chat.
+ skill set:
	- Microarchitecture study of next-generation MN-Core
	- We will work on various studies to improve the performance, power, and area of the next-generation MN-Core microarchitecture.
	- Communication Language: Japanese
	- Knowledge of computer architecture
	- Advanced Verilog HDL/System Verilog coding skills
	- Experience in verifying RTL by logic simulation
	- Experience in using synthesis/place-and-route tools
	- Knowledge of STA
	- Basic knowledge of deep learning
+ skill set:
	- AIML - Sr. Software Engineer, On-Device Machine Learning, Foundation Models
	- Help us bring state-of-the-art foundation models to the phone in your pocket, enabling the next generation of ML-based experiences in a privacy-preserving way! Our team is responsible for the core framework that launches neural-network workloads on Apple devices. We build the bridge between the compute resources available on Apple hardware and an entire universe of ML models, trained by feature teams throughout Apple and by our developer community. Your work on our team will enable increasingly sophisticated models throughout our products, from the computer vision models that process every camera frame in the Apple Vision Pro, to potential large language models that could transform how we interact with our computing devices. By developing the underlying representation and pipeline for these workloads, and the mechanisms for mapping them to the CPU, GPU, and Neural Engine, you will play a critical role in expanding what is possible in the Apple ecosystem.
	- Excellent C/C++ programming and debugging skills
	- Passion for API design and software architecture
	- Outstanding verbal and written communication skills
	- Experience with modern neural-network architectures and deep learning libraries
	- Expertise with performance optimization (preferred)
	- Design and implement improvements to Apple’s Model Intermediate Language (MIL), the intermediate representation of neural-network workloads shared across the inference stack
	- Develop the mechanisms for analyzing and transforming MIL workloads
	- Build the tightly integrated pipeline that optimizes and compiles models and then orchestrates their execution on device
	- Collaborate with CPU, GPU, and Neural Engine hardware backends to push inference performance and efficiency
	- Work closely with feature teams to facilitate and debug the integration of increasingly sophisticated models, including large language models
	- BS/MS/PhD in Computer Science or Electrical Engineering
	- Solid industry experience (2+ years)
+ skill set:
	- Neural Engine HW Modeling Engineer, Platform Architecture
	- At Apple, Platform Architecture is responsible for connecting our hardware and software into one unified system. Join this team, and you'll collaborate with engineers across Apple to design how all of our technologies work in unison. In this role, you will be part of the Neural Engine IP architecture team to define, architect, design, implement and deploy models for Neural Engine IP.
	- 3+ years in developing models for hardware validation
	- Domain experience in hardware IP including ML HW accelerator, GPU and image/video processing units.
	- Proficient experience developing C++ bit accurate models for hardware verification.
	- Experience working in a chip development environment with RTL designers and verification engineers.
	- Experience integrating IP models into chip simulation platforms
	- Experience debugging complex models
	- Experience working with C++ modeling tools, including C++, SystemC and scripting languages such as Perl and/or Python
	- As a Neural Engine Modeling Engineer, you will be responsible for developing, integrating and maintaining software models for Neural Engine.
	- Define, document and implement C/C++ bit-accurate and transaction level models with SoC and Neural Engine arch teams
	- Collaborate with design and verification teams to define C-model interfaces for validation and debug
	- Develop and maintain architecture test cases and automated workflows to verify the correct functionality of the models
	- BS with 3 years relevant industry experience. M.S. or Ph.D. preferred.
+ skill set:
	- Sr Staff Chip Engineer
	- Lightmatter is a photonic computer company that is redefining what computers and human beings are capable of by building the engines that will drive discoveries and progress sustainably. With modern human progress relying heavily on computers, the world has hit a dead end with traditional transistors, and the prospect of constantly building data centers is an environmental nightmare. Lightmatter builds chips for artificial intelligence computing. Our architecture leverages the unique properties of light to enable a fast and efficient compute platform. We created a solution in photonic computing: using photons instead of electrons to take advantage of their higher bandwidth.
	- If you have a passion for working cross-functionally, leading projects to success, and doing impactful work, like helping build the world's first optical computers, you should join the team at Lightmatter!
	- We are hiring a Sr Staff Chip Engineer to join our Chip team to guide the next generations of Lightmatter's innovative AI/ML silicon designs. If you have high-performance and/or AI/ML accelerator chip design experience, this would be a great opportunity!
	- In this job you will work in constant collaboration with hardware and software engineering teams across the company to create chips and systems that address a broad range of generative AI workloads. You'll be understanding the target workloads, managing technical risks, and innovating to solve problems these new applications face. You will work with company and engineering leadership to develop and manage the roadmap.
	- Own the chip architecture and roadmap for the AI Inference product line.
	- Work with hardware, software, program management and supply chain teams to define state-of-the-art silicon products.
	- Understand AI/ML models for existing and upcoming applications in terms of hardware acceleration architecture. Specifically, translate AI model architecture trends to specific hardware architecture requirements.
	- Develop and maintain competitive analysis of the AI/ML hardware landscape relative to Envise product line.
	- Collaborate with silicon design, verification and implementation teams to build efficient and performant chips.
	- Communicate the value proposition of Envise internally and externally.
	- Collaborate with sales and field teams to drive customer engagements.
	- 8 years and a Master’s degree; or a PhD with 5 years experience; or equivalent experience.
	- At least 3+ years of previous experience in high-performance or AI/ML semiconductor chip architecture resulting in silicon products.
	- Experience in Python, C++ or other performance simulation tools.
	- Strong teamwork skills with the ability to collaborate with multiple functional teams across a variety of fields.
	- Ability to react to change and thrive in a fast-paced (startup) environment.
	- Attention to detail and strong ability to multitask.
	- Experience influencing decisions and providing technical leadership in a matrix environment.
	- Enthusiastic, responsive, and passionate about finding innovative solutions to challenges.
	- Excellent communications and technical presentation skills
	- Previous experience working on startups is a plus
+ skill set:
	- Machine Learning Engineer - Generative AI and Deep Learning
	- USA - California - Mountain View/Sunnyvale
	- Synopsys’ Generative AI Center of Excellence defines the technology strategy to advance applications of Generative AI across the company. The GenAI COE pioneers the core technologies – platforms, processes, data, and foundation models – to enable generative AI solutions, and partners with business groups and corporate functions to advance AI-focused roadmaps.
	- As an ML Engineer in Gen-AI, you will innovate and translate cutting edge research into user experiences on questions like the following:
	- How to prompt an LLM like Llama-2, GPT-4, etc. effectively?
	- How to build a foundation model for a specific domain like EDA?
	- How to blend prompt engineering, retrieval augmentation, and fine-tuning to customize models with the least human time and effort?
	- Design and implement machine learning models and algorithms for Generative AI and Deep Learning applications.
	- Conduct experiments to evaluate model performance, identify areas for improvement, and implement optimizations.
	- Partner with cross-functional teams to design and develop scalable solutions that meet business goals.
	- Stay up to date with the latest research and advancements in the field of Generative AI and Deep Learning and apply this knowledge to improve our models and algorithms.
	- Communicate complex technical concepts and findings to both technical and non-technical stakeholders.
	- Participate in code reviews, testing, and deployment of machine learning models and algorithms.
	- BS with 5+ years’ experience or MS degree with 1+ years’ experience in computer science, Electrical Engineering, Mathematics, or related field.
	- A Ph.D. in machine learning or a related area with good publication history would be a good fit for this position. We would also love to hear from people with similar skill sets acquired through other career paths.
	- 2-5 years of experience in machine learning engineering, with a focus on AI/ML and Deep Learning.
	- Proven familiarity with python, and excellent background in data structures and algorithms.
	- Good expertise of Probability and Statistics concepts, including Probability, Conditional Probability, Bayes Theorem, Normal Distribution, and Central Limit Theorem.
	- Sound knowledge of Linear Algebra and Calculus concepts
	- Sound knowledge of deep learning architectures like Recurrent Neural Networks (RNNs), Long-Short-Term-Memory models (LSTMs), and Convolutional Neural Networks (CNNs).
	- Experience with deep learning frameworks like Tensorflow or PyTorch.
	- Experience with LLMs, Encoder-Decoder Models, and other Generative AI techniques.
	- Experience with Natural Language Processing (NLP) and Text Generation using Deep Learning.
	- Excellent problem-solving skills and ability to work autonomously as well as collaboratively in a team environment.
	- Excellent communication and presentation skills, with the ability to communicate complex technical concepts to both technical and non-technical stakeholders.
	- Good expertise with hands-on experience in data cleansing and modeling for deep learning models in at least one domain (language, image, graphs, etc.)
	- Experience with cloud-based machine learning platforms such as AWS, GCP, or Azure
	- Proven publication record in top-tier conferences and journals in the field of Machine Learning or NLP or Generative AI.
	- Experience with standard machine learning frameworks and tools (Huggingface Transformers, NumPy, Scikit-learn, Pandas, PyTorch, TensorFlow, etc.) and machine learning cloud infrastructure and accelerators (AWS, Google Cloud, GPUs and TPUs).
	- Familiarity with supervised and unsupervised learning algorithms like linear regression, logistic regression, random forests, and k-means.
	- Prior exposure to AI/ML workflows and tools
	- Knowledge and/or exposure to cloud computing technologies like containerization platforms (Docker, Kubernetes, microservices)
	- Broad expertise and understanding of AI, NLP, LLM, and generative AI trends.
	- Proficiency in advanced concepts and techniques like Proximal Policy Optimization (PPO) and RLHF for building generative models is a big plus
	- Experience prototyping, experimenting, and testing with large datasets and training models.
	- BS in EE or CS with 5+ years of relevant experience.
	- MS in EE or CS with 1+ years of relevant experience.
	- PhD in EE or CS with published academic papers and/or relevant experience.
	- The base salary range across the U.S. for this role is between $106,000 to $185,000.
+ skill set:
	- Machine Learning Engr, II
	- USA - California - Mountain View/Sunnyvale
	- The Artificial Intelligence team designs and develops the next generation Machine Learning, Artificial Intelligence and Cloud architecture for Synopsys tools. We are looking for a ML engineer to work in areas such as microservices architecture, containers, distributed systems, webservers fine-tuning, and Application Program Interface design. The ML engineer will help develop and deploy robust software components that meet our high-quality requirements both on-prem and in public cloud.
 	- Designing, developing, troubleshooting, or debugging on-prem and cloud infrastructure for ML/ Artificial Intelligence based tools and applications.
 	- This role may span many tiers such as data management, backend services, visualization, presentation, and APIs for the infrastructure.
 	- The candidate will work with other members of the engineering team to design, code and deploy optimal solutions.
 	- Expected to exercise judgment in selecting methods and techniques to obtain solutions. Work on diverse problems where study of situations or data requires evaluation of various factors.
 	- Work with engineering project management system. Provides regular updates to manager/team on project status. Able to work autonomously and collaboratively. Exceptional communications and synergy skills are a must.
 	- BS/MS EE/CS/CE
 	- Expertise in one or more languages such as C/C++, Python, Java, JavaScript
 	- Requires a sound background in file systems, data structures, algorithms, performance, and scalability.
 	- Demonstrates good investigation and problem-solving skills.
 	- Knowledge of microservices-based software design and architecture
 	- Knowledge about containerization, distributed computing, fault tolerance, throughput and latency tuning.
 	- Knowlegde about CI/CD pipelines, ML model versioning and deployment.
 	- Knowledge of Data Engineering tools such as Kafka, Spark, Hive. Also HDFS, Airflow, NOSQL Databases, in-memory databases such as redis, ignite, and/or similar.
 	- Knowledge of designing / deploying micro services application on Kubernetes both on-prem and public clouds
 	- Knowledge about working with AWS, GCP, and/or Microsoft Azure. May include configuring, deploying, managing and monitoring.
 	- Certifications are a plus: AWS Solutions Architect, Cloud Security Certification, OpenStack Certification
+ skill set:
	- Senior RTL Design Engineer
	- AlphaIC, based in the US and India, designs and manufactures AI/ ML Co-Processor targeted for vision inference applications. Gluon, AlphaIC’s first AI/ ML Co-Processor, has been successfully taped out and we are planning to develop a more advanced AI/ ML Co-Processor which can be used in Edge Inference as well as Edge Learning applications. We have applied for multiple patents among which quite a few are granted and will continue to apply more to enhance our IP portfolio. The engineering team is led by Industry experts from SoC Design, System Design and Edge AI/ ML Application Development areas. We have raised Series A funding and in the process of raising further funds from leading VC firms
	- 1) Will work with system architects to understand the functional and performance requirements of a unit or feature.
	- 2) Define micro-architecture and develop RTL for a module and feature.
	- 3) Responsible for IP / sub-system level micro-architecture development and RTL coding.
	- 4) Prepare block/sub-system level timing constraints.
	- 5) Integrate IP/sub-system.
	- 6) Work with verification and physical design team to ensure that the unit or feature is accurately verified and implemented.
	- 7) Silicon bring-up in conjunction with post silicon validation and SW teams.
	- 1) BS / MS in Electrical Engineering or Computer Science from a leading university.
	- 2) Strong analytical and problem-solving skills.
	- 3) 5+ years of minimum experience as RTL Design Engineer.
	- 4) Expertise in using design and verification tools (Synthesis, lint, Xcelium or equivalent simulation tools, debug tools etc.)
	- 5) Experience with high-speed interfaces such as LPDDR, AXI, PCIe, etc.
	- 6) Experience with Memory sub-system, DDR interfacing.
	- 7) Experience with Deep Learning and Deep Learning HW acceleration.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.














###	Start-ups related to VLSI Deep Learning & Embedded Deep Learning

+ Covariant: https://covariant.ai/careers/


































##	Application engineers of different EDA products

Skill sets for application engineers of different EDA products:
+ Bus protocols such as AMBA-AXI, AHB, APB, I2C, SPI
+ Expert in coding SV Testbench, drivers, monitors, scoreboards, checkers
+ Experience in C/C++,Shell/Perl scripting.
+ Understanding of AHB, AXI and other bus protocols and system architecture is a plus.
+ Expert in System Verilog and OVM/UVM based verification.
+ Preferred Expertise in MIPI UniPro/UFS Protocol and UVM.
+ To help the team to verify the existing design (UFS/UniPro)
+ Preferable: Experience in one/more of the following areas PCI_Express, USB, SATA, SDIO, MIPI and /or AMBA standards (OCP, AXI, AHB etc.)
+ Experience with verification methodology like OVM/VMM/UVM
+ Experience in constrained-random verification is a strong plus




+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










#	Artificial Intelligence + Machine Learning










##	Machine Learning, ML



###	Notes about Machine Learning, ML

+ ML/AI compiler design
	- Poplar framework for IPU architecture compiler.
+ ML/AI compilers, or deep learning compilers:
	- MLIR
	- TVM
	- Glow
	- XLA
+ ML/AI frameworks
	- ***JAX***:
		* JAX = Autograd + XLA (from TensorFlow)
			+ Google JAX
		* for high-performance machine learning research
		* https://github.com/google/jax
	- ***TensorFlow***
		* TensorFlow Lite, TFLite
		* TFX, TensorFlow Extended
		* TensorFlow Probability
		* ***Keras***
	- ***PyTorch***
		* based on ***Torch*** library
		* PyTorch JIT
		* includes:
			+ Deep Speed
				- deep learning optimization engine
				- Zero Redundancy Optimizer, ZeRO, can optimize deep learning models with >1 trillion parameters
			+ Caffe, Convolutional Architecture for Fast Feature Embedding, and Caffe2, which has been merged into PyTorch.
			+ Torch, which is based on Lua
	- Open Neural Network Exchange, ONNX
		* open-source AI ecosystem
		* Neural Network Exchange Format, NNEF
	- ***scikit-learn***
		* mlpy
		* SpaCy, for NLP
		* Natural Language Tookit, NLTK, for NLP
		* Orange
		* PyTorch
		* TensorFlow
		* Infer.NET, for Bayesian inference in graphical models, for probabilistic programming
		* scikit-multiflow, for multi-output/multi-label and stream data
	- ***pandas***, for data analysis
	- high-performance inference frameworks, or inference engines:
		* ArmNN
		* ONNX Runtime
		* OpenVINO
		* ***NVIDIA TensorRT***, or ***TensorRT***
		* edge inference frameworks:
			+ ***PyTorch Mobile***
			+ ***Tensorflow Lite, TFLite***
		* inference server or model serving frameworks
			+ Triton
			+ TFServe
			+ ***KubeFlow***
		* related tools:
			+ CuBlas
			+ CuDNN
			+ CuFFT
			+ HugeCTR
			+ MLIR
			+ MNN
			+ OpenPPL
			+ Paddle
			+ TNN
			+ TVM
			+ XLA
	- [***Lightning, or Lightning AI***](https://lightning.ai)
		* Or, ***PyTorch Lightning***
	- Chainer/ChainerMN
	- ***ML.NET***
		* Microsoft Cognitive Toolkit, CNTK, The Microsoft Cognitive Toolkit (deprecated)
	- ***Apache MXNet***, for deep learning
	- ***OpenVINO toolkit***, Open Visual Interference and Neural Network Optimization
		* includes:
			+ nGraph
	- ***BigDL***, distributed deep learning framework for Apche Spark
	- Dlib
	- PaddlePaddle
	- Caffe and Caffe2
	- Theano
	- Apache SINGA
	- Horovod
	- Deepspeed
	- K-means
	- Dask
	- Flux machine learning framework, Julia based
	- Flax, from Google Brain
	- PlaidML, portable tensor compiler
	- Optax, for gradient processing and optimization, by DeepMind
	- [***Chainer***](https://chainer.org)
	- Apple Core ML:
		* https://pypi.org/project/coremltools/
			+ pip install coremltools
		* https://developer.apple.com/documentation/coreml
		* https://coremltools.readme.io/docs
			+ https://developer.apple.com/machine-learning/core-ml/
		* https://developer.apple.com/machine-learning/create-ml/
		* https://developer.apple.com/machine-learning/models/
		* https://developer.apple.com/machine-learning/
		* https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml
	- reinforcement learning:
		* RLax, for reinforcement learning agents, by DeepMind
		* OpenAI Gym: https://github.com/openai/gym
		* RLLib:
			+ https://github.com/ray-project/ray/tree/master/rllib
			+ https://github.com/ray-project/ray/
			+ https://github.com/ray-project
			+ https://docs.ray.io/en/latest/rllib/index.html
		* Stable Baselines 3:
			+ https://stable-baselines3.readthedocs.io/en/master/
			+ https://github.com/DLR-RM/stable-baselines3
			+ https://pypi.org/project/stable-baselines3/
	- for gradient boosting:
		* LightGBM
		* XGBoost
	- for Python software:
		* PyMC
			+ formerly PyMC3
			+ for Bayesian statistical modeling and probabilistic machine learning
				- Markov chain Monte Carlo
				- variational fitting algorithms
		* TomoPy, tomographic data processing and image reconstruction
	- for C++ software:
		* mlpack
		* OpenNN
	- for Java software:
		* Massive Online Analysis, MOA
			+ SAMOA, derivative of MOA
		* Eclipse Deeplearning4j
	- for distributed machine learning or deep learning:
		* ***Horovod***, open-source software framework for distributed deep learning training
			+ TensorFlow
			+ Keras
			+ PyTorch
			+ Apache MXNet
			+ managed by Linux Foundation AI, LF AI
	- Orange
	- from sanctioned entities:
		* CatBoost, for gradient boosting
	- ***for MLOps***:
		* MLflow
		* Kubeflow
		* Seldon Core
		* TFServing, TensorFlow Serving
		* MLeap
		* Airflow, but for generic pipelined workflow managemnt
	- ***machine learning and data science pipeline management and version control***
		* goals:
			+ pipeline computation
			+ track data
			+ track machine learning models
				- or track statistical models
			+ track experiments
			+ make machine learning models shareable
			+ make experiments reproducible
			+ track versions of the following:
				- data
				- machine learning models, or statistical models
				- machine learning pipelines, or data science pipelines
		* ***DVC, Data Version Control***
		+ [***MLflow***, An open source platform for the machine learning lifecycle](https://mlflow.org/)
		* Git LFS
		* Dolt
		* lakeFS
		* deep learning models:
			+ ImageNet and VGG
			+ GoogleNet
			+ ResNet
			+ NASNet
			+ DeepSpeech
			+ Large Language Models (LLMs)
		* Data Parallel DNN Training
			+ Basic Solutions for CPU- and GPU-based Training
			+ NVIDIA NCCL, Baidu-allreduce, Facebook Gloo
			+ Co-Designs
			+ Deep Learning and Big Data
		* Model Parallel DNN Training
			+ Out-of-core DNNs
		* Elastic Training
+ data science frameworks, including libraries and tools for data visualization and information visualization:
	- Madagascar
	- Statsmodels, for statistical analysis
	- Dataplot, for statistical analysis and data visualization
	- Fityk, curve fitting and data analysis application
		* fit analytical, bell-shaped functions to experimental data
	- for Python software:
		* ***matplotlib***, for data visualization
		* ***Bokeh***, for data visualization
		* ***Plotly***, for data visualization
		* ***wxPython***, for data visualization
		* ***PLplot***, for data visualization
		* Gnuplot-py, for data visualization
		* Biggles, for data visualization
		* Chaco, for data visualization
		* MayaVi, for data visualization
		* SQLAlchemy, open-source SQL toolkit and object-relational mapper, ORM
	- PSPP, open-source equivalent of IBM SPSS Statistics (or SPSS Statistics, or Statistical Package for the Social Sciences)
	- other data analytics tools:
		* Apache Arrow
		* Apache Spark, or Spark
		* Baremetrics
		* Chartmogul
		* ***Dask***
			+ "flexible open-source Python library for parallel computing maintained by OSS contributors across dozens of companies"
			+ Scale the Python tools you love
			+ https://www.dask.org
		* Databricks
		* Gephi
			+ for ***graph visualization***
		* Google Analytics
		* Google Data Studio
		* Heap Analytics
		* Jupyter
		* Looker
		* Mesos
		* Mode Analytics
		* Noteable
		* Numba: A High Performance Python Compiler
			- https://numba.pydata.org/
		* Periscope
		* Tableau
			+ alternatives include:
				- Apache Superset
				- Business Intelligence and Reporting Tools, BIRT
				- Domo
				- GoodData
				- Grafana
				- Knime Analytics Platform
				- Looker
				- Metabase
				- Microsoft Power BI???
				- Noteable
				- Oracle Analytics Cloud
				- Pentaho Community Edition
				- Plotly-Dash
				- Qlik Sense
				- QlikView
				- RAWGraphs
				- Redash
				- Retool
				- Sisense
				- SpagoBI
				- Talend
				- ThoughtSpot
				- TIBCO Software
				- Trevor.io
	- for Elasticsearch dashboards
		* Kibana, for data visualization
			+ substitute is: ***Opensearch Dashboards, for Opensearch***
	- for SQL, NoSQL, and NewSQL databases:
		* Amazon DocumentDB
		* Apache Cassandra
		* Apache Hive
		* IBM Db2
		* large-scale databases
			+ THIN
		* MariaDB
		* massively parallel processing databases, MPP databases
			+ BigQuery
			+ Redshift
			+ Snowflake
			+ Vertica
		* Microsoft Access
		* Microsoft Azure SQL Database
		* Microsoft SQL Server
		* MongoDB
		* MySQL
		* Oracle Database
		* PostgreSQL
		* Snowflake
		* SQLAlchemy
			+ open-source SQL toolkit and object-relational mapper, ORM
		* SQLite
		* Teradata Vantage
		* important principles, and properties, to abide by when designing database systems or hardware accelerators for data management (and computation, such as in-memory computing):
			+ common parallel DBMS architectures
				- shared memory architecture
				- shared disk architecture
				- shared nothing architecture
			+ ***CAP theorem, or Brewer's theorem***, in system design and theoretical CS guarantees:
				- consistency: every read receives the most recent write or an error
					* different from consistency definition in ACID properties
				- availability: each request receives a non-error response, without the guarantee that it contains the most recent write
				- partition tolerance: system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes
				- When a network partition failure occurs, it has to do one of the following:
					* cancel the operation and decrease the availability but ensure consistency
					* proceed with the operation and provide availability but risk inconsistency
				- therefore, when a design choice for network partition is made, it has to choose between consistency and availability.
			+ ***ACID = atomicity, consistency, isolation, durability***
				- consistency/correctness: database transactions must change affected data only in allowable ways (as specified by rules):
					* database constraints
					* cascades
					* triggers
					* combination thereof
					* subsequent starts of transactions will have to deal with the effects of past and current transactions
					* database consistency are about transactions
					* atomic consistency refers to the property of each request/response operation sequence
				- isolation is the goal of concurrency control
				- durability guarantees that committed transactions will be completed, even when there is system failure, such as power outage or crash
					* record completed transactions or their effects in NVRAM (or non-volatile memory)
			+ ***CRUD = create, read, update, and delete***
				- essential operations of ***database engine***, or ***storage engine***
				- basic operations of ***persistent storage***
					* data definition, or definition for data organization
						+ creation
						+ modification
						+ removal
					* update (data)
						+ insert data
						+ modify data
						+ delete data
					* retrieval
						+ access data in directly usable format
						+ access data for further processing
					* administration
						- register users
						- monitor users
						- enforce data security
						- monitoring performance
						- maintain data integrity
						- concurrency control
						- recover information corrupted by events, such as unexpected system failure.
				- design for:
					* ***predominantly "persistent data"***, infrequently accessed and not likely to be modified
					* ***predominantly "dynamic data", or predominantly "transactional data"***, asynchronously/periodically update information as new data becomes available
						+ new data can come at any time
						+ ***transaction data***, category of data describing information that refer to ***master data*** and/or ***reference data***, such as dates, times, time zones, and currencies
							- financial transactions
							- work transactions
							- logistics transactions
							- Note: master data and reference data provide context for transaction data (or transactions)
							- Note: reference data also provides information/data for classification and categorization of other data, such as:
								* units of measurement
								* country codes
								* corporate codes
								* fixed conversion rates for mass/weight, temeperature, and length
								* calendar structure and constraints
					* ***streaming data, event stream processing, data stream processing, or distributed stream processing***, constant flow of information
						+ encompasses:
							- dataflow programming
							- reactive programming
							- distributed data programming
					* ***static data, or unchanging data***, which does not change
					* ***unstructured data, or unstructured information***
						+ has no pre-defined data model
						+ not organized in pre-defined manner
						+ text heavy
							- can include data about:
								* dates
								* numbers
								* facts
							- can result in irregularities and ambiguities when processed by structured databases
						+ can be ***semi-structured data***, by containing data that has some structure associated with relational databases, RDBMS
							- XML, extensible markup language
								* XML databases
									+ native XML databases
								* XML-enabled databases
								* data-centric XML databases
							- JSON, JavaScript Object Notation
								* supported by:
									+ MongoDB
									+ Couchbase, formerly Membase
										- CouchDB???
							- Object Exchange Model, OEM
						+ can be highly structured, such that the structure is unanticipated and unannounced
						+ periods of inactivity between data arrival can exist
						+ for unstructured data database
					* ***semi-structured database model***
						+ no separation between data and database schema
					* types of database applications:
						+ online transaction processing, OLTP
							- facilitate and manage transaction-oriented applications
							- goals:
								* availability
								* speed
								* concurrency
								* recoverability
							- requirements:
								* high throughput
								* insert- or update- intensive database management
						+ online analytical processing, OLAP
							- multidimensional analysis, MDA
								* MDA queries
							- complex queries
							- small volume
							- for data analytics
								* such as business intelligence or business analytics
							- types:
								* multidimensional OLAP, MOLAP
								* relational OLAP, ROLAP
								* hybrid OLAP, HOLAP
						+ hybrid transactional/analytical processing, HTAP
							- for "in business real time" decision making
					* ***database administration tools***
						+ to manage DBMS such as:
							- MySQL
							- PostgreSQL
							- SQLite
					* relational database model, or relational model or RM
						+ ***relational databases, relational database management systems, RDBMS, or RDB***
							- row-oriented DBMS
							- column-oriented DBMS, columnar DBMS
								* store data tables based on columns rather than rows
					* SQL
						+ applications of SQL:
							- relational databases, relational database management systems, RDBMS, or RDB
							- relational data stream management systems, RDSMS
								* for stream processing
						+ NewSQL
							- extend relational databases, RDBMS, with scalability of NoSQL for online transaction processing, OLTP, while maintaining ACID guarantees of traditional RDBMS (or SQL databases)
							- primarily uses SQL interface
							- distributed computing uses a cluster of shared-nothing nodes, such that each node uses a subset of the data
								* components include:
									+ distributed concurrency control
									+ flow control
									+ distributed query processing
							- uses optimized storage engines for SQL
								* scale better than built-in engines
							- transparent sharding
								* split databases across multiple nodes using consensus algorithms:
									+ Raft
									+ Paxos
					* NoSQL
						+ non-SQL database, or non-relational database, or not only SQL database
						+ database management systems, DBMS, for data storage and retrieval beyond tabular relations in relational databases, relational database management systems, RDBMS, or RDB
						+ can support the following:
							- ACID properties
							- join operations
						+ applications:
							- Big Data applications
							- real-time Web applications
						+ see Wikipedia entry for NoSQL for comparing data models based on:
							- performance
							- scalability
							- flexibility
							- complexity
							- functionality
							- list of data models that are compared:
								+ [x] key-value database, key-value store
								+ [x] document-oriented database, document store
								+ [x] graph database, GDB
								+ relational database model, or relational model or RM
									- [x] relational databases, relational database management systems, RDBMS, or RDB
									- [x] column-oriented DBMS, columnar DBMS
						+ examples:
							- HBase
							- Cassandra
							- MongoDB
							- DynamoDB
					* ***multi-model database***
						+ database management system, DBMS, that is designed to support multiple data models with an integrated back-end
						+ other DBMSes are organized around a data model that determines how data is:
							- organized
							- stored
							- manipulated
						+ examples of supported data models:
							- relational database model, or relational model or RM
								* relational databases, relational database management systems, RDBMS, or RDB
							- key-value database, key-value store
							- graph database, GDB
							- document-oriented database, document store
						+ examples of multi-model databases/DBMS:
							- PostgreSQL:
								* [x] relational database model, or relational model or RM
									+ relational databases, relational database management systems, RDBMS, or RDB
								* [x] key-value database, key-value store
								* [x] graph database, GDB
								* [x] document-oriented database, document store
									+ JSON
									+ XML
								* [x] object database model
									+ object-relational database model, ORD, for object-relational database management systems, ORDBMS
					* navigational databases
						+ navigational databases use references from objects to find records or other objects
						+ hierarchical database model
						+ network database model
						+ graph database, GDB
					* entity relational database models, entity-relationship models, ER models
						+ enhanced entity relational database model, enhanced entity relationship model, EER model
					* entity-attribute-value database model, EAV
						+ or, object-attribute-value data model
						+ or, vertical database model
						+ or, open schema
					* physical database model, physical data model, or physical database design
						+ inverted index, postings list, postings file, or inverted file
						+ flat file, for 2-D arrays of data
							- for flat-file databases
							- examples are:
								* CSV, standard comma-separated-values
								* TSV, standard tab-separated-values
								* Awk, flat-file processor
					* star schema database model
					* semantic database model
					* named graph
					* correlational database model
					* dimensional database model
					* triplestore, or RDF store
						+ semantic triples, RDF triples, or triples
							- atomic data entities in resource description framework, RDF, data model
						+ uses resource description framework, RDF
						+ for storage and retrieval of triples, through semantic queries
					* uncertain databases
						+ includes the following types of uncertain databases:
							- probabilistic databases, probabilistic DBMS
								* possible worlds have associated probabilities
								* types of uncertainties:
									+ tuple-level uncertainty
									+ attribute-level uncertainty
					* non-relational database models:
						+ graph database model
							- ***graph database, GDB***
								* graph database processing is different from graph computing
									+ a separate component for graph databases is needed for graph computing to implement graph algorithms
								* examples:
									+ GraphQL
						+ ***key-value database, key-value store***
							- associative array, map, symbol, dictionary
							- hash table, dictionary
						+ document database model
							- ***document-oriented database, document store***
								* based on semi-structured database model
						+ ***wide-column store, extensible record store***
							- name and format of columns can vary between rows within a table
							- can be interpreted as a 2-D key-value database, key-value store
						+ multidimensional database model
							- resource space database model
							- multivalue database model
							- useful for:
								* online analytical processing, OLAP, applications
									+ multidimensional OLAP, MOLAP
									+ relational OLAP
									+ hybrid OLAP
						+ object database model
						+ object-relational database model, ORD, for object-relational database management systems, ORDBMS
							- object-relational impedance mismatch, can occur when object-oriented software interact with RDBMS
							- includes:
								* terminology-oriented database, or terminology-oriented database management system, or terminology-oriented DBMS
							- or, Object Relational Mapping (ORM)
					* blockchain-based database
						+ combines traditional database with distributed database
						+ supported by multiple layers of blockchains
						+ use features from SQL and NoSQL databases, with blockchain properties
							- data integrity
							- integrity assurance
							- decentralized control
							- Byzantine fault tolerance
							- transaction traceability
					* cloud database
						+ database/DBMS that runs on cloud computing platform
						+ deployment models
							- virtual machine image
								* run databases on the cloud independently, using virtual machine image
							- database-as-a-service, DBaaS
								* purchase/paid access to database service, maintained by cloud database providers
						+ can support SQL and NoSQL databases
						+ *join operations perform poorly*
					* spatial database
						+ usually implemented with relational databases, relational database management systems, RDBMS, or RDB
						include spatial data that represent objects
							- 3-D objects
							- topological coverages
							- linear networks
							- triangulated irregular networks, TINs
						+ includes:
							- geographical database, geodatabase, or georeferenced spatial database
								* "georeferenced" from georeferencing or georegistration
								* applications of coverage data
									+ geographical information systems, GIS
									+ geospatial content and services
									+ GIS data processing
									+ data sharing
								* types of data:
									+ 1-D sensor time series
									+ 2-D satellite images
									+ 3-D x/y/t image time series or x/y/z geo tomograms
									+ 4-D x/y/z/t climate and ocean data
					* temporal database
						+ store data about time instances
							- valid time
							- transaction time
							- decision time
					* real-time database
						+ DBMS that uses real-time processing to handle workloads that can be constantly changing
							- most of the data is not persistent data that is unaffected by time
						+ applications
							- accounting
							- banking
							- law
							- medical records
							- multimedia applications
							- process control
							- reservation systems
							- scientific data analysis
					* ***in-memory database, IMDB***, main memory database system, MMDB, memory resident database
						+ Can support ACID properties: atomicity, consistency, isolation, durability
						+ Can be implemented with NVRAM.
					* ***database security***
					* ***database scalability***
					* ***slowly changing dimension, SCD***
			+ Armstrong's axioms, references to infer functional dependencies on a relational database
			+ Codd's 12 rules, or Codd's twelve rules, for relational database management systems, RDBMS
+ ML/AI conferences
	- AAAI, IJCAI, NeurIPS, ACL, SIGIR, WWW, RSS, NAACL, KDD, IROS, ICRA, ICML, ICCV, EMNLP, EC, CVPR, AAMAS, HCOMP, HRI, ICAPS, ICDM, ICLR, ICWSM, IUI, KR, SAT, WSDM, UAI, AISTATS, COLT, CORL, CP, CPAIOR, ECAI, OR ECML







###	Machine Learning Scientist & Deep Learning Scientist Roles







***Machine Learning Scientist*** and ***Deep Learning Scientist*** roles:
+ skill set:
	- Principal Software Engineer - Applied Machine Learning
	- As the Principal Software Engineer for our Machine Learning team you will be responsible for ensuring that the development of ML systems and services meets all technical and quality standards. You will work with Product Management and other technical teams within Splunk, incorporating new requirements and providing technical information related to this sophisticated ML Platform as needed.
	- work with a team of senior ML engineers, applied researchers and security researchers, and experts within their own specialty. You will set an example for this group, as well as set high standards on quality, communication and ability to deliver with deadlines.
	- contribute to architecture and technical decisions while also mentoring junior members within the team.
	- be working in a multi-office, multi-location development environment and prior experience working with local and remote teams or groups will be a plus.
	- While expertise with ML products and their application within enterprise solutions is highly desirable, it is not required, provided you are willing to quickly come up to speed and you have some prior experience of ML technology and its application.
	- 12+ years software development with focus on large scale distributed systems.
	- Some Machine Learning application development experience, this is NOT a data scientist role, but a services/platform development role.
	- Ability to communicate effectively in conversations with researchers and engineers from academia background.
	- Passionate about building and encouraging good engineering practices and processes such as continuous integration and deployment.
	- Experience developing and putting into production test automation and CI/CD systems.
	- Expertise in developing software with container deployment and orchestration technologies at scale, with strong knowledge of the fundamentals including service discovery, deployments, monitoring, scheduling, load balancing.
	- Strong background in building streaming applications or streaming analytics platforms.
	- Expert in one of the streaming platforms, preferably Flink.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
+ ***Capsule Networks***, or capsule neural networks
+ skill set for ***Autodesk AI Lab, Pier.9 at San Francisco***:
	- [BrickBot](https://www.fastcompany.com/90204615/autodesks-lego-model-building-robot-is-the-future-of-manufacturing)
	- [Auto Sketching and Vectorization](https://canvasdrawer.autodeskresearch.com/)
	- [Topology Optimization for Specific Manufacturing Processes](https://www.autodesk.com/customer-stories/general-motors-generative-design)
	- Exploring and developing new Machine Learning models and techniques
	- Constantly reviewing relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduces creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Planning and designing research projects: specifying the problem and defining the project scope
	- Connecting with academics and institutions to build relationships and collaborate
	- Realizing solutions through prototypes
	- Exploring new data sources and discovering techniques for best leveraging data
	- Collecting and performing data analysis to validate and further new theories and discoveries
	- Publishing and talking at conferences
	- Working closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meeting with customers to understand how ML could be applied to their problems
	- Thinking strategically about research directions
	- Mentoring more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- Truly excited by the pace of advancement in AI research and technology
	- Understanding of fundamental CS algorithms and their scaling behaviors
	- ***Solid background in statistical methods for Machine Learning: Bayesian methods, dimensional reduction, SVD, clustering, classification, forms of regression, etc***
	- ***Strong familiarity with Deep Learning techniques: various network architectures (CNNs, GANs, RNNs, Auto-Encoders, etc.); regularization; embeddings; loss-functions; optimization strategies; etc***
	- ***Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, PaddlePaddle, etc***
	- Experience training and debugging networks
	- Strong coding abilities in: Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- ***Reinforcement Learning and other areas of Control Theory***
	- Distributed Systems and High Performance Computing methods
	- ***Geometric Shape Analysis***
	- ***Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc***
	- ***Knowledge Representation (semantic models, graph databases, etc.)***
+ skill set:
	- Our AI Labs focus on research in: ***deep learning, control systems, simulation and knowledge representation applied to diverse areas such as: geometry, robotics, advanced sensing, design exploration and sustainable engineering or construction practices***. The labs also host product engineers resulting in early productization of our research, so you can see your work in action.
	- You will be a senior researcher focusing on problems related to geometry understanding, manipulation and synthesis.
	- The Lab brings together AI Researchers, Software Engineers and specialists in various problem areas to create novel AI solutions in all the areas mentioned above and more. They work closely with experts in: geometric modeling, simulation systems, robotics, knowledge representation, sensing and computer vision, industrial manufacturing and construction techniques.
	- Explore and develop new Machine Learning models and techniques
	- Constantly review relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduce creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Plan and design research projects: specifying the problem and defining the project scope
	- Connect with academics and institutions to build relationships and collaborate
	- Realize solutions through prototypes
	- Explore new data sources and discover techniques for best leveraging data
	- Collect and perform data analysis to validate and further new theories and discoveries
	- Publish and talk at conferences
	- Work closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meet with customers to understand how ML could be applied to their problems
	- Think strategically about research directions
	- Mentor more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- ***Solid theoretical background in geometry and geometric methods (e.g. shape analysis, topology, differential geometry, discrete geometry, functional mapping, etc.)***
	- ***Good background in statistical methods for Machine Learning (e.g. Bayesian methods, HMMs, Graphical Models, dimensional reduction, clustering, classification, regression techniques, etc)***
	- ***Familiarity with Deep Learning techniques (e.g. Network architectures; regularization techniques; learning techniques; loss-functions; optimization strategies etc)***
	- ***Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, Chainer, etc.***
	- Strong coding abilities in: Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- ***Reinforcement Learning and other areas of Control Theory***
	- Distributed Systems and High Performance Computing methods
	- ***Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc.***
	- ***Knowledge Representation (semantic models, graph databases, etc.)***
+ [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
	- [TensorBoard, TensorFlow's visualization toolkit](https://www.tensorflow.org/tensorboard)
	- https://databricks.com/tensorflow/visualisation
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Participate in cutting edge research in machine intelligence and machine learning applications.
	- Develop solutions for real world, large scale problems.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- ***Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.***
	- ***Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).***
		* Chainer: open-source deep learning framework
+ skill set:
	- Solid Machine Learning background and familiarity with standard speech processing and machine learning techniques
	- Experience with one or more deep learning libraries and platforms (e.g., ***TensorFlow, Caffe, Chainer or PyTorch***).
	- Industry or academic experience in deep learning research.
	- ***Strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, KDD, PAMI, JMLR, TACL, IJCV).***
+ skill set:
	- **Salesforce Research and Einstein.AI (formerly MetaMind) are looking for extraordinary deep learning or research engineers.**
	- As a deep learning or research engineer, you will work with research scientists and engineers to develop and productize new cutting edge models and associated artifacts such as data preparation pipeline and model characterization logic. You will ensure these models are developed to support accuracy, performance or other specific customer requirements.
	- You will work with platform team to support deployment of these models. In other words, you are problem solver, a deep learning model designer, and an engineer who makes sure the model is deployed at scale to serve our customers with state-of-the-art speech, vision, and language technologies.
	- You have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, voice, or reinforcement learning. Additionally, applicants should have in-depth experience with problems such as text categorization, information extraction, question answering, text summarization, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Partner with product managers to understand customer requirements
	- Conduct research (including reviewing relevant literature) and collaborate with our research team to identify appropriate solution candidates
	- Develop prototypes, then design and carry out experiments to validate and improve the prototypes
	- Bring the ideas to production
	- Monitor model behaviors in production and iteratively improve quality of services over time
	- Work on cutting-edge research in machine learning
	- MA/MS or PhD degree in computer science, artificial intelligence, machine learning, speech recognition, natural language processing, or related technical field such as operations research, computational mathematics, etc.
	- Research experience or contributions in deep learning, machine learning, NLP, computer vision, reinforcement learning, or optimization.
	- Solid Machine Learning background and familiarity with machine learning techniques
	- Problem solving and ability to reuse, customize, and implement latest research
	- Experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, or PyTorch)
	- Industry experience in deep learning research
	- Can thrive in team environments; using agile methodology and interacting with Product Leaders, Scientists and Engineers to solve technology's greatest challenges
	- In particular, we are looking for experienced engineers with Deep Learning experience and domain expertise around Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Vision to provide the best possible experience for our customers.
	- Experience designing and implementing machine learning pipelines in production environments.
	- Experience in building speech recognition and natural language processing systems (e.g. commercial or government-funded speech products) is a huge plus.
	- We value professional industry experience; advanced degrees alone do not replace real world experience.
	- Excellent communication, leadership, and collaboration skills.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- ***Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.***
	- Experience with one or more general purpose programming languages including but not limited to C/C++ or Python.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, object detection or reinforcement . Our postdoctoral researchers have the ability to give talks, attend conferences and build relationships with academic institutions if desired.
	- Collaborate on research to advance the science and technology of artificial intelligence.
	- Contribute to cutting edge research projects in machine intelligence and machine learning applications that can be infused into our world-class CRM.
	- Develop solutions for real world, large scale problems.
	- Influence progress of relevant research communities by producing publications.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Create a year long project proposal with research managers.
	- ***First-author publications at AI conferences and journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).***
+ skill set:
	- Salesforce Research Asia is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision,
		* speech recognition, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue systems, language and speech, machine translation, language and vision, image classification, object detection, or image semantic segmentation, etc.
	- ***Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, ECCV, SIGKDD, PAMI, JMLR, TACL, IJCV) are preferred.***
	- Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	- Experience with one or more deep learning libraries and platforms, e.g. PyTorch, TensorFlow, Caffe, or Chainer
	- Strong background in machine learning, natural language processing, speech, computer vision, or reinforcement learning
	- Strong algorithmic problem solving skills
	- Programming experience in Python, Java, C/C++, Lua, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- ***Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV) are preferred.***
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area
	-  Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	-  Experience with one or more deep learning libraries and platforms, e.g. Torch, TensorFlow, Caffe, or Chainer
	-  Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	-  Strong algorithmic problem solving skills
	-  Programming experience in Python, Lua, Java, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for an outstanding entry level research scientists focused on ethics in AI. It is our belief in the words of our CEO Marc Benioff, “The business of business is improving the state of the world." The way we behave — with integrity, transparency, alignment, and accountability — builds trusted relationships. We believe that companies can do well and do good in the world. We know technology is not inherently good or bad. It's what we do with it that matters. With AI, we believe that we can go even further to advance and support its effectiveness by ensuring equality, transparency, and accountability in the models we create and how we implement them in our products.
	- As a research scientist, you discover new research problems, develop novel models, design careful experiments and generally advance the state of the art in AI. At Salesforce, the research team is committed to collaboration with the wider research community. In this unique role, you will have the opportunity to work directly on advancing technologies that nonprofits use to solve problems in the real world that create positive impact for the world while accomplishing publications at major conferences. We believe that making substantive progress on hard problems can drive and sharpen the research questions we study, and, in turn, scientific breakthroughs can spawn entirely new applications. With this in mind, the team maintains a portfolio of projects, some with an immediate path to production, others that may not find an application for several years. Research scientists have the freedom to set their own research agenda and move between pure and applied research.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area (e.g., Machine Learning, AI, AI ethics, law and policy)
	- Excellent understanding of deep learning models and techniques (i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods)
	- Experience with one or more deep learning libraries and platforms (e.g. PyTorch, TensorFlow)
	- Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	- Programming experience in Python or a similar language
	- Strong algorithmic problem-solving skills
	- Demonstrable experience implementing machine learning models and algorithms, e.g., through open-source implementations, or shareable code
	- Strong presentation and communication skills
	- Experience applying deep learning models to ethical issues in AI or social causes (e.g., racial disparity in facial recognition, explainability of AI for redress and remediation)
	- Experience researching artificial intelligence ethics, including areas such as fairness, safety, privacy and transparency in artificial intelligence
	- ***Published in top-tier conferences or journals (e.g., FAT\*, NIPS, AIES, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV)***
	- Open-source implementations of machine learning research projects.
	- The ideal candidate will have a keen interest in producing new science to understand intelligence and technology and how to apply it safely and fairly in real-world settings.
+ ***Open-source projects that demonstrate relevant skills and/or publications in relevant conferences and journals (e.g. NIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICASSP)***
+ skill set:
	- Machine Learning Researcher
	- Machine learning is a critical pillar of Jane Street's global business, and our ever-changing trading environment serves as a unique, rapid-feedback platform for ML experimentation. 
	- Researchers at Jane Street are responsible for building models, strategies, and systems that price and trade a variety of financial instruments. As a mix of the trading and software engineering roles, this work involves many things: analyzing large datasets, building and testing models, creating new trading strategies, and writing the code that implements them.
	- We’re looking for people to join the research team with deep ML experience in either an applied or academic context. A good candidate should have a deep understanding of a wide variety of ML techniques, and a passion for tinkering with model architectures, feature transformations, and hyperparameters to generate robust inferences. We also want people who are good communicators, with the ability to quickly absorb the context of a new problem, carefully consider tradeoffs, and recommend possible solutions.
	- As an ML researcher, your expertise will also shape the firm's future ML developments including hiring new ML researchers, attending conferences, teaching techniques to teammates, and setting firmwide goals.
+ skill set:
	- Research Scientist
	- We are looking for Research Scientists in the machine learning discipline who are passionate about generative models and creative applications of AI. In particular, we are looking for people who share our mission of open-source research; people who do not believe AI models should be controlled by a centralized gatekeeper behind a closed wall, but rather be truly open and in control by all. We want highly creative researchers who are motivated to push the boundaries of generative models research, not just in state-of-the-art performance, but in pushing the efficient frontier between performance and resource usage. You will have access to state-of-the-art high performance computing resources and you will be able to work alongside top researchers and engineers to truly make an impact in the fast growing world of generative AI.
	- Work with research team to execute research agenda
	- Build the next generation of creative generative AI models
	- Publication of results at top conferences or journals, and blog posts
	- Work with open-source community on model release and tooling
	- Work with engineering / business teams on model deployment and customized training
	- Publication of papers, projects, and blog posts that had a high impact in generative AI
	- Experience with dataset curation, rather than rely solely on spoonfed research datasets
	- Ability to communicate research ideas effectively through writing and visualization
	- Experience with Python scientific stack, PyTorch, creating Jupyter/Colab notebooks
	- Experience with training large models on a compute cluster environment is a plus
+ skill set:
	- AI/ML Research Scientist
	- Santa Clara Valley (Cupertino), California, United States
	- Apple 3DML team is looking for software engineers to develop and deploy computer vision technologies (2D and 3D) for next generation of Apple products. You’ll join a phenomenal team of researchers and engineers with deep experience in deep learning, machine learning, computer vision and software engineering. The team has an outstanding track record in shipping high visibility products empowering Apple new hardwares. More specifically, you will be working closely with ML Applied research scientists in the team to bring CVML prototypes to life and deploy them on shipping frameworks. You’ll participate in crafting systems, algorithm, and optimization efforts to ship product with impact. No matter whether you have worked on computer vision and machine learning before, we provide you with the best experience, to learn, grow and contribute.
	- Strong academic and publication record in computer vision.
	- Solid programming skills with Python.
	- Deep understanding large foundation models.
	- Deep understanding of ***multi-task, multi-modal machine learning domain***.
	- Familiarity with ***deep learning toolkits***.
	- Familiar with challenges associated with training large models and working with large data.
	- ***Agile integration of deep learning, data collection and failure analysis.***
	- ***Designing and evaluating experiments monitoring key performance measures.***
	- Visualization and deeper understanding of deep learning networks.
	- Ability to communicate the results of analyses in a clear and effective manner.
	- Are you passionate about building AI/ML products with focus on Augmented Reality (AR) and Virtual Reality (VR) applications? Are you passionate about Generative AI and large foundation models? Are you passionate about solving hard problems? Video Computer Vision org at Apple is looking for machine learning researcher to join the team of highly accomplished and deeply technical scientist. You will design and implement new machine learning algorithms while collaborating with the most innovative product development teams at Apple. We provide the right balance between research and product to deliver Apple quality state of the art experiences on various apple devices. 
	- You will have the opportunity to touch the life of millions of people through amazing products. Our team researches new machine learning algorithm and techniques and has a track record of shipping some of the most impactful CVML products ever shipped by Apple. Examples include ***FaceID, Animoji/Memoji, Scene Understanding, People Understanding and Positional Tracking (VIO/SLAM)***. We want new ambitious team members to join our research ML group. In this role, we create new models and algorithms, and actively engage with the academic community. 
	- You will also have the opportunity to contribute to impactful projects at Apple, and use your machine learning and computer vision skills to transfer your ideas into solutions for some of the most complicated technical problems in the next generation of products that will delight millions of people.
	- M.S. or PhD in Computer Science with relevant publication and research background.
	- At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $116,100 and $208,300 annualized, and your base pay will depend on your skills, qualifications, experience, and location.
+ skill set:
	- Proactive Intelligence, Applied Research Scientist — Generative AI
	- AI represents a huge opportunity to elevate Apple’s products and experiences for billions of people globally. We are looking for Applied Research Scientists with a background and interest in Generative AI. You will be leveraging state-of-the-art Generative models to ship extraordinary products, services, and customer experiences for the iPhone, Mac, Apple Watch, iPad and more. 
	- The mission of Proactive Intelligence is to improve Apple platforms by better understanding, anticipating, and adapting to user behavior by using machine learning to build phenomenal features that are built right into Apple platforms. Our team provides an opportunity to be part of an incredible research and engineering organization within Apple. The ideal candidate for this role will have industry experience working on a range of modeling problems e.g., Conversational Agents, Sequential Decision Making, Reinforcement Learning, Autonomous Systems, Human Preference Learning and Large Language Models (LLMs). Working knowledge of large-scale data processing especially with structured data, probabilistic modeling and statistics will broaden your role and effectiveness in this position.
	- Strong programming skills in Python and/or C++ with 5+ years of experience in using these languages for machine learning (ML) modeling and applied research
	- Proficiency in using ML toolkits such as PyTorch, TensorFlow, SkLearn etc.
	- Fundamental knowledge of ML concepts and hands-on experience in building deep-learning systems
	- Strong software engineering skills to create scalable and robust infrastructure for deep learning data, modeling, and evaluation systems
	- Proven ability to train and debug deep learning systems: defining metrics and datasets, performing error analysis and training models in a modern ML framework
	- Familiarity with researching current ML literature and math including optimization methods and modeling techniques
	- Passionate about building extraordinary autonomous systems with Generative AI
	- Creative, collaborative and project focused with an ability to work hands-on in multi-functional teams
	- As an Applied Research Scientist on our team, you will design and implement ML algorithms that process data in different Apple products. You will train Generative AI models and agent behaviors using deep reinforcement learning to solve hard problems/tasks. Where necessary, you will also work on integrating ML/RL frameworks into our products to train large-scale agents and leverage cloud services to enable scalable and distributed training/simulation of agent behaviors. You will communicate advanced ideas to a focused team of researchers in the spirit of developing innovative tools and metrics that change the way we look at problems. You will work closely with other cross-functional teams to align messaging, contribute to roadmaps and contribute software back into different repos for proper integration with core systems. You will write clean, maintainable and production code with appropriate documentation and tests. You will contribute to architecture decisions, design reviews and peer code reviews.
	- M.S. or PhD in Computer Science, or a related fields such as Electrical Engineering, Robotics, Statistics, Applied Mathematics or equivalent experience. A minimum of 5 years of experience in applied ML and/or product development.
	- Publications in top-tier conferences in a plus e.g., ***NeurIPS, ICML, ICLR, ICRA*** etc.
	- Hands-on development experience within OSS Libraries and RL environments such as OpenAI Gym, MuJoCo, RLLib, Stable Baselines 3, Apple Core ML etc.
	- Experience in applying deep learning to robotics problems and predicting multimodal behaviors for agents via-techniques such as MDP, Monte-Carlo methods, TD learning, policy approximations etc.
	- Experience with hardware specific optimization of ML models and deployment
	- Experience developing software for mobile devices and heterogenous compute environments (eg. iOS, watchOS)
+ skill set:
	- Proactive Intelligence, Applied Research Scientist — Generative AI
	- AI represents a huge opportunity to elevate Apple’s products and experiences for billions of people globally. We are looking for Applied Research Scientists with a background and interest in Generative AI. You will be leveraging state-of-the-art Generative models to ship extraordinary products, services, and customer experiences for the iPhone, Mac, Apple Watch, iPad and more. 
	- The mission of Proactive Intelligence is to improve Apple platforms by better understanding, anticipating, and adapting to user behavior by using machine learning to build phenomenal features that are built right into Apple platforms. Our team provides an opportunity to be part of an incredible research and engineering organization within Apple. The ideal candidate for this role will have industry experience working on a range of modeling problems e.g., Conversational Agents, Sequential Decision Making, Reinforcement Learning, Autonomous Systems, Human Preference Learning and Large Language Models (LLMs). Working knowledge of large-scale data processing especially with structured data, probabilistic modeling and statistics will broaden your role and effectiveness in this position.
	- Strong programming skills in Python and/or C++ with 5+ years of experience in using these languages for machine learning (ML) modeling and applied research
	- Proficiency in using ML toolkits such as PyTorch, TensorFlow, SkLearn etc.
	- Fundamental knowledge of ML concepts and hands-on experience in building deep-learning systems
	- Strong software engineering skills to create scalable and robust infrastructure for deep learning data, modeling, and evaluation systems
	- Proven ability to train and debug deep learning systems: defining metrics and datasets, performing error analysis and training models in a modern ML framework
	- Familiarity with researching current ML literature and math including optimization methods and modeling techniques
	- Passionate about building extraordinary autonomous systems with Generative AI
	- Creative, collaborative and project focused with an ability to work hands-on in multi-functional teams
	- As an Applied Research Scientist on our team, you will design and implement ML algorithms that process data in different Apple products. You will train Generative AI models and agent behaviors using deep reinforcement learning to solve hard problems/tasks. Where necessary, you will also work on integrating ML/RL frameworks into our products to train large-scale agents and leverage cloud services to enable scalable and distributed training/simulation of agent behaviors. You will communicate advanced ideas to a focused team of researchers in the spirit of developing innovative tools and metrics that change the way we look at problems. You will work closely with other cross-functional teams to align messaging, contribute to roadmaps and contribute software back into different repos for proper integration with core systems. You will write clean, maintainable and production code with appropriate documentation and tests. You will contribute to architecture decisions, design reviews and peer code reviews.
	- M.S. or PhD in Computer Science, or a related fields such as Electrical Engineering, Robotics, Statistics, Applied Mathematics or equivalent experience. A minimum of 5 years of experience in applied ML and/or product development.
	- Publications in top-tier conferences in a plus e.g., ***NeurIPS, ICML, ICLR, ICRA*** etc.
	- Hands-on development experience within OSS Libraries and RL environments such as ***OpenAI Gym, MuJoCo, RLLib, Stable Baselines 3, Apple Core ML*** etc.
	- Experience in applying deep learning to robotics problems and predicting multimodal behaviors for agents via-techniques such as MDP, Monte-Carlo methods, TD learning, policy approximations etc.
	- Experience with hardware specific optimization of ML models and deployment
	- Experience developing software for mobile devices and heterogenous compute environments (eg. iOS, watchOS)
+ skill set:
	- Scientist - Machine Learning
	- The DeepPumas team works on augmenting traditional pharmacological modeling frameworks with seamlessly embedded machine learning algorithms. This enables structural enforcement of prior scientific knowledge through statistical and dynamical modeling while enabling data-driven augmentation. We work with neural networks, universal differential equations, symbolic identification, Bayesian/Convolutional/Graph neural networks and more, all of which are deeply embedded in statistical and dynamical models. Our work is groundbreaking and unprecedented, taking us across the horizontal of basic scientific research to applications in real-world healthcare issues. Throughout the process, we are engaging with and building credibility in the scientific community by publishing papers, presenting (and winning awards) at prestigious international conferences. Our goal is to democratize the resulting methodology through well-crafted software and learning material for widespread use.
	- At DeepPumas, we are currently taking on few but scientifically challenging and long-running consulting projects. These are projects where apply the DeepPumas methodology of augmenting traditional knowledge-based phramacological modelling with data-driven AI. The projects typically involve solving problems that require refinement and further development of our methodology itself – including but not limited to the AI augmentation. As DeepPumas grows and we expand our ability to perform specific analyses quickly, this position might also evolve. Now, however, our projects are similar to post-docs, where we continually update collaborators on our progress but where the project is expected to take more than a year to complete. Every such project needs someone who can take ownership of it. Someone who will be the primary contact to our external partners; who gets to know the biology behind the problem; who charts the course, breaks down the problem into smaller tasks, takes on many of those tasks themselves but outsources the parts that require specific expertise beyond their own. So far, the work is typically done using AI-augmented nonlinear mixed effect models and will thus build on pharmacology, statistics, dynamics and ML.
	- Solve real-world healthcare problems using advanced AI-augmented pharmacological modeling.
	- Develop general theory and methodology needed for the current and future projects to achieve its goals.
	- Establish excellent working relationships with our external partners.
	- Responsibilities
		* Project Owner
			+ Take ownership over specific DeepPumas scientific projects and lead them to successful completion
			+ Communicate internally and acquire the advice and the hands-on help that you need to successfully complete your projects.
			+ Communicate and collaborate internally to successfully meet deliverables.
			+ Interface with our external scientific partners and clients
			+ Research, design, implement, and document methodology
			+ Present your work at scientific conferences and write scientific papers
			+ Mentor interns
		* Company responsibilities
			+ Develop and communicate appropriate project documentation, including project overview, scope, team structure, status reports, issues management reports, change control reports, meeting notes, etc. as required and in a timely manner.
			+ Ensure adherence to company SOPs (Standard Operating Procedures), policies, and guidelines at the project level.
			+ Participate in Pumas-AI's continuous process improvement initiatives by, amongst other activities, assuring that project quality metrics align with company, client, and clinical operations objectives.
			+ Track identified project risks and issues and assist with following up with actions to ensure adherence to contractual agreement.
	- PhD in related field or equivalent experience that satisfies the requirement and responsibilities stated.
	- Effective communication skills.
	- Experience with mathematical modelling.
	- Proficiency in statistical reasoning and modelling
	- Proficiency in dynamical modelling
	- Proficiency with machine learning
	- Proficiency of pharmacology
	- Software development skills
	- Proficiency with the Julia programming language
+ skill set:
	- AI RESEARCH SCIENTIST
	- Two Sigma is a financial sciences company, combining data analysis, invention, and rigorous inquiry to help solve the toughest challenges in investment management, insurance technology, securities, private equity, and venture capital.
	- We are looking for creative experts who are interested in applying general machine learning and specifically deep learning techniques to many types of problems, but particularly those with large amounts of noisy data.
	- Develop effective techniques and infrastructure, from the initial idea to the running prototype and product
	- Similar to a research environment, you will write code, use the latest machine learning tools, run experiments, and generally develop techniques and processes to improve our understanding of how financial data influences the world around us
	- Partner with teams across Two Sigma to implement your ideas into their products
	- Remain connected to the broader research community by partnering with internal and external collaborators and participate in relevant conferences
	- Advanced degree in Computer Science, Engineering, or other STEM field
	- Excellent programming skills in Python, C++, Tensorflow, PyTorch or similar languages
	- Internships/work experience with machine learning, deep learning, reinforcement learning
	- Background in machine learning techniques with large amounts of noisy data, curiosity in applying it to financial problems
	- Relevant research experience (publications at NeurIPS, ICML, ICLR or similar are preferred)
	- Experience with cloud environments and multi-machine setups
	- Participation in open source community
+ skill set:
	- Machine Learning, AI Research Internship (FALL)
	- Our mission is to build the Covariant Brain, a universal AI to give robots the ability to see, reason and act on the world around them. Bringing AI from research in the lab to the infinite variability and constant change of our customer’s real-world operations requires new ideas, approaches and techniques.
	- Success in the real world requires a team that represents that world: diversity of backgrounds, points of view, and experiences. Our common denominator: ambitious expectations, love of learning, empathy for those around us, and a team-first mindset.
	- The Covariant Brain is a Universal AI Platform that powers all robotic applications at Covariant. The Brain is a collection of state-of-the-art models, algorithms, and APIs that enable all intelligent behavior of the robot, from perception to 3d object understanding, grasp sampling, motion planning, and control. 
	- We are seeking talented and motivated interns to join our AI Research team. As an AI Research Intern, you will work directly under the guidance of a research mentor, contributing to cutting-edge projects in the areas of artificial intelligence, robotics, and machine learning. This is an opportunity to gain hands-on experience with working on real-world applications and expand your knowledge while having the potential to transform and leave a lasting impact on our company's technology stack.
	- Strong knowledge and experience in any of the following areas: modern computer vision (including but not limited to depth estimation, segmentation, occupancy estimation, NERFs), large language models, motion planning, dynamics modeling, robotics simulation
	- Knowledge and experience in training and analyzing ML models and robotics applications (in simulation, or in the real world)
	- A solid mathematical and statistical foundation with an understanding of how to apply ML concepts (e.g. training, optimizers, regression, classification, etc.)
	- Proficiency in Python and experience with tensor libraries (e.g. NumPy, PyTorch, TensorFlow)
	- Clear communication, organization, and collaboration
+ skill set:
	- AI Software Engineer
	- AlphaICs is looking for a world-class AI/Deep Learning Research Intern to research and develop neural network based learning algorithms. In this role, you will work with algorithm experts to come up with state of the art algorithms in the area of Artificial Intelligence, Computer Vision, natural language processing, including Object detection, and Image recognition. We are looking for candidates who can work with us for more than 3 months.
	- If you are someone with a good understanding of deep learning, reinforcement learning, advanced mathematical concepts and good academic record, we will be happy to talk to you about the work we are doing.
	- 1) Bachelor / Master degree in computer science or AI related field with excellent academic records (Students are also welcome to apply)
	- 2) Strong mathematical foundation and applied research experience in machine learning and deep learning, reinforcement learning.
	- 3) Experience working with deep learning frameworks (Tensorflow, Pytorch, etc.) and related tools in each special field. Solid understanding and hands on experience with C/C++/Python.
	- 4) Publication record in top AI conferences (ACL, CVPR, NAACL, ICML etc) is a plus.
	- 5) Experience with CUDA or GPU computation is a plus.
	- 6) Excellent communication and documentation skills
	- 7) Ability to learn quickly, starting with little information and becoming an expert in your domain.
	- 8) Ability to communicate/collaborate with other researchers/engineers including our team members.
	- 9) Solid Github/ Kaggle profile is a plus.
+ skill set:
	- Artificial Intelligence Intern
	- AlphaICs is looking for a world-class AI/Deep Learning Research Intern to research and develop neural network based learning algorithms. In this role, you will work with algorithm experts to come up with state of the art algorithms in the area of Artificial Intelligence, Computer Vision, natural language processing, including Object detection, and Image recognition. We are looking for candidates who can work with us for more than 3 months.
	- If you are someone with a good understanding of deep learning, reinforcement learning, advanced mathematical concepts and good academic record, we will be happy to talk to you about the work we are doing.
	- 1) Bachelor / Master degree in computer science or AI related field with excellent academic records (Students are also welcome to apply)
	- 2) Strong mathematical foundation and applied research experience in machine learning and deep learning, reinforcement learning.
	- 3) Experience working with deep learning frameworks (Tensorflow, Pytorch, etc.) and related tools in each special field. Solid understanding and hands on experience with C/C++/Python.
	- 4) Publication record in top AI conferences (ACL, CVPR, NAACL, ICML etc) is a plus.
	- 5) Experience with CUDA or GPU computation is a plus.
	- 6) Excellent communication and documentation skills.
	- 7) Ability to learn quickly, starting with little information and becoming an expert in your domain.
	- 8) Ability to communicate/collaborate with other researchers/engineers including our team members.
	- 9) Solid Github/ Kaggle profile is a plus.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
































***Machine Learning Engineer***, and ***Deep Learning Engineer***, roles:
+ Experience in Deep Learning and Deep Learning development frameworks
+ Proficiency in programming languages such as Python, Java and C++, and experience with machine learning libraries (e.g., TensorFlow, PyTorch, scikit-learn)
+ You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
+ tech stack:
	- Experience with Deep Learning frameworks, e.g., PyTorch, DeepLearning4J, TensorFlow
	- Experience in SPARK (using Python or Scala). Knowledge of AWS services will be appreciated.
+ skill set:
	- 3+ years of experience in machine learning, data mining, natural language processing, information retrieval, or statistical analysis
	- Experience working with large data sets using open source technologies such as Spark, Hadoop, and NoSQL
	- Experience developing and productizing real-world AI/ML applications such as prediction, personalization, recommendation, content understanding and NLP
	- Experience working with at least 3 of the following popular machine learning frameworks/libraries: sklearn, tensorflow, pytorch, caffe, keras, theano, cntk, mxnet, spark mllib
	- Experience developing and deploying deep learning NLP models is a plus
	- Experience working with a knowledge graph is a plus
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
+ skill set:
	- Individuals in this role should be experts in machine learning and NLP and have experience working on problems such as language models, discourse analysis, question-answering, word-sense disambiguation, automatic summarization etc.
	- Improve our existing NLP and Machine Learning systems using your expertise
	- Identify new opportunities to apply NLP and Machine Learning to different parts of the Quora product
	- Work with other engineers to implement algorithms, abstractions and systems in an efficient way, with strong positive impact on our user-facing products
	- Take end to end ownership of Machine Learning systems - from data pipelines and training to realtime prediction engines
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
+ skill set:
	- At Quora, we use Machine Learning in almost every part of the product - feed ranking, answer ranking, search, topic and user recommendations, spam detection etc.
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building internet applications and large systems
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
	- Passion for learning
+ skill set:
	- We use a variety of algorithms — everything from linear models to decision trees and deep neural networks.
	- To that end, we are looking for engineers to help us build our company-wide ML development platform. In this role, you will be the part of a small team solving very interesting technical problems at the intersection of various exciting domains like Machine Learning, Distributed Systems and High Performance Computing.
	- Build and maintain large scale distributed systems to support the whole pipeline from data collection and training to deployment
	- Write efficient implementations of ML algorithms over CPUs & GPUs
	- Integrate our in-house systems with open source libraries like Spark and Tensorflow
	- Build abstractions to automate various steps in different ML workflows
	- Build tools to debug, visualize and inspect various features and models
	- Work with the engineers who use the platform, and help them be more impactful by improving the platform
	- Experience with designing large-scale distributed systems
	- Experience with building end-to-end machine learning systems
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building end to end Machine Learning systems
+ ***Capsule Networks***, or capsule neural networks
+ skill set:
	- Experience working with relational SQL and NoSQL databases
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Fluency with one or more programing language: Python, Java, Scala, etc
	- Good understanding of CS fundamentals, e.g. algorithms and data structures
	- Experience with data science tools and libraries, e.g. R, pandas, Jupyter, scikit-learn, TensorFlow, etc
	- Familiarity with statistical concepts and analyses, e.g. hypothesis testing, regression, etc
	- Familiarity with machine learning techniques, e.g. classification, clustering, regularization, optimization, dimension reduction, etc
	- Guide the utilization or development of a robust CI/CD capability.
	- Identify key performance & effectiveness metrics, monitor & adjust to goals
	- Work with test automation team to lay the groundwork for automated API testing framework and test cases
	- Prioritize backlog & drive product releases
	- Distill strategic intent into structured product release roadmaps that are compelling and achievable
	- Ability to execute and manage performance and expectations within a cross-functional, matrix management environment
+ A background in machine learning and related sub-areas including ranking, personalization, search, recommendation, explore/exploit, causal learning, reinforcement learning, deep learning and probabilistic modeling.
+ skill set:
	- As a Deep Learning Engineer at Simbe Robotics, you will be part of a talented team designing and training state of the art deep learning algorithms to identify placement, presentation, pricing, and availability of products in retail stores across the globe.  
	- In this role you will lead various initiatives designing, developing, and training in-house character recognition and image caption algorithms powered by deep learning.
	- Participate in planning and prioritizing, write functional specifications and lead design reviews for our character recognition and image caption algorithms.
	- Generate, clean, and curate real world training datasets
	- Create photorealistic synthetic training data for augmentation
	- Develop, test, tune, and deploy character recognition and image caption systems across a wide variety of customers
	- Evaluate existing character recognition and image caption methods for speed and accuracy performance improvements
	- Collaborate with other developers, quality engineers, product managers, and documentation writers
	- Ph.D. or M.S. preferred
	- Strong machine learning background, with 2+ years of hands-on experience in building real systems
	- Deep understanding of state of the art machine learning and deep learning algorithms, techniques and best practices
	- Solid understanding of linear, non-linear, and dynamic programming
	- Experience using or building synthetic image generation systems, data augmentation pipelines, and OCR/image caption systems
	- Proficient in at least one of the following: Tensorflow, Keras, PyTorch. Tensorboard knowledge is a plus
	- Must be fluent in Python, other languages are a plus
	- Should be familiar with training and running deep learning models on GPUs (both commodity and otherwise)
	- A good understanding of recurrent neural networks (including LSTMs and GRUs)
	- Experience in debugging and diagnosing performance problems with ML algorithms
	- Must have excellent written and verbal communication skills
	- Experience with attention models, text localization, Google Cloud Platform, AWS, and serverless is a plus
	- Strong Linux & Command Line background
	- Ability to work hands-on in cross-functional teams with a strong sense of self-direction
+ skill set:
	- You have industry experience with writing code (e.g., Python, Scala, PySpark, Java) and taking ML models/ algorithms to production. Preference for 5+ years of industry experience (without PhD); at least 2-3+ years of industry experience with PhD. This is not an entry level / new college graduate role.
	- Experience with Apache Spark platform (including Datasets, SparkML) and/or experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe or PyTorch).
+ skill set:
	- As a research engineer at Salesforce Research, your role will be at the intersection of software engineering and research, and may range from implementing novel research models to rapid-prototyping demos that show off applications of deep learning on production data. You will work closely with research scientists to develop models, prototypes, and experiments that push the state of the art in AI research, paving the way for innovative products for the Einstein AI Platform. You will have the opportunity to take on real-world problems from Salesforce's enterprise customers with the latest deep learning models.
	- You have strong programming skills and a background in one or more of the following domains: deep learning, machine learning, natural language processing, or computer vision, with applications such as: text categorization, text summarization, sentiment analysis, information extraction, question answering, dialogue learning, language and vision, image classification, image segmentation, and object detection.
	- Knowledge of linear algebra, calculus, statistics, and machine learning.
	- Practical experience in natural language processing, computer vision, crowdsourcing, or information retrieval.
	- Exposure to industry or academic research, particularly in deep learning, neural networks, or related fields.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Experience with Amazon Web Services and Mechanical Turk.
	- Strong computer systems experience in topics such as filesystems, server architectures, and distributed systems.
	- Experience in GPU programming, data visualization, or web development.
+ skill set:
	- Machine learning. You should be able to understand and apply major machine learning methods, such as logistic regression, SVM, Decision Trees, Principal Component Analysis and K-means. Completion of Andrew Ng's Machine Learning course on Coursera is sufficient to meet this criterion.
	- Deep learning. You should be able to understand and apply major deep learning methods, including neural network training, regularization, optimization methods (gradient descent, Adam), and be familiar with major neural network architecture types such as Convolutional Networks, RNN/LSTM. Completion of the deeplearning.ai specialization is sufficient to meet this criterion.
	- Implementation. You should have prior experience taking a dataset, cleaning it if necessary, and applying a learning algorithm to it to get a result. You should be able to implement a learning algorithm “from scratch” using a framework such as NumPy, Tensorflow, Pytorch, Caffe, etc.
	- General coding. You should be able to code non-trivial functions in object-oriented programming, such as popular sorting or search algorithms.
	- Mathematics (including probabilities and statistics.) You should be able to use mathematical notations and linear algebra (matrix/vector operations, dot products, etc.), and understand basic probability theory (distributions, independence, density functions, etc.) as well as statistics (mean, variance, median, quantiles, covariance, etc.)
	- Software Engineering. You should know how to use your terminal, work with version control systems (Git), relational databases, APIs, and build the back-end of web or mobile applications.
	- Mean Stack
		* MEAN is a free and open-source JavaScript software stack for building dynamic web sites and web applications.
		* The MEAN stack is MongoDB, Express.js, AngularJS (or Angular), and Node.js.
+ skill set:
	- The steps of an end-to-end machine learning project. This includes, but is not limited to:
		* Conducting a structured and deep literature review of a specific field.
		* Strategizing your machine learning project end-to-end.
		* Collecting, cleaning, labeling, and augmenting your own dataset.
		* Training a model for a real-world application.
		* Setting-up an efficient and organized experimentation process.
		* Defining task-specific metrics to optimize in your experiments.
		* Performing error analysis to improve your models.
		* Deploying an AI product.
		* Exposure to real-world problems that multiple AI teams in our community work on.
	- Hands-on experience in designing, building, and deploying end-to-end AI solutions through curated content and instructor-led workshops.
	- Career mentorship and connections with teams aligned with your career aspirations.
	- Meet and share experiences with other machine learning engineers and data scientists.
	- Everyone who successfully completes the Bootcamp will be awarded a certificate of completion and join the AI Bootcamp Alumni community.
	- Machine learning Engineers and Data Scientists who have already worked on Machine Learning projects and want to get exposed to different Machine Learning problems.
	- Demonstrated AI, data science and/or data analysis experience from previous work experience or publications.
	- Demonstrated strong coding from previous work experience or publications. This means you're able to write a non-trivial program in Python, Java, or C++.
	- Solid CS foundation (including but not limited to Operating Systems, Computer Networks, Database, etc.)
+ Experience working with modern deep learning software architecture and frameworks including: Tensorflow, MxNet, Caffe, Caffe2, Torch, and/or PyTorch.
+ experience creating machine learning products
	- deploy model
	- MLOps
+ skill set for machine learning architect:
	- optimize workflows and solve problems using a data-driven approach
	- provide machine learning as a service (MLaaS) for users
	- co-develop machine learning as a service (MLaaS) platform, with the following features:
		* data ingestion
		* data indexing
		* data labeling
		* visualization
		* dashboards
		* data viewers
	- collaborate with team on production quality service development with:
		* unit testing, integration testing for MLaaS
		* CI/CD for MLaaS
		* DevOps for MLaaS
	- develop and maintain services with:
		* high production quality standards
		* components for feature data storage
		* ML model training and inferencing
		* ML model storage and management
		* model evaluation metrics
	- demonstrate experience in MLOps and deep learning related infrastructure
	- solid foundation for machine learning, deep learning architecture and service development experience
	- provide capacity to use and fine-tune latest state-of-the-art machine learning and deep learning models, depending on the use cases
		* develop lifecycle management tool for these machine learning and deep learning models to be deployed as a service and made available for inferencing
	- detailing processes and workflows
	- Programming skills in:
		* Java
		* Python
		* Web service development, using REST API
			+ front-end Web development
				- React
				- Ember.js
	- object-oriented design patterns
	- ability to learn quickly and adapt to different platforms as per the need of the project
	- crafted/developed production quality microservices
	- knowledge of:
		* Hadoop
		* Hive
		* Spark
		* Kafka
		* cloud/distributed infrastructure
		* SQL and NoSQL data platforms
+ skill set:
	- deep knowledge of distributed training concepts and frameworks, such as Megatron and Deepspeed
		* Megatron-Turing Natural Language Generation model, MT-NLG,
	- design and develop analysis tools to drive efficient research, such as:
		* deep cleaning
		* analysis of training dynamics
		* grdient quality
	- design and develop software for scaling models and large-scale experimentation
	- design and develop ML workflows and user interfaces for novel algorithms
	- deep knowledge of machine learning framework, such as TensorFlow and PyTorch
+ skill set:
	- wafer scale engine, WSE
	- NLP models:
		* BERT
		* GPT
	- computer vision models:
		* ResNet
		* Vision Transformer
	- sparse and low-precision training algorithms for reduced training time and increased accuracy
	- compute- and memory- efficient training techniques, such as reversibility and low-rank
	- Scaling laws for increasing model size: accuracy/loss, architecture scaling, hyperparameter transfer 
	- Optimizers, initializers, normalizers to improve distributed training on large scale clusters 
	- Develop novel training algorithms and demonstrate on state-of-the-art large DNNs 
	- Develop novel network architectures and layers such as normalization, activation functions, optimizers, and parameter layers 
	- Design and run research experiments to prove novel algorithms are effective and robust 
	- Analyze results to gain research insights, including training dynamics, gradient quality, and dataset cleaning techniques 
	- Publish and present research at leading machine learning conferences 
	- Collaborate with engineers in co-design of the product to bring the research to customers 
	- Strong grasp of machine learning theory, fundamentals, linear algebra, and statistics 
	- Experience with state-of-the-art DNNs models, such as BERT and GPT 
	- Experience with machine learning frameworks, such as TensorFlow and PyTorch
	- Experience with distributed training concepts and frameworks such as Megatron and Deepspeed 
	- Fluency in a programming language, such as Python 
	- Strong track record of relevant research success through relevant publications/patents at top conferences or journals (e.g. ***ICLR, ICML, NeurIPS***)
+ experience with modern compiler frameworks:
	- TVM
	- LLVM
	- MLIR
	- GLOW
	- XLA
+ skill set:
	- ***deep learning runtimes: ONNX Runtime, TensorRT***
	- ***inference server or model serving frameworks***
		* ***Triton***
		* ***TFServe***
		* ***KubeFlow***
	- distributed systems collective
		* NCCL
		* OpenMPI
	- deploy ML workloads on distributed systems, in a multitenancy environments
	- MLOps, from definition to deployment, including training, quantization, sparsity, model preprocessing, deployment
	- training, tuning, and deploying ML models for:
		* computer vision, such as ResNet
		* natural language processing, such as BERT, GPT
		* recommendation systems, DLRM
+ machine learning model compression techniques, such as quantization or pruning
+ skill set:
	- Machine Learning Research Engineer, Generative AI
	- Performant model code, high quality data, and robust evaluation methods form the foundation of an AI system. Scale’s leading end-to-end solutions for the ML lifecycle based on real-world data will continue to set the bar for the data-centric AI movement. Scale’s Generative AI team focuses on building models to accelerate AI adoption for some of the largest companies in the world. 
	- Your focus will be on developing Models as a Service using a variety of Machine Learning techniques. You will be involved end-to-end from coordinating with operations to create high quality datasets to productionizing models for our customers. If you are excited about shaping the future of the data-centric AI movement, we would love to hear from you!
	- Apply state of the art models, developed both internally and from the community, in production to solve problems for our customers and data labelers. 
	- Work with product and research teams to identify opportunities for ongoing and upcoming services.
	- Explore approaches that integrate human feedback and assisted evaluation into existing product lines. 
	- Work closely with customers - some of the most sophisticated ML organizations in the world - to quickly prototype and build new deep learning models targeted at multi-modal content understanding problems.
	- At least 3 to 5 years of model training, deployment and maintenance experience in a production environment.
	- Strong skills in NLP, LLM and deep learning.
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment.
	- Experience in dealing with large scale AI problems, ideally in the generative-AI field. 
	- Demonstrated expertise in large vision-language models for diverse real-world applications, e.g. classification, detection, question-answering, etc. 
	- Published research in areas of machine learning at major conferences (***NeurIPS, ICML, EMNLP, CVPR***, etc.) and/or journals. 
	- Strong high-level programming skills (e.g., Python), frameworks and tools such as DeepSpeed, Pytorch lightning, kuberflow, TensorFlow, etc. 
	- Strong written and verbal communication skills to operate in a cross functional team environment. 
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $176,000 - $240,960. Compensation packages at Scale include base salary, equity, and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Scale employees are also granted Stock Options that are awarded upon board of director approval. You’ll also receive benefits including, but not limited to: Comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous PTO. Additionally, this role may be eligible for additional benefits such as a commuter stipend.
+ skill set:
	- Scale Spellbook is a developer platform for prompt engineering, evaluation, deployment, knowledge retrieval and more. We are looking for a strong product engineer to join our team and help us scale and grow our product. The ideal candidate will have a strong understanding of software engineering principles and practices, as well as experience with large-scale distributed systems. You will be responsible for owning large new areas within our product, working across backend, frontend, and interacting with LLMs and ML models. You will solve hard engineering problems in scalability and reliability.
	- Own large new areas within our product
	- Work across backend, frontend, and interacting with LLMs and ML models
	- Deliver experiments at a high velocity and level of quality to engage our customers
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- 5+ years of full-time engineering experience, post-graduation
	- Experience scaling products at hyper growth startups
	- Experience tinkering with or productizing LLMs, CV, vector databases, and the other latest AI technologies
	- Proficient in Python or Javascript/Typescript, and Sql database
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $212,800-$258,121.
+ skill set:
	- Experience  in using the following systems in production: AWS, Typescript, Node, MongoDB, MLflow, Python (note that we are mostly language-agnostic and are open to using whatever is the best tech for the problem at hand)
	- Experience working with Docker, Kubernetes, and Infrastructure as code (eg terraform); bonus points for running GPU/ML workloads
	- Experience building systems that process large volumes of data.
	- Experience with core AWS technologies such as VPC, EC2, ALB, ASG, Spot Instances
	- Experience in operating or managing Infrastructure such as Spark, Presto, Hive
+ skill set:
	- Staff Machine Learning Research Engineer, Generative AI
	- Scale's Generative AI Data Engine powers the most advanced LLMs and generative models in the world through world-class RLHF/RLAIF, data generation, model evaluation, safety, and alignment.
	- As the Lead of the Generative AI team, you will be responsible for managing and leading a group of talented researchers and engineers. Your primary focus will be to leverage your expertise in LLMs, generative models, and other foundational models to create and execute an AI roadmap which will help Scale accelerate our customers' Generative AI initiatives forward. This is an exciting opportunity to work on cutting-edge technologies and collaborate with industry-leading professionals.
	- We are building a large hybrid human-machine system in service of ML pipelines for dozens of industry-leading customers. We currently complete millions of tasks a month and will grow to complete billions monthly.
	- Led a team of highly effective researchers and engineers. Provide guidance, mentorship, and technical leadership to a team of researchers and engineers working on Generative AI projects. Develop and evaluate methods for integrating machine learning into human-in-the-loop labeling systems to ensure high-quality and throughput labels for our customers.
	- Implement and improve on state-of-the-art models developed internally and from the community and put them into production to solve problems for our customers and taskers.
	- Work with product and research teams to identify opportunities for improvement in our current product line and for enabling upcoming product lines.
	- Work with massive datasets to develop both generic models as well as fine-tune models for specific products.
	- Work with customers and 3rd party research groups to understand their goals and define how we can enable them.
	- Build a scalable ML platform to automate our ML services, including automated model retraining and evaluation.
	- Be able and willing to multi-task and learn new technologies quickly.
	- Must be able to commute to the San Francisco Office 1-2x weekly. 
	- 7+ years of full time work experience using LLM, deep learning, deep reinforcement learning, or natural language processing in a production environment. Especially training foundational AI models through pre-training, fine-tuning, and RLHF.
	- A vision for where the field should go and what Scale should do to enable it.
	- Strong programming skills in Python, experience in PyTorch or Tensorflow
	- Experience with MLOps and the automation of model training & evaluation
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Solid background in algorithms, data structures, and object-oriented programming
	- Deep appreciation for building high-quality, robust, reusable machine-learning software
	- Degree in computer science or related field
	- Graduate degree in Computer Science, Machine Learning or Artificial Intelligence specialization
	- Publication experience in the field or related topics.
	- Experience with model optimization techniques for both training and inference
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $176,000 - $250,000.
+ Experience working fluently with standard orchestration & deployment technologies like Kubernetes, Temporal, Terraform, Docker, etc in multiple clouds
+ skill set:
	- Senior Software Engineer, AV / CV
	- At Scale AI, we are building tools to across the AI development lifecycle. Data is the new code, and Scale AI helps companies get the data they need, whether it’s for self-driving vehicles, artificial general intelligence, or robotics. The AV-CV Team (Autonomous Vehicles / Computer Vision) builds the infrastructure for labeling Lidar, Mapping, and Camera data. If seeing autonomous vehicles excited you, you’ll be seeing a lot of that on the AV-CV team. The team builds the tools for labeling Lidar pointclouds, annotating image and video feeds, and linking them together to build perception models.
	- Own large new areas within our product
	- Become an expert in working closely with customers across many CV industries
	- Build technologies ranging from frontend and backend to automated ML systems
	- Work deeply with sales and marketing to run demos and increase customer engagement
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- 5+ years of full-time engineering experience
	- 2+ years working with TypeScript/JavaScript, HTML, CSS, and related web technologies (React, Next.js, Webpack)
	- Experience working with distributed systems and cloud environments
	- Experience working with a production database (Postgres, MongoDB, MySQL, MS SQL) and schema migrations
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Excitement to work with AI technologies
	- Strong written and verbal communication skills
	- Strong problem-solving skills, and be able to work independently or as part of a team.
	- The base salary range for this full-time position in our hub locations of San Francisco is $153,000 - $215,000.
+ skill set:
	- Familiar with CUDA ecology, experience in NCCL and RDMA development is preferred;
	- Understand common large-scale distributed training optimization strategies, familiarity with fsdp/deepspeed/accelerate/galvatron is preferred.
+ skill set:
	- Familiarity with distributed training, NCCL communication library, etc. is preferred; familiarity with large model training frameworks such as DeepSpeed, Megatron, etc. is preferred;
	- Proficiency in CUDA and other heterogeneous programming is preferred; familiarity with GPU/GPGPU architecture is preferred;
	- Familiar with CNN, RNN, Transformer and other deep neural networks is preferred;
	- Familiarity with AI compilation technologies such as TVM and MLIR is preferred.
+ skill set:
	- Proficient in Gtest or Pytest framework, and be familiar with Jenkins, K8s and Docker environment.
	- Experience in PyTorch/TensorRT/Tensorflow programming is a big plus.
	- Experience in KMD/UMD/Operator development is a big plus.
+ Familiar with and have used one or more mainstream deep learning frameworks (Tensorflow/PyTorch/PaddlePaddle/MindSpore, etc.);
+ skill set:
	- Research and analyze common algorithm models (CNN/RNN/Transformer, etc.) in scenarios such as computer vision, speech recognition, natural language processing, and advertisement recommendation, and analyze their implementation, performance, and accuracy on mature GPUs;
	- Based on Biren's chip products (GPGPU), participate in the transplantation, acceleration, precision tuning of deep learning algorithm models on Biren GPU, and performance tuning of application pipelines;
	- Closely track industry trends and technology trends, focusing on hot algorithms/models that will be applied in the industry.
	- Master’s degree or above in EECS or related majors, with more than 5 years of relevant work experience;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorflow/PyTorch/PaddlePaddle/MindSpore, etc.);
	- Familiar with the principles of deep learning algorithms, have experience in precision and performance tuning, and be familiar with the application of algorithms in business scenarios;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Have a developer's perspective understanding of one or more mainstream deep learning frameworks, and have certain insights and experience in framework design or tuning;
	- Familiar with large-scale distributed training, understand the principle and implementation of large models such as GPT;
	- Have experience in algorithm and engineering collaborative optimization, and be able to make a reasonable trade-off between accuracy and performance;
	- Experience in GPGPU programming, experience in domestic GPGPU/AI chips;
	- Have experience in AI compiler.
+ skill set:
	- System Architect - HW/SW Architecture - Apple Vision Pro
	- Apple is where individual imaginations gather together, committing to the values that lead to great work. Every new product we build, service we create, or Apple Store experience we deliver is the result of us making each other’s ideas stronger. That happens because every one of us shares a belief that we can make something wonderful and share it with the world, changing lives for the better. It’s the diversity of our people and their thinking that inspires the innovation that runs through everything we do. When we bring everybody in, we can do the best work of our lives. Here, you’ll do more than join something — you’ll add something.
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- 6+ years of working experience
	- Strong system / SoC architecture background with deep understanding of low power processing.
	- Experience in development of SoC Power and Performance simulation models for use case level analysis and optimization.
	- Strong coding skills in Python, C/C++, SystemC.
	- You have background on NN algorithms and Neural HW architecture.
	- You have background on HW acceleration for image / display / audio signal processing workloads.
	- Ability to drive HW / SW partition and co-optimization in a data driven way.
	- Strong verbal communication skills across organizations and at the executive level.
	- Excellent written skills for clear reporting of data analysis and conclusions.
	- We are seeking a highly motivated, system architect to define compute acceleration strategy into our products. You must have excellent understanding of computer architecture (CPU, GPU, Neural Processing), System / SoC level power architecture, emerging compute engines RISC-V, Low power neural processing architecture. You will be mapping system level use cases to the SoC and system and identify HW/SW partitioning opportunities. With this knowledge you are able to apply it to analyzing requirements tradeoffs impacting system performance and product experience. 
	- The job will have a focus on use case level analysis and development of system level models for power, performance and latency to drive overall product architecture.
	- This cross-functional role requires excellent interpersonal skills, working with camera module, architecture, system EE, and product development teams and communicating across teams and to leadership.
	- MS/PhD in relevant field (EE, CS, Computer Eng)
	- At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ ***Experience with metrics systems such as Grafana.***
+ skill set:
	- ML Systems & Performance Engineer - SPG
	- The key to successful Machine Learning Teams is an efficient, robust, flexible and scalable architectural foundation. The Apple Special Projects Group on Autonomous Systems is looking for a talented, dedicated and result oriented software engineer to help build and improve our Machine Learning Systems. This fundamental and highly impactful work enables us to architect, train, evaluate and deploy state of the art machine learning models for Autonomous Systems.
	- You will be part of a world class team with a highly diverse skillsets. From implementing CUDA kernels, optimizing cache coherency of complex algorithms, reducing memory footprints all the way to optimizing distributed training setups and implementing efficient large scale cloud compute schedules, ML Systems & Performance engineers are taking a holistic view at the whole stack to eliminate bottlenecks resulting in a lean and optimally efficient implementation.
	- 3+ years of professional software development experience.
	- Familiarity with modern Machine Learning frameworks like PyTorch.
	- Familiarity with optimization techniques like Quantization, sparsity and/or pruning.
	- Experience in architecting and implementing large-scale cloud pipelines., e.g using spark, kafka, kubeflow, postgres
	- High proficiency in Python, C++ and/or CUDA.
	- Passion for optimizations and efficient implementations.
	- High software engineering standards: desire to write clean, well-tested and well-structured code.
	- Track record of collaborations across teams, including requirement specifications and successful project delivery.
	- Excellent communication and presentation skills.
	- Curiosity to learn new things and push the boundary towards new approaches without fear of changing existing paradigms.
	- Improve ML Training efficiency to models train faster by scaling out and scaling up.
	- Develop strategies for training distributed models to accommodate model growth beyond state-of-the-art.
	- Design and implement a highly scalable inference pipelines for large scale evaluation.
	- Architect a data processing framework training data preparation and auto-labeling.
	- Collaborate with ML engineers to remove bottlenecks and improve turn-around times.
	- Build visualizations and dashboards to monitor cloud resource utilization.
	- At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ ***Good experience with applying Big Data tools (MapReduce, Hadoop, Hive and/or Pig, Spark) to large quantities of textual data***
+ skill set:
	- AIML - Software Engineer, Machine Learning Platform & Technologies
	- The Machine Learning Platform Team at Apple is looking for a Senior Engineer who has extensive experience in CI/CD, orchestration pipelines, build, release, and code management to manage critical parts of our development lifecycle. You will work with a dedicated team of engineers that will deliver the tooling and pipelines that enable consistent development of a high performance search stack, ensuring high quality delivery and enabling fast engineering turnaround.
	- 5+ years of experience in developing developer tooling, pipelines, automations and API
	- Thorough understanding of software development lifecycle, DevOps (build, continuous integration, deployment tools) and best practices.
	- Strong programming skills in Go, Python, or other language
	- Strong experience with Spinnaker and/or other delivery platforms
	- Strong experience with workflow platforms (Argo, Jenkins, or other)
	- Solid Kubernetes, AWS or other cloud experience
	- Strong communications and collaboration skills required
	- We design and build infrastructures to support features that empowers billions of Apple users. Our team processes trillions of links to find the best content to surface to users through search. We also analyze pages to extract critical features for indexing, ranking. We apply statistical analysis to improve link selection, freshness, retrieval rates, extraction quality, and many others. You’ll have the opportunity to work with large scale systems with trillions of rows and many petabytes of data and incredible complexity.
	- Work in a team of engineers to deliver core tooling and pipelines for builds, validation, and deployment
	- Build tooling and SCM integrations that ensures code consistency, test coverage, and reliability
	- Develop pipelines for continuous, incremental delivery to preproduction environments
	- Extend tooling that helps engineers test, maintain, and deploy solutions across multiple repos and languages
	- Collaborate with DevOps and Release Engineering to enhance release speed and reliability
+ skill set:
	- On the GPGPU-based heterogeneous system, research and implement distributed core modules such as tensorflow/pytorch/horovod for scenarios such as voice/image/NLP, and research multiple hybrid parallel strategies such as data parallelism/model parallelism/pipeline parallelism to improve the scalability of large-scale distributed training;
	- On the heterogeneous system based on GPGPU, research large-scale sparse model training framework for scenarios such as search/advertising/recommendation, support TB-level models, and support multi-machine efficient expansion;
	- Participate in MLPerf rankings to enhance the international influence of domestic GPU chips.
	- Computer, electronics, mathematics and related majors, more than 2 years of relevant work experience;
	- Proficiency in C++/Python programming;
	- Experience in developing Tensorflow/pytorch/horovod and other domestic frameworks;
	- Proficiency in NCCL, RDMA and other communication technologies is preferred;
	- Familiar with data parallel/model parallel/pipeline parallel, familiar with Deep Speed, etc. is preferred;
	- Familiar with CNN/ RNN such as Bert/GPT and other networks is preferred;
	- Familiarity with advertising/recommendation scene algorithms such as Wide&Deep/DLRM/DeepFM/DIEN is preferred, and familiarity with large-scale sparse model training frameworks such as AIBox/HugeCTR/XDL is preferred.
+ skill set:
	- Biren-based chip products (GPGPU), responsible for the implementation of customer applications, including the transplantation and acceleration of the customer's algorithm model on the Biren chip, as well as the performance tuning of the application pipeline;
	- Have certain insights and experience in AI models (CNN/RNN/Transformer, etc.) and pipelines corresponding to scenarios such as computer vision, speech recognition, search advertisement recommendation, and natural language processing;
	- Closely track industry trends and technology trends, and participate in the exploration of new technology solutions.
	- Bachelor degree or above in EE, CS or related majors, more than 5 years of work experience;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.);
	- Familiar with heterogeneous computing systems, and have at least participated in the model transplantation and optimization of one or more AI chips;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Bonus points: 1. For one or more mainstream deep learning frameworks (Tensorflow/PyTorch/MXNet, etc.) have developers; understanding of perspective, certain insights and experience in framework design or tuning;
	- Familiar with and like performance optimization, have certain development experience in CUDA/OpenCL/OpenMP, etc. or have certain insights and experience in performance distributed solutions such as MPI/CCL;
	- Familiar with GPGPU/AI ASIC/CPU micro-architecture, have certain insights and experience in joint optimization of software and hardware;
	- Experience in compiler, video codec, GPU virtualization, K8s cluster management and scheduling is preferred.
+ skill set:
	- AI Software Library Senior Development Engineer (C++).
	- Participate in the design, implementation, testing, performance optimization and problem fixing of deep learning acceleration library
	- Participate in the design, implementation, testing, performance optimization and problem fixing of AI compilation module
	- Participate in the design, development and continuous optimization of the basic framework for AI operator development
	- Participate in the interface design of AI operator docking with the upper framework, and the design and development of debugging tools
	- Proficient in C++11/14/17/20, proficient in STL
	- Proficient in commonly used data structures and algorithms, it is best to have your own implementation
	- Proficient in C++ multi-thread development under multi-platform, have a deep understanding of thread safety, anti-deadlock, thread-related design patterns and functional programming and the ability to get started immediately
	- Proficient in Python
	- Those who have studied the source code of the deep learning framework (Caffe/Tensorflow/PyTorch/MXNet) and have done relevant development are preferred
	- Must have excellent programming style
	- Familiar with deep learning framework (Caffe/Tensorflow/PyTorch/MXNet) or experience in using TensorRT is preferred
+ skill set:
	- Intelligent Platform Inference Engine R&D Engineer
	- Responsible for the design and implementation of the programming model of the intelligent processor reasoning optimization engine;
	- Responsible for the function realization and performance optimization of the intelligent processor reasoning optimization engine;
	- Responsible for the implementation of the intelligent processor reasoning optimization engine, including function realization, performance optimization and problem repair;
	- Participate in the design, development and continuous optimization of deep learning-related solutions;
	- Continue to carry out intelligent processor compilation optimization and integration work.
	- Bachelor degree or above, computer, mathematics, software engineering, automation, communication, microelectronics and other related majors, more than three years of relevant work experience;
	- Familiar with object-oriented programming methods, familiar with C++ programming development, compilation and debugging process in Linux environment, familiar with python, have a solid programming foundation, good programming style and working habits;
	- Familiar with deep learning algorithms, familiar with at least one deep learning programming framework (one of caffe/tensorflow/pytorch/mxnet), familiar with or have experience in using TensorRT is preferred;
	- Have good learning and understanding ability, strong logical thinking;
	- Possess a good teamwork spirit, strong sense of responsibility, do things practically and conscientiously, be able to actively complete relevant work, and have certain ability to resist pressure.
+ skill set:
	- Math Library Development Engineer
	- Participate in the hardware implementation and performance optimization of common neural network models, such as ResNet, BERT, EfficientNet, DLRM, etc., and propose architecture and microarchitecture improvements based on various neural network model architectures;
	- Participate in the development and performance optimization of AI operator libraries and other mathematical libraries;
	- Participate in the function and performance verification of the chip, and propose architecture and micro-architecture changes according to the verification results;
	- Participate in the development and improvement of various tools, which will be used to support the development and verification of AI operators and other mathematical libraries;
	- Cooperate with the hardware design department to support hardware verification;
	- Cooperate with the software department to support the development of AI framework/software/compiler.
	- Master degree or above, major in computer science, applied mathematics, etc.;
	- Proficient in using Python, familiar with algorithms and data structures;
	- Understand computer architecture, familiar with CPU/GPU architecture is preferred;
	- Understand the basic principles of neural networks, experience in the development and use of neural networks is preferred;
	- Good communication skills and teamwork skills.
+ Experience in ***PyTorch/TensorRT/Tensorflow*** programming is a big plus.
+ Experience in ***CUDA/cuDNN*** programming is a big plus
+ skill set:
	- Inference Engine Architect
	- On the heterogeneous system based on GPGPU, research and realize the reasoning engine for voice/image/NLP and other scenarios, and be responsible for related architecture design, performance optimization and business implementation;
	- On the heterogeneous system based on GPGPU, research and realize the prediction engine for search/advertising/recommendation and other scenarios, and be responsible for related architecture design, performance optimization and business implementation;
	- Responsible for the integration of Biren's self-developed reasoning/estimation engine and third-party frameworks such as TF or the reasoning/estimation engine of major Internet customers, to strengthen ecological construction and promote business implementation;
	- Participate in MLPerf rankings to enhance the international influence of domestic GPU chips.
	- Computer, electronics, mathematics and related majors, more than 5 years of relevant work experience;
	- Proficiency in C++/Python programming;
	- Experience in open source framework or engine architecture design such as ***Tensorflow/Pytorch/Paddle/TensorRT/TNN/MNN/OpenPPL/HugeCTR***;
	- Familiar with AI compilation technology such as TVM/MLIR/XLA is preferred;
	- Familiar with CNN/ RNN and other network reasoning optimization / landing priority;
	- Familiarity with Wide&Deep/DeepFM/DIN and other advertising recommendation scene models is preferred, and familiarity with sparse model estimation engine architecture design and implementation is preferred.
+ skill set:
	- Deep Learning Framework Engineer
	- On the heterogeneous system based on GPGPU, research and implement the core module of the deep learning programming framework;
	- Algorithm research and framework implementation of large-scale cluster distributed training.
	- Computer, mathematics and related majors, more than 1 year of relevant work experience, doctor is preferred;
	- Master at least one language (C++/Python);
	- Mastering a mainstream deep learning framework (Tensorflow, PyTorch, MXNet, NNVM/TVM, etc.) is preferred;
	- Familiar with CUDA, OpenCL, OpenMP and other heterogeneous programming is preferred;
	- Familiarity with CNN, BERT, RNN and other networks is preferred; 6. Familiarity with compilation principles and LLVM is preferred.
+ skill set:
	- Deep Learning Framework Architect
	- On the GPGPU-based heterogeneous system, research and implement the core module of tensorflow;
	- According to the characteristics of the model and hardware, optimize the calculation graph, as well as optimize the memory and synchronization scheduling.
	- Computer, mathematics and related majors, more than 5 years of relevant work experience, doctorate is preferred (more than 3 years);
	- Familiar with C++/Python;
	- More than 2 years of experience in the development of mainstream deep learning frameworks, including Tensorflow, PyTorch, Mindspore, PaddlePaddle, Oneflow, MegEngine, MXNet, NNVM/TVM, etc.;
	- Proficiency in heterogeneous programming such as CUDA/OpenCL/Vulkan/OGL/DX Compute is preferred;
	- Familiar with GPU/GPGPU architecture;
	- Familiar with CNN, BERT, RNN and other networks is preferred;
	- Familiar with compilation principles and LLVM is preferred.
+ skill set:
	- Deep Learning Framework Engineer
	- Research and implement the core modules of the deep learning programming framework on the heterogeneous system based on GPGPU;
	- Algorithm research and framework implementation of large-scale cluster distributed training.
	- Computer, mathematics and related majors, with more than 1 year of relevant work experience, doctor is preferred;
	- Master at least one language (C++/Python);
	- Mastering a mainstream deep learning framework (Tensorflow, PyTorch, MXNet, NNVM/TVM, etc.) is preferred;
	- Familiar with CUDA, OpenCL, OpenMP and other heterogeneous programming is preferred;
	- Familiarity with CNN, BERT, RNN and other networks is preferred; 6. Familiarity with compilation principles and LLVM is preferred.
+ skill set:
	- Chip System Architecture Engineer
	- Responsible for driving the architectural design of our general computing/AI processors based on the latest general computing and AI accelerator applications, providing new technologies for products in new markets and existing markets
	- Participating in the development of modeling tools and infrastructure needed to facilitate exploratory design and performance studies;
	- Promote the development of new GPGPU/AI technologies to improve the user experience of various products and applications.
	- Bachelor degree or above in computer science/electronic engineering/applied mathematics, master/doctorate preferred;
	- More than 7 years of experience in general computing/AI hardware architecture, microarchitecture and design;
	- Have a deep understanding of modern general-purpose computing application programming interfaces, such as Cuda, OpenCL;
	- Have a deep understanding of deep learning technology, with artificial neural network framework;
	- Familiar with C/C++, Python, excellent software development skills;
	- Excellent teamwork skills, self-motivation and focus on results.
+ skill set:
	- Senior Software Library Engineer/Architect
	- Design and develop general basic mathematics library;
	- Discuss and implement possible optimization methods for specific hardware implementations;
	- Coordinate with other software engineers to provide solutions to problems and performance tuning in various general purpose platforms.
	- Master degree or above, more than 3 years of work experience;
	- Proficient in C/C++ programming, experience in Python programming is preferred;
	- Understand computer architecture;
	- Understand compute shader programming;
	- Experience in one or more common platforms or mathematical libraries is preferred: such as ***CuFFT, CuBlas, CuDNN, Tensorflow, TensorRT***, etc.;
	- Have a strong ability to analyze and solve problems;
	- Possess strong communication skills, independent working ability and team driving ability, and can coordinate all relevant teams to promote the completion of the plan.
+ skill set:
	- AI/GPU Heterogeneous Platform Software Development Engineer
	- Parallel algorithm implementation and high-performance template library development in AI/GPU software framework;
	- Analyze and research open source machine learning and deep learning training engines, optimize and improve the implementation of operators and models;
	- Participate in the architecture design, key technology research and core code development of heterogeneous computing framework;
	- Development of test cases in general computing and heterogeneous computing programming models.
	- Computer, mathematics and related majors, master degree or above or more than 3 years of relevant work experience, doctor is preferred;
	- Proficient in C/C++ and open source project development tools under Linux; familiar with heterogeneous programming models such as CUDA/HIP/cupy;
	- Strong learning ability, familiar with compilation principles and architecture is preferred;
	- Familiarity with deep learning frameworks, accelerator cards, and typical deep learning networks is preferred.
+ skill set:
	- GPU application optimization engineer
	- Biren-based chip products (GPGPU), responsible for the transplantation and acceleration of deep learning algorithm models on Biren GPU, and performance tuning of application pipelines;
	- Have certain insights and experience in AI models (CNN/RNN/Transformer, etc.) corresponding to scenarios such as computer vision, speech recognition, search advertisement recommendation, and natural language processing;
	- Closely track industry trends and technology trends, and participate in the exploration of new technology solutions.
	- Master degree or above in EECS or related majors;
	- Proficiency in operating Linux system, proficiency in C/C++/Python/Shell, solid programming foundation and debugging experience;
	- Familiar with and have used one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.);
	- Familiar with heterogeneous computing systems, experience in cuda or parallel computing is preferred;
	- Diligent in thinking, willing to solve problems, and have the spirit of cooperation.
	- Have a developer's understanding of one or more mainstream deep learning frameworks (Tensorlow / PyTorch / MXNet, etc.), and have certain insights and experience in framework design or tuning;
	- Familiar with GPGPU/AI ASIC/CPU micro-architecture, have certain insights and experience in joint optimization of software and hardware;
	- Experience in compiler, video codec, GPU virtualization, K8s cluster management and scheduling is preferred.
+ skill set:
	- Machine Learning Engineer - SIML, ISE
	- Would you like to help shape the next set of ML features of iPhone? Would you like to contribute to the field of generative AI? Want to contribute to transforming how people interact with AI technologies?
	- The System Intelligence and Machine Learning team is in charge of creating datasets that power many of Apple’s intelligent software. Our datasets range from very small targeted sets to Petabyte scale datasets. As a data scientist on our team you will be in charge of selecting the right assets, removing harmful and toxic assets and extracting insights from the datasets, assessing & reducing harmful biases, and maximizing fairness and inclusion of various ML features.
	- We are looking for an experienced Machine Learning Engineer who can help create and improve the datasets used in Generative AI through solid understanding and usage of ML and stats. You will be using Apple technologies to refine our datasets, remove toxicity and select the right images, videos or texts through active selection and model-in-the-loop methodologies. Focus areas range from text processing across many languages (toxic language detection and removal, identification of colloquial vs formal language) to image and video understanding, deduplication and processing.
	- Familiarity with a broad range of Machine Learning techniques and relevant statistical packages to engineer Machine Learning solutions end-to-end.
	- Experience in contributing to production code bases. Ability to rapidly prototype algorithmic ideas in notebook environments and translate them into production code.
	- Proficient in state-of-the-art ML techniques particularly in the field of Generative AI and Large Language Models (Transformer architecture, CLIP and various visual and text embedding models, GPT and BERT style language models).
	- Exceptional communication and presentation skills and the ability to explain difficult technical topics to everyone from data scientists to engineers to business partners.
	- Strong proficiency with Python (Scikit learn, Jupyter), PyTorch, SQL-based languages. Working proficiency with Git.
	- As a Machine Learning Engineer on the Data Team, you will be working to deepen our understanding of how various datasets can improve the quality of Apple’s ML models on a range of products. You will particularly help shape Apple’s Datasets that are used for generative AI by removing irrelevant or toxic assets, selecting the right assets by employing various asset selection algorithms, utilizing Apple proprietary ML models. For this, you will also use your stats and ML background to build models and algorithms that can select the right assets for ML experiences from a large pool of available assets. And you will work with our data engineers to put your models in data pipelines to run on large scale datasets.
	- In our team, you are expected to collaborate with other AIML product stakeholders and partners to understand needs, design Machine Learning models that help us better understand our data and automatically pick the right assets for ML training. Our Data Scientists actively evaluate and present the progress of their work. Your creative problem solving skills will be used daily.
	- Masters or Phd degree in Computer Science, Engineering; or equivalent practical experience. 
	- Strong analytical product intuition: able to understand the user experience and use data to guide the development of products.
	- 2+ years of experience in an Applied Scientist role, preferably in a technology company.
	- Ability to understand a technically complex product, and work with engineering leads and data engineers.
	- Proficiency in data science and analytics, including statistical data analysis and machine learning. Experience crafting, conducting, analyzing, and interpreting experiments and deep-dive investigations.
	- Ability to build relationships across multiple functions and establish strong partnerships.
+ Proficiency ***in building end-to-end ML pipeline from data ingestion to feature engineering to model training to deploying and scaling the model in production***.
+ Experience ***in building end-to-end ML pipeline from data ingestion to feature engineering to model training to deploying and scaling the model in production***.
+ skill set:
	- Bonus if you have prior data analysis experience (SQL, Python/Jupyter Notebooks, Tableau, Superset)
	- Bonus if you have prior experience exploring data formats and tools for big data systems (e.g. Parquet, Avro, Protobuf).
	- Bonus if you have prior experience working with data engineers (e.g. engineers who use tools such as ***Airflow, Luigi, Jenkins, MapReduce, Pig, Spark, Hadoop, Hive, HBase, Greenplum, Vertica***, etc.)
+ skill set:
	- 2+ years of experience in developing end to end machine learning pipelines on distributed systems.
	- You have experience in deploying machine learning solutions on marketplaces. Any previous experience in advertising technology in areas such as conversion modeling, automatic bidding, keyword/bid/budget recommendation, creative optimization, advertiser churn prediction, automatic targeting and auction design will be a plus.
	- You can drive multi-functional collaboration with product, data engineering, experimentation and machine learning operations.
	- You can actively participate in investigations into multiple streams of ads quality data, draw conclusions from data, and recommend actions.
	- You are comfortable with alternative experiment designs such as budget splitting, switch backs, interleaving in addition to traditional a/b testing.
	- You have extensive experience developing in Python.
	- You are familiar with databases, SQL, and scripting languages.
	- You have a practical understanding of some of the modern machine learning applications to rare events modeling, natural language processing, ranking, clustering and embedding generation.
	- You enjoy working closely with operational teams on deployment, monitoring, and management concerns.
+ skill set:
	- Senior Software Engineer, Autonomous Systems - SPG
	- Apple SPG (Special Projects Group) is seeking an experienced software engineer to work on developing and implementing high-quality software for autonomous systems. Our organization is engaged in conducting world-changing research that requires the development of novel algorithms that need to run in real-time.
	- Proficient in C++ and Python. Language-agile.
	- Experience with building production code.
	- Strong understanding of scientific and numerical programming in domains such as robotics, controls, computer graphics.
	- Familiarity with code developer workflows and tooling.
	- Strong interest in engineering problem solving.
	- Help design and build production-grade software to solve historically difficult problems in the field of autonomous systems under strong engineering constraints
	- To be experienced in designing and building production-grade software in C++
	- Have deep understanding of patterns (and anti-patterns) of architecture
	- Familiarity with scientific programming and numerical techniques
	- A minimum of a BS in Computer Science or related fields.
	- 10+ years of experience on working on production software in scientific and numerical domains.
+ skill set:
	- Perception System Engineer - SPG
	- As Perception System Engineer on a revolutionary Apple project, you will be working on an autonomous system built on state of the art sensing technologies and ground breaking machine learning algorithms. The Perception team provides sense capabilities such as detection, classification, tracking, and observed maps in complex environments using a range of sensing modalities. You will play a key role in measuring end-to-end system performance, identifying key issues and provide detailed feedback for performance improvement. You will engage cross functionally with a wider range of experts to build a robust and scalable triage and measurement system. You will use statistical modeling and develop expertise in Perception system performance trends, forecasting methodologies, and synthesize key findings for leadership reviews.
	- 5+ years of experience in testing, QA or algorithm development for Autonomous Perception systems
	- A deep understanding of perception functions its impact on motion planning
	- Knowledge of machine learning models and deep learning fundamentals
	- A background in statistical analysis, system-level triage of complex systems
	- Proficient in data analysis, scripting and automation using python
	- Familiarity with data products from optical sensors like lidar and camera is desired
	- You will be developing and maintaining a Perception performance measurement pipeline that provides continuous feedback to developers for performance improvement and debugging. The work involves significant cross functional interaction with system test engineers, model, and tooling developers.
	- Defining procedure and tooling requirements for a triage and test pipeline that identifies, classifies, and measures perception failure rates.
	- Engaging with relevant partners to ensure timely implementation and delivery. 
	- Synthesize failure rate data to derive meaningful trends and sensitivities, and track measured improvements and regressions over time. 
	- Create and own dashboards for leadership reviews and develop expertise in observed system performance. 
	- Root causing and failure analyses in partnership with deep learning model developers will be essential to be effective in this role. 
	- You will also have opportunities to develop statistical models to forecast full system performance using developer metrics, critical scenario testing, and past performance.
	- Masters degree in engineering, data science, statistics, or mathematics
	- 5+ years of relevant Industry experience in robotics or autonomous systems
+ skill set:
	- Computer Vision Architect
	- Apple is looking for a computer vision architect with exceptional technical and communication skills to contribute on high-impact projects that will enable game-changing future Apple products. The role requires deep knowledge of image processing and ability to define, analyze and optimize the architecture of video pipelines. Successful candidate must possess good understanding of both hardware and software solutions used in battery operated devices, including tradeoffs in optimal performance vs. power consumption.
	- Ability to evaluate existing and emerging imaging systems against use case requirements, including, understanding of hardware engines (CPU, GPU, Neural Network Accelerator, DSP, etc.) and how to choose the right one for a given application.
	- Understanding of difference between classes of algorithms and how to optimize their efficiency on available hardware engines (Computer Vision vs DeepLearning) as well as resource sharing and arbitration.
	- Understanding of the impact processing flow selection will have on overall system performance, power consumption and resource sharing by various algorithms.
	- Deep knowledge of sensors and camera technologies, including, image acquisition (sensor+lensing+ISP), rolling vs global, color vs monochrome, rectilinear vs equidistant and their impacts on algorithms, color management, performance optimization in low light conditions, etc.
	- Understanding of system resources supporting image collection and camera calibration in multi-camera systems and ability to define common system pre-processing blocks for camera streams.
	- Demonstrated proficiency in system architecture and performance analysis, including, input interface definition/optimization of image streams serving multiple algorithms, estimation of system compute requirements based on required performance.
	- Ability to define system-wide policies optimizing resources in multi-sensor streaming systems with memory constraints as well as estimation of system responsiveness based on latency analysis and system margins.
	- Ability to translate product goals to feature level architectures and the ability to derive for each feature the end-to-end requirements (with rationale) and decompose them down to module/component specific requirements.
	- Ability to work cross-functionally (Algorithms, Embedded, ML, Mechanical, Electrical, Optical, Human Factors, Vision science etc.) with stakeholders to define CV features, understand trades, sensitivities, risks and perform budgeting across full system.
	- Ability to effectively communicate and align with peers while navigating complex trade-spaces and the ability to summarize learnings, take-aways and recommendations in a succinct way to executive leadership.
	- Ideal candidate must have proven track record of having participated in the development of multiple consumer electronic products. Should have broad understanding of hardware and software technologies that are employed in low-power hand-held / wearable devices. Must be a self-starter who is highly-motivated and an organized thinker with excellent communication and presentation skills.
	- BE + 15 yrs of relevant industry experience.
	- Masters/PhD + 10yrs of relevant industry experience
+ skill set:
	- Robotics Software Engineer, Autonomous Systems - SPG
	- Apple SPG (Special Projects Group) is looking for talented robotics and software engineers to join our team to push the boundaries of autonomous planning algorithms (behavior, predictions, motion planning, and architecture).
	- Background in any of the following areas: behavior planning/decision-making, predictions, machine learning, motion planning (sampling, search based planning, optimization), estimation, control, and/or high-performance real-time algorithms.
	- Experience programming autonomous robots, modeling multi-agent systems, and developing algorithms for them.
	- Strong C++ and/or Python development skills.
	- Solid understanding of advanced algorithms and data structures.
	- 2+ years of professional or equivalent experience.
	- You must be hands-on, eager, curious and never satisfied with the status quo.
	- You must love learning and being challenged.
	- You will develop cutting edge robotics technology at the intersection of machine learning, AI, and classical robotics. This involves the design and implementation of algorithms that run on a robot in real-time in a safety critical application that involves autonomous interactions with the surrounding world in an uncontrolled environment. You will test and deploy your work in simulation and in the real world on state-of-the-art robotics hardware. You will contribute to the development of an ambitious and innovative projects as part of a dedicated team of world-class engineers.
	- M.Sc., or Ph.D. in computer science, engineering, or equivalent professional experience.
	- Designed one or more machine-learned approaches to solve a robotics problem.
	- Experience with cloud-based tools to automate experiments and analysis at scale.
	- Familiarity with real-time, multi-process, multi-threaded coding.
	- Comfort using the command line in Linux.
	- Experience with 3D geometric math.
	- Comfortable with collaboration tools for programming
+ skill set:
	- Data Collections Lead - SIML, ISE
	- Would you like to help shape the next set of ML features of iPhone? Would you like to contribute to the field of generative AI? Want to contribute to transforming how people interact with AI technologies? 
	- The System Intelligent and Machine Learning team is in charge of creating datasets that power many of Apple’s intelligent software. Our datasets range from very small targeted sets to Petabyte scale datasets. As part of the Data Collection team, you will be managing end-to-end data collection projects for a wide variety of features. Our data team is responsible for designing and building high quality datasets at scale. At the heart of machine learning, data defines how Apple features and products operate and what is the final user experience that will impact millions of our customers. This is an exciting time to join us: grow fast and have an impact on multiple key features on your first day at Apple!
	- Excellent project management, communication, interpersonal, analytical, and organizational skills
	- Passion for creating great products and understanding the challenges associated with building datasets for machine learning features; while addressing the challenges of inclusion, bias removal, and fairness.
	- Ability to define/design/develop data collection efforts that focus on the end-to-end user experience, including anticipating potential failure modes, edge cases, and anomalies
	- Capacity to multitask and manage several projects in parallel while meeting deadlines and providing access to clients and partners
	- Problem solving & critical thinking capacities, with an eye for innovation and continuous optimization (improving the diversity and quality of assets, reducing time to delivery and cost)
	- Ability to understand data needs and define concrete project deliverables
	- Our SIML Data team focuses on data acquisition, data science, annotation, data QA and robustness analysis. We utilize generative AI tools to help generate and evaluate data. Each year, we power dozens of features and work closely with ML teams across the entire company. Apple's commitment to deliver incredible experiences to a global and diverse set of users with a full respect of their privacy leads our team to explore innovative data collection processes.
	- In this position, you will be responsible for leading and managing data collection projects end-to-end and ensuring the quality of the data delivered to R&D. 
	- Connect with R&D teams to understand expectations and define specs of data collection efforts, with a constant focus on fairness and on potential biases
	- Co-define a data collection workflow with the partners, in accordance with Apple values
	- Coordinate the efforts of internal teams (privacy, legal, procurement, security) and own the administrative setup
	- Lead the data collection project, including working with internal and external teams
	- Define and dynamically adapt the data collection methodology to foster efficient quality analysis and annotation
	- Track quantities and quality delivered
	- A strong understanding of applied machine learning concepts is desired
	- Formal or informal experience working with generative AI tools (including personal projects) is desired
	- Experience in data operations supporting R&D work related to generative AI is a strong plus
	- Ability to maintain and develop relationships with multi-functional teams
+ skill set:
	- Generative AI Applied Researcher - SIML, ISE
	- Are you excited about Generative AI? We are looking for experts in this space to join our applied ML R&D team at Apple! You will be inventing and shipping the next generation of these core technologies with a focused team. Our purpose is to surprise and delight users and developers worldwide.
	- The team comprises domain experts in Computer Vision & Natural Language Processing (NLP) who contribute to a variety of shipping workflows you may already regularly use, including: Photos Search, Curation, Memories, Intelligent Auto-crop, Visual Captioning for Accessibility, Federated Learning on visual content, Real-time Classification & Saliency in Camera, Semantic Segmentation in Camera, and several on- device feature extractors across the system. Further, several of our projects are surfaced to third party developers through Vision & CoreML. Shipping APIs include image tagging, image similarity, saliency estimation and prints for transfer learning. The team collaborates extensively with various teams across Apple in bringing experiences to life across our devices, services and 1st/3rd party APIs. 
	- Selected references to our team’s work:
	- https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon
	- https://machinelearning.apple.com/research/on-device-scene-analysis
	- https://machinelearning.apple.com/research/panoptic-segmentation
	- 5+ years of industry experience with strong ML fundamentals
	- Hands-on experience with building Deep Learning applications
	- Proficiency in using ML toolkits, e.g., PyTorch
	- Strong analytical and problem solving skills
	- Strong programming skills in Python, C and C++
	- You're aware of the challenges associated to the transition of a prototype into a final  product
	- You're familiar with the challenges of developing algorithms that run efficiently on  resource constrained platforms
	- You've demonstrated leadership in both applied research and development
	- Excellent written and verbal communications skills, be comfortable presenting  research to large audiences, and have the ability to work hands-on in multi-functional teams
	- We are looking for a candidate with a proven track record in applied ML research. Responsibilities in the role will include training large scale multimodal (vision-language) models on distributed backends, deployment of compact neural architectures such as transformers efficiently on device, and learning adaptive policies that can be personalized to the user in a privacy preserving manner. Ensuring quality in the field, with an emphasis on fairness and model robustness would constitute an important part of the role. You will be interacting very closely with a variety of ML researchers, software engineers, hardware & design teams cross functionally. The primary responsibilities associated with this position range from algorithm design and implementation, ability to integrate research into production frameworks, and collaborating closely with product teams before and after feature launch.
	- M.S. or PhD in Electrical Engineering/Computer Science, or a related field (mathematics, physics or computer engineering), with a focus on computer vision and/or machine learning or comparable professional experience; or equivalent experience.
	- Familiarity with Multi-modal ML, Graph ML and/or Reinforcement Learning (RL) in a distributed large-scale training environment is desirable.
	- Experience in neural network deployment optimizations is desirable.
+ skill set:
	- Software Engineer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- Passionate about Spacial Computing
	- Passionate about working with hardware architectures
	- Great interest or background in Computer Vision and/or Machine Learning
	- Solid fundamentals in Linear Algebra
	- Strong proficiency in C/C++
	- Performance and optimization oriented
	- Self-motivated and great teammate
	- VPG (Vision Product Group) is the group that is responsible for many of the key algorithms for Apple Vision Pro. We are looking for versatile engineers who are passionate about building products for millions of customers around the world. You’ll be working on cutting-edge technology and develop algorithms that enable a high-quality user experience across a range of tentpole use cases and applications. As a part of our team, you will closely collaborate with HW engineers (cameras, silicon, electrical engineering, product design) and other technology development software teams (computer graphics, video engineering, data generation/annotation, drivers/OS). You can make a difference by researching and prototyping novel deformable object tracking algorithms beyond the state of the art and/or by optimizing the performance of real-time algorithms running on Apple silicon.
	- M.Sc. or B.Sc. degree in Computer Science or similar, alternatively a comparable industry career with a consistent track record of successful projects.
+ Familiarities with Knowledge Graph and Traversal Algorithms
+ skill set:
	- Machine Learning Engineer — NLP
	- Superior verbal and written communication and presentation skills, ability to convey rigorous mathematical concepts and considerations to non-experts
	- Thorough understanding of common machine/deep learning algorithms and practical experience in one or more of the following areas: prompt-based learning, reinforcement learning, BERT, GPT, T5, transformers, conversational models, large-scale NLP model training and fine-tuning
	- Working knowledge of distributed training and parallel computing on machine learning tools, such as AWS SageMaker
	- Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and Spark
	- Ability to implement data intensive pipelines and applications in a general programming language such as Python, Scala, Java or C++
	- Ability to comprehend and debug complex systems integrations spanning toolchains and teams
	- Ability to extract meaningful business insights from data and identify the stories behind the patterns
	- Creativity to engineer novel features and signals, and to push beyond current tools and approaches
	- Excellent verbal and written communication skills, in both Mandarin Chinese and English
	- Engage with others to find opportunities, understand requirements, and translate those requirements into technical solutions 
	- Design internal search and conversational system concerning business and sales topics
	- Design machine learning approach, applying tried-and-true techniques or developing custom algorithms as needed by the business problem 
	- Collaborate with data engineers and platform architects to implement robust production real-time and batch decisioning solutions 
	- Ensure operational and business metric health by monitoring production decision points
	- Investigate adversarial trends, identify behavior patterns, and respond with agile logic changes
	- Communicate results of analyses to business partners and executives 
	- Research new technologies and methods across data science, data engineering, and data visualization to improve the technical capabilities of the team
	- Ph.D. in Computer Science, Machine Learning, Statistics, Operations Research or related field; or 
	- Ph.D. in Math, Engineering, Economics, or hard science with data science fellowship; or 
	- M.S. in related field with 3+ years experience applying deep learning to real business problems
+ Deep technical knowledge in classic Computer Vision (pixel processing and geometry) or Machine Learning are a must. Bayesian filtering, Signal processing, Optics, Camera calibration and/or Mathematics expertise is desirable as well.
+ skill set:
	- Algorithm & Performance Engineer - SPG
	- The Apple Special Projects Group, working on autonomous systems, we are implementing highly complex algorithms. We are looking for a talented, dedicated and result oriented C++ Software Engineer to help improve, expand and further optimize our stack.
	- You will be part of a world class team with a highly diverse skillsets. Implementing complex numerical algorithms in a well designed, testable manner is as much part of an Algorithm & Performance Engineer’s day-to-day as optimizing cache coherency of existing implementations, applying SIMD optimization or reducing memory footprint of modules. You will be addressing a vast variety of challenges from implementing GPU kernels over deploying and optimizing machine-learned models all the way to architecting, implementing and testing a complex software stack.
	- 3+ years of professional software development experience.
	- Expert knowledge in Modern C++.
	- Experience in either ComputerVision, High Performance Computing or Numerical Algorithms.
	- Familiarity with SIMD, concurrency and/or GPU kernels.
	- Passion for optimizations and efficient implementations.
	- High software engineering standards: desire to write clean, well-tested and well-structured code.
	- Excellent communication and presentation skills.
	- Track record of collaborating across teams, gathering requirements and delivering results.
	- Efficient, correct, clean C++ implementation of complex numerical algorithms using efficient data structures.
	- Low-level optimization, for example using SIMD, concurrency, cache optimizations, GPU kernels
	- Design, implementation, testing and maintenance of a complex software stack.
	- Implement visualization tooling to enable insights into complex algorithms.
	- Collaborate with testing and verification teams to ensure correctness and reliability of our stack.
+ skill set:
	- AIML - Engineering Manager, Machine Learning Data Platform
	- The Machine Learning Data Platform group builds cloud-native systems that enable customers across all Apple to design, build and innovate with ML-driven product features rapidly and at scale. These systems provide cloud-native solutions for data exploration, data pre-processing, ETL, ML fine-tuning, and large-scale batch inference for ML models including LLMs. We are looking for an experienced leader who wants to bring their passion for infrastructure & distributed systems to build world-class data platforms/products at a very large scale across cloud environments.
	- 5+ years of experience building, influencing, and growing successful infrastructure teams that consist of senior software engineers and first-line managers.
	- Strong experience in strategizing, planning, and delivering very large-scale cloud-native distributed systems and data infrastructure.
	- Experience managing expectations and relationships with multiple collaborators and project teams across various functional areas.
	- Excellent communication and leadership skills. Ability to influence across the organization and customer leadership.
	- Demonstrated ability to partner with recruiting to grow technical teams.
	- Experience with building production Machine Learning inference pipelines is a plus.
	- Experience building GPU cost/performance observability systems and optimizing them for cost and speed is a plus
	- Our platform is built using a variety of systems and services, from bare metal to managed infrastructure services, and everything in between. We use existing and open-source systems when possible, but do not shy away from building new products ourselves. As the Engineering Leader, you will work closely with customers, and cross-functional teams and lead the planning, execution, and success of technical projects. You also take full ownership of the strategic direction of products and solutions and influence customers, executives, and tour teams. This role requires technical expertise, experience in driving strategy, influence across the leadership stack, growing and mentoring senior engineers and first-time managers, relationship management, and multi-functional coordination.
	- Work closely with customers, cross-functional teams and lead the planning, execution, and success of technical projects. 
	- Lead, mentor, and grow a team of senior software engineers and first-line managers
	- Foster a healthy and collaborative culture as well as a high-output group 
	- Strategize, plan, and execute technical and cross-functional projects and provide leadership in an innovative and fast-paced environment 
	- Measure and improve the reliability and security of data infrastructure systems
+ skill set:
	- State Estimation Engineer — World Representation / Localization
	- Apple is looking for passionate, talented, and results-oriented robotics / state-estimation engineers to join our team and work on exciting technologies for autonomous systems. In this position, you will have the opportunity to work with a cross-functional team on innovative hardware/software technologies, enabled by the systems you build.
	- Joining our team, you will get to work with a fantastic group of talented and dedicated engineers and researchers. We hope you’re excited about the values that drive us: 
		* Passion for the mission:  we’re here to make something great. We take on whatever work is right for the mission and strive for the best possible results. 
		* Humility:  the right answer is more important than being right. We search for solutions as a team and value clear-eyed feedback. 
		* Lean habits:  you can’t grow without limits. Time constraints and big goals encourage us to sharpen our focus and learn to make great decisions.
	- Experience developing and integrating state estimation algorithms for localization, calibration, mapping, or SLAM applications in robotic systems.
	- Experience developing algorithms which fuse many sensor modalities is a plus.
	- Very strong background in C++ development for Linux. Ability to understand and prototype in a python codebase is a plus.
	- Excellent communication: strong interpersonal, verbal and written skills.
	- Experience with machine learning and recent literature is a plus.
	- Develop, deploy, and scale both online and offline state-estimation algorithms for real-world robotic systems.
	- Build software, tools, and processes that accelerate your rate of experimentation, monitor performance, and inform what to try next.
	- Work with large quantities of sensor data on challenging real-world problems.
	- Collaborate with the team to deploy your work in a mission critical environment.
	- Masters or PhD in Computer Science, Robotics, Machine Learning, Engineering or equivalent professional experience.
	- The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.
+ skill set:
	- AIML - Machine Learning Engineer - Machine Learning Platform Technology (MLPT)
	- Want to build the training platform that engineers rely on to develop next-generation Apple products and services?  As a machine learning engineer on our team, you will create software systems and algorithms to enable performant, scalable training for Apple's AI-driven experiences.  Join our team of highly skilled, impact-focused engineers!  This role also includes opportunities to open source your work and publish at top ML conferences.
	- Strong Python programming skills
	- Understanding of data structures, software design principles, and algorithms
	- Experience with deep learning frameworks, such as PyTorch, JAX, or TensorFlow
	- Experience developing model parallel and data parallel training solutions and other training optimizations
	- Experience building large-scale deep learning infrastructure or platforms for distributed model training
	- Experience with parallel training libraries such as torch.distributed, DeepSpeed, etc.
	- We're looking for strong machine learning engineers to help build next-generation tools for training deep learning models at scale. You'll be part of a small team of training technology experts, focusing on training speed and scalability. We're looking for candidates with polished coding skills as well as passion for machine learning and computational science. 
	- Design and develop components for our centralized, scalable ML platform. Help push the limits of existing solutions for large-scale training. Develop novel techniques to circumvent the limitations of these solutions. Deploy your techniques on high-impact tasks from our partners across the company building new Apple products and services. 
	- We encourage publishing novel work at top ML conferences such as ***MLSys or NeurIPS*** and releasing contributions as open source. 
	- In exchange, we offer a respectful work environment, flexible responsibilities, and access to world-class experts and growth opportunities—all at one of the best companies in the world.
	- PhD or Masters in the area of Computer Science or equivalent years of industry experience
+ skill set:
	- AIML - Machine Learning Engineer, Siri and Information Intelligence
	- The Siri information intelligence team is creating groundbreaking technology for artificial intelligence, machine learning and natural language processing! The features we create are redefining how hundreds of millions of people use their computers and mobile devices to search and find what they are looking for. Siri’s universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup. As part of this group, you will be doing large scale machine learning and deep learning to improve Query Understanding and Ranking of Siri Search and developing fundamental building blocks needed for Artificial Intelligence. This involves developing sophisticated machine learning models, using word embeddings and deep learning to understand the quality of matches, online learning to react quickly to change, natural language processing to understand queries, taking advantage of petabytes of data and signals from millions of users and combining information from multiple sources to provide the user with results that best satisfies their intent and information seeking needs.
	- 5+ years of experience in machine learning, information retrieval, indexing and retrieval
	- Mastery of two of following languages: Python, Go, Java, C++
	- Excellent knowledge and good practical skills in major machine learning algorithms
	- Excellent data analytical skills
	- Experience with large scale search and machine learning systems is highly desired
	- ***Experience with Hadoop, Hive, and/or Impala is a plus***
		* https://en.wikipedia.org/wiki/Apache_Impala
	- Good interpersonal skills
	- Good team player
	- Analyzing search suggestion ranking and relevance requirements, issues and opportunities
	- Understanding product requirements, translate them into modeling tasks and engineering tasks
	- Building machine learned models for search relevance, ranking and query understanding problems
	- Integrating search functions into Apple products, such as Siri, Spotlight, Safari, Messages, Lookup, etc.
	- Building end-to-end production system including query understanding, ranking and recommendation to power search
	- Utilizing Spark, Hadoop MapReduce, Hive, Impala to perform distributed data processing
+ skill set:
	- AIML - Software Engineer, Machine Learning Platform & Technologies
	- Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there’s no telling what you could accomplish.
	- The Machine Learning Platform Team at Apple is looking for a Senior Engineer who has extensive experience in CI/CD, orchestration pipelines, build, release, and code management to manage critical parts of our development lifecycle. You will work with a dedicated team of engineers that will deliver the tooling and pipelines that enable consistent development of a high performance search stack, ensuring high quality delivery and enabling fast engineering turnaround.
	- 5+ years of experience in developing developer tooling, pipelines, automations and API
	- Thorough understanding of software development lifecycle, DevOps (build, continuous integration, deployment tools) and best practices.
	- Strong programming skills in Go, Python, or other language
	- Strong experience with Spinnaker and/or other delivery platforms
	- Strong experience with workflow platforms (Argo, Jenkins, or other)
	- Solid Kubernetes, AWS or other cloud experience
	- Strong communications and collaboration skills required
	- We design and build infrastructures to support features that empowers billions of Apple users. Our team processes trillions of links to find the best content to surface to users through search. We also analyze pages to extract critical features for indexing, ranking. We apply statistical analysis to improve link selection, freshness, retrieval rates, extraction quality, and many others. You’ll have the opportunity to work with large scale systems with trillions of rows and many petabytes of data and incredible complexity.
	- Work in a team of engineers to deliver core tooling and pipelines for builds, validation, and deployment
	- Build tooling and SCM integrations that ensures code consistency, test coverage, and reliability
	- Develop pipelines for continuous, incremental delivery to preproduction environments
	- Extend tooling that helps engineers test, maintain, and deploy solutions across multiple repos and languages
	- Collaborate with DevOps and Release Engineering to enhance release speed and reliability
+ skill set:
	- AIML - Senior Data Infrastructure Software Engineer, Machine Learning Platform and Technology
	- The Data Infrastructure group within the AI/ML organization powers the analytics, experimentation and ML feature engineering that powers the Machine Learning technologies we all love in our Apple devices. Our mission is to provide cutting edge, reliable and easy to use infrastructure for ingesting, storing, processing and interacting with data while keeping Apple’s users’ data private and secure.
	- Are you a passionate about building scalable, reliable, maintainable infrastructure and solving data problems at scale? Come join us and be part of the Data Infrastructure journey.
	- 5 years of experience in software engineering with deep knowledge in computer science fundamentals.
	- Strong in data structures and algorithms. Must write good quality code with test cases and review PR's in fast faced environment.
	- Expert in one or more functional or object-oriented programming languages (Scala, Java)
	- Fluent in at least one scripting or systems programming language (Python, Bash and Go etc.)
	- ***Experience or knowledge in distributed data systems like Hadoop, Spark, Kafka or Flink.***
	- Experience or knowledge in public cloud is a big plus, preferably AWS.
	- Strong collaboration and communication (verbal and written) skills to work with diff
	- The role involves managing petabytes of data for machine learning applications and designing and implementing new frameworks to build scalable and efficient data processing workflows and machine learning pipelines. The successful candidate will be responsible for ensuring complete data lineage and legal workflow integration while optimizing performance and scalability. You will also be responsible for monitoring the performance of the system, optimizing it for cost and efficiency, and solving any issues that arise. This is an exciting opportunity to work on cutting-edge technology and collaborate with cross-functional teams to deliver high-quality software solutions. The ideal candidate should have a strong background in software development, experience with public cloud platforms, and familiarity with distributed databases.
	- ***Familiarity with distributed databases, such as DynamoDB, MongoDB, or Cassandra.***
	- ***Experience with containerization and orchestration technologies, such as Docker and Kubernetes.***
+ skill set:
	- AIML - Sr Product Manager, Siri Knowledge
	- Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Do you love taking on challenges that create a positive impact? Here, you could play a leading part in revolutionizing how people use their computers and mobile devices by empowering many ground-breaking intelligent experiences.
	- The Siri team is crafting innovative technology for virtual assistants, knowledge graph, and algorithmic search using machine learning, natural language processing, and artificial intelligence. We’re looking for someone to lead product management efforts for Search and Knowledge in Siri, Spotlight, and Safari. You would work with top tier engineering and product teams to serve over 1 billion customers, crafting the type of magnificent user experiences that Apple customers expect and love. As a PM for Siri Knowledge and Search, you would drive search relevance, machine learning, and user experience improvement projects to build and improve end-to-end knowledge products on all major Apple platforms including iPhone, CarPlay, macOS, AppleTV and HomePod.
	- 5+ years experience as a Product Manager, Software Engineer, Engineering Manager, or similar role leading cross-functional product or software engineering teams.
	- 2+ years experience bringing a product from concept to completion, ideally related to joining together sophisticated backend and front-end systems with the user experience in mind
	- Experience driving large cross-functional projects with multiple workstreams
	- Self-motivated and proactive, with demonstrated creative and critical thinking capabilities
	- Self-sufficient in analyzing and drawing conclusions about the quality and product opportunity from raw and refined product data
	- Ability to, directly and indirectly, lead large teams for success
	- Experience shipping machine-learned products
	- Lead roadmap planning and detail execution of Siri Knowledge and Search in Siri, Spotlight, and Safari
	- Lead cross-functional teams from the product ideation/creation to full-stack delivery to our customers
	- Keep teams focused on the right priorities to meet aggressive deadlines; clearly communicate project progress to leads and executive team.
	- Collaborate with engineering, product, and executive leadership to drive outstanding year over year product delivery across large product areas at Apple.
	- Analyze product metrics to measure performance and plan the evolution of search and knowledge for immediate impact and longer-term (5+ years)
	- Be the voice for our customers; do whatever it takes to deliver the highest quality experience to our customers
	- Lead machine learning best practices in Siri team
	- Collaborate with other Apple teams to promote cross-team technology sharing
+ skill set:
	- Machine Learning Engineer
	- Machine learning is a critical pillar of Jane Street's global business, and our ever-changing trading environment serves as a unique, rapid-feedback platform for ML experimentation. 
	- Our ML Engineers develop the infrastructure that makes all of that possible. The work is wide-ranging, including things like:
		* Developing libraries for automating ML workflows and experiment evaluation, and helping us evaluate and onboard existing open-source tools
		* Digging in to the internals of open-source ML tools to extend their capabilities and fix fundamental bugs
		* Optimizing our systems to match the needs of our trading systems, whether that be for efficient training or low-latency inference
		* Building tools to train and evaluate models in parallel over large datasets
	- And much more besides. As an ML Engineer, you’ll help drive the direction of an ML platform that is used daily by traders and researchers across every corner of the firm.
	- We’re looking for accomplished and disciplined software engineers with deep experience in machine learning techniques and the software systems that power them. A good candidate will be curious, with a wide-ranging understanding of the open-source ML ecosystem, and an interest in the latest advances in ML, in both academic and industrial contexts.
+ skill set:
	- Senior Video Processing Algorithm Engineer
	- As a Senior Video Processing Algorithm Engineer, you will explore opportunities to research and develop video process algorithms in order to both improve real-time video quality & performance, and add new features on Zoom video products. 
	- Work across our stack, developing software ranging from Web Server to business application layer for our distributed, cloud-hosted backend. You will work alongside fellow experts in the field, you will deliver happiness to our users, and grow your knowledge base each and every day.  
	- Conduct performance research evaluations on image/video processing algorithms 
	- Perform feasibility analysis and validation, develop corresponding demos, and cooperate with team members for feature deployment on various platforms
	- Develop and prototype innovative algorithms in Zoom’s video processing pipeline
	- Design new video features to tackle new and existing problems on Windows, macOS, IOS, Android and Linux systems
	- Collaborate with internal stakeholders across the business to drive the delivery of features, processes and happiness
	- PhD degree or Master degree with 4 years of working experiences in Computer Science, Electrical Engineering, or a related STEM field
	- Excellent C++/C and Python programming skills 
	- Strong experiences with libraries for deep learning, such as TensorFlow, PyTorch, Keras, Caffe, etc.
	- Solid knowledge of math, including linear algebra, numerical optimization, calculus, etc.
	- Hands-on experiences with video processing techniques (traditional method and deep learning method), such as image/video synthesis and generation, image enhancement, etc.
	- Hands-on experiences in ***deep learning (neural network, neural rendering, generative model, discriminative model, transfer learning, one-shot or few shot learning, Neural Architecture Search, etc.)***
	- Experience on deep learning structure (Transformer, CNN, RNN, etc.) optimization and acceleration
	- Ability to crystallize vague concepts into concise plans with clear documentation
	- Detail oriented, organized, ethical, responsible, and self-motivated
	- Strong communication skills and a desire to learn something new 
	- Basic understanding of Mandarin
+ skill set:
	- Video Processing Algorithm Engineer
	- Conduct research on image/video processing algorithms. Conduct performance evaluations on image/video processing algorithms. Leverage signal processing, machine learning and deep learning techniques to solve computer vision problems. Possess strong skills in the areas of development and real-time implementation of video processing system. Analyze factors that impact algorithm runtime on various platforms and come up with solutions for real-time implementation without sacrificing algorithm performance. Analyses at both algorithm level and coding level (x86/x64/Arm neon assembly optimization, data structure optimization, multiple thread, GPU acceleration, etc.) are conducted to achieve the goal. Develop and prototype innovative algorithms in Zoom’s video processing pipeline. Develop a deep understanding of Zoom’s video processing architecture and develop new features on top. Design new video features to tackle new and existing problems on Windows, macOS, IOS, Android and Linux systems. Perform feasibility analysis and validation, develop corresponding demos and cooperate with team members for feature deployment on various platforms. 
	- Master’s degree in Electrical/Computer Engineering, Computer Science, a related field, or foreign equivalent and 1 year of post-baccalaureate experience in job offered or related. Applicants must have 1 year of experience with the following (1) C++/C and Python programming skills to develop computer vision/video processing features; (2) deep learning frameworks including TensorFlow, PyTorch, Keras, and Caffe; (3) video/image processing including semantic segmentation, object detection, object tracking, and image enhancement (traditional method or deep learning method); (4) Convolutional Neural Networks or (CNN) structure optimization and acceleration; and (5) project development skills including build and release, quality assessment, third party SDKs integration. Telecommuting Permitted.
+ skill set:
	- Machine Learning Engineer
	- You will be part of a team whose focus is to solve cutting edge AI problems and deploying models that constantly advance the state-of-the-art. You will be working across various Natural Language Processing (NLP) areas like summarization, topic segmentation, language modeling, coreference resolution and other interesting challenges that are challenging at Zoom's scale.
	- A person in this role is expected to able to carry out independent research without much supervision, collaborate with other researchers on larger scale projects, and provide directions to junior engineers on their research/engineering tasks.
	- Build and scale Machine Learning (ML) services which enable Zoom’s products.
	- Research, build and deploy state-of-the-art Machine Learning models for Natural Language.  
	- Processing use cases – mostly in the conversational domain.
	- Take ML models from research all the way to production.
	- Integrate and support the ML services with product and operations teams.
	- PhD degree in Computer Science, Machine Learning, or related degree, or Masters with 3+ years of relevant experience
	- Strong expertise in NLP
	- Experience with one or more of the following: text summarization, natural language understanding, natural language generation, conversational AI, and multimodal AI modeling
	- Experience with deep learning modeling
	- Experience in machine learning toolkits (TensorFlow, PyTorch, Scikit, etc.)
	- Experience with large-scale data processing.
	- Strong coding skills in Python
	- Experience in distributed training and performance optimization on GPU’s
	- Experience working on multi-modal ML
	- Familiarity with large-scale data processing and distributed systems.
	- Microservice, Docker, Kubernetes, REST API, AWS
+ skill set:
	- Research Scientist / Research Engineer
	- MosaicML is a deep learning startup with a mission to make ML training more efficient for everyone through fundamental innovations in algorithms, systems, and platforms. We believe that large scale training should be available beyond the well-resourced companies, and bridging the gap between research and industry is core to our success. Our products will enable our customers to train the best neural networks efficiently as possible within given time, cost, or other resource constraints – and to do so with a great user experience.
	- Develop methods for efficient neural network training. You will survey ideas in the literature and develop ideas of your own to change the way neural networks are trained to improve efficiency. This involves exacting scientific inquiry, rigorous empirical analysis of large-scale experiments, and building high-quality research artifacts.
	- Systematize knowledge and adjudicate scientific truth. At MosaicML, we are focused on transforming scientific knowledge into practical efficiency improvements. To do so, we navigate the messy machine learning literature, determine what holds up under scrutiny, and use that knowledge to build better models.
	- Advance the frontier of deep learning. You will drive ambitious research projects that push the limits of existing technology and explore new approaches that go beyond the state of the art.
	- Love our customers. Our goal is to make our customers successful when they train large deep learning models. We seek to encode our scientific expertise in our tools for the benefit of our customers. We love our customers, and we expect you to love them too!
	- Experience training large models. We're looking to hire researcher scientists and research engineers who have experience training modern neural networks for computer vision, natural language processing, and multimodal settings. Ideally, you will have experience training at large scales (100M+ parameters, and ideally 1B+ parameters) and conducting multi-node training. 
	- Extensive experience with NLP for deep learning. We are looking to hire research scientists who specialize in natural language processing.
	- Experience with data preparation and data quality for deep learning. We are looking to hire research scientists who have experience cleaning and curating large-scale data corpora (1B-100B examples) and using those corpora for deep learning.
	- Experience with deep generative model evaluation and improvement. We are looking to hire research scientists with experience evaluating generative models and using those insights to improve the models. 
	- A PhD is NOT required for this role. We are open to hiring candidates with bachelor's and master's degrees and to new graduates. We are open to hiring candidates who are currently in "research engineer" roles at other companies.
	- Keeping up to date with the research literature and thinking beyond the current state of the art.
	- Developing and implementing methods that improve the efficiency and efficacy of deep learning.
	- Rigorously evaluating these methods and communicating the results of your findings.
	- Proficiency with the fundamentals of deep learning.
	- Proficient software engineering skills and proficiency with PyTorch.
	- Knowledge of the systems aspects of how neural networks train and the resources used in the process of doing so.
	- Research experience in deep learning.
	- US work authorization required
	- Salary Range: $145K - $295K
	- Also includes equity (stock options) and benefits
+ skill set:
	- Principal Software Engineer
	- MosaicML is hiring a Principal Software Engineer to play a key role in building our product and in developing our team of software engineers.
	- Serve as a technical lead, overseeing and supervising projects and engineers on the team.
	- Play a leading hands-on role in the design and implementation of ML infrastructure and cloud platform software technologies
	- Drive our technology vision and roadmap
	- Establish software development best practices, and lead by example in applying them
	- Develop our engineering organization and culture through hiring, mentoring, and feedback.
	- ***8+ years of hands-on programming experience with at least one modern language such as Python, Go, or C++***
	- 8+ years of experience contributing to the architecture and design of large scale distributed systems and/or ML systems and tools
	- Strong sense of software design and usability of ML systems
	- Experience applying software engineering methodologies and best practices including coding standards, code reviews, build processes, testing, and security.
	- Prior experience in developing public cloud services or open source ML software is an advantage.
	- We value candidates who are curious about all parts of the company's success and are willing to learn new skills and technologies along the way.
	- Salary Range: $200K - $300K
+ skill set:
	- (Senior) Julia machine learning specialist
	- You love applying mathematical models to real data? You are expert about how to automatically learn from data?
	- What we offer
		* you will be part of an inspiring team of Julia professionals
		* every Friday is time for knowledge sharing, SDGs or generally trying out new product ideas
		* an open, reflective, and caring environment where you can feel like among friends
		* fair salary
	- What you will do
		* understand the kind of machine learning the customer needs
		* communicate with the customer
		* implement the models
		* connect the machine learning part with given data sources and sinks
		* build dashboard solutions
		* present results to the customer
		* give workshops and trainings about Julia, data science and machine learning
		* use your Fridays to empower SDGs (Sustainable Development Goals) with Julia
	- What you bring
		* completed Bachelor or Master or PhD in data science, statistics, computer science, mathematics, physics or a comparable education
		* programming experience in Julia
		* knowledge about Databases and SQL
		* expertise in two fields of machine learning
		* generic strategies for problem solving
		* good soft skills
		* enthusiasm about Julia
+ skill set:
	- (Senior) Julia machine learning engineer
	- You love to see machine learning models in production? You care more about the infrastructure than about the actual model itself?
	- What you will do
		* understand the production demands of the customer
		* communicate with the customer
		* setup appropriate architectures (e.g. for DevOps & MLOps)
		* setup monitoring and alerting
		* automatize training and evaluation of machine learning models
		* present results to the customer
		* give workshops and trainings about Julia and MLOps
		* use your Fridays to empower SDGs (Sustainable Development Goals) with Julia
	- What you bring
		* completed Bachelor or Master or PhD in data science, statistics, computer science, mathematics, physics or a comparable education
		* experience with DevOps & MLOps
		* expertise with docker and kubernetes
		* cloud knowledge, including infrastructure-as-code
		* generic strategies for problem solving
		* good soft skills
		* enthusiasm about Julia
+ We have built and contributed to foundational AI technologies like TensorFlow, PyTorch, TF Lite, XLA, TPUs, ONNXRuntime, Android, Apple Neural Engine, MLIR, LLVM, Clang, Swift and more. We are world-leading experts in compilers, runtimes, distributed computation, and hardware, and have deployed production workloads to billions of users and devices.
+ skill set:
	- Mojo Kernel Engineer
	- We believe that AI is a net positive force in the world. Our vision and mission are to help rebuild AI infrastructure to advance humanity and our environment. We will do whatever it takes to empower our customers, team, and company to benefit from that pursuit. You can read about our culture and careers here to understand how we work and what we value.
	- We are owners and advocates for the underlying technologies, developer platforms, product components, and infrastructure. These essential building blocks form the high-quality and coherent experiences our users expect. We aim to drive the pace of innovation for every AI/ML developer.
	- ML developers today face significant friction in taking trained models into deployment. They work in a highly fragmented space, with incomplete and patchwork solutions that require significant performance tuning and non-generalizable/ model-specific enhancements. At Modular, we are building the next generation AI platform that will radically improve the way developers build and deploy AI models.
	- A core part of this offering is providing a platform that allows developers reuse deployment specific tuning and enhancements across model families and frameworks. As an AI Kernel Engineer you will own developing and tuning performance libraries for AI models. You will develop kernels and algorithms to increase performance of kernels, reduce the activation volumes, speedup data pre- and post-processing, and in general increase the end-to-end performance of the model.
	- Design and optimize high-performance ML numeric and data manipulation kernels/operators.
	- Utilize low-level C/C++/Assembly programming to achieve state of the art performance. Your work will also entail potentially introducing new novel compiler and tools support.
	- Work with compiler, framework, runtime and performance teams to deliver end-to-end performance that fully utilizes today’s complex server and mobile systems.
	- Collaborate with architects and hardware engineers to co-design future accelerators, including ISA for new hardware features and evolving ISA.
	- Collaborate with machine learning researchers to guide system development for future ML trends.
	- 6+ years of relevant work experience.
	- In-depth knowledge of C++ and low-level (micro)architectural performance is required.
	- 2+ years of experience working on complex code and systems.
	- Experience with HPC programming and accelerator languages such as CUDA, OpenCL, SYCL, etc.
	- Experience with performance modeling and performance data analysis.
	- Understanding of Parallelization techniques for ML / HPC Acceleration.
	- Deep interest in machine learning technologies and use cases.
	- Creativity and curiosity for solving complex problems, a team-oriented attitude that enables you to work well with others, and alignment with our culture.
+ skill set:
	- Machine Learning Engineer, Beijing, China
	- Research and develop key technologies of AI compilers and automatic optimization tools
	- Research and develop software and hardware co-design methods and related tools
	- For the IPU chip architecture, develop new AI compilation optimization methods
	- Participate in the work of the AI compiler open source community
	- Strong background in computer architecture, in-depth understanding of at least one AI chip architecture
	- Experience with assembly-level performance tuning on AI chips (including GPUs or other AI chips)
	- Solid C++ system development ability
	- Understand the basic principles of deep learning training/inference framework
	- Experience with AI processor adapting mainstream deep learning framework is preferred
	- Bachelor degree or above in computer, electronics, communication or mathematics
	- Excellent teamwork ability, self-motivation and results-driven
+ skill set:
	- Lead ML & AI Engineer
	- Develop a Tech driven innovation strategy for TIL based on the principles of Design Thinking.
	- Identify and Define relevant channels of engagement across HERE for TIL to expand the impact footprint of the business vertical
	- Define IP monetization strategy that will enable HERE to earn revenue /enter new market
	- Define startup/university engagement framework for successful collaboration with these bodies, in line with HERE’s business strategy.
	- Identify the domains in technology where TIL and HERE should engage and defining a strategy for realization of same through development of disruptive prototypes in line with HERE business strategy
	- Enabling invention of new frameworks /solutions in the domain of AIML, Blockchain, Location intelligence, distributed computing,quantum computing, tinyML, etc
	- Improving the innovation index of HERE, by getting research papers published in ***NeurIPS, IEEE***, and awarded recognition in international forums like CES, Stevie ,etc
	- Enabling y-O-y increase in number of patents being filed by the team
	- Work with the Manager to Identify and Define lead KPIs related to effective assessment of performance at individual level and impact assessment of innovation implementation at business unit level
	- Support team to design experiments and stay up to date with industry inventions in AIML(esp Computer Vision), blockchain, quantum computing, Location Intelligence , distributed computing LiDAR and Drone processing, 3D modelling , AR/VR
	- Identify relevant research papers in above mentioned tech domains and enable team to design solutions for the same
	- MS/ MSc/ PhD in the field of Computer Science, Machine Learning, Mathematics, Location Intelligence , Signal Processing or related quantitative field
	- Understanding of statistical analysis of data and mathematical modeling
	- Knowledge of Clustering, Regression, Decision Trees, Forecasting, RandomForest, XGBoost, SVM, Deep Neural Networks (CNN, GRU, LSTM, Activation and Pooling layers), GAN, Encoders, Transformers, BERT, Graph Neural Network , diffusion models
	- Experience in Object Detection, Localization, Segmentation using Computer Vision and Deep Learning
	- Ability to handle large datasets and images of high resolution (4K) using Scala, Hadoop, Pyspark
	- Candidates with MSc/MS in Mathematics/Computer Science, Machine learning and AI or related quantitative needs 10yrs of experience. Candidates with PhD in related fields may have 5 yrs of experience
	- The candidate should possess a certified degree in the field of quantum physics and other related fields.
	- Should have adequate knowledge about machine learning and artificial intelligence.
	- Prior experience through internships and volunteering with professionals will also be needed to apply for the quantum engineer profile.
	- Should possess in-depth knowledge about the emerging and simulating fields and approach challenges with unique perspectives.
	- The candidates should also possess a basic knowledge of the different programming language
	- 8 or more years of experience in research and/or corporates with multi tech domain exposure
	- Demonstrated experience in setting up new business verticals and leading them to success
	- Demonstrated experience in successful team and performance management
	- Demonstrated experience in designing business strategy and successful execution of the same
	- Have prior experience in designing innovation strategy with organization wide impact
	- Have minimum 10 patents/ international publications in high impact factor journals like ***NeurIPS, IEEE***, etc
	- Shows a combined experience in both research and business field
	- Startup experience is advantageous and highly desirable
+ skill set:
	- A data scientist experienced with using real-world data to analyze and develop algorithms that are incorporated into commercially profitable product lines. Someone who can leverage clustering algorithms to model non-Gaussian statistics;  who is able to study the physical characteristics of a problem and design mathematical models that solve problems from first principles;  has a desire to learn, develop, and keep up to pace with the state-of-the-art machine learning based algorithms; has the ability to be a subject matter expert on HERE’s technologies and product lines and leverage that knowledge to identify derivative algorithms to create new growth opportunities.
	- Knowledge of data mining and analytic methods.
	- Perform your day-to-day work by programming in Python and work with associated statistical and analytics packages in python.
	- Leverage AWS technologies and use them to develop modelling pipelines.
	- Proficiency with statistical analysis packages.
+ Experience tinkering with or productizing LLMs, CV, vector databases, and the other latest AI technologies
+ skill set:
	- Software Engineer, Generative AI
	- Software is eating the world, but AI is eating software. We live in unprecedented times – AI has the potential to exponentially augment human intelligence. Every person will have a personal tutor, coach, assistant, personal shopper, travel guide, and therapist throughout life. As the world adjusts to this new reality, leading platform companies are scrambling to build LLMs at billion scale, while large enterprises figure out how to add it to their products. To make them safe, aligned and actually useful, these models need human eval and reinforcement learning through human feedback (RLHF) during pre-training, fine-tuning, and production evaluations. This is the main innovation that’s enabled ChatGPT to get such a large headstart among competition.
	- At Scale, our Generative AI Data Engine powers the most advanced LLMs and generative models in the world through world-class RLHF, human data generation, model evaluation, safety, and alignment. The data we are producing is some of the most important work for how humanity will interact with AI.
	- We’re looking for entrepreneurial Software Engineers to join our team. In this role, you'll be given the opportunity to build any of these products to meaningfully drive millions of dollars in revenue. You’ll also get widespread exposure to the forefront of the AI race as Scale sees it in enterprises, startups, governments, and large tech companies.
	- The ideal person is a natural entrepreneurial engineer who can take an ambiguous scope and lead the execution of outcomes, doing what it takes to hit them incl coding, talking to customers, defining requirements, etc. We strongly believe the best engineers own outcomes and deeply understand customer problems. This tweet by Greg Brockman summarizes it well: https://twitter.com/gdb/status/1514291063233474560
	- You’re excited about solving customer problems, and you pick the technologies and tactics that balance speed, function, and long-term robustness.
	- Own large new areas within our product
	- Work across backend, frontend, and interacting with LLMs and/or other ML models
	- Deliver experiments at a high velocity and level of quality to engage our customers
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- Collaborating with cross-functional teams to define, design, and ship new product features and experiences.
	- 5+ years of full-time engineering experience, post-graduation
	- Proficiencies in one or more of Python, Node, React, Next.js and MongoDB
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Experience scaling products at hyper-growth startups
	- Excitement to work with AI technologies
	- Strong written and verbal communication skills
	- Strong problem-solving skills, and be able to work independently or as part of a team.
	- Strong knowledge of software engineering best practices.
	- Have experience with AI platforms and technologies, including generative models and LLMs.
	- Experience building ML infrastructure and AI-powered solutions.
+ skill set:
	- Senior Software Engineer, AV-CV
	- At Scale AI, we are building tools to across the AI development lifecycle. Data is the new code, and Scale AI helps companies get the data they need, whether it’s for self-driving vehicles, artificial general intelligence, or robotics. The AV-CV Team (Autonomous Vehicles / Computer Vision) builds the infrastructure for labeling Lidar, Mapping, and Camera data. If seeing autonomous vehicles excited you, you’ll be seeing a lot of that on the AV-CV team. The team builds the tools for labeling Lidar pointclouds, annotating image and video feeds, and linking them together to build perception models.
	- Own large new areas within our product
	- Become an expert in working closely with customers across many CV industries
	- Build technologies ranging from frontend and backend to automated ML systems
	- Work deeply with sales and marketing to run demos and increase customer engagement
	- Work across the entire product lifecycle from conceptualization through production
	- Be able, and willing, to multi-task and learn new technologies quickly
	- 5+ years of full-time engineering experience
	- 2+ years working with TypeScript/JavaScript, HTML, CSS, and related web technologies (React, Next.js, Webpack)
	- Experience working with distributed systems and cloud environments
	- Experience working with a production database (Postgres, MongoDB, MySQL, MS SQL) and schema migrations
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Excitement to work with AI technologies
	- Strong written and verbal communication skills
	- Strong problem-solving skills, and be able to work independently or as part of a team.
+ skill set:
	- Machine Learning Research Engineer - Public Sector
	- The goal of a Machine Learning Engineer at Scale is to bring techniques in the fields of computer vision, deep learning and deep reinforcement learning, or natural language processing into a production environment to improve Scale's products and customer experience. Our research engineers take advantage of our unique access to massive datasets to deliver improvements to our customers.
	- We are building a large hybrid human-machine system in service of ML pipelines for Federal Government customers. We currently complete millions of tasks a month, and will grow to complete billions of tasks monthly.
	- Take state of the art models developed internally and from the community, use them in production to solve problems for our customers and taskers.
	- Take models currently in production, identify areas for improvement, improve them using retraining and hyperparameter searches, then deploy without regressing on core model characteristics
	- Work with product and research teams to identify opportunities for improvement in our current product line and for enabling upcoming product lines
	- Work with massive datasets to develop both generic models as well as fine tune models for specific products
	- Build the scalable ML platform to automate our ML service
	- Be a representative for how to apply machine learning and related techniques throughout the engineering and product organization
	- Be able, and willing, to multi-task and learn new technologies quickly
	- US citizenship and US Government Security Clearance is a requirement (TS/SCI preferred)
	- Extensive experience using computer vision, deep learning and deep reinforcement Learning, or natural language processing in a production environment
	- Solid background in algorithms, data structures, and object-oriented programming
	- Strong programing skills in Python or Javascript, experience in Tensorflow or PyTorch
	- Graduate degree in Computer Science, Machine Learning or Artificial Intelligence specialization
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Experience with generative AI models
+ skill set:
	- Machine Learning Research Engineer, Generative AI
	- Performant model code, high quality data, and robust evaluation methods form the foundation of an AI system. Scale’s leading end-to-end solutions for the ML lifecycle based on real-world data will continue to set the bar for the data-centric AI movement. Scale’s Generative AI team focuses on building models to accelerate AI adoption for some of the largest companies in the world. 
	- Your focus will be on developing Models as a Service using a variety of Machine Learning techniques. You will be involved end-to-end from coordinating with operations to create high quality datasets to productionizing models for our customers. If you are excited about shaping the future of the data-centric AI movement, we would love to hear from you!
	- Apply state of the art models, developed both internally and from the community, in production to solve problems for our customers and data labelers. 
	- Work with product and research teams to identify opportunities for ongoing and upcoming services.
	- Explore approaches that integrate human feedback and assisted evaluation into existing product lines. 
	- Work closely with customers - some of the most sophisticated ML organizations in the world - to quickly prototype and build new deep learning models targeted at multi-modal content understanding problems.
	- At least 3 to 5 years of model training, deployment and maintenance experience in a production environment
	- Strong skills in NLP, LLM and deep learning 
	- Solid background in algorithms, data structures, and object-oriented programming
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Experience in dealing with large scale AI problems, ideally in the generative-AI field
	- Demonstrated expertise in large vision-language models for diverse real-world applications, e.g. classification, detection, question-answering, etc. 
	- Published research in areas of machine learning at major conferences (***NeurIPS, ICML, EMNLP, CVPR***, etc.) and/or journals
	- Strong high-level programming skills (e.g., Python), frameworks and tools such as DeepSpeed, Pytorch lightning, kuberflow, TensorFlow, etc. 
	- Strong written and verbal communication skills to operate in a cross functional team environment
+ skill set:
	- Stability AI is a community and mission driven, open-source artificial intelligence company that cares deeply about real-world implications and applications. Our most considerable advances grow from our diversity in working across multiple teams and disciplines. We are unafraid to go against established norms and explore creativity. We are motivated to generate breakthrough ideas and convert them into tangible solutions. Our vibrant communities consist of experts, leaders and partners across the globe who are developing cutting-edge open AI models for Image, Language, Audio, Video, 3D and Biology.
	- We are looking for a versatile Software Engineer who will do anything it takes to design and implement our projects for Japanese partners, clients, and the community. We will focus on projects related to image/video models, large language models, and chatbots.
	- You will adapt quickly as we try various approaches to various industries in a fast-changing environment. You will have access to state-of-the-art high-performance computing resources, and you will be able to work alongside top researchers and engineers to truly make an impact in the fast-growing world of generative AI. 
	- Lead efforts to drive the design, development, and productionization of ML systems, and present the solutions to partners and clients in Japan
	- Work on the commercial side - productionizing generative models and building the infrastructure to serve them at scale; collaborate with ML Engineers to deploy customized models for commercial applications
	- Be a strategic thought partner both internally and externally on driving business impact through machine learning
	- Build pipelines to ingest and process data (e.g. images and text) for feeding into ML models
	- Provide technical advice to partners/clients on the integration of generative models into their products
	- Good communication skills with fluency in Japanese and business-level English proficiency
	- 5+ years of software development experience with high proficiency in 2 or more languages (Python required, Go is a plus) across a variety of projects
	- Experience with MLOps
	- Experience with data engineering (data pipelines for ML projects)
	- Experience with Linux and command line tools
	- Experience with cloud computing and APIs
	- Experience with web scraping/crawling is a plus
	- Experience with Tensorflow/PyTorch is a plus
	- Experience with Kubernetes and containers is a plus
+ skill set:
	- Research Engineer, Music
	- We are looking for ML Research Engineers who are passionate about generative models for music creation. In particular, we are looking for people who can explore new ideas and architectures for music generation models; highly creative people who straddle research and engineering and who are motivated to push the boundaries of generative music research, not just in state-of-the-art performance, but also in balancing performance and resource usage. You will have access to state-of-the-art, high-performance computing resources, and you will work alongside top researchers and engineers to truly make an impact in the fast growing world of generative AI.
	- Work with the rest of the research team and the open-source community on developing the next generation of generative audio models
	- Prototype and productionize model architecture improvements and new features
	- Maintain and innovate on open-source code repositories for generative AI audio models, including custom model code, training code, and fine-tuning code
	- Work with Product, Engineering and Commercial teams on model deployment and customized training
	- Create interactive demos and interfaces for generative models, demonstrating simple use cases in an intuitive and fun way
	- Optimize model architectures and inference code for performance on consumer devices
	- Publish results at top conferences, in journals, and in blog posts
	- Keep up to date with the latest research advancements in the field and work them into open-source repos, reimplementing as needed to ensure an open license
	- 3+ years working on machine learning projects, including training, fine tuning and refining models
	- Publication of papers, projects, and blog posts that had a high impact in generative AI
	- Experience maintaining high-quality, well-documented open-source code repositories for AI models
	- Experience with music generation models, preferably working in the time domain (Jukebox, SampleRNN, RAVE, etc.)
	- Ability to iterate quickly on public code-bases with attention to backwards compatibility, usability, and readability
	- Experience with Python scientific stack, PyTorch, and creating Jupyter/Colab notebooks
	- Ability to communicate machine learning concepts and results effectively through writing and visualization
	- Experience training and/or deploying ML models with Amazon AWS (Sagemaker a plus) or Google Cloud
	- Experience with data engineering, including cleaning and maintaining large heterogeneous datasets
	- Experience building interactive web demos that serve generative ML models
	- Experience with the open-source ML ecosystem (HuggingFace, W&B, etc.)
	- Experience with Linux and command line tools
	- Familiarity with digital signal processing and audio engineering concepts
	- Experience with Python audio processing libraries such as librosa, torchaudio, or similar
+ skill set:
	- Machine Learning Engineer, Audio
	- We are looking for Machine Learning Engineers to work on our audio team who are passionate about generative models and creative applications of AI. In particular, we are looking for people who have experience of developing model serving pipelines to operate at scale and have knowledge of state of the art techniques for optimisation and feature development. We want highly creative ML engineers who are motivated to push the boundaries of generative audio models. You will have access to state-of-the-art high performance computing resources and you will be able to work alongside top researchers and engineers to truly make an impact in the fast growing world of generative AI.
	- Lead efforts to drive the design, development and production of customer-facing ML music, speech and audio generation systems, with specific reference to inference and API environments
	- Work with the Audio, Platform and Inference teams on building pipelines for the next generation of models, where you may assist with areas such as optimization, model tuning and deployment, HPC clusters, and tooling
	- Be a strategic thought partner for leaders across the organization on driving business impact through machine learning
	- Work on the commercial side - productionizing generative models, and building the infrastructure to serve them at scale
	- Produce events and metrics in our data warehouse so that we can analyze critical business metrics like cost, performance, reliability, etc.
	- Be part of the team that brings new Stability audio models and pipelines into existence for API customers
	- Prototype and productionize inference platform improvements and new features 
	- 5+ years working on machine learning projects, including inference and pipeline development
	- Solid knowledge of Python scientific stack, PyTorch and at least one high-performance inference framework (e.g. ***TensorRT***)
	- Experience profiling and optimizing deep neural networks, including knowledge of GPU profiling tools such as NVIDIA Nsight
	- Experience with Python audio processing libraries such as librosa, torchaudio, or similar
	- Experience with cloud orchestration systems such as Kubernetes and cloud providers such as AWS, GCP, and Azure
	- Ability to rapidly prototype solutions and iterate on them with tight product deadlines
	- Experience with training and/or deploying ML models with Amazon AWS (Sagemaker a plus) or Google Cloud
	- Strong communication, collaboration, and documentation skills
	- Experience with Linux and command line tools
	- Evidence of interest in music / audio projects is valued
+ skill set:
	- Machine Learning Engineer, 3D Research
	- We are looking for experts in Machine Learning and 3D Graphics, who are passionate about generative models and creative applications of AI. In particular, we are looking for people who share our mission of open-sourcing machine learning models; people who do not believe AI models should be controlled by a centralized gatekeeper behind a closed wall, but rather be truly open and in control by all. You will have access to state-of-the-art high performance computing resources and you will work alongside other creative, hard-working top researchers and engineers, to make a lasting, positive impact in the fast-growing world of generative AI.
	- Contribute to the creation and improvement of applications and use cases such as text-to-3D, 2D-to-3D, text-to-4D, simple and fast 3D editing… and make systematic progress towards enabling anyone to easily and quickly create complex, animated and interactive 3D characters and environments
	- Take ownership of ML and 3D graphics new and existing features, across the whole product lifecycle, from developing prototypes, to assisting the teams charged with production and maintenance - with minimal supervision
	- Collaborate within Stability and in the open-source community on developing the next generation of 3D-aware neural models, where you may assist with areas such as optimization of model training, model tuning, dataset engineering, automation of processes, tooling, data translation, integration in game engines, etc.
	- Always look for opportunities to delight our customers and improve their life through AI
	- ***If you want: publish new ideas on arxiv and in major conferences***
	- 3+ years working on state-of-the-art ML or 3D graphics projects (ideally both), including training, fine-tuning ML models, neural graphics, raytracing, etc.
	- Experience researching and implementing SOTA algorithms related to ML or 3D, e.g Neural Radiance Fields, real-time 3D graphics, diffusion models, 3D file format conversion, etc.
	- Experience with Python scientific stack, PyTorch or similar ML frameworks
	- Communicate ML insights and results effectively: verbally, in writing and through visualization
	- Self-motivated, well-organized (e.g. keep track of your own priorities and refine the plan according to new insights), self-reliant (able to debug complex issues with minimal supervision)
	- ML and 3D open-source projects or influential publications
	- Performance optimization, such as profiling shaders and/or ML systems, C++/CUDA, algorithmic improvements, ML quantization and distillation
	- 3D graphics and game development, e.g. OpenGL, DirectX, Vulkan, Unreal Engine, Unity, Godot
	- JAX, TPUs, compiler development
+ skill set:
	- Machine Learning Engineer
	- We are looking for a versatile ML Engineer who will train and deploy generative models for Japanese partners, clients, and the broader community.  You will adapt quickly as we try various approaches to various industries in a fast-changing environment. We will focus especially on image/video models, large language models, and chatbots. 
	- You will have access to state-of-the-art high-performance computing resources and you will be able to work alongside top researchers and engineers to truly make an impact in the fast-growing world of generative AI.
	- Lead efforts to drive the design, development, and productionization of ML models and systems, and present the solutions to partners and clients in Japan
	- Work on the commercial side - productionizing generative models and building the infrastructure to serve them at scale; collaborate with other engineers and researchers to build customized models for commercial applications
	- Be a strategic thought partner both internally and externally on driving business impact through machine learning
	- Conduct experiments on e.g. fine-tuning image generation models or LLMs for the Japan market
	- Prototype and productionize model architecture improvements and new features
	- Provide technical advice to partners/clients on generative models
	- Good communication skills with fluency in Japanese and business-level English proficiency
	- 5+ years working on machine learning projects, including training, fine tuning and refining models
	- Familiarity with recent, important papers and projects in the generative machine learning space
	- Experience with Python scientific stack, PyTorch, creating Jupyter/Colab notebooks
	- Experience with the open-source ML ecosystem (HuggingFace, W&B, etc.)
	- Experience with training and/or deploying ML models with Amazon AWS (Sagemaker a plus) or Google Cloud
	- Experience with Linux and command line tools
+ skill set:
	- Machine Learning Engineer, Research
	- We are looking for Machine Learning engineers who are passionate about generative models and creative applications of AI. In particular, we are looking for people who share our mission of open-source machine learning models; people who do not believe AI models should be controlled by a centralized gatekeeper behind a closed wall, but rather be truly open and in control by all. We want highly creative ML engineers who are motivated to push the boundaries of generative models. You will have access to state-of-the-art high performance computing resources and you will be able to work alongside top researchers and engineers to truly make an impact in the fast growing world of generative AI.
	- Lead efforts to drive the design development and production of ML systems, and  present the solutions to customers 
	- Work with Research team on developing the next generation of models, where you may assist with areas such as optimization of model training, model tuning, dataset engineering, HPC clusters, tooling, and work on open-source efforts
	- Be a strategic thought partner for leaders across the organization on driving business impact through machine learning
	- Work on the Commercial side - productioning generative models, and building the infrastructure to serve them at scale, or work to build customized models for commercial applications
	- Prototype and productionize model architecture improvements and new features 
	- 3+ years working on machine learning projects, including training, fine tuning and refining models
	- Experience with Python scientific stack, PyTorch, creating Jupyter/Colab notebooks
	- Experience with JAX / TPUs / CUDA-level / JavaScript (TensorFlow.js etc) a plus
	- Ability to communicate machine learning concepts and results effectively through writing and visualization
	- Experience with training and/or deploying ML models with Amazon AWS (Sagemaker a plus) or Google Cloud
	- Experience with building interactive web demos that serve generative ML models
	- Experience with the open-source ML ecosystem (HuggingFace, W&B, etc.)
	- Experience with Linux and command line tools
+ skill set:
	- Machine Learning Engineer [Dept: Business Analytics]
	- Master’s degree or foreign equivalent in Operation Research, Machine Learning, Industrial Engineering or related field and 3 years of experience in the job offered or related occupation.
	- 1 year of experience with each of the following skills is required:
	- Statistics
	- Machine Learning
	- Linear and convex optimization
	- Convex and Stochastic Optimization
	- Large scale mixed integer Optimization
	- Metaheuristics
	- Python
	- Multiple positions available in Cupertino, California and various unanticipated locations throughout the USA. Design Apple’s future supply chain and planning processes leveraging linear, mixed-integer and stochastic optimization, machine learning and statistics. Collaborate with business teams to find opportunities, understand requirements, prepare technical solutions and drive critical projects. Design data science approach, applying tried-and tested techniques or developing custom algorithms as needed by the business problem. Collaborate with data engineers and infrastructure partners to implement robust solutions and operationalize models. Enhance and evolve solutions to meet changing business needs with agility. Work collaboratively across teams and present results of analyses and models to partners and senior leaders. Research new technologies and methods across data science, data engineering, and data visualization to improve the technical capabilities of the team. 40 hours/week.
+ skill set:
	- AIML - Software Engineer, MLR
	- As part of Apple’s Machine Learning Research organization, we do world-class scientific research and build the technologies that will power future products at Apple.  The techniques and tools we create will impact ML solutions across Apple, which in turn power most of the features we deliver to billions of consumers worldwide.  We are looking for highly motivated, result-oriented engineers with a strong background in system engineering and software development to join our team. In this position, you will work with researchers across the Machine Learning Research group to build scalable, distributed training and research pipelines in the latest and greatest generative models.
	- Expert in Python programming
	- Expert in at least one ML framework (PyTorch preferred but tensor flow or jax are ok as well).
	- Experience with CUDA programming, and/or High-Performance Computing and/or distributed computing.
	- Experience with open-source projects and collaborative software development.
	- Excellent communication skills.
	- Experience and passion to circumvent unexpected roadblocks
	- Past research experience is a plus
	- Work with researchers on the team to build high-performance and scalable software addressing novel ML research algorithms - Apply solid software engineering skills, leverage experience to deal with the unexpected, explore research software solutions and pave the way to future Machine Learning toolboxes. Be part of a small team dedicated to advancing ML algorithms and techniques - Is this you? If so we'd love to hear from you.
	- M.S. or PhD in Computer Science (or related fields) or related fields or equivalent experience.
+ skill set:
	- AIML - Machine Learning Engineer, Siri Information & Intelligence (SII)
	- Search ranking is the technology that presents the most relevant results to a search query, giving the user a rewarding experience with Apple’s search products. The Siri Information Intelligence teams are building groundbreaking technology using algorithmic search, machine learning, natural language processing, and artificial intelligence.
	- The features we build are redefining how hundreds of millions of people interact with their favorite devices. Siri’s universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup. As a part of this group you will have an opportunity to contribute to on-device search technologies and imagine and build products that delight our customers every single day.
	- Strong problem-solving skills, ability to abstract out details and simplify a problem
	- Strong programming skills, e.g. C/C++, python
	- Strong background in computer science: algorithms and data structures.
	- Knowledge of data mining and machine learning
	- Strong interpersonal skills; able to work independently as well as in a team
	- Our team builds the technology for search ranking that brings the “ideal” search results to the top. In pursuit of this, we are interested in ways to rank search results in a way that is sensitive to user-privacy, and in ways to evaluate the quality of answers that are being returned to user queries. This leads us to different adventures, ranging from building indexes for very fast retrieval and search, reimagining query processing using state-of-the-art text processing methods, statistical methods for result evaluation, and many more.
	- BMS, Ph.D. in a related field , or equivalent experience
+ skill set:
	- Machine Learning Engineer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- Our team builds technology that defines industry standards, and we are seeking people who thrive to innovate and strive to build best-in-class high-impact products. We value passion for excellence and a deep commitment to excellence, and if you want to impact millions of customers by working on the most advanced technology solutions, we want to talk to you.
	- Strong programming skills in Python and/or C++ with 5+ years of demonstrated ability in using these languages for machine learning (machine learning) modeling and applied research
	- Hands-on experience with building deep learning applications
	- Expertise in using machine learning toolkits such as PyTorch, TensorFlow, etc
	- Experience developing and optimizing algorithms that run efficiently on resource constrained platforms
	- Ability to drive early-stage research projects with risks and ambiguity
	- Passionate about delivering high-quality products, seeking to solve everyday problems in innovative ways
	- Excellent programming, problem solving and analytical skills
	- Communication and collaboration skills in a multi-functional setting
	- Ability to work hands-on with multi-functional teams
	- Ability to work under tight schedules and deliver under pressure
	- Ability to thrive in a collaborative environment and communicate clearly and confidently with partner teams
	- The Vision Products Group at Apple is actively looking for a highly motivated Machine Learning Engineer to contribute to and build Apple’s future technologies in the spatial computing space. The successful candidate will demonstrate deep knowledge of, and hands-on experience, with designing, implementing, and optimizing machine learning algorithms to tackle ambitious problems. Candidate is expected to be proficient in machine learning and deep learning and be comfortable in applying their machine learning background and problem-solving skills to develop high-quality machine learning solutions that contribute to Apple's revolutionary roadmap.
	- As an Machine Learning Engineer in the Vision Products Group at Apple, you will partner with the algorithm designers to collaboratively design machine learning based solutions to solve high-impact problems on Apple product(s).
	- The primary responsibilities associated with this role, include algorithm design, implementation and optimization, integrating ground breaking research into production frameworks, and collaborating closely with product teams before and after feature launch.
	- You will work multi-functionally with multiple teams at Apple, drive requirements and deliver the end solution
	- You will help evaluate various candidate approaches for optimizing machine learning pipelines for training and inference - these could include (but are not limited to) algorithm tuning, hyper parameter tuning, hardware and software co-design.
	- You will write clean, maintainable and production code with appropriate documentation and tests.
	- You will debug quality related issues in machine learning pipelines.
	- You will contribute to architecture decisions, design reviews and peer code reviews.
	- You will be a force-multiplier, by enabling team-members to be more productive
	- M.S or Ph.D. in deep learning/computer vision/natural language processing/machine learning/computer science with 5+ years of equivalent industry experience (or in exceptional cases, BS with proven track record of relevant industry experience).
	- Bonus: Strong publication record at top conferences
	- The base pay range for this role is between $170,700 and $300,200, and your base pay will depend on your skills, qualifications, experience, and location.
+ skill set:
	- Machine Learning Engineer (Dept: SII Info Intelligence US)
	- Master’s degree or foreign equivalent in Computer Science, Statistics, Software Engineering, Biomedical Engineering or related field and 2 years of experience in the job offered or related occupation.
	- 1 year of experience with each of the following skills is required:
		* Machine Learning and generative models;
		* Deep learning and language models;
		* Natural language processing;
		* Continuous integration;
		* Python frameworks needed for applied ML research;
		* Data-centric ML; and
		* Information retrieval and knowledge graphs
	- Multiple positions available in Cupertino, CA and various unanticipated locations throughout the USA. Developing and implementing production-ready algorithms and method. Co-developing machine learning solutions. Providing architectural guidance on transitioning prototypes to high-performance production models. Providing feedback on tools and new features needed back to platform teams.. Design, implement, and ship new machine learning algorithms and techniques and collaborate with production development team and engineers. Build the platform that enables teams across Apple to develop machine-learning solutions that power intelligent user experiences. Provide technical guidance to product teams on the choice of machine learning approaches appropriate for a task.
+ skill set:
	- ISE, SIML - Robustness Analysis & AI Safety ML Engineer
	- Are you passionate about inclusion, fairness and safety in AI powered features that ship on 1.5B Apple products across the globe? Are you excited about Generative AI and motivated to build out robust and safety capabilities of generative models? 
	- We are the Intelligence System Experience (ISE) team within Apple’s software organization. The team works at the intersection between multimodal machine learning and system experiences. System Experience (Springboard, Settings), Keyboards, Pencil & Paper, Shortcuts are some of the experiences that the team oversees. These experiences that our users enjoy are backed by production scale ML workflows. Visual Understanding of People, Text, Handwriting & Scenes, multilingual NLP for writing workflows & knowledge extraction, behavioral modeling for proactive suggestions, and privacy preserving learning are areas our multi disciplinary ML teams focus on.
	- We have multiple ongoing efforts involving generative models, and we are looking for talented candidates to ensure that features built on top of such models are safe for deployment, and perform equally well for diverse customers within Apple's global user base.
	- Good ML fundamentals
	- Experience in training machine learning models (NLP or computer vision)
	- Familiarity with challenges associated to building robust ML datasets and models: definition and coverage of target data distribution, potential biases, potential failure modes
	- Demonstrated experience in accessing and addressing potential risks and ensuring the safety and fairness in generative models. Prior experience in LLM Safety is a big plus.
	- Strong programming skills in Python to operate models and data at scale
	- Familiarity with ML toolkits, e.g., PyTorch
	- Excellent drive, problem solving skills and analytical approach
	- Strong communication and collaboration skills in a cross-functional setting. Ability to work hands-on with multi-functional teams
	- Apple’s commitment to deliver incredible experiences to a global and diverse set of users, in full respect of their privacy, has led to the development of a dedicated Robustness Analysis function. With the generative experience, creating a safe and robust platform is vital to our mission. Team’s responsibilities include monitoring ML model performance on relevant axes, and surfacing, measuring and mitigating ML failure modes, in order to improve overall user experience and reduce risks, with specific attention given to safety, inclusion and fairness.
	- In this position, you will join a team of people passionate about leading RA operations for key future facing Apple features with focus on ensuring safety and robustness for generative models.
	- research and develop approaches to mitigate harmful and risk behaviors in generative models 
	- define product-centered axes of analysis relevant to target feature, in collaboration with model DRI and feature DRI
		* Directly Responsible Individual (DRI)
		* designated response individual, DRI
	- develop processes (models, tools and data) to identify other potential biases or failure modes
	- when applicable, benchmark model using targeted public datasets
	- characterize potential biases in training set and model along chosen axes
	- request data collection efforts and/or implement automated pipelines based on advanced ML technology and humans/models in the loop to create test sets covering the various axes of investigation
	- report progress and issues found in technical and sponsor meetings
	- suggest mitigation options (data and/or model) and lead mitigation experiments, when issues are found
+ Experience integrating LLMs or other generative models with data platforms a plus
+ skill set:
	- VIO/SLAM Algorithm Engineer
	- The Video Computer Vision organization is working on exciting technologies for future Apple products. Our focus is on ML based solution around real time image and video. We have contributed to the FaceID and FaceKit project in the past and more recently the new LIDAR iPad sensor. We are looking for the right Engineer to help us take our efforts to the next level.
	- In this role, you will work together with similar minds in a unique development team where your skills and expertise will be put into the Apple products. This role is highly multi-functional and you will work very closely with various highly skilled software development / ML teams developing groundbreaking algorithms.
	- Multi-view geometry
	- SLAM - Simultaneous Localization and Mapping experience
	- Traditional ML or deep learning domain knowledge for the above areas is a plus
	- Understanding of visual inertial sensor fusion / or general sensor fusion is a plus
	- Solid programming skills with c/c++
	- Passion on cutting edge computer vision/machine learning technologies and product delivery
	- Ability to communicate the results of analyses in a clear and effective manner
	- You will create 3D computer vision algorithms to deliver AR/VR experiences that are impactful, meaningful, and influential. We work closely with Apple’s best-in-class designers to ensure the products we ship are more than technical demos – they resonate with users at a personal level.
	- In this role you will be working on a wide range of responsibilities: core technology algorithm development in support of future user experiences; communicating with and supporting external teams that use our algorithms; supporting low-level, cross-platform efforts; participating in code reviews; and being a constant advocate within the team for high quality results.
	- PhD in computer vision, robotics or machine learning; alternatively a comparable industry career, with significant experience on delivering products using state-of-the-art computer vision, machine learning and robotics technologies
+ skill set:
	- AIML - Machine Learning Engineer, Operations Research
	- Do you want to innovate on Operations Research and Machine Learning solutions to transform Apple product’s lifecycle to the next level? The Data and Machine Learning Innovation team is looking into innovative ways to forecast key business indicators and optimization throughout end-to-end product lifecycle. We are a R&D team with strong expertise in Machine Learning, Operations Research, and Data Engineering. The team works broadly with Finance, Sales, Operations and Marketing to advance capabilities in forecasting and supply chain optimization with innovative ML/OR techniques and applications.
	- As part of our team, you will work together with domain experts in finance, econometrics and supply chain operations where your skills and expertise will impact decisions that optimizes Apple’s business. This role is highly multi-disciplinary and you will collaborate closely with top notch engineers to build end-to-end ML/OR applications.
	- 2+ years experience of building Operations Research applications.
	- Deep understanding of Linear Programming, Integer Programming, and Stochastic Optimization.
	- Being proficient in solving LP/IP problems using Gurobi/CPLEX/OR-Tools.
	- In-depth knowledge about machine learning and statistical algorithms, especially forecasting algorithms.
	- Being self-driven, understanding the underlying logic of the business, adapting to rapid business changes
	- Good communication skills and courage to change business.
	- Intellectual curiosity, versatility and track record in the field of Operations Research and Machine Learning.
	- Background in supply chain planning or optimization is a plus.
	- You will have the unique opportunity to use innovative techniques from OR and ML to develop accurate forecasting models and supply chain optimization systems. Your work is particularly exciting as it will impact Apple business planning. Strong candidates operations research and machine learning skills are encouraged to apply.
+ skill set:
	- Machine Learning Performance Engineer
	- Our mission is to build the Covariant Brain, a universal AI to give robots the ability to see, reason and act on the world around them. Bringing AI from research in the lab to the infinite variability and constant change of our customer’s real-world operations requires new ideas, approaches and techniques.
	- Success in the real world requires a team that represents that world: diversity of backgrounds, points of view, and experiences. Our common denominator: ambitious expectations, love of learning, empathy for those around us, and a team-first mindset.
	- The Covariant Brain is a Universal AI Platform that powers all robotic applications at Covariant. The Brain is a collection of state-of-the-art models, algorithms, and APIs that enable all intelligent behavior of the robot, from perception to 3d object understanding, grasp sampling, ranking, motion planning, and control. Each ML Performance Engineer will be expected to own a product family and use their understanding of the Covariant Brain to make improvements toward making robots highly autonomous.
	- Own all performance aspects of a robotic application 
	- Understand and iterate on all aspects of the Brain (models, algorithms, tooling, etc.) to resolve production failures and deliver human-level autonomy
	- Analyze robot performance for different errors, understand their root causes, and test/deploy improvements to production
	- Build tools to analyze a highly complex system of robots, models, components, and sensors to understand errors and enable more people across the org to make improvements to the system
	- Take real-world challenges and engineer ML & robotics solutions by training, testing, tuning, iterating, and deploying new and existing models to solve production problems
	- Collaborate closely with research, SW, HW, and infrastructure teams to deliver highly reliable robotic applications
	- Work closely with the HW team on the design of a robotic application including the gripper, vision suite, and station layout
	- Identify throughput bottlenecks in the application and work with the SW team to pinpoint and resolve them
	- Take experimental models from the research team, test them to validate improvement, and deploy them to solve application problems
	- Travel to the customer site as needed to resolve issues with robots in production (not expected for more than once per quarter, if needed at all)
	- Obsession with building products that deliver value to customers through solving hard ML and robotics challenges
	- 3+ years of professional experience working with real-world software systems, AI products, or AI research
	- Proficiency in Python and experience with tensor libraries (e.g. NumPy, PyTorch, TensorFlow)
	- Working knowledge of Linux, SQL, and web (e.g. HTML, Javascript, etc.)
	- A solid mathematical and statistical foundation with an understanding of how to apply ML concepts (e.g. training, evaluation, testing, fine-tuning, data sampling, etc.)
	- Demonstrated strong problem-solving ability: analyzing real-world problems and formulating solutions, iterating and formulating, shipping, and making an impact on products for customers
	- Clear communication and collaboration across teams: taking requests from customers, and PMs, and prioritizing work across SW and HW teams
	- Trained, deployed, and analyzed ML models or robotics applications in production
	- Strong understanding of the state of the art in Computer Vision and Robotics literature
	- Experience working with complex data infra and highly concurrent SW systems
+ skill set:
	- ***Machine learning engineer role with an EDA company to build machine learning frameworks, and machine learning infrastructure (in the context of MLOps)***
	- AI/ML Senior Engineer
	- Lahore, Pakistan
	- Siemens Digital Industry is an innovation leader in automation & digitization. We are part of Technology & Innovation department of Siemens Factory Automation, an international team of dedicated and passionate engineers working on the next generation of pre-products for Industry 4.0. We work on the complete business lifecycle from concept ideation to incubation. We are not tied to specific technology; we build our own technology. We have filed multiple patents for our innovations. Our current portfolio includes a fully managed multi-tier orchestrated industrial IoT Platform and a distributed machine learning flow manager, a no code solution for domain experts & low code solution for data scientists. We work in Machine Learning, Industrial devices and Industrial IoT on the entire stack ranging from Devices & connectivity to Platform services and UI. Our strategic search fields include Industrial grade AI, Industrial IoT & Edge Computing, IT/OT convergence & Digital Twin.
	- Design & develop functionality for our new product using technologies like Generative AI & LLMs
	- Research existing LLMs to find appropriate fit for the team’s need, fine-tuning or retraining existing LLMs available in the opensource
	- Research different AI related topics which are in the interest of the team to create new product ideas and implement POCs and then translating them to complete product. These topics include Generative AI, Adoption of AI, AI Safety, Easy AI, etc.
	- Construct jobs & scripts using industry standard toolsets to automate various data querying, loading, aggregating and reporting tasks
	- Perform data analysis to identify new data sources and to optimize/identify features in existing data sources
	- Develop and evaluate the performance of predictive statistical models and selecting features, building and optimizing classifiers using machine learning techniques.
	- Design & development of enterprise scale Intelligent self-healing system including:
	- Requirement Gathering
	- Statistical Model Designing & Live deployment in production
	- Application Design & Implementation
	- Documentation
	- We're looking for experienced data scientists/machine learning engineers with either a PhD degree in Computer Science or related field with minimum 3 years of industry experience, or an MS degree in Computer Science or related field with minimum of 8 years of industry experience working in the area of Machine Learning and Data Science techniques and understanding the parameters that affect their performance.
	- 4+ years of experience with Python, C++, SQL, NoSQL
	- Good understanding of Deep Learning concepts like Time Series Analysis, Root Cause Analysis, Neural Networks, Supervised Learning, Unsupervised Learning, Regression, Classification, Reinforcement Learning, Knowledge Graphs, Transformers, Generative AI, LLMs
	- Hands-on experience with one or more of the following ML/DL frameworks: Tensorflow, Tensorflow Lite, Keras, MXNet, PyTorch, KNIME, Data Annotation Tools
	- Experience with standard build and deployment tools such as Eclipse, Maven, Git/Stash, and Jira.
	- This is what gives you an edge:
		* Previous experience with LLMs, Generative AI for Images/Vide
		* Previous experience in the field of Computer Vision
		* Knowledge of big data technologies such as Apache Cassandra, Apache Spark and experience of building systems that utilize large data sets
	- Passion to build industrial grade software systems using cutting edge AI
	- Willingness to do hands-on development
	- Strong problem-solving skills and ability to think outside the box for better solutions with an aptitude for research and innovation in AI/ML and their application
	- Strong communication skills and comfortable working with geographically distributed teams
	- Good self-management skills to efficiently work from home whenever required
	- Willingness to take ownership of the assigned responsibilities and ability to work independently with minimal supervision
+ skill set:
	- ***Machine learning engineer role with an EDA company to build machine learning frameworks, and machine learning infrastructure (in the context of MLOps)***
	- Technical Lead (ML)
	- Sri Lanka
	- Synopsys Central Engineering (SCE) team is looking for Software Engineering Experts with an excellent focus on solving complex problems, and capable of leading & building an excellent team.
	- As a part of the Synopsys SCE team, you will be collaborating with a world-class team of software engineers and architects on the mission to build our state of art tools, and you would be expected to conceptualize and develop new tools & applications with cutting-edge technology. You will work together from requirement elicitation, design, implementation, and testing phases to production deployment in the cloud environment and on-prem.
	- Leading and driving the team technically,
		* To implement & productize AI/ML based solutions.
		* To incorporate AI/ML technology into existing tools
		* To re-design and develop existing applications for key R&D productivity
		* BSc/MSc in the domain of Computer/Electronics Engineering or Computer Science
		* 4+ years of relevant industry experience in developing and productizing software solutions
		* Exemplary leadership and team-working skills
		* Solid background in Data Structures & Algorithms
		* Knowledge of Machine Learning & Data Science
		* Excellent programming skills, preferably in Python
 	- Synopsys, Inc. is the Silicon to Software™ partner for innovative companies developing the electronic products and software applications we rely on every day. As the world’s 15th largest software company, Synopsys has a long history of being a global leader in electronic design automation (EDA)and semiconductor IP and is also growing its leadership in software quality and security solutions.
	- At Synopsys, we’re at the heart of the innovations that change the way we work and play. Self-driving cars. Artificial Intelligence. The cloud. 5G. The Internet of Things. These breakthroughs are ushering in the Era of Smart Everything. And we’re powering it all with the world’s most advanced technologies for chip design and software security. If you share our passion for innovation, we want to meet you.
+ skill set:
	- ***Machine learning engineer role with an EDA company to build machine learning frameworks, and machine learning infrastructure (in the context of MLOps)***
	- Sr. Technical Lead (ML)
	- Sri Lanka
	- Staff Machine Learning Engineer
	- Synopsys Central Engineering (SCE) team is looking for Software Engineering Experts with an excellent focus on solving complex problems, and capable of leading & building an excellent team.
	- As a part of the Synopsys SCE team, you will be working with a world-class team of software engineers and architects on the mission to build our state of art tools, and you would be expected to conceptualize and develop new tools & applications with cutting-edge technology. You will work together from requirement elicitation, design, implementation, and testing phases to production deployment in the cloud environment and on-prem.
	- Leading and driving the team technically,
		* To design and implement  AI/ML based solutions
		* To incorporate AI/ML technology into existing tools
		* To re-design and develop existing applications for key R&D productivity
	- BSc / MSc in the domain of Computer/Electronics Engineering or Computer Science
	- 7+ years of relevant industry experience in developing and productizing software solutions
	- Exemplary leadership and team-working skills
	- Solid background in Data Structures & Algorithms
	- Knowledge of Machine Learning & Data Science
	- Excellent programming skills, preferably in Python
	- Knowledge of cloud technologies
	- Exposure to distributed computing will be an added advantage
	- Quick learning and adaption to new technologies
+ skill set:
	- Staff Machine Learning Engineer - Language Modeling (Remote)
	- Our small Natural Language Processing team works on providing key NLP competence to improve Quora users’ experience. We build end-to-end industry-scale NLP systems for moderation (e.g. spam detection) and content-based recommendation (e.g. topic tagging and modeling content relevance). The team directly supports moderation and growth-related initiatives at Quora. As a remote-first company, our engineers have a high degree of flexibility and autonomy.
	- We are looking for an experienced Staff Machine Learning Engineer to join our growing NLP engineering team. You will have a unique opportunity to have high impact by advancing our NLP systems, as well as uncovering new opportunities to apply large language models to the Quora product. You will also play a key role in developing tools and abstractions that our other developers would build on top of.
	- Improve our existing machine learning systems using your core coding skills and ML knowledge
	- Identify new opportunities to apply large language models to different parts of the Quora product
	- Perform experiments and comparative analysis to evaluate the effectiveness of different model architectures and fine-tune hyperparameters for optimal performance
	- Take end-to-end ownership of machine learning systems - from data pipelines, feature engineering, candidate extraction, model training, as well as integration into our production systems
	- Continuously research and stay up-to-date with the latest developments in NLP and large language models, utilizing innovative techniques and methodologies to enhance our models
	- Collaborate with the manager to lead the effort to transform our NLP systems to a new era of large language models
	- Ability to be available for meetings and impromptu communication during Quora's “coordination hours" (Mon-Fri: 9am-3pm Pacific Time)
	- 6+ years of professional software development experience in machine learning, ideally in an NLP team and/or as tech lead
	- BS, MS or PhD in Computer Science, Mathematics, or a related field with a focus on machine learning and NLP
	- Previous experience building end-to-end machine learning systems
	- Strong innovator who understands the nuances of large language models and hands-on experience working with large language models like GPT, LLAMA, BERT, or Transformer-based architectures
	- Proficient knowledge of NLP techniques, including tokenization, language modeling, and embeddings
	- Experience with distributed training and optimization techniques for large-scale machine learning models
	- Excellent communication skills and the ability to collaborate effectively in a cross-functional team.
	- 2+ years of experience writing Python or C++ code
	- Experience with leading large-scale multi-engineer projects
	- For Colorado based applicants, the minimum salary range is $174,400 - $216,000 USD + equity + benefits. For California, New Jersey, New York, and Washington based applicants, the minimum salary range is $174,400 - $255,000 USD + equity + benefits.
	- British Columbia candidates only: For British Columbia applicants, the minimum salary range is $173,800 - $252,000 CAD + equity + benefits.
+ skill set:
	- Deep Learning Engineer
	- Deep Learning Engineer will develop AI applications on AlphaICs Gluon Inference Platform. We are looking to hire a trainee (6 months) or a full time engineer. 
	- 1) Development experience in Deep learning application on embedded/AI boards. (Jetson/Movidius etc)
	- 2) Experience in TensorFlow /pyTorch/Python C/C++ programming.
	- 3) Experience in profiling/ benchmarking/ validating applications in embedded/DSP/ AI boards.
	- 4) Experience working with camera interfaces Develop camera, 3D and other sensors.
+ skill set:
	- Deep Learning Software Library Development Engineer
	- In this role, you will be responsible for developing highly optimized libraries for Deep Learning operators. This role is a very critical role in our AI Processor’s SDK development. It involves working on libraries, performance tuning and analysis, implementing new algorithms etc.
	- 1) Bachelors, Masters or equivalent experience in Computer Science, Artificial Intelligence, Applied Math, EE or related fields.
	- 2) Excellent C/C++ programming and software design skills, including debugging, performance analysis, and test design.
	- 3) Good working understanding of TensorFlow operators and maths behind it.
	- 4) Strong in Linear Algebra.
	- 5) Good knowledge of CPU (ARM, X-86 etc.) or GPU architecture.
	- 6) Architecting or optimizing software libraries for one or more from the list: HPC, OpenCL, CUDNN, BLAS, Eigen, LAPACK, DSP Software Library.
	- 7) Convolution Neural Networks.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
















Skill sets for managing machine learning engineers, or machine learning engineering managers or machine learning managers:
+ skill set:
	- Manager, Machine Learning Research Engineer, Generative AI
	- Scale's Generative AI Data Engine powers the most advanced LLMs and generative models in the world through world-class RLHF/RLAIF, data generation, model evaluation, safety, and alignment.
	- As the Manager of the Generative AI team, you will be responsible for managing and leading a group of talented researchers and engineers. Your primary focus will be to leverage your expertise in LLMs, generative models, and other foundational models to create and execute an AI roadmap which will help Scale accelerate our customers' Generative AI initiatives forward. This is an exciting opportunity to work on cutting-edge technologies and collaborate with industry-leading professionals.
	- We are building a large hybrid human-machine system in service of ML pipelines for dozens of industry-leading customers. We currently complete millions of tasks a month and will grow to complete billions monthly.
	- Manage a team of highly effective researchers and engineers. Provide guidance, mentorship, and technical leadership to a team of researchers and engineers working on Generative AI projects. Develop and evaluate methods for integrating machine learning into human-in-the-loop labeling systems to ensure high-quality and throughput labels for our customers.
	- Implement and improve on state-of-the-art models developed internally and from the community and put them into production to solve problems for our customers and taskers.
	- Work with product and research teams to identify opportunities for improvement in our current product line and for enabling upcoming product lines.
	- Work with massive datasets to develop both generic models as well as fine-tune models for specific products.
	- Work with customers and 3rd party research groups to understand their goals and define how we can enable them.
	- Build a scalable ML platform to automate our ML services, including automated model retraining and evaluation.
	- Be able and willing to multi-task and learn new technologies quickly.
	- Must be able to commute to the San Francisco Office 1-2x weekly. 
	- 7+ years of full time work experience using LLM, deep learning, deep reinforcement learning, or natural language processing in a production environment. Especially training foundational AI models through pre-training, fine-tuning, and RLHF.
	- A vision for where the field should go and what Scale should do to enable it.
	- Strong programming skills in Python, experience in PyTorch or Tensorflow
	- Experience with MLOps and the automation of model training & evaluation
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Solid background in algorithms, data structures, and object-oriented programming
	- Deep appreciation for building high-quality, robust, reusable machine-learning software
	- Degree in computer science or related field
	- Graduate degree in Computer Science, Machine Learning or Artificial Intelligence specialization
	- Publication experience in the field or related topics.
	- Experience with model optimization techniques for both training and inference
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






























##	Applied Machine Learning, Applied ML



For applications of machine learning, or ML, in the following fields, see the *Markdown* document for [](bio-biochem-biotech-pharma.md)
+ bioinformatics
+ bio design automation, BDA
+ bio manufacturing automation
+ biology
+ biochemistry
+ biotechnology
+ medicinal chemistry
+ pharmacy
+ pharmaceutical science



For applications of machine learning, or ML, in finance, see the *Markdown* document for [Financial Engineering, Computational Finance, and Financial Analytics](financial-engr-n-finance-x.md)



Skill sets for generic applied machine learning, applied ML
+ counter AI engineering
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Knowledge of Deep Learning (CNN, RNN, ANN) using TensorFlow.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 1 year of building Business Data Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies (e.g  Alation, Informatica or custom portals).
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of experience designing and building data models to support large scale BI, Analytics and AI solutions for Big Data.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
+ skill set:
	- Director of Applied Machine Learning Experiences
	- The Machine Learning team is building a new platform called Sparta and this platform will be used to build assistive intelligence experiences (AIEs).  These AIEs enable Splunk customers to accomplish their tasks while using real-time user feedback to build self-tuning cloud services. The focus of the selected candidate will be to manage the AIE portfolio and automate simple tasks and processes by harnessing the power of Big Data, cloud, and data science to aid in actionable decision-making and system remediation.
	- Create the capability to make intelligent recommendations, detect anomalies, and help users prioritize events and alerts.
	- Create libraries, services, and SDK to accelerate AIE development.
	- Lead a team of engineers and researchers that collaborate with product and applied research teams to build ML features into a wide variety of products across Splunk. 
	- Recruit, mentor, and grow world class engineers and managers
	- Partner with cross functional team members to develop and maintain a well-defined roadmap, while balancing technological excellence.
	- Manage the AIE portfolio (the assistive intelligence experiences)
	- Help define technical direction and architecture with engineering team members.
	- Facilitate coordination of multiple scrum teams to successfully deliver committed feature sets.
	- Drive practices in training and development, and drive efficiencies across multiple feature teams.
	- Build a culture of continuous learning, growth, and sharing of technological insights.
	- 10+ years of hands-on technical experience.
	- 4+ years of direct management experience of highly technical managers and engineers.
	- Bachelor’s degree in Computer Science or another quantitative field. We will consider equivalent practical experience
	- Experience in delivering multiple complex technical projects within an Agile environment.
	- Familiar with machine learning and data science workflows.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
	- Proficiency with backend systems built using microservices, containerized infrastructure, and modern continuous delivery practices.
	- Demonstrated ability to build a culture of team building and people management.
	- Demonstrated ability to reach stretch goals in a fast-paced environment
	- Outstanding written and verbal communication skills.
+ skill set:
	- Director of Engineering - Machine Learning
	- Recruit, mentor, and grow world class engineers and managers.
	- Partner with cross functional team members to develop and maintain a well-defined roadmap, while balancing technological excellence.
	- Help define technical direction and architecture with engineering team members.
	- Facilitate coordination of multiple scrum teams to successfully deliver committed feature sets.
	- Drive practices in training and development, and drive efficiencies across multiple feature teams.
	- Build a culture of continuous learning, growth, and sharing of technological insights.
	- 10+ years of hands-on technical experience.
	- 4+ years of direct management experience of highly technical managers and engineers.
	- Bachelor’s degree in Computer Science or another quantitative field. We will consider equivalent practical experience.
	- Experience in delivering multiple complex technical projects within an Agile environment.
	- Familiar with machine learning and data science workflows.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
	- Proficiency with backend systems built using microservices, containerized infrastructure, and modern continuous delivery practices.
	- Demonstrated ability to build a culture of team building and people management.
	- Demonstrated ability to reach stretch goals in a fast-paced environment.
	- Outstanding written and verbal communication skills.
+ skill set:
	- Imagimob is a fast-growing, high-tech startup with an exciting future ahead. We are currently developing our next generation hybrid AI platform that allows for advanced motion detection for smaller Internet-of-things-articles, of virtually every kind. For example, the technology is today being used in projects ranging from the automotive and manufacturing industry to the health sector.
	- We are looking for a Machine Learning / AI Application Engineer to join our development team in Stockholm. Do you have excellent programming skills and are interested in working in the frontline of artificial intelligence? Then this position might be something for you.
	- Working with us you will get the opportunity to become part of our cross functional team with creative and innovative software engineers and AI researchers building the next generation AI beyond Deep Learning. Since we are still in a startup phase, you will also be able to develop in the areas you find the most interesting.
	- Has excellent programming skills in one or several languages, preferably in C, C# or Python
	- Experience with a deep learning framework (e.g. tensorflow, keras, Torch, caffe)
	- University degree or equivalent experience in computer science, electrical engineering, engineering physics or similar
	- A passion for Artificial Intelligence and Machine Learning technologies
	- Extreme ownership and go-get attitude
	- Has experience from programming on embedded platforms
	- Good knowledge in signal processing, statistics and its practical applications
	- Experience from Artificial Intelligence and Machine Learning technologies
	- And if this is not enough, you will get the chance to change history and shape the future of humanity...
	- The opportunity to be part of building the next generation AI beyond deep learning
	- Being part of an excellent international engineering team with highly motivated individuals striving for a common goal
	- A chance to get to solve real world problems using AI
	- A prestigeless and an open minded company culture
	- Short decision paths, we love getting things done
+ skill set:
	- 2+ years of work experience developing and deploying production-quality code
	- Foundational knowledge of commonly used machine learning techniques, such as cluster analysis, classification methods, and linear and nonlinear regression modeling
	- Experience developing applications using Natural Language Processing techniques.
	- Experience working with cross-functional teams in a dynamic environment
	- Hands-on experience building deep learning models on text corpora, preferably using PyTorch and Tensor Flow
	- Experience building machine learning models in the healthcare domain
	- Experience using AWS infrastructure and tools for machine learning
	- Experience with other back-end software engineering frameworks
	- [Talkspace](https://www.talkspace.com/)
+ skill set:
	- Data/Model Validation Engineer
	- We are looking for someone passionate about learning how machine learning systems are developed to assist with validating and processing training data to evolve our state of the art systems.
	- Engage with software engineers on the Perception team to identify and collect training data to evolve our machine learning systems.
	- Working with engineers on the Perception team, train new machine learning models and perform analysis to quantify how they perform based on changes to the training data sets.
	- Provide feedback on tools and processes for efficient workflow of training data creation and validation.
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ skill set:
	- You've got a Master's degree in statistics, econometrics, mathematics, or deep learning architectures including convolutional, recurrent, autoencoders, GAN's, and ResNets
	- You're a coding wizard with Python, C# (.NET), Scala, MxNet, CNTK, R, H2O, TensorFlow, PyTorch, cuDNN, NumPy, and SciPy
+ skill set:
	- At least 4 years' experience in deep learning, machine learning or artificial intelligence applications like virtual agent, robotic process automation and video/image/text analytics
	- Minimum of 2 years' experience in AI/ML/RPA functional expertise with developing use cases and building/leading Proofs of Concept
	- At least 2 years architecting AI Pipelines orchestrating multiple analytics engines
+ skill set:
	- Minimum 5 years of developing machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other.
	- Strong quantitative and analytical skills with minimum 3 years of experience with data science tools, including Python, R, Scala, Julia, or SAS
	- Ability to technically lead data science projects
	- Deadline-driven, organized and able to multi-task
	- Familiarity with using cloud services (AWS, Google, Azure) or Big Data tools (Hadoop, Hive, Spark) in data science solutions
+ skill set:
	- Proven experience with caching, queuing, RPC frameworks and other building blocks of a large scale distributed systems.
	- Experience with NoSQL AWS data stores like DynamoDB, CloudSearch or their open source equivalents like Cassandra, HBase, Solr or ElasticSearch
	- Experience with React or other modern javascript frameworks.
	- Experience with MySQL, Redis, Memcache and related web-backend technologies.
	- Experience with data pipelines (Kafka, AWS Kinesis, AWS Data Pipeline)
	- Experience building web applications, widgets, or interactive experiences.
+ skill set:
	- Cortex is a team of software engineers and machine learning scientist to developing state-of-the-art machine learning capabilities to refine and transform our products.
+ skill set:
	- solid understanding and experience using fundamental data fusion and tracking techniques, such as:
		* Kalman filtering
		* batch processing
		* multiple-hypothesis data association
		* multiple dynamic models
	- experience fusing across multiple real-world sources whose data may exhibit characterstics, such as:
		* sub-dimensioned data
		* time-late data
		* biased data
		* negative data
+ skill set:
	- scikit-learn
	- TensorFlow
	- PyTorch
	- MXNet
	- neural network architectures
		* CNNs
		* RNNs
		* autoencoders
		* generative models
	- other machine learning frameworks
	- experience deploying decision policy algorithms:
		* MDPs
		* RL algorithms
		* tree search
		* rules engines
		* path planning
+ knowledge of state estimation and data fusion algorithms
+ skill set:
	- ***LightGBM***, light gradient-boosting machine
	- ***compare bagging versus boosting***
	- usage of ***KDD data set***
	- ***MLP***
	- ***stacking***
+ skill set:
	- deep learning frameworks:
		* TensorFlow
		* PyTorch
		* PaddlePaddle
+ Expert in prototyping traditional ML (GBMs, scikit, etc.) and AI frameworks (keras, tensorflow, mxnet, pytorch, etc.) for a variety of applications
+ ***Experience with one of the ML platforms: Python / scikit-learn, Spark, vowpal wabbit, etc***
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
	- BS, MS, or PhD in Computer Science, Engineering, Statistics or a related technical field
	- Love of the Quora product
+ skill set:
	- BS, MS or PhD in Computer Science, Machine Learning, NLP or a related technical field
	- 5+ years of industry experience preferred
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
	- Love of the Quora product
+ ***Experience building shallow or deep learning models (GBDT, CNN, RNN, LSTM), toolkits e.g. OpenCV, Matlab, RStudio, Weka, MLLib and frameworks PyTorch, TensorFlow, CNTK***
+ ***Expertise in multivariate analysis, graphical models, Bayesian hierarchical modelling, Markov chain Monte Carlo (MCMC), mixture models, stochastic processes, generalized linear models (GLMs), dimensionality reduction (PCA/CCA/MDS/tSNE) and other machine learning techniques***
+ ***Familiar with one or more machine learning, statistical modeling tools such as R, Matlab, scikit learn and deep learning frameworks, such as tensorflow, keras, caffe, torch.***
+ Knowledge in machine learning framework - Tensorflow, Caffe, Torch or Theano
+ Spark, Kafka
	- Spark, for large-scale data science and applied machine learning
	- Kafka, distributed stream-processing platform
+ skill set:
	- Apache Beam or Google Dataflow
		* https://beam.apache.org/
		* "Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream (continuous) processing.[2] Beam Pipelines are defined using one of the provided SDKs and executed in one of the Beam’s supported runners (distributed processing back-ends) including Apache Flink, Apache Samza, Apache Spark, and Google Cloud Dataflow."
			+ https://en.wikipedia.org/wiki/Apache_Beam
	- MapReduce frameworks
		* Hadoop
		* Spark, Apache Spark
			+ https://en.wikipedia.org/wiki/Apache_Spark
			+ Spark MLlib, MLlib Machine Learning Library
		* BigQuery, BigQuery SQL, Google BigQuery
	- Bayesian methods, MCMC, Gibbs sampling
	- significance testing
	- active learning methods
		* multiarm-bandit
		* active Thompson sampling
		* causal inference using instrumental variables and other forms of multi-factor attribution methods
		* sequential testing methods
		* PyMC3
	- real-time metrics tracking systems
+ skill set:
	- ***NVIDIA TensorRT***
		* https://developer.nvidia.com/tensorrt
			+ NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.
		* https://github.com/NVIDIA/TensorRT
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




























###	Computer Vision


####	Notes about Computer Vision

Topics in computer vision:
+ 3-D computer vision
	- 3D pose estimation
	- 3D scene modeling
+ based on:
	- machine learning
		* deep learning
+ event detection
+ feature extraction
+ ***frameworks, tools, and libraries***
	- ***OpenCV***
+ geometric computer vision
+ image restoration
+ image segmentation
+ image understanding systems, IUS
+ indexing
+ learning
+ motion estimation, motion analysis
	- egomotion, visual odometry
	- optical flow, optic flow
	- tracking, video tracking
+ object categorization
+ object recognition, or object classification
	- object detection
	- object identification
	- specialized tasks:
		* 2-D code reading
		* content-based image retrieval
		* facial recognition
		* object character recognition, OCR
		* pose estimation
		* shape recognition technology, SRT
+ scene reconstruction
+ system methods:
	- image acquistion
	- pre-processing
	- feature extraction
	- detection / segmentation
	- high-level processing
		* verification of model-based and application-specific assumptions
		* estimation of application-specific parameters
			+ object pose
			+ object size
		* image recognition
		* image registration
	- decision making
+ template finding
+ video tracking
+ visual prototyping
+ visual servoing
	- vision-based robot control, VS







Applications of machine learning -based computer vision:
+ automatic inspection in post-manufacturing quality control/assurance
+ autonomous vehicles
+ assisting humans in identification tasks, such as species identification system of plants and animals
+ event detection
+ human-computer interaction, HCI
+ medicine
+ modeling objects or environments
+ navigation
+ organization of information
+ process control
+ tactile feedback
+ tracking surfaces or planes in 3-D coordinates
	- enables augmented reality experiences




Machine learning -based frameworks for computer vision:
+ HALO AI










####	Skill Sets about Computer Vision


Skill sets for computer vision:
+ knowledge of CUDA / OpenCL / OpenCV
+ skill set:
	- Design and implementation of state of the art monocular computer vision algorithms
	- Solve problems involving odometry, landmark detection, structure from motion and segmentation in large scale outdoor environments
	- Integrate vision based algorithms into our probabilistic fusion framework
	- Help in identifying core requirements for camera sensors
	- Code development in C++/Python
	- Work with real data on our self driving car
	- Masters or PhD Computer Science, Electrical Engineering or both.
	- Deep Experience in SfM, VO, and classical computer vision algorithms
	- Expert knowledge in computational geometry
	- Experience in machine learning, feature detection and classification
	- Experience with open source computer vision and linear algebra frameworks
	- A solid background in statistics, probability and linear algebra
	- Experience with real world datasets
	- Experience with real time algorithm implementation
	- Ability to work independently without direct supervision
	- Experience with CV algorithm packages (Eigen, OpenCV, etc.)
	- Knowledge of Deep learning techniques applied to CV
	- Experience in Linux based environments
	- Experience in SLAM and/or motion planning
	- Experience with CUDA, OpenCL or other GPU frameworks
	- Experience with automotive systems or UAV systems
	- Ability to lead a small technical team that balances research and application
+ skill set:
	- Strong knowledge of the state-of-the-art in computer vision and machine learning algorithms with a solid understanding of OpenCV
	- Experience working with point cloud processing and Point Cloud Library (PCL)
+ skill set:
	- Develop pipeline for data tagging, labelling and munging to be consumed for training of ML models for vision based tasks.
	- Architect and train machine learning models for object detection and tracking
	- Build testing environment to test the model and simulate edge-case performance scenarios
	- Experience with at-least one of Tensorflow/Caffe/Theano/Torch
+ skill set:
	- skills and knowledge of topics, such as:
		* object recognition
		* semantic segmentation
		* human recognition
		* SLAM, simultaneous localization and mapping.
			+ 3-D SLAM, or localization
		* 6-D object pose estimation algorithm
			+ human pose estimation
		* ray tracing
		* physically-based rendering
		* efficient 3-D map representation
	- performance tuning for robots in real environments
	- performance improvement of machine learning model using simulation
	- robot system development, using ROS 1, ROS 2
	- cloud-based distributed learning experience
	- transfer learning from simulation to real machine
	- accelerating algorithms with OpenCL and CUDA
	- sensor simulation:
		* camera
		* LIDAR
		* RBG-D sensors
+ skill set:
	- generative AI models
	- image/layout generation space
	- develop untried approaches
	- analyze data sources
	- generate, collect, and prepare data for commercial licensing in mind
	- experiment with large-scale cloud infrastructure with high-end hardware
	- set up data augmentation pipelines and perform hyperparameter searches to build production-ready AI solutions
	- CNNs, transforms, diffusion models (stable diffusion)
	- deep learning frameworks
		* TensorFlow
		* Keras
		* Theano
		* MXNet
	- Python 3
	- NumPy
	- OpenCV
	- Docker, Kubernetes
+ skill set:
	- OpenCV
	- Point Cloud Library, PCL
		* open-source library of algorithms for point cloud processing tasks, 3D geometry processing, 3-D computer vision
		* https://pointclouds.org
	- ***TensorRT, MLIR, TVM, XLA***
	- C++1x
	- ML SW stack, machine learning software stack, cuDNN, cuBLAS
	- SIMD programming 
		* avx2
		* neon
+ skill set:
	- Machine Learning Engineer, ML Developer Tools - Video Engineering
	- The Video Computer Vision org is a centralized applied research and engineering team responsible for developing real-time on-device Computer Vision and Machine Perception technologies across Apple products. We focus on a balance of research and development to deliver Apple quality, state-of-the-art experiences. Our team prides itself on innovating through the full stack, and partnering with HW, SW and ML teams to influence the sensor and silicon roadmap that brings our vision to life!
	- Experience in developer tools (ML tools preferred).
	- Experience in HCI to build robust and user-friendly products.
	- Strong prototyping skill to build demos for early feedback.
	- Strong software engineering skills (e.g., Javascript, Python; Vue.js/React; Flask/FastAPI)
	- Experience in LLM (or vision model) application for productivity.
	- Experience on machine learning model development is a big plus.
	- Strong communication skills; great work ethic and teamwork.
	- Our team builds tools that Apple in-house ML experts use everyday to optimize many models shipped in Apple devices. You will work with world-class talents in visualization, on-device optimization, ML tools/platforms.
	- We looking for a researcher / engineer who can develop reliable and scalable web services for ML developers: e.g., effective ML dev workflow, infrastructure to serve internal service using large models (e.g., LLM, vision model).
	- We also encourage publishing novel research at top HCI / ML conferences.
	- PhD in HCI, Machine Learning or M.S. with industry experience (3+ year of experience on developer tools or machine learning).
+ skill set:
	- Senior Algorithm Evaluation Engineer
	- The Video Computer Vision org is a centralized applied research and engineering organization responsible for developing real-time on-device Computer Vision and Machine Perception technologies across Apple products. We balance research and product to deliver state-of-the-art experiences, innovating through the full stack, and partnering with HW, SW and ML teams to bring our visions to life. Examples include FaceID, Animoji/Memoji, Scene Understanding, People Understanding and Positional Tracking (VIO/SLAM), and the Apple Vision Pro.
	- 4+ years experience developing or evaluating computer vision or machine learning algorithms.
	- Passion for quality, outstanding attention to detail.
	- Strong knowledge of software development lifecycle, testing terminology and processes.
	- Excellent written and verbal communication skills, able to describe and document complex topics clearly.
	- Comfortable working with technical teams specialized in a variety of fields.
	- Solid programming skills in Python, familiar with packages like Numpy, Scikit-learn, etc
	- Familiarity with the challenges of working with large scale data sets.
	- Experience using data visualization and presentation tools, such as Tableau.
	- Project and small team leadership and organizational skills preferred.
	- The Algorithm Analysis team is looking for a Senior Evaluation Engineer to enhance our data analysis capabilities for new and exciting technologies. As a member of our fast-paced group, you will have the unique and rewarding opportunity to work with world-class engineers to build detailed evaluations enabling deep understanding of algorithm performance and use case stability. Responsibilities include:
	- Oversee requirements of software design, testing, release cycles, and related processes.
	- Lead technical discussions, system architecture, and other coding requirements with multi-functional teams.
	- Define roadmap, strategic priorities, and business requirements as related to larger organizational objectives, and work within the allocated time.
+ skill set:
	- Computer Vision and Machine Learning Engineer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- In this position, you will join a team of computer vision and machine learning researchers and engineers to discover and build solutions to previously-unsolved challenges and push the state of the art. We are looking for a driven and dedicated computer vision/machine learning engineer or researcher, optimally with experience in 3D computer vision algorithms, such as object detection, pose estimation, generative models, or tracking. As a member of a fast-paced team, you have the unique and rewarding opportunity to shape upcoming products that will delight and inspire millions of people every day.
	- 2+ years of industry experience in CVML or similar fields
	- Strong experience in 3D Computer Vision algorithms
	- Experience in machine learning, preferably in 3D computer vision applications
	- Solid foundation in 3D geometry and linear algebra
	- Good experience in Python and science libraries like Numpy, Pandas, Matplotlib
	- Solid in deep learning frameworks like PyTorch or TensorFlow
	- Experience in C++ is a big bonus
	- Experience in 3D graphics and rendering is a big plus
	- Sensor fusion experience is a big bonus
	- Excellent communication and collaboration skills
	- Excellent problem solving and analytical thinking skills
	- Track record of successfully building and shipping products or open source projects
	- Creativity and curiosity for solving highly complex problems
	- You’ll be working in a team of computer vision and machine learning researchers and engineers to implement world class algorithms that pushes the state of the art.
	- Inventing and implementing state of the art computer vision algorithms to solve cutting edge problems
	- Designing such algorithms to work reliably and efficiently on mobile devices
	- Collaborating with other teams in software and hardware to ensure the full pipeline runs efficiently and utilizes Apple hardware effectively
	- Cooperating with your team members to prepare presentations, papers, and talks to explain your inventions
+ skill set:
	- Algorithm Evaluation Engineer
	- The Video Computer Vision org is a centralized applied research and engineering organization responsible for developing real-time on-device Computer Vision and Machine Perception technologies across Apple products. We balance research and product to deliver state-of-the-art experiences, innovating through the full stack, and partnering with HW, SW and ML teams to bring our visions to life. Examples include ***FaceID, Animoji/Memoji, Scene Understanding, People Understanding and Positional Tracking (VIO/SLAM), and the Apple Vision Pro***.
	- 1+ years experience developing or evaluating computer vision or machine learning algorithms
	- Passion for quality, outstanding attention to detail
	- Strong knowledge of software development lifecycle, testing terminology and processes
	- Excellent written and verbal communication skills, able to describe and document complex topics clearly
	- Comfortable working with technical teams specialized in a variety of disciplines
	- Strong programming skills in Python, familiar with packages like Numpy, Scikit-learn, etc
	- Familiarity with the challenges of working with large scale data sets
	- Experience using data visualization and presentation tools
	- Strong leadership and organizational skills preferred
	- The Algorithm Analysis team is looking for an Evaluation Engineer to enhance our data analysis capabilities for new and exciting technologies. As a member of our fast-paced group, you will have the unique and rewarding opportunity to work with world-class engineers to build detailed evaluations enabling deep understanding of algorithm performance and use case stability. Responsibilities include:
	- Oversee requirements of software design, testing, release cycles, and related processes
	- Partake in technical discussions, system architecture, and other coding requirements with multi-functional teams.
	- Help define roadmap, strategic priorities, and business requirements as related to larger organizational objectives, and work within the allocated time
+ skill set:
	- Senior Computer Vision and Machine Learning Engineer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.  In this position, you will join a team of computer vision and machine learning researchers and engineers to discover and build solutions to previously-unsolved challenges and push the state of the art. We are looking for a driven and dedicated computer vision/machine learning engineer or researcher, optimally with experience in 3D computer vision algorithms, such as object detection, pose estimation, generative models, or tracking. As a member of a fast-paced team, you have the unique and rewarding opportunity to shape upcoming products that will delight and inspire millions of people every day.
	- 4+ years of industry experience in CVML, or Ph.D./M.Sc. in similar field
	- Strong experience in 3D Computer Vision algorithms
	- Experience in machine learning, preferably in 3D computer vision applications
	- Solid foundation in 3D geometry and linear algebra
	- Good experience in Python and science libraries like Numpy, Pandas, Matplotlib
	- Solid in deep learning frameworks like PyTorch or TensorFlow
	- Experience in C++ is a big bonus
	- Experience in 3D graphics and rendering is a big plus
	- Sensor fusion experience is a big bonus
	- Excellent communication and collaboration skills
	- Excellent problem solving and analytical thinking skills
	- Track record of successfully building and shipping products or open source projects
	- Creativity and curiosity for solving highly complex problems
	- You’ll be working in a team of computer vision and machine learning researchers and engineers to implement world class algorithms that pushes the state of the art.   
	- Inventing and implementing state of the art computer vision algorithms to solve cutting edge problems 
	- Designing such algorithms to work reliably and efficiently on mobile devices 
	- Collaborating with other teams in software and hardware to ensure the full pipeline runs efficiently and utilizes Apple hardware effectively 
	- Cooperating with your team members to prepare presentations, papers, and talks to explain your inventions
+ skill set:
	- Machine Learning Imaging Scientist
	- Do you want to push the limits of the best mobile phone camera in the world? The Camera Algorithms R&D team delivers algorithms that drive some of the iPhone cameras ground-breaking features such as Night mode. In this position, you will have the opportunity to be part of an extraordinary team of image processing and computer vision experts to devise and build solutions to previously-unsolved problems in digital camera image processing and push the state of the art in camera algorithms that will change the way people capture images.
	- We are looking for a driven and dedicated machine learning/computer vision scientist, optimally with experience in creating deep learning architectures for image processing. As a member of a fast-paced team, you will have the unique and rewarding opportunity to impact trillions of images and videos that are shot on the iPhone every year.
	- Strong experience in deep learning architectures and frameworks such as PyTorch
	- Experience in computer vision and image processing
	- Solid foundation in mathematics and statistics
	- Excellent problem solving and analytical thinking skills
	- Excellent communication and collaboration skills
	- Technical publication record in deep learning and/or computer vision is desirable
	- Experience in neural network optimization is a plus
	- Proficiency with C/C++ and Python
	- You’ll be working in a team of image processing and computer vision experts to push the state of the art in camera algorithms.
	- Inventing and implementing innovative machine learning and computer vision algorithms to solve cutting edge problems in image processing.
	- Designing such algorithms to work reliably and efficiently on mobile devices.
	- Collaborating with other teams in software, firmware, and hardware to ensure the highest possible image quality.
	- Collaborating with your team members to prepare presentations, talks, and disclosures to explain your inventions.
	- Industry experience of 5+ years in machine learning, image processing, computer vision, or Ph.D. in similar fields.
+ skill set:
	- Computer Vision Researcher & Developer - Apple Vision Pro
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- The Computer Vision Researcher responsibility is to develop innovative solutions for capturing and reconstructing of 3D and 4D representations of detailed and deforming surface geometry and materials. They will develop and deploy research prototypes into production. We value researchers who are self- motivated and enjoy a highly collaborative environment with minimal supervision. There is a substantial R+D component to our development and production. We want an Engineer that is excited about defining new workflows, clear-headed about risks inherent with invention, but passionate about pushing the designs to their maximum potential.
	- Algorithm development in the areas reflectance capture, material estimation and photogrammetry
	- Fluent in C/C++, Python (programming and debugging)
	- Experience HLSL, GLSL, Metal
	- Knowledge of parallel computing, CUDA, OpenCL, GPGPU
	- Knowledge of software optimization and embedded programming
	- 12+ years of related experience
	- 3D and 4D geometry and reflectance reconstruction algorithms with optimization workflows
	- Engineering capture systems for face, body, and hands
	- Capturing and processing data from machine vision cameras and pipeline to production including color calibration, camera intrinsics
	- Automation of capture and processing tasks
	- Creating tools and workflow for integration into shader and rendering pipeline
+ skill set:
	- Computer Vision Engineer
	- The candidate will be required to spend considerable hands-on time designing, coding, optimizing, debugging, and testing Computer Vision applications on AlphaICs’s AI Processor running Linux operating systems.
	- 1) 2-3 years' experience with Programming Languages such as C/C++, Python and experience with hands-on coding to collaborate with HW designers to adopt the proposed.
	- 2) Embedded system software development experience i.e device driver, BSP development.
	- 3) Experience enabling vision algorithms on imaging and vision accelerators or GPUs.
	- 4) Experience with OpenCV, OpenCL, OpenVX, TensorFlow, Caffe or equivalent computer vision library.
	- 5) Experience programming for raspberry pi, NVIDIA Jetson, IMX8, Google Coral, or similar SOC's.
	- 6) Experience in Design/building computer vision end to end solutions for edge devices.
	- 7) Preferable working experience in Convolutional Neural Networks.
	- 8) Experience in the Yocto build system, camera integration and V4l2 is a plus.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.





























###	Natural Language Processing, NLP


####	Notes about Natural Language Processing, NLP



Topics for natural language processing, NLP:
+ word embeddings/vectors:
	- GLoVe
	- fastText
	- word2vec
+ contextualized embeddings
	- ELMo
	- CoVe
+ encoder-decoder models
	- seq2seq
	- vanilla transformers
+ tranformer language models
	- GPT-3
	- GPT-2
	- BERT
	- XLnet
+ named entity recognition
+ POS tagging
+ parsing
+ sentiment analysis
+ clustering
+ text prediction





Libraries for natural language processing, NLP:
+ spaCy, text processing
+ Kaldi, speech recognition/processing and signal processing




Companies involved in natural language processing, NLP:
+ https://emerj.com/ai-sector-overviews/machine-translation-14-current-applications-and-services/
+ https://localizejs.com/articles/types-of-machine-translation/





####	Skills for Natural Language Processing, NLP



Skills for natural language processing, NLP:
+ Experience with Natural Language Processing topics, such as:
	- Topic Modeling
	- Document Classification
	- Document Summarization
	- Sentiment Analysis
+ skill set:
	- experience with ML deployment frameworks
	- work with highly imbalanced data sets to improve detection performance of our existing NLP models and algorithms
	- work closely with product and platform teams, participate in design reviews, and demo your work
	- apply the latest cutting-edge advancements in AI/ML research to our current solutions in a scalable manner for online detection
	- take full ownership of ML models, including:
		* collecting training data
		* evaluating and deploying models to production with ongoing quality monitoring
+ skill set:
	- build and operate high volume distributed systems
	- design and build systems in a microservice-based architecture
	- experience with ML deployment frameworks
	- design, implement, and deploy NLP models
	- experience working with high growth venture-backed start-ups
+ hybrid approach to NLU combines symbolic human-like comprehension and machine learning to transform language-intensive processes into practical knowledge, providing the insight required to improve decision making throughout organizations
+ skill set:
	- Computational Linguist
	- As a Computational Linguist you will join project teams to build top-notch NLP/NLU solutions for our Customers and Partners worldwide, ensuring that all issues are dealt with in the most effective and satisfactory way. Not only you have a firm grasp of general linguistics (semantics, syntax, morphology) but you have also experience with artificial intelligence, machine learning, natural language processing and computational linguistics.
	- Identify logical patterns underlying the language, write semantic rules on Expert.ai’s proprietary IDE to meet Customers’ expectations
	- Perform tests and validations to verify the effectiveness of the implemented solution
	- Contribute to the continuous improvement of the core technology
	- Identify suitable solutions for clients in the field of Natural Language Processing
	- Work with Project Managers, Software Engineers and other Knowledge Engineers on Customer’s projects
	- Graduated in Computational Linguist or Languages
	- Experienced in coding with Python and/or C++: write data processing scripts to transform or extract knowledge from unstructured data, including hands-on experience with regular expressions
	- Analytical person with excellent written and verbal communication skills in a technical context
	- Independent worker with the ability to effectively operate with flexibility in a fast paced, constantly evolving team environment
	- Team player with exceptional interpersonal and solution-oriented attitude
	- Fluent English, both oral and written (the knowledge of other languages will be considered a plus)
+ skill set:
	- Knowledge Engineer Intern
	- If you are completing your studies in Computational Linguistics or Languages and you would like to be part of a growing company that works in an international environment, you can send us your application: we are looking for a Knowledge Engineer Intern for our Paris office.
	- As knowledge Engineer intern you will support the Professional Services team that works on Artificial Intelligence projects aimed at helping big Companies and Public Institutions in solving the daily practical issues linked to the digital transformation and the processes’ automation. During your internship you will be trained in COGITO technology and you will develop your skills in the field of Natural Language Processing / Natural Language Understanding; you will learn the use cases of NLP for Companies and carry out a first experience as a consultant. Finally, you will contribute to the continuous development of our core technology from the linguistic point of view and you will test and verify the effectiveness of the platform.
	- Degree in Languages, Literature, Linguistics or Computational Linguistics;
	- Excellent knowledge of English and another language;
	- Good knowledge of linguistics will be necessary for the rapid handling of our Cogito technology;
	- Basic knowledge of programming languages like Java and Python will be considered a plus;
	- Good general culture knowledge;
	- Attention to the quality of work and results;
	- Team work skills;
	- Ability to analyze and structure information;
	- The knowledge of the Arabic language will be considered as a plus.
+ skill set:
	- NLP/ML Lead at SciSpace (Formerly Typeset)
	- As one of the first engineers in Content Intelligence at Typeset, you will be required to design infrastructure and models for helping our users discover their research papers better. At Typeset, We are on a mission to help researchers connect the dots faster and accelerate scientific innovation. We want to use Machine Learning to provide an assisted discovery experience to our users.
	- Your job responsibilities would include:
		* 1. Choosing a technology stack for machine learning operations
		* 2. Identify problems in our domain that can benefit from ML
		* 3. Create a data collection/curation system
		* 4. Craft models to help solve problems
		* 5. Train/Tune your models
		* 6. And most importantly, deliver an amazing discovery experience to our users
	- This is an end-to-end gig. Typeset believes in full-stack teams who contribute to all parts of the product delivery and take total ownership of the product/features they work on.
	- Ph.D. holder and 2+ years of prior experience working in a team that centered on NLP/ML
	- Prior experience working with NLTK, Spacy, or similar NLP libraries
	- Strong knowledge of Python, Data Structures and Algorithms
	- Prior experience with any/both of the following will be given preferences a. Statistical modeling techniques, such as conditional random fields b. Deep learning fundamentals, such as RNN, LSTM, CNN, Language Models
	- Published papers in top-tier NLP/ML conferences or journals
	- Experience in Scholarly Document Processing (SDP) would be highly-valued
	- Hands-on experience with AWS cloud infrastructure is a bonus.
+ linguistic quality assurance process, LQA process
+ skill set for speech processing/recognition in NLP (natural language processing):
	- ASR
	- MT
	- NLP
	- NLU
	- TTS
	- DM
	- ASP
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Experience with NLP libraries such as SpaCy, Stanford CoreNLP, OpenNLP, or NLTK
+ NLP library: spaCy, NLTK, GATE, CoreNLP, gensim
+ Deep Learning applied to NLP, for example through distributed representations (e.g. Word2Vec, fastText, etc)
+ Publish papers in:
	- ACL
	- EMNLP
	- SIGIR
	- NeurIPS
	- Interspeech
	- ICASSP
	- ICML
	- WSDM
	- WWW
	- RecSys
	- KDD
	- AAAI
+ skill set:
	- Software Engineer, AI Product
	- Adept's mission is to build Useful General Intelligence. We are solving open problems in AI in order to train models that can do arbitrary things on a computer, and we are solving open product problems in order to package these models into a form factor that best enhances human experience and performance. 
	- We’ve recently raised a $350M Series B led by General Catalyst and Spark, on top of a $65M Series A in 2022 with Addition and Greylock. We’re fortunate to be supported by amazing firms and angels such as Chris Re, Andrej Karpathy, Root Ventures, Howie Liu, Dara Khosrowshahi,  and others, and were recently highlighted by Forbes. Adept is backed by a coalition of strategic partners, including Atlassian, Microsoft, NVIDIA, and Workday.
	- Adept thrives at the intersection of research and product. In this role, you will rapidly build new capabilities of our product leveraging a combination of in-house and external tooling, and help safely deploy applications built on top of large models at scale. You’ll collaborate closely with the product engineering, design, and product teams to build practical solutions that address real user needs.
	- The ideal candidate is a full-stack software engineer who has built and launched applications on top of LLMs, worked with a variety of common tools for LLM-enabled software, and is familiar with research in prompt engineering techniques. You are familiar with or even have contributed to open-source libraries related to LLMs and love a 0 to 1 challenge.
	- We deeply value software engineers who can engage with new problems and get things done at a startup, and our team members come from a variety of backgrounds and experiences. 
	- 6+ years of experience as a software engineer, preferably building apps and interfaces
	- Proficiency with Python and JavaScript
	- Experience with frontend frameworks like React and REST APIs
	- Experience building applications on top of LLMs, with a familiarity of tradeoffs across performance, latency, and ease of deployment
	- Experience with LLM-enabled software tooling including frameworks such as LangChain and LlamaIndex, retrieval mechanisms like vector databases, caching solutions such as Redis, and monitoring tools
	- Familiarity with techniques to maintain secure deployments that are robust to prompt injection and have safe outputs
	- Excellent communication and collaboration skills, both verbal and written
	- Bonus: Experience building LLM-powered agents capable of using APIs
+ skill set:
	- Natural Language Generation, Research Engineer, Input Experience
	- Text generation is a key enabler for accelerated text input and intelligent interaction on Apple platforms. Our team is working on redefining user interaction with generative models for text generation. If you want to be part of an ambitious, organized and collaborative team that ships user experiences with pioneering ML applied to NLP, come join the Input Experience NLP team in Software Engineering. Recent announcements at WWDC 2023 for running Transformer models on every keystroke for autocorrection, sentence level correction and inline completions were conceived and built end-to-end on our team.
	- You will work with a hard-working and dedicated set of outstanding ML and software engineers on a wide range of text generation technologies such as long-form text generation, summarization, question-answering, etc. Our team has been working in this area for years and own the NLP and ML text input stack for the keyboard input that includes autocorrection, predictive typing on all Apple platforms. We also work on full stack ML applied to NLP and expose these key technologies across Apple on device and also to third party applications through the Natural-Language framework. If you want to amplify your strong ML and NLP skills into user experiences that will reach every person around you, this is the perfect opportunity!
	- We exemplify Apple’s outstanding integration of hardware and software to create seamless input experiences. You will have the opportunity to go from building offline pioneering NLP models to optimization of the models for different hardware backends and user interfaces that make the experience magical. Our vision always includes a deep dedication to strengthening Apple’s privacy policy by achieving all of the above on device with powerful ML.
	- We are looking for a NLG Research Engineer to innovate and enhance the input experience across all Apple platforms. If you are passionate about using pioneering ML for building phenomenal ML/NLP technologies, products and impacting user experience across billions of users, this is the job for you. The role will require you to set new directions, perform hands-on experimentation and implement your vision into deliverables. You will have wide impact across Apple with strong multi-functional collaboration across teams in hardware, software and design. Come join us!
	- Here are a selection of relevant WWDC presentations focusing on technologies:
		* https://developer.apple.com/videos/play/wwdc2023/10042/
		* https://developer.apple.com/videos/play/wwdc2020/10657/
		* https://developer.apple.com/videos/play/wwdc2019/232/
		* https://developer.apple.com/videos/play/wwdc2018/713/
		* https://developer.apple.com/videos/play/wwdc2017/208/
	- KEY R&D PROJECTS PRODUCTIZED ON THE TEAM:
		* Autocorrection
		* QuickType: Predictive typing
		* Quickpath: Swipe input
		* Image captioning
		* Private Federated Learning for Language Models
		* Natural Language APIs, NLP in CreateML
	- Exemplary ability to design, perform experiments and influence engineering direction and product roadmap
	- Hands-on expertise in NLP and machine learning
	- Experience being part of engineering teams and transferring research ideas to production
	- Proven record of research innovation and visionary leadership in NLP and ML
	- Track-record of publications history in credible conferences and journals (optional)
	- Experience solving analytical problems with quantitative approaches
	- As a NLG research engineer on our team, you will research, design and develop machine learning models as well as tools/pipelines for input experience across all Apple platforms. Your role will have a direct impact on shaping the future of input technology roadmap. You will work closely with the engineering teams, perform hands-on experiments applying groundbreaking NLP and ML, derive insights from experiments and convert them into features that reach the hands of billions of users.
	- PhD in Computer Science or a related field OR MS in Computer Science or related field with at least 5 years of industry experience
+ skill set:
	- AI/ML - Sr Software Engineer, Siri in the Home
	- An opportunity to join the Siri in the Home engineering team and contribute to the platform that is redefining conversational computing for our homes.
	- We are seeking a Sr Software Engineer who brings a passion for connected home technology to help us advance Siri’s capabilities whilst delivering intuitive and delightful experiences to millions of customers.
	- BS/MS in Computer Science or equivalent
	- 8+ years of professional engineering experience
	- Proven track record of architecting performant scalable software, ideally both client and server side
	- Demonstrated ownership of large cross-functional and multi-team projects
	- Proven technical leadership, education, and team mentorship
	- Solid fundamentals in OO design, patterns, data structures, and algorithms
	- Are you interested in being responsible for innovating, implementing, delivering new customer facing features for Siri? Then this may be the role for you!
	- As a Siri engineer, you will blend the latest machine learning technologies with conversational and product design to deliver exceptional experiences to millions of homes globally.
	- You will join a hands-on development team that fosters creativity and generates outstanding solutions to deliver exceptional product experiences by partnering with a variety of multi-functional teams across the company.
	- You will prioritize tasks in a fast paced environment, remain flexible and calm in the face of uncertainty, and collaborate to deliver excellent results.
	- You will be a technical leader to engineers in your team and across the organization. Communicating clearly and having the flexibility to learn new technologies, while continuously developing your skills will be key to your success. You should be comfortable both giving and receiving feedback.
	- BS/MS in Computer Science or equivalent
+ skill set:
	- Software Development Engineer [Dept: ***Machine Translation***]
	- Master’s degree or foreign equivalent in Information Technology, Computer Science, Computer Engineering or related field and 1 year of experience in the job offered or related occupation.
	- Experience and/or education must include:
		* Swift;
		* Xcode Developer Tools;
		* XCTests;
		* Python;
		* macOS or Unix administration and command line usage;
		* HTTP protocol; and
		* REST APIs
	- Multiple positions available in Seattle, Washington. Triage software quality issues and user feedback for improvements. Build tooling for automated triaging. Write, debug, and maintain automation tools using tools such as Swift and Python. Program in Swift or Objective C. Write XCTest Unit Tests and XCText UI automation tests. Build command line tools using Python or Swift. Provide reports on statistics around radars. Coordinate internal processes around software triaging and reporting. 40 hours/week.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.













###	Recommender Systems



Sets of skills for recommender systems, recommendation systems (or, recommendation platforms/engines):
+ skill set:
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
+ skill set:
	- Recommender Systems and Personalization. Almost every aspect of the Netflix experience is personalized, and much of that personalization is driven by our various flavors of recommendation algorithms. You'll apply a number of techniques, from the latest in deep learning, reinforcement learning, to causal inference.
	- Search Ranking and Query Understanding. You'll work on the algorithms that allow our members to interactively query and explore our catalog. Using the latest in NLP techniques, you'll solve problems including: query understanding, knowledge graph discovery, and learning to rank across our global catalog of titles.
	- Large Scale Machine Learning. Netflix is available in over 190 countries, with over 148+ million members. This gives us a unique dataset to work with, but also unique challenges in how we scale our models. You'll work on cutting edge techniques to scale your models for use in our production systems.
	- Strong background in machine learning with a broad understanding of unsupervised and supervised learning methods
	- Strong software development experience
	- Successful track record of delivering results in complex cross-functional projects
	- Strong mathematical skills with knowledge of statistical methods







###	Legal Informatics & Computational Law



This subsubsection includes skill sets for applied machine learning roles in legal services, including:
+ legal informatics
+ computational law











###	MLOps, or ML Ops, ModelOps, & AIOps 





####	MLOps, or ML Ops



MLOps is the set of practices at the intersection of Machine Learning, DevOps and Data Engineering.

MLOps or ML Ops is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently.

Machine learning models are tested and developed in isolated experimental systems. When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems.

Similar to DevOps or DataOps approaches, MLOps seeks to increase automation and improve the quality of production models, while also focusing on business and regulatory requirements. While MLOps started as a set of best practices, it is slowly evolving into an independent approach to ML lifecycle management.

MLOps applies to the entire lifecycle:
+ integrating with model generation
	- software development lifecycle
	- continuous integration/continuous delivery, CI/CD
+ orchestration
+ deployment
+ health
+ diagnostics
+ governance
+ business metrics

MLOps is a subset of ModelOps
+ MLOps is focused on the operationalization of ML models, while ModelOps covers the operationalization of all types of AI models.
+ MLOps is the intersection of:
	- machine learning engineering
	- DevOps
	- data engineering



Steps in a machine learning lifecycle:
+ data collection
+ data processing
+ feature engineering
+ data labelling
+ model design
+ model training
+ optimization
+ endpoint deployment
+ endpoint monitoring




Goals of enterprise machine learning that can be achieved through MLOps systems:
+ Deployment and automation
+ Reproducibility of models and predictions
+ Diagnostics
+ Governance and regulatory compliance
+ Scalability
+ Collaboration
+ Business uses
+ Monitoring and management


A common architecture of an MLOps system would include data science platforms where models are constructed and the analytical engines where computations are performed, with the MLOps tool orchestrating the movement of machine learning models, data and outcomes between the systems.



Need to address:
+ model effectiveness
+ model compliance
+ model life cycle (MLC) management as a cross-functional process




***Skill sets for MLOps***:
+ We work with the best of open source technologies - ***Akka, Scala, Undertow, Spark, Spark ML, Hadoop, Cassandra, Mongo***.
+ skill set:
	- develop distributed traning infrastructure for faster training of ML models
	- efficiently deploy ML models into production
	- create automation pipelines for continuous training, evaluation, and deployment of models
	- improve tracking of models, data, and experiments
	- create the interfaces, infrastructure, and clusters to process data efficiently
	- advance monitoring to identify model drift and active learning opportunities
	- establish scalable, efficient, automated processes for data analyses, model deployment, model validation, and model implementation
	- create and deploy new product features via collaboration with data scientists and software developers
	- ***champion engineering excellence and culture, establish metrics for regular improvement***
	- automate ML pipelines and deploy ML models in production environments
	- design and build systems in a microservices-based architecture
	- experience with ML deployment frameworks
	- experience with containerized applications, databases, and distributed computing
	- experience with deploying ML models as a Web service, and building scalable machine learning systems spanning multiple teams and organizations
+ skill set:
	- Principal Software Engineer - Cloud Service and Machine Learning
	- As the Principal Software Engineer for our Machine Learning team you will be responsible for ensuring that the development of ML systems and services meets all technical and quality standards. You will work with Product Management and other technical teams within Splunk, incorporating new requirements and providing technical information related to this sophisticated ML Platform as needed.
	- work with a team of senior ML engineers, applied researchers and security researchers, and experts within their own specialty. You will set an example for this group, as well as set high standards on quality, communication and ability to deliver with deadlines.
	- contribute to architecture and technical decisions while also mentoring junior members within the team.
	- be working in a multi-office, multi-location development environment and prior experience working with local and remote teams or groups will be a plus.
	- While expertise with ML products and their application within enterprise solutions is highly desirable, it is not required, provided you are willing to quickly come up to speed and you have some prior experience of ML technology and its application.
	- 12+ years software development with focus on large scale distributed systems.
	- Some Machine Learning application development experience, this is NOT a data scientist role, but a services/platform development role.
	- Ability to communicate effectively in conversations with researchers and engineers from academia background.
	- Passionate about building and encouraging good engineering practices and processes such as continuous integration and deployment.
	- Experience developing and putting into production test automation and CI/CD systems.
	- Expertise in developing software with container deployment and orchestration technologies at scale, with strong knowledge of the fundamentals including service discovery, deployments, monitoring, scheduling, load balancing.
	- Strong background in building streaming applications or streaming analytics platforms.
	- Expert in one of the streaming platforms, preferably Flink.
	- Expertise in developing software on a public cloud platform (AWS, GCP, Azure).
	- Expertise in developing software with stream processing technology (Kafka, AWS Kinesis).
+ skill set:
	- You will also work closely with our data scientists to make sure our customers have the necessary tools to perform high quality data integrations by building out the Machine Learning and AI infrastructure for entity resolution, automated data mapping, predictive analytics, and risk analysis.
	- As a Software Engineer Intern you will work with a mentor to improve storage, compute, privacy, security, and compliance features necessary to support the operational workflows that help people get the assistance they need.
+ skill set:
	- We are seeking a strategic technical leader who will be responsible for delivering the core infrastructure for machine learning on Databricks. This includes the ML runtime (a packaged environment containing Spark, Tensorflow, and other frameworks), our own machine learning algorithms, storage and IO optimizations, as well as higher level abstractions such as hyper parameter tuning and feature registries.
	- Grow a team of application developers responsible for the Databricks ML Runtime.
	- Grow Databricks' machine learning capabilities - increase YoY product revenue and adoption at > 100%
	- Manage technical debt, including long term technical architecture decisions and balance product roadmap
+ [***MLflow***, An open source platform for the machine learning lifecycle](https://mlflow.org/)
+ skill set:
	- The Machine Learning Platform team is hiring strong engineers to help us design MLflow, an open source tool for managing the Machine Learning lifecycle. In this role you will help define the APIs creating the standard that organizations use to manage their Machine Learning, from tracking offline experimentation through deployment to production systems. You will also build the services supporting the APIs in the open source and their integration into the Databricks product, a unified analytics platform that helps manage data processing and machine learning workloads in a collaborative, enterprise grade product.
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Be responsible for full software development lifecycle - design, development, testing, operating in production
	- Architect solutions to achieve a high level of reliability, scalability and security
	- Communicate effectively with other engineers in the same team, with other teams and with various other stakeholders such as product managers
	- Mentor junior engineers or other engineers on the team to help level up their skillset
	- 7+ years of production experience developing services in: Java, Scala, C++, Go, or Python
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- Preferred Experience working on a SaaS platform or with Service Oriented Architectures
	- Preferred Experience with software security and systems that handle sensitive data
+ skill set:
	- Development and optimization of storage systems for deep learning and simulation
	- Our goal is to develop the technology to deliver the training data in storage media (either HDD or SSD), to the memory of GPU or MN-Core, as well as the technology to store the data obtained by simulation into our storage media.
	- Communication Language: English/Japanese
	- System programming in Linux (TCP/IP, Ethernet, system calls, file I/O, FUSE)
	- Kernel programming in Linux (VFS, kernel modules, etc.)
+ skill set:
	- Research and development of large-scale computing infrastructure (infrastructure technology) for machine learning
	- We will work on research and development of large-scale computers (clusters) using GPUs and MN-Core. We plan to adjust the content of the work from themes such as performance improvement of calculation infrastructure, verification of elemental technologies, and better operation technology (visualization and automation).
	- Communication Language: Japanese
	- Ability to work on research projects voluntarily and ambitiously
	- Basic computer science knowledge and hardware skills
+ skill set:
	- distributed machine learning
	- query assistance
		* autocomplete, and popular and related searches
	- user experience, with universal search systems and diversity-aware ranking
	- indexing, visual content representation and content understanding
	- retrieval, query understanding and language understanding
	- ranking
		* topical, contextual, personalized, and business objective feature modelling and ranking systems
	- metrics and experimentation
		* development of sensitive offline and online metrics, and more efficient and predictive experimentation systems
	- information retrieval technologies
		* OpenSearch
		* ElasticSearch
		* Solr
		* Learning to Rank algorithms and toolkits
		* build ML solutions that meet SLA guidelines, beyond ML model training
	- skills in:
		* Learning to Rank algorithms and toolkits
		* machine learning
		* information retrieval
		* search-specific experimentation and metrics
- skills to develop:
	- Deep understanding of at least one popular server side MVC Framework (e.g Django, Rails, AngularJS etc).
	- Knowledge of backend storage systems like MySQL, HBase, Memcached, Redis, Kafka etc.
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
+ tech stack:
	- Experience with NoSQL databases. MongoDB is a plus
	- Experience with real-time and streaming data processing
	- Experience with queuing platforms like Kafka
	- Knowledge of BigQuery
	- Familiarity with GCP/AWS cloud services
	- Familiarity with TensorFlow
	- Comfortable with CI/CD Pipelines
	- Experience with Git version control
+ skill set:
	- Software Engineer, Platform
	- The Platform team is responsible for building the core abstractions and infrastructure on which the products can be built and iterated rapidly. The team owns how data flows throughout the scale platform. We’re looking for people with a strong background or interest in building distributed systems, data-intensive applications, and machine learning infrastructure. You have a growth mindset and are comfortable learning new technologies.
	- Build engineering foundations for Scale, such as machine learning infrastructure, data platform, APIs framework and task dispatching systems.
	- Collaborate with stakeholders across the organization, such as production engineers, machine learning scientists, customer operations, etc.
	- Own services or systems and define their long-term health goals, while also improving the health of surrounding components.
	- Mentor other engineers and become deeply involved in design and code review.
	- Improve our high engineering standards, tooling, and process.
	- Work directly with our engineering and sales teams to create backend and infrastructure solutions to meet their challenging data and security needs.
	- Work with our Security Team on security compliance, pen tests and mitigations that improve security across Scale.
	- Build systems capable of handling millions of frames of data every day, making it available to both our workforce and our internal teams with high availability.
	- 2-7 years of industry experience as a software engineer post graduation
	- Systems engineering experience with real-time and distributed system architecture.
	- Experience building systems that process large volumes of data.
	- Experience  in using the following systems in production: AWS, Typescript, Node, MongoDB, MLflow, Python (note that we are mostly language-agnostic and are open to using whatever is the best tech for the problem at hand)
	- Experience working with Docker, Kubernetes, and Infrastructure as code (eg terraform); bonus points for running GPU/ML workloads
	- Prior startup experience to help us grow responsibly
	- Experience with core AWS technologies such as VPC, EC2, ALB, ASG, Spot Instances
	- Experience in operating or managing Infrastructure such as Spark, Presto, Hive
	- Mentored and grown members of your team or been a tech lead on large projects
+ skill set:
	- MLOps Engineer - Deep Learning & HPC
	- We are looking for a talented MLOps Engineer with a focus on Deep Learning and High-Performance Computing that will work with a growing multidisciplinary team of talented research scientists and machine learning engineers to improve and scale the efficiency within our computing capacity. 
	- Optimizing Deep Learning Workflows: 
		* Monitor reports and dashboards and detect low utilization jobs, projects, users
		* Partner with researchers to check their workflow when they lack performance
		* Identify bottlenecks and suggest scripting optimisations
		* For high-scale jobs, introduce AWS proprietary profiler and libraries to boost performance
		* Scale-up gating process: check the scripts performance and vet requests to scale up
		* Build a knowledge base / best practices documentation for all researchers
		* Implement and monitor CPU usage levels for our CPU clusters; identify users that need assistance in properly coding to maximize usage of CPU’s
		* Train researchers on best practices on how to implement automatization strategies to minimize human oversight on jobs.
	- Develop and Test Strategies for Future Workloads:
		* Benchmark new systems capabilities and identify strategies to properly utilize them (H100, TRN2, TPUv5, Intel Gaudi)
		* Define the minimum needs for storage speeds and find better data loading strategies to support high processing demands of the new accelerators
	- High-Performance Computing:
		* Maintain HPC cluster operations
		* Monitor dead nodes and recover them; document dead nodes and their fixes
		* Monitor shared volumes health, usage, and clean-up needs, pursue users to clean-up
		* Partner with users that do not adequately use POSIX permissions on shared storage
		* Monitor the HPC Help Center and solve user problems
		* Assist users in properly launching their jobs
		* Maintain the future S3 access permissions, debug problems, etc
		* Monitor all CPU clusters for users 
		* Create and maintain processes around authentication, authorization and accounting for clusters usage
		* Develop processes around security aspects of the HPC clusters, including tools to in case of security risks are identified (globally, by user, by team, by location, etc)
		* Convert and deploy SLURM scheduling for all clouds and all resource types; integrate TPUs into our larger enterprise approach when SLURM becomes available. 
		* If needed to use k8s infrastructure for research, then maintain SLURM on top of K8S
		* Solve SLURM support tickets with Sched MD's bug management  tools
		* Maintain AWS resources associated with the HPC clusters (login nodes, S3 buckets, FSx volumes, VPCs, subnets, NAT Gateways, S3 VPC Endpoints, routing tables)
	- At least 8+ years of relevant experience
	- Applied programming experience in Python, C, and/or C++
	- Experience with libraries and tools like PyTorch and CUDA
	- Experience in building, productizing and monitoring orchestration pipelines for AI and Machine Learning pipelines
	- Experience with training frameworks like Megatrong, NVIDIA or similar frameworks
	- Experience in leading more junior engineers
	- Experience with AWS and/or GCP
	- Experience/exposure to CI tools infra tools is a nice to have (Kubernetes)
	- Experience with Linux-based environments and scripting (Shell Scripting, Python, Powershell)
	- Ability to work well as an individual contributor as well as within a multidisciplinary team environment
	- Strong communicator with excellent interpersonal skills and can-do attitude to work and thrive in a fast-paced team environment
+ skill set:
	- Machine Learning Solutions Engineer
	- We are very optimistic about opportunities for Japanese content and the Japan market, and so we are fully committed to serving Japanese creators, companies, and organizations. We will provide free and open models that incorporate Japan-specific datasets to the community as well as custom private models to clients and partners. Our current focus is on generative models related to images/video and text/chat.  
	- We are looking for a versatile Solutions Engineer who can successfully manage and deliver software projects for our clients/partners. This role will work closely with other engineers and BizDev/sales, as well as directly collaborating with clients/partners. You will adapt quickly as we try various approaches to various industries in a fast-changing environment. You will have access to state-of-the-art high-performance computing resources, and you will be able to work with top talent in Japan and globally to truly make an impact in the fast-growing world of generative AI.
	- Develop and maintain positive relationships with clients/partners, identifying their needs and requirements and finding solutions to meet those needs
	- Take ownership of architecting, executing, and delivering software projects to clients/partners, including hands-on coding for model-fine tuning
	- Communicate with key client stakeholders to align expectations on the proposed solution. Write project proposals, manage technical evaluations, define solution architecture, facilitate product demonstrations, etc
	- Collaborate with other technical staff and help plan their work to ensure that projects are delivered in a highly satisfactory way that will lead to more business
	- Provide technical expertise/support/training to clients/partners; demonstrate to partners (such as cloud vendors and system integrators) how to use and fine-tune our models
	- Work closely with the BD/sales team to identify and pursue new business opportunities with existing clients
	- Speak at industry events to evangelize our models/products, build our brand, and network with potential clients/partners/users
	- Make good use of media (blog posts, podcasts, etc.) to make our solutions understood and appreciated by a broad audience
	- Fluent in spoken/written Japanese and business-level English
	- 8+ years of experience in the software industry and 5+ years of experience in a customer-facing (or partner-facing) technical role
	- 5+ years of programming experience, including 2+ years of ML engineering
	- Experience owning and delivering entire projects to a client/partner from beginning to end with several past examples of successful delivery
	- Software project management experience
	- Strong teamwork and relationship management skills
	- Ability to quickly understand new technical topics and explain them to a non-technical audience
	- Excellent oral communication and writing skills - including comfort in front of large audiences
	- Solid interest and experience in working with Generative AI models such as Stable Diffusion or large language models is a big plus
	- Experience architecting cloud solutions (especially on AWS) is a big plus
+ skill set:
	- MLOps Engineer, DL & HPC
	- We are looking for a talented MLOps Engineer with a focus on Deep Learning and High-Performance Computing that will work with a growing multidisciplinary team of talented research scientists and machine learning engineers to improve and scale the efficiency within our computing capacity. 
	- Optimizing Deep Learning Workflows: 
		* Monitor reports and dashboards and detect low utilization jobs, projects, users
		* Partner with researchers to check their workflow when they lack performance
		* Identify bottlenecks and suggest scripting optimisations
		* For high-scale jobs, introduce AWS proprietary profiler and libraries to boost performance
		* Scale-up gating process: check the scripts performance and vet requests to scale up
		* Build a knowledge base / best practices documentation for all researchers
		* Implement and monitor CPU usage levels for our CPU clusters; identify users that need assistance in properly coding to maximize usage of CPU’s
		* Train researchers on best practices on how to implement automatization strategies to minimize human oversight on jobs.
	- Develop and Test Strategies for Future Workloads:
		* Benchmark new systems capabilities and identify strategies to properly utilize them (H100, TRN2, TPUv5, Intel Gaudi)
		* Define the minimum needs for storage speeds and find better data loading strategies to support high processing demands of the new accelerators
	- High-Performance Computing:
		* Maintain HPC cluster operations
		* Monitor dead nodes and recover them; document dead nodes and their fixes
		* Monitor shared volumes health, usage, and clean-up needs, pursue users to clean-up
		* Partner with users that do not adequately use POSIX permissions on shared storage
		* Monitor the HPC Help Center and solve user problems
		* Assist users in properly launching their jobs
		* Maintain the future S3 access permissions, debug problems, etc
		* Monitor all CPU clusters for users 
		* Create and maintain processes around authentication, authorization and accounting for clusters usage
		* Develop processes around security aspects of the HPC clusters, including tools to in case of security risks are identified (globally, by user, by team, by location, etc)
		* Convert and deploy SLURM scheduling for all clouds and all resource types; integrate TPUs into our larger enterprise approach when SLURM becomes available. 
		* If needed to use k8s infrastructure for research, then maintain SLURM on top of K8S
		* Solve SLURM support tickets with Sched MD's bug management  tools
		* Maintain AWS resources associated with the HPC clusters (login nodes, S3 buckets, FSx volumes, VPCs, subnets, NAT Gateways, S3 VPC Endpoints, routing tables)
		* At least 8+ years of relevant experience
		* Applied programming experience in Python, C, and/or C++
		* Experience with libraries and tools like PyTorch and CUDA
		* Experience in building, productizing and monitoring orchestration pipelines for AI and Machine Learning pipelines
		* Experience with training frameworks like Megatrong, NVIDIA or similar frameworks
		* Experience in leading more junior engineers
		* Experience with AWS and/or GCP
		* Experience/exposure to CI tools infra tools is a nice to have (Kubernetes)
		* Experience with Linux-based environments and scripting (Shell Scripting, Python, Powershell)
		* Ability to work well as an individual contributor as well as within a multidisciplinary team environment
		* Strong communicator with excellent interpersonal skills and can-do attitude to work and thrive in a fast-paced team environment
+ skill set:
	- Tools and Pipeline Software Engineer - Apple Vision Pro
	- Apple is where individual imaginations gather together, committing to the values that lead to great work. Every new product we build, service we create, or Apple Store experience we deliver is the result of us making each other’s ideas stronger. That happens because every one of us shares a belief that we can make something wonderful and share it with the world, changing lives for the better. It’s the diversity of our people and their thinking that inspires the innovation that runs through everything we do. When we bring everybody in, we can do the best work of our lives. Here, you’ll do more than join something — you’ll add something.
	- Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space. It will allow us to do the things we love in ways never before possible — all while staying connected to the people around us.
	- We are looking for a driven and dedicated Tools and Pipeline Software Engineer to join our fast-paced team. As a member of the Face and Body technologies team, you have the unique and rewarding opportunity to work on upcoming releases of Apple products that delight and inspire millions of people every day. We invite you to contribute to our current spatial computing software platform and craft the future of this technology.
	- 2+ years of production experience in tools and pipeline development with track record of successful projects
	- Strong proficiency in Python, writing clean and well structured code
	- Strong understanding of data structures and algorithms
	- Experience working in distributed processing
	- Passion for delivering high quality software to end-users
	- Self-motivated with proven ability to effectively prioritize and deliver tasks on schedule
	- Excellent communication and experience working with cross-functional teams
	- Apple's Vision Products Group (VPG)Face and Body technologies team is looking for a skilled Tools and Pipeline Software Engineer with a passion to accelerate the development of new algorithms and help the team achieve confidence in their results at scale. In this role, you will perform research and development work to design, implement, benchmark, optimize and distribute large scale dataset processing for ambitious computer graphics and computer vision projects for Apple Vision Pro.
	- Our team delivers algorithms that power Apple Vision Pro's Eyesight, Persona and other visionOS technologies. In this position, you will have the opportunity to be part of our extraordinary team of Computer Vision and Deep Learning researchers and engineers to discover and build solutions to previously-unsolved challenges and push the state of the art in Computer Vision / Machine Learning, 3D Reconstruction and Neural Rendering algorithms that will change the way people experience the world!
	- BS, MS or PhD in computer vision, machine learning, computer science, computer engineering or related fields
+ skill set:
	- AIML - Sr Engineering Program Manager, Text to Speech
	- Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Do you love taking on challenges that create a positive impact? Are you passionate about empowering many ground-breaking intelligent experiences to be made? The Apple AIML org is building groundbreaking technology... and we are looking for people like you!
	- The AIML org Text-To-Speech team is building innovative Text-To-Speech solutions to enable most accurate, natural and connected voice and audio experiences for Apple users. We’re looking for a strong Senior Program Manager with a solid track record of building and maintaining sophisticated software technologies, tools and processes that deliver the best experiences that delight customers. The program manager will lead projects in Text-To-Speech program to serve our over 1 billion customers. You will work with world level deep learning and AI engineers applying innovative machine translation and AI technologies to solve challenges on speech generation, both on user experiences and technologies.
	- 8+ years of experience in driving large scale program building machine learning powered products
	- Excellent program management skills including program structuring and managing multiple work streams interdependently
	- Have personally orchestrated the delivery of multiple significant client/server software products requiring large organizations (100+) to complete
	- Solid knowledge of technical system architecture and systems design tradeoffs
	- Capability to work across a large number of teams to enforce process and efficient outcomes in a data-driven environment
	- Strong analytical thinking, analysis, and problem-solving skills
	- Attention to details
	- Strategic mentality focusing on driving what matters
	- Self-motivated and proactive, with demonstrated creative and critical thinking capabilities required to forge a path to success
	- Strong leadership skills, including coaching, team-building, conflict resolution and management
	- Ability to communicate abstract ideas clearly and independently manage complex project objectives
	- Strong verbal and written communication skills including negotiation, presentation and influence
	- Lead annual and quarterly planning for Text-To-Speech core engineering team and dependency feature teams via strong prioritization framework  
	- Create and manage project timelines with clear dependencies, critical path and systematic methodology to communicate status  
	- Break down complex issues into discussion topics, lead the discussion and drive alignment  
	- Keep teams on critical path to accomplish quarterly goals   
	- Proactively identify issues, manage risks and mitigations, and re- plan as events warrant  
	- Provide clear, timely and objective communication, including regular program status updates and one-off reviews as needed to executive team  
	- Effectively drive requirements, dependency management and alignment cross-functionally across Text-To-Speech core engineering team, dependency feature teams and other function teams  
	- Collaborate with other teams within and outside of the organization to promote cross-team technology sharing  
	- Lead competitor analysis to understand competitor landscape and opportunities for Apple Text-To-Speech technologies  
	- Network with internal customers to continuously look out for opportunities for Text-To-Speech services or technologies to improve user experience and (or) Apple values  
	- Lead measurements to help team understand and measure success, including quality, usage scale and other critical metrics   
	- Reflect on how the team operates, propose and implement new ways to work in order to improve the rate at which the team delivers value to customers
	- Bachelors or Master’s degree in Computer Science, or related field and equivalent experience
+ skill set:
	- AIML - Machine Learning Engineer, Visual Intelligence
	- Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there’s no telling what you could accomplish.
	- Do you want to make Siri and Apple products smarter for our users? Do you want to be a part of redefining how people use their computing devices to search and access information? Are you excited by early stage initiatives with potential for huge impact?
	- The AIML Information Intelligence teams are building groundbreaking technology for algorithmic search, machine learning, natural language processing, , computer vision and artificial intelligence. The features we build are redefining how hundreds of millions of people use their computers and mobile devices to search and find what they are looking for. Our universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup.
	- Join a new team in Apple AIML that is investigating novel visual capabilities across all Apple products that will transform the way people engage with the world around them! We’re looking for strong engineers to work on cutting-edge technology, collaborate with experts across Apple, and deliver new end-to-end experiences that delight customer
	- 3+ years of professional experience in machine learning for computer vision applications
	- Excellent knowledge and good practical skills in major machine learning algorithms
	- Proven track record of developing and productionalizing high-quality computer vision algorithms
	- Strong interpersonal skills able to work independently as well as in a team
	- Excellent software design problem solving and debugging skillsFluency in Python and another language (C/C++, Go, Rust)
	- Experience with relevant deep learning software packages (Keras, Tensorflow, PyTorch…)
	- As a machine learning engineer you are excited to seek and tackle high impact problems using deep learning and large data sets. You will stay up to date with the latest research in detection, segmentation and metric learning and work with a team of highly qualified computer vision and machine learning specialists to develop innovative computer vision/machine learning systems in the area of visual search. You'll be involved in all phases of model development including data analysis, prototyping, testing, deployment.
	- We work closely with teams across Apple worldwide and are looking for expert Applied ML Engineers to join our Agile development teams. You will have great technical skills, a drive for high quality software and the ability to innovate creative solutions. Communicating clearly and having the flexibility to learn new technologies, while continuously developing your skills will be key to your success. You will fit into our teams, be a fantastic collaborator, comfortable with giving and receiving feedback and able to thrive in a dynamic environment.
	- MS, Ph.D. in a related field , or equivalent experience
+ skill set:
	- Develop, deploy, and refine solutions centered around infrastructure and how it hooks into generative AI platform and our GPU ETL/AI pipelines
	- Spearhead data pipelines and solutions focusing on OSINT applications of our platform, incorporating diverse data sources such as using social media data, news media, government records, and enterprise data
	- Act as a pivotal interface with enterprise, tech, and multinational government teams, aiding sales engineering efforts around onboarding and tuning rapidly evolving deployments
	- Collaborate on special projects for premium customers
	- Forge partnerships with senior staff from other technology providers to create innovative integrations around their traditional SIEM platforms  (Splunk, Zeek, Kafka), modern data stacks (BigQuery, Snowflake, Databricks), and new Graphistry ecosystem technologies (Python, Jupyter, Streamlit, Graphistry, Louie.AI, Nvidia RAPIDS, Nvidia Moprheus, Apache Arrow, graph neural networks, Kubernetes, generative AI)
	- Contribute to shaping our engineering culture and approach to company building
	- Ideally, share knowledge through avenues such as public speaking, private training sessions, customer demos, tutorials, and blog posts
	- Enjoy Python and PyData ecosystem: Linux/Docker, notebooks, dataframes
	- Experience with GB and ideally TB or even PB data volumes such as with tools like ELK/OS, Splunk, Spark, SQL
	- Practice IaaS and reliable software engineering
	- Security/fraud/etc investigation platform experience, especially using SIEM, UEBA, SOAR, and code
	- Broader investigative experience such as sigint, osint, and fraud
	- Built custom investigation platforms and content, including data engineering, detection as code, and dashboards
	- Experience in on-premise, cloud, and Kubernetes environments
	- Experience in both startup, government, and enterprise environments
	- Developed or used traditional ML models and neural networks, and especially graph or NLP
	- Used GPU computing, including in bigger-than-memory settings
	- Used graph technologies like graph databases, graph analytics, graph visualization, and graph AI, and especially in at-scale detection and investigative scenarios
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.





















####	ModelOps



MLOps is a subset of ModelOps
+ MLOps is focused on the operationalization of ML models, while ModelOps covers the operationalization of all types of AI models.


ModelOps (model operations), as defined by Gartner, "is focused primarily on the governance and life cycle management of a wide range of operationalized"
+ artificial intelligence (AI) models
+ decision models
	- machine learning models
	- knowledge graphs models
	- rules
	- optimization
	- linguistic models
	- agent-based models
	- decision optimization models
+ optimization models
+ transformational models



ModelOps has overlaps wothin:
+ DataOps
+ DevOps


ModelOps is a programming model for reusable, platform-independent, and composable AI workflows.
+ Mitigate the accumulation of AI and machine learning models that are:
	- undeployed
	- unused
	- unrefreshed
	- manually deployed
+ Support model management of AI and machine learning models
+ address the gap between model deployment and model governance
+ ensure that all models are running in production with strong governance, and aligned with technical and business KPIs while managing the risk
+ programmatic solution for AI-aware staged deployment and reusable components that would enable model versions to match business apps, and which would include AI model concepts such as:
	- model monitoring
	- drift detection
	- active learning
+ cloud-based framework and platform for end-to-end development and lifecycle management of artificial intelligence (AI) applications
+ extend the principles of software lifecycle management to enable the following for AI model pipelines:
	- automation
	- trust
	- reliability
	- traceability
	- quality control
	- reproducibility
+ includes:
	- routine deployment of machine learning models
	- continuous retraining
	- automated updating
	- synchronized development
	- deployment of more complex machine learning models




References:
+ Hummer, Waldemar; Muthusamy, Vinod. ModelOps: Cloud-based Lifecycle Management for Reliable and Trusted AI. IEEE International Conference on Cloud Engineering. Parijat Dube, Kaoutar El Maghraoui. p. 1.


The ModelOps process focuses on:
+ automating the governance, management and monitoring of models in production across the enterprise
+ enabling AI and application developers to easily plug in life cycle capabilities
	- bias-detection
	- robustness and reliability
	- drift detection
	- technical, business and compliance KPIs
	+ regulatory constraints and approval flows for putting AI models into production as business applications



The ModelOps process starts with a standard representation of candidate models for production that includes a metamodel (the model specification) with all of the component and dependent pieces that go into building the model such as:
+ data
+ hardware and software environments
+ classifiers
+ code plug-ins
+ business and compliance/risk KPIs




Skill set for ModelOps:
+ Spring Boot, Spring Data Rest, and Microservice Development experience
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












####	AIOps

AIOps, a similarly named, but different concept - using AI (ML) in IT and Operations.



Skill set for AIOps:
+ skill set:
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Cloud computing platforms and large web-scale distributed systems



















##	Logical AI + Other AI




















##	Differential Machine Learning











##	Differential Privacy





+ Hestia - Differential Privacy - Data Anonymization Challenge, https://www.topcoder.com/challenges/30082341
	- https://github.com/uber/sql-differential-privacy
	- https://github.com/arx-deidentifier/arx










##	Information Retrieval




+ Experience with search and information retrieval systems, either custom or commercial (Elasticsearch, Solr).
+ You have used GraphQL in production environments



























##	Data Science + Data Engineering + DataOps


###	Notes about Data Science + Data Engineering + DataOps

This section provides information about data science roles and skills set regarding:
+ [Generic data science positions]()
+ [Business analytics]()
+ [Sports Analytics]()
+ [Data Science for public health]()
+ [Data Science for Advocacy, Lobbying, Think Tanks]()
+ []()
+ []()
+ []()
+ []()
+ []()




For skill sets in data science roles regarding finance, see the *Markdown* document [](financial-engr-n-finance-x.md).





For skill sets in data science roles regarding the following fields, see the *Markdown* document [](bio-biochem-biotech-pharma.md).
+ bioinformatics
+ bio design automation, BDA
+ bio manufacturing automation
+ biology
+ biochemistry
+ biotechnology
+ medicinal chemistry
+ pharmacy
+ pharmaceutical science






Notes:
+ In database normalization, unnormalized form (UNF), also known as an unnormalized relation or non first normal form (N1NF or NF^2),[1] is a database data model (organization of data in a database) which does not meet any of the conditions of database normalization defined by the relational model. Database systems which support unnormalized data are sometimes called non-relational or NoSQL databases. In the relational model, unnormalized relations can be considered the starting point for a process of normalization. It should not be confused with denormalization, where normalization is deliberately compromised for selected tables in a relational database.







###	Generic Data Science Roles



Sets of skills for generic data science roles, or data scientist positions (i.e., junior to mid-level data scientists):
+ ***Experience with metrics systems such as Grafana.***
+ ***Experience working with analytics tools such as Google Analytics, Heap Analytics, Chartmogul, Baremetrics, Periscope, Tableau, Mode Analytics, Looker, or similar***
+ Data engineering experience and data pipeline tooling (e.g. Airflow, DBT) experience is a plus
+ skill set:
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data analyst experience with 4+ years of proven industry experience in a large scale environment(PBs scale, globally distributed teams).
	- Strong experience in Python, R, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ skill set:
	- Partner and align with business leaders, stakeholders, product managers and internal teams to understand the business and product challenges and goals and address them using predictive analytics in a globally distributed environment.
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Understand business/product strategy and high-level roadmap and align analysis efforts to enable them with data insights and help achieve their strategic goals.
	- Strong audience focused presentation and storytelling skills focused on key takeaways in a crisp and concise manner.
	- Define hypothesis driven models and best practices to derive and publish model scores/insights/learnings at scale within the company.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data scientist experience with 4+ years of proven industry experience in a large scale environment (PBs scale, globally distributed teams).
	- Proven lead in driving multi-million dollar revenue generator models for the company and setting up data science related best practices at a company.
	- 2+ years experience with a fast-growing SaaS business based company is preferred.
	- Strong experience in Python, R, Spark, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ Experience with data analytics platforms, such as Semantic Pro, Semantic Cortex, IBM i2
+ tech stack:
	- 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Experience  communicating effectively with non-technical audiences
	- Strong ability to devise data-driven solutions to business problems
	- Strong competency with SQL
	- Experience with or exposure to a scripting language (Python preferred)
+ tech stack:
	- 5+ years of relevant analytical experience working with data or MS in a relevant technical field and 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Comfort and expertise communicating effectively with a wide-range of audiences (including product managers, engineers, business development managers, occasionally executives)
	- Strong ability to devise data-driven solutions to business problems
	- Ability to drive impact by thoughtfully tackling open-ended problems
	- Strong data intuition and deep understanding of and experience with statistical and/or ML modeling techniques
	- Strong competency with SQL
	- Fluency in a scripting language (Python preferred)
	- A plus: Experience with large scale data processing tools (Apache Spark) or implementing systems in production at scale
+ skill set:
	- Strong proficiency in Python a necessity, especially the Python data science stack
	- Experience developing data science models, workflows, and software for real world applications and working with imperfect data
	- Exploratory analysis, modeling, and visualization in Jupyter notebooks
	- Integrating data sources, creating subsets (ex. train/test) for modeling, and assessing potential datasets, tools, and approaches
	- Translating the results of analysis into implications for people and problems
	- Developing well-organized code that can be collaboratively reviewed, reproduced, and integrated into applications
	- Quickly assessing and becoming productive in relevant new technologies and methods
	- Experience with core data science tools (ex. pandas, scikit-learn, numpy, jupyter)
	- Experience working with messy data and real-world applications
	- Experience using IaaS like Amazon AWS
	- Working on a small team means doing a little bit of a lot of things. We're looking for somebody who can ask the right questions to figure out what is important, iterate between brainstorming together, working independently, and managing other data scientists, scope new data science projects, and exercise sound judgment to make reasonable decisions under conditions of ambiguity.
	- Communication is a core data science skill at DrivenData. Doing client-facing work involves articulating concepts, interpreting results, and selecting the method that satisfies the constraints of the project.
	- Working familiarity with the tools and practices used in software engineering and deployment (including test-driven deployment, containerization (ex. Docker), platform as a service (ex. Heroku), and infrastructure as a service (ex. AWS, Azure)
+ skill set:
	- Use Python and SQL to draw insights from data at scale
	- Extract actionable insights from broad, open-ended questions
	- Create dashboards and develop metrics to track the success and growth of the product
	- Design and evaluate experiments to measure the impact of product changes
	- Analyze data from across the product to uncover the root causes of metric movements
	- Communicate results to cross-functional stakeholders to inform product decisions
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Improve the work of other data scientists through mentorship and by bringing industry best practices to the team
	- Experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ skill set:
	- Identify new methods to test product changes where traditional A/B testing is not possible
	- Drive adoption of good experimental and statistical practices across the company
	- Apply statistical techniques to increase the efficiency and rigor of our experimental analyses
	- Proactively identify ways to optimize and scale up the way we run experiments, and to increase data scientists' impact in general, and create processes and tools to meet these needs
	- Mentor other data scientists in experimental design and causal inference techniques
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ skill set:
	- Experience in data store design (data lakes; relational, columnar, NoSQL databases, analytics/OLAP cubes)
	- AWS and DevOps experience with AI/ML pipelines
	- Create the vision to build and evolve the team’s data infrastructure and tools. Technically lead for the design, building, and launching of new data models and data pipelines
	- Ensure production quality methods to retrieve, condition, validate, synthesize, and manipulate data
	- Full-stack build custom integrations between cloud-based systems using APIs
	- Continuously refine and improve the data architecture and delivery as new requirements surface
	- Experience ensuring data integrity, security, and encryption
+ skill set:
	- Associate Data Scientist (Internship):
	- VMware Tanzu Labs partners with organizations worldwide to accelerate innovation, while reducing operating costs and risk.  The data science team at VMware Tanzu Labs is primarily a consulting practice; we work with our customers to solve real world problems.  Our customers, like us, are cross-disciplinary. We service engagements with use cases running from customer churn to optimization to detecting fraud and misconduct.  We are not just doers; we are also educators and enablers. programs.
	- You have a passion for exploring data and connecting the value hidden in data with business outcomes and user needs.  You’re agile and retrospective, and not afraid to identify what we’re doing wrong so we can fix it, and what we're doing right so we can improve on it.  You’ll be working on a wide variety of data problems for a diverse range of clients. You will often be asked to learn new technologies and domains on the fly.   Above all, you judge your success by the success of your team and the happiness of your customers.
	- Be given an opportunity to attend fun and educational events to kickstart your career, meanwhile, you’ll get a better feel for our culture. 
	- Get hands-on exposure with cutting edge technology from managers, mentors, and fellow team members.
	- Acquire the tools and knowledge to contribute on a large scale.
	- Have the ability to advance your career in the direction you choose in the future. 
	- While there is no such thing as a “typical day”, these are activities we frequently find ourselves doing the following:
	- Partnering with clients to uncover and frame new opportunities for data science. Clients often come to us without a clear understanding of what we can do, so this is our chance to open their eyes to new possibilities for their businesses.
	- Pairing and writing code together with clients around engineering features, training models, tuning hyperparameters and evaluating results.  We emphasize rigor, because data science done right at this stage leads to models that shine in production.
	- Taking the models we build into production. This is an exciting stage for anyone who likes collaborating with product teams and seeing their model become real when users interact with it.
	- Helping our clients develop their internal data science practices, through mentoring and pair programming, so that they can be successful when we hand off the project.
	- Continuous learning by building demos and prototypes on new technologies, methodologies, and frameworks.  Presentation of learnings and findings for internal audiences, external conferences, and blog posts.
	- Bachelor’s degree in an analytical or quantitative field, or currently pursuing a master’s or doctorate degree in an analytical or technical field (e.g. applied mathematics, statistics, computer science, operations research, economics, data science, etc.). 
	- Clear and empathetic communicator. You’ll be the one sharing your insights with clients and stakeholders. As such, frequent communication and tireless empathy are essential to succeed in this role.
	- Fluent speaking and writing ability in Chinese and English.
	- Advanced knowledge of statistical modeling and/or machine learning methods. These are the technical skills we need to iterate and improve data science pipelines.
	- Strong exploratory data analysis skills. Every engagement starts with an investigation of the data, and thorough EDA saves us a lot of headaches in the long run.
	- Strong programming skills in SQL and Python/R
	- Hands-on experience working in a distributed computing environment
+ ***A project around the areas of big data, searching and statistical modelling***
+ skill set:
	- ***Familiarity with analytics notebooks (Jupyter, RStudio, DataBricks)***
	- ***Strong programming skills and ability to utilize a variety of data/analytic software/languages/tools; e.g., Spark (ML, Mllib, Spark SQL), R (caret, ggplot2), Python (pandas, numpy, scipy, scikit-learn), Scala, Hive, SQL, SAS, Tableau, etc.***
	- Familiarity with cloud computing (in AWS or Azure)
	- Deep knowledge of a variety of statistical and data mining concepts and procedures including: generalized regression, machine learning algorithms, deep learning, media mix algorithms, and statistical graphics
	- Predictive Analytics experience desired
	- Experience with big data- Spark, Hive, Hadoop desired
	- Designing and overseeing implementation of solutions for non-routine problems utilizing a large array of know-how areas within analytics e.g. generalized regression, decision tree, non-parametric; and machine learning, e.g., gradient boosting, random forest, neural networks, clustering, pattern recognition
	- Developing best practices and repeatable processes for routine problems arising in business problems business cases including driving targeted marketing campaigns for tune-in and product adoption, creating an enhanced consumer experiences, and developing digital/social advertising audience segments
	- Assisting with strategic decisions about processes, frameworks and standards
+ skill set:
	- Proficient in SQL/Hive
	- Proven ability to apply scientific methods to solve real-world problems
	- Knowledgeable about the machine learning trade-offs and model evaluation
	- Over 4 years of industry experience with proven ability to apply scientific methods to solve real-world problems on web scale data
	- Ability to lead initiatives across multiple product areas and communicate findings with leadership and product teams
+ Hands-on experience with open source big data platforms (Hadoop, Hive, Presto) and familiarity with data visualization (Tableau, D3) technologies
+ Familiarity with implementing metric logging and interpreting metrics to make decisions
+ Must possess knowledge in SW to Enterprise, SaaS and on Premise, and cloud technologies Elasticsearch for text indexing, MongoDB for structured data storage, Mysql/postgres for SQL storage, JanusGraph for Properties’ based graph storage, ArangoDB for Documents/Key Value/Graph storage, and Amazon AWS.
+ skill set:
	- Deep understanding of machine learning, statistical modeling and data mining techniques such as gradient boosting, neural networks, natural language processing and clustering
	- Understanding of experimental design and adaptive sampling
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Experience working with relational SQL and NoSQL databases
	- Familiar with ML and statistical modeling tools such as R, SparkML, TensorFlow, SciKit
	- Proven track record overseeing multiple data science projects at all stages, from initial conception to implementation and optimization
	- Good programming skills using analytics-oriented languages such as Python, R and Scala
+ skill set:
	- Hands-on experience with AWS (Lambda, SAM, S3, DynamoDB, CloudFormation, EC2, IAM)
	- Experience building and interpreting data models and analytics dashboards
+ skill set:
	- Experience with data quality processes, data quality checks, validations, data quality metrics, definition, and measurement.
	- Ability to operate with cross-functional teams (for example, customer support, data science, engineering, and sales), including a willingness to balance the changing needs of a client-facing team with a demand for data engineering best practices and the ability to communicate the tradeoffs.
+ skill set:
	- Experience with presentation or data visualization software, such as Microsoft PPT, Tableau, Shiny, etc.
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ skill set:
	- Proficiency with statistical programming languages (R, Python, etc.) and proven ability to work pragmatically with statistical concepts
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ Proficiency with machine learning and statistical modeling (e.g., scikit-learn, TensorFlow, Stan)
+ Experience identifying data quality and developing automated QC checks and/or reports
+ A senior analytics professional with a proven track record of data analysis, reporting and visualization (e.g. Tableau, D3)
+ skill set:
	- Use Databricks to build internal data warehouse and integrate it with BI and CRM services used internally
	- Use Databricks to analyze usage data, and create dashboards and reports
	- Build self-serving internal data products to make data simple within the company
	- Work closely with Product Management and other stakeholders to understand product usage patterns and trends and to make data-driven decisions and forecasts
	- Provide product feedback to PM and Engineering teams
	- Strong desire to work at a rapidly growing startup
	- Knowledge of data processing and applied statistics
	- Proficient in data analysis and visualization using R or PyData
	- Familiar with SQL and databases like MySQL or PostgreSQL
	- Experience with distributed data processing systems like Spark and Hadoop
	- General-purpose languages such as Python and Scala
	- Desire to explore lots of data to find unexpected insights
	- Strong communication and presentation skills
	- [Plus] Advanced degrees in statistics, computer science, math, or similar fields
	- [Plus] Familiarity with interactive data visualization using tools like D3.js
+ Research, evaluate, and present statistical or Machine Learning methods to provide actionable insights.
+ Enforce SOX & GDPR compliance across the analytics database and reporting tools
+ Good grasp of statistical concepts (e.g. hypothesis testing, regression)
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ **Experience and knowledge of programming languages, data analysis packages (e.g., Python, R, SAS, MatLab, Stata, GAMS, SPSS, Hadoop, BigML, Pandas). Experience and knowledge of visualization tools (e.g., Tableau, Sigma JS) is preferred.**
+ Working knowledge of data analysis packages (e.g., SAS, MatLab, Stata, GAMS) is strongly preferred.
+ skill set:
	- analytical mindset and strong experience with data-driven product decisions using analytics tools, such as:
		- Amplitude
		- Retool
	- AI-driven insights and recommendation engines to help creators build better businesses
	- category defining analytics platform to help creators better understand their audience
+ Expertise in statistical inference including experimentation and observational methods to causal inference
+ Strong coding experience. Experience with open-source ML packages (specifically sklearn, TensorFlow/Keras/PyTorch).
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
	- Experience in optimization algorithms and numerical computation
	- Experience with Spark, TensorFlow, or Keras
	- Experience with cloud computing platforms and large web-scale distributed systems
	- Open source contributions
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Optimization algorithms and numerical computation
	- Spark, TensorFlow, or Keras
	- Cloud computing platforms and large web-scale distributed systems
+ Expertise in additional statistical methods (e.g., Bayesian approaches, dyadic analysis, causal inference approaches, factor analysis, SEM)
+ Experience with Databricks or Spark, EMR
	- Databricks: data lakehouse architecture, cluster management for computer clusters (in the context of cluster computing), and associated machine learning technology
	- Adobe Spark, or Spark: open-source unified analytics engine for large-scale data processing
		* large-scale, multi-language engine extreme data engineering, data science, and machine learning on single-node machines or clusters
		* for the following tasks:
			+ machine learning
			+ data science, or data analytics, at scale
			+ batch/streaming data
			+ SQL analytics
		* for programming languages:
			+ Python
			+ SQL
			+ Scala
			+ Java
			+ R
	- Amazon Elastic MapReduce, Amazon EMR
		* for applications in:
			+ machine learning
			+ extract, transform, load (ETL)
			+ real-time streaming
			+ genomics
			+ clickstream analysis
		* accessed using:
			+ AWS Command Line Interface, AWS CLI
			+ AWS Console
		* Not EMR, electronic medical record = electronic health record
+ build multivariate simulation data pipelines on a cloud for integration tests
+ skill set:
	- experience with defining, implementing, and analyzing A/B and multivariate experiments
	- capacity as data analysts to work with data engineers and data scientsts as appropriate to design, scope, and work through new projects
	- statistical modeling and analysis in R and/or Python
	- reporting and analysis using major Web analytics tools, such as:
		* Google Analytics
		* Adobe Analytics
		* Parse.ly
		* comScore
		* SimilarWeb
	- analyze data and build dashboards using business intelligence, BI, tools, such as:
		* Data Studio
		* Looker
		* Tableau
	- use SQL-based database applications for handling large data sets:
		* Snowflake
		* BigQuery
		* Athena
		* RedShift
+ A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
+ skill set:
	- B.S., M.S., or Ph.D. in a quantitative field
	- 4+ years work experience in an analytical or quantitative role as a Data Scientist
	- 2+ years experience working on product analytics in a two-sided marketplace
	- Extensive experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
	- Active Quora user with curiosity about the product
	- Deep experience with MySQL, NoSQL data stores like HBase or similar
	- Strong grasp of Configuration Management (Chef, Puppet, Ansible, Salt Stack)
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ ***Experience with big data techniques (such as Hadoop, MapReduce, Hive, Pig, Spark)***
+ skill set for data science:
	- ***Technical mastery in one or more of the following languages/tools to wrangle and understand data: Python (NumPy, SciPy, scikit-learn), Spotfire, Tableau.***
	- ***Experience with Spark (MapReduce, PIG, HIVE)***
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
+ data science:
	- Knowledge of ElasticSearch/Solr/Lucene is a big plus.
	- Understanding in Java server platform and system tuning is a plus.
	- Knowledge with vector space models, text classification and categorization.
	- Implement high-quality code in an agile software development environment.
+ data science skill set:
	- Implement scalable algorithms and services using technologies such as Scala, Akka, elasticsearch, Kafka, Cassandra and Hadoop technologies such as Hive, Spark or MapReduce
	- Hands-on experience in analyzing large datasets (e.g. with SQL, Python, R, Hive, etc.)
	- Some knowledge and experience in working with technologies such as Kafka, Cassandra, Elasticsearch, Akka, Kubernetes, etc.
	- Experience in Scala or Java is a plus
	- You are fluent in English; German skills are a plus
+ skill set:
	- Python
	- R
	- SQL
	- Jupyter Notebooks
	- Tableau
	- Looker Studio
+ skill set:
	- Data Scientist
	- Algorithmically agile including experience with a variety of classical and deep learning based machine learning capabilities. Should be familiar with packages like scikit-learn, keras, spacy, open cv, tensorflow, pytorch.
	- Strong programming skills including at least 1 dynamic (preferably Python) and 1 compiled language (e.g. Java, Scala, C++).
	- BS or advanced degree (MS or PhD preferred) in computer science, engineering, or related technical field.
	- Great communication skills including the ability to communicate directly with customers communicating challenging machine learning concepts. Ability to create great data visualizations is necessary.
	- Experience working in a distributed computing environment.
	- Published academic work in core machine learning (ML) and/or open-source NLP software contributions.
	- Experience developing, testing, and shipping production software.
	- Knowledge of cloud services in AWS, for example EC2, S3, etc.
	- Familiarity with distributed source control management system such as GIT.
	- Experience accelerating startups and working on green space problems.
	- Passionate about building innovative products that delight customers
	- Excited to work on multiple ML problems in a variety of domains with different data types
	- Fun to work with and thrilled to make an impact
	- The type that doesn’t whine or complain, but rather tries and instigates change or disagrees and still commits
	- Thrilled to work on a small team of full stack data scientists
+ skill set:
	- Data Scientist, Generative AI
	- Scale is looking for a data scientist to join our team to help advance the development of AI. As a member of the data science team, you will lead the charge of building our data science infrastructure for Gen AI products and driving insights that lead to step-function improvements in how we operate. The ideal candidate is detail oriented, rigorous about validating results, talented at distilling down complexity, and loves tackling and solving hard problems.
	- Build evaluation frameworks to measure LLMs efficacy, ground truth dataset quality, and guide product development roadmap
	- Adapt statistical models to solve specific hard problems in fields of economics, price theory, and marketplace experimentation
	- Be a proactive partner to your business stakeholders and provide insights and conclusions rather than just data outputs/models
	- Tackle business-critical questions by developing and testing hypotheses, and aid evidence-based decision making
	- Partner with Product Managers, Data Engineers, Data Scientists, and Business Stakeholders to drive business decisions and product roadmaps
	- 4+ years of industry experience in a highly analytical role
	- Degree in a quantitative field (e.g., Maths, Engineering)
	- Expert-level proficiency in writing complex SQL queries across large datasets
	- Experience NLP/NLU models
	- Strong proficiency in python, experience in writing production-grade code
	- Expertise in designing metrics and diagnosing data inconsistencies
+ skill set:
	- Experience working with non-Transact-SQL databases and SQL dialects, preferably Oracle PL/SQL or Cerner Command Language
	- Experience with large-scale data analysis systems, such as Databricks, Hadoop, Pig, Scala, Spark or MPP databases
+ skill set:
	- The role’s primary technologies include Graphistry, Cypher+SPARQL, Python, Jupyter, StreamLit, pandas / RAPIDS.ai, Docker, React, and Linux/AWS/Azure. Of additional interest are graph neural networks, huggingface, mlflow, networkx/graphx, Tableau/PowerBI, and SQL.
	- Required: Comfortable with pydata - Python, Jupyter, pandas, viz, cloud (Linux/AWS/Azure)
	- Ideal: Experience with graphs - databases (cypher/sparql), algorithms, visualizations, & neural networks; Dashboarding (StreamLit/Ploty/Tableau/PowerBI/...); GPU PyData (RAPIDS, Tensorflow, numba, ...)
	- Required: Comfortable with coding (Python, minor HTML/JS/CSS)  and/or ML (hands-on or courses around linear algebra, statistics, classical ML, neural networks)
+ skill set:
	- You will collaborate with our users, technology partners, and Graphistry staff as part of a new locally-based team and growing community. Together, you will augment security data platforms with Graphistry’s graph AI and GPU visualization capabilities. Due to your privileged access, you will be a key innovator and implementer. The role provides significant room for growth, ownership, and startup experience. You will have day-to-day flexibility and competitive compensation.
	- Develop, deploy, and iterate on visual and automated security AI solutions for detection, correlation, recommendation, prioritization, and investigation
	- Learn, develop, integrate, and scale using graph neural networks, end-to-end GPU acceleration, vector search, UMAP, and other AI technologies in the Graphistry ecosystem, including Nvidia RAPIDS, Nvidia Morpheus, DGL, PyTorch, and PyGraphistry[AI]
	- Share knowledge through public speaking opportunities, private trainings & presentations, customer demos, and tutorials & blog posts on topics such using the Graphistry ecosystem and workflows you build on top to tackle problems like cybersecurity investigations, anti-misinformation, and anti-fraud
	- Enjoy Python and PyData ecosystem: Linux/Docker, notebooks, dataframes, visualization
	- Experienced with neural networks, time series analysis, classical machine learning, underlying math, and computing over GB+ datasets
	- Experienced with cybersecurity use cases, or equivalent digital forensic scenarios like fraud or sigint, such as for anomaly detection, entity fingerprinting, and analyzing machine logs from large systems
	- Developed and deployed security AI models in operational settings
	- Used GPU computing, including in bigger-than-memory settings
	- Used graph technologies like graph databases, graph analytics, graph visualization, and graph AI, and especially in at-scale detection and investigative scenarios
	- Strong communicator in-person, in writing, and in presentations
	- History of project leadership & ownership from both a technical perspective and business
	- Graphistry is a visual graph AI startup that spun out of UC Berkeley to accelerate investigation processes. It is a Gartner Cool Vendor and features the first GPU-accelerated visual graph analysis environment, low-code investigation automation, autoML for deploying graph AI, and integrates with popular data science & database technologies. Profitably growing 3X+ year-over-year, Graphistry is helping enterprise, tech, and public sector teams begin their modern graph journey, especially around digital crime and commercial analytics. It helped start the modern GPU dataframe compute movement (Apache Arrow, Nvidia RAPIDS) and is working to scale & democratize graph neural networks.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.



















Sets of skills for more senior data science roles, such as senior data scientists and management of data science teams (i.e., data science managers):
+ skill set:
	- At the heart of Netflix Product Innovations is an experimentation driven culture led by Science & Analytics (S&A).  In this role, you will lead teams of data scientists and analysts responsible for shaping UI and Content Innovations decisions through experimentation (A/B, quasi) and empirical studies to guide product strategy.
	- Set an impact-focused, strategic science roadmap to guide product innovations.
	- Recruit and inspire exceptional data scientists focused on the span of causal inference, behavioral research, and analytical activities.
	- Uphold the culture of rigor in product decision-making through active participation in product debates.
	- Lead and contribute to cross functional initiatives between product development (product management, design, engineering), content, and marketing.
	- Define a team culture that balances supporting high impact business needs with forward looking research.
	- Serve as thought partner to product development executives across product management, engineering, and design.
	- 5+ years experience in building and inspiring a high-performing data science and analytics team.
	- Capacity and passion to translate business objectives into actionable analyses, and analytic results into business and product recommendations
+ skill set:
	- Starsky Robotics is looking for a full-time Senior Data Scientist. Your job will tackle a wide variety of problems in autonomous vehicles. From finding every time a car cut in front of our truck, to figuring out how to report on the quality of autonomous driving, to creating new tools and statistical methods for robotics engineers to characterize the behavior of their systems, we're looking for someone motivated to attack self-driving problems with mounds of data. Tackling these problems will require learning about the whole suite of robotics fields applied to make autonomous vehicles: motion planning, controls, perception, and behavior planning.
	- You'll own high-level decisions such as “How do we determine if a route fits our current driving capabilities”. Day-to-day projects may have mission statements as technical as “Help us solve this spike in cross-track error on curves”, or as business-focused as “Can we get a heatmap of all the places our trucks have driven over the last year”.
	- Additionally, you can bring best-practices for data-science to the company, including helping build up the base platform and infrastructure necessary to speed up data-centric work. Starsky has a solid base of tooling around our data, but it is ripe for improvement.
	- Demonstrated expertise in the data scientists modern toolkit: Pandas, R, SQL, etc, and don't mind sharing your experience with the team
	- Deep quantitative thinker: Masters or PhD in a quantitative field, or multiple years of experience in a quantitative-focused position
	- Relish delivering answers and metrics and seeing change affected by your work
	- Can take high level directives and take them through from research project, proof of concept, to applied & implemented feature.
	- Are constantly looking for problems that could be solved with liberal application of data
+ skill set:
	- AIML - Head of Data Engineering, Data & Machine Learning Innovation
	- Are you passionate about data, data quality, efficiency, and scale?  Would you like to play a critical part in accelerating data-informed product development to drive innovations and amazing user experiences?
	- As the Head of Data Engineering, you and your team will be responsible for the crucial role of designing, operating, and supporting the increasingly complex environments that power modern data analytics and machine learning use cases for Apple products such as Siri, Dictation, Translations, Apple Search and others.
	- You will partner closely with cross-functional teams and be responsible for the planning, execution, and success of technical projects as well as help drive scalable practices to accelerate the evolution of our products. You will lead existing data resources, build new datasets and data pipelines, and implement new technologies and tools to enable rigorous scalable science and analytics, and data-informed product development. Your team and vision will enable data to inform product engineering teams at scale, with the ultimate purpose of improving the product experience for Apple users across hardware, software, and services, while preserving Apple’s privacy values. You will drive a strong culture of data excellence!
	- 10 years in a managerial role, growing and scaling organizations
	- Experience in multiple large-scale data processing systems, data telemetry, data-driven performance and reliability improvement across platforms and products, the data foundation for advanced machine learning problems, and data/analytics.
	- Demonstrated passion and leadership in data engineering, data-informed product development, building and shaping data culture
	- Proven ability to proactively drive a data engineering agenda and prioritize work based on impact and strategic investment
	- Experience with modern data engineering stack, processing technologies, data, and analytic pipelines, including batch (Spark, others), streaming systems (Flink, etc.), and scalable query engines (e.g. Trino, Druid, Pinot, etc.)
	- Solid understanding of both relational and NoSQL database technologies and experience in engineering datasets and metrics out of massive and complex data
	- Expertise in data engineering partnering with analytics and ML teams (comprised of data scientists and statisticians) to enable horizontal and vertical use cases
	- Demonstrated ability to establish strong partnerships and drive processes across organizations, e.g. to manage the creation, processing, and use of instrumentation to enable using data impactfully in developing products
	- Outstanding communication and presentation skills, written and verbal, to all levels of an organization
	- Proactively drive the vision for Data Warehouse, Data Platform, Analytics, and ML analytics to accelerate the building and improvement of our products, and define and execute a plan to achieve that vision.
	- Grow and scale a large, established data engineering team and build strong relationships with partner teams
	- Drive the design, building, and launching of new data models and data pipelines in production
	- Stay focused and prioritize a heavy workload while achieving exceptional quality and driving long-term vision
	- Bachelors or Master in CS, Engineering, Math, Statistics, or a related field, or equivalent practical experience in engineering
+ skill set:
	- The FSQ BI app manager will lead a small team of Backend and Frontend engineers to build and develop the FSQ BI application. The candidate should have a relevant background in building analytic dashboard products, with an understanding of basic SQL, charts, and maps.
	- Manage, lead, and coach a team of backend and frontend engineers to develop the FSQ business intelligence application
	- Own the team's technical roadmap and direction, working closely with Product Management, Program Management, Client Success, Engineering, and stakeholders to achieve company goals
	- Lead the development of a scalable and robust application stack for delivering business intelligence dashboards
	- Contribute hands-on engineering solutions as needed
	- Partner with engineering, product teams, and department stakeholders to drive forward broader engineering and company initiatives
	- Recruit talent to grow the team in line with Foursquare company growth and priorities
	- Proven experience with business intelligence application development, and building data analytic products
	- In-depth knowledge of data analytic concepts, tools, and methodologies
	- Hands-on experience with data analytics tools (e.g., Tableau, Power BI, SQL, Python, etc.)
	- Familiarity with data modeling, ETL processes, and data warehousing techniques
	- Knowledge of data visualization, charts, and maps
	- Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and data warehouses (e.g., BigQuery, Snowflake)
+ skill set:
	- Data Science & Engineering Manager - Auction/Yield, Ad Platforms
	- At Apple, we work every single day to build products that enrich people’s lives! Our Advertising Platforms group makes it possible for people around the world to easily access informative and visionary content on their devices while helping publishers and developers promote and monetize their work. We are a team of passionate scientists and technologists, dedicated to helping publishers and developers find their audience, and changing the way advertising works with data. Our technology and services power advertising in Apple News and Search Ads in App Store. Our platforms are highly-performant, deployed at scale, and setting new standards for enabling effective advertising while protecting user privacy. Our auctions power rapidly growing marketplaces and are critical to the health and diversity of our ecosystem.
	- We are looking for an exceptional data science and engineering manager who can thrive in a fast-paced environment and is highly knowledgeable in the space of auctions and marketplace optimization for online advertising. You will lead teams and systems that drive marketplace optimization, budget pacing, pricing, and winner selection algorithms. You will drive the conception, development, and delivery of innovative capabilities that differentiate our products and are core to our business. You should have experience developing and implementing machine learning algorithms, ideally within the ads space. You will have an excellent understanding of scalable architectures and thrive working in Agile environments.
	- Recognized expertise in the areas of marketplace optimization and auction theory.
	- Demonstrated success mentoring and growing a team technically as well as developing growth plans for its members.
	- Ability to apply and implement research concepts, ultimately in production quality code.
	- Experience defining clear, testable research hypotheses, including intended impact on the business.
	- Deep knowledge of design of experiments, online experimentation approaches, preferably at scale.
	- Ability to formulate and advocate for R&D objectives and results to multi-functional team members including executive business leadership and product management.
	- Experience contributing and/or reviewing research for top conferences and publications.
	- Experience owing real time production systems.
	- Deep fluency in Java or Python.
	- Experience with Spark, Hadoop or other distributed frameworks.
	- You will have the opportunity to work on optimizing a marketplace that serves ads millions of times a day. You will own the auction systems and marketplace optimization framework which have strong requirements for both technical SLA performance and delivering a fair and vibrant marketplace for discovery. We operate a platform with extreme scale requirements and value. 
	- You will, in partnership with our product management colleagues, propose, design and analyze new features and algorithms to improve the performance of our auctions. You will work closely with operational teams on deployment, monitoring, management concerns. You will participate and lead Apple internal Data Science & Machine learning interest groups, meet ups, and conferences. You will join a team passionate about data privacy, fairness, and trust, with profound responsibility to ensure those things, while protecting the value our customers and developers count on from our advertising products for.
	- PhD in a quantitative field (or BS/MS with equivalent work experience) with 5+ years experience in managing a data science team.
+ skill set:
	- Senior Data Scientist (Remote)
	- Quora is a "remote-first" company. This position can be performed remotely from anywhere in North America. Please visit careers.quora.com/eligible-countries for details regarding employment eligibility by country.
	- Quora’s mission is to grow and share the world’s knowledge. To do so, we have two knowledge sharing products:
		* Quora: a global knowledge sharing platform with over 400M monthly unique visitors, bringing people together to share insights on various topics and providing a unique platform to learn and connect with others.
		* Poe: a platform that lets people ask questions, get instant answers, and have back-and-forth dialogue with various AI language models (bots). As AI capabilities rapidly advance, Poe provides a single platform to instantly integrate and utilize these new models.
	- Behind these products are passionate, collaborative, and high-performing global teams. We have a culture rooted in transparency, idea-sharing, and experimentation that allows us to celebrate success and grow together through meaningful work. Join us on this journey to create a positive impact and make a significant change in the world.
	- The Data Team is highly empowered at Quora, helping navigate complexity and influencing product and company strategy directly. Quora's outsized commitment to data is visible in everything we do, from our rigor-driven experimentation processes to the backgrounds of our leaders. With this emphasis on data and empirics, we aim to balance rigor and pragmatism, searching for scrappy solutions in pursuit of our mission. In joining Quora's strong data team, you'll both benefit from and work to advance our culture of rational decision making.
	- As a member of our team, you'll work closely with product managers, product designers, and engineers to devise appropriate measurements and metrics, design randomized controlled experiments, build visualizations, and tackle hard, open-ended problems that uncover usage patterns and opportunities for the company. Quora has a wide range of rich data, giving you ample room for exploration and creativity. Examples of some projects our data scientists have worked on include modeling our long-term growth, improving the relevance and personalization of the homepage feed, and exploratory analysis of factors driving question-asking behavior. For more about our work, see Quora's Data blog at https://data.quora.com/.
	- As a Senior Data Scientist, you will draw on your experience to excel not only in your own work but also to elevate data’s impact at a company-wide level. You will provide team mentorship that propels the work of your colleagues while helping to establish best practices for data usage across Quora.
	- Extract actionable insights from broad, open-ended questions
	- Design and evaluate experiments to measure the impact of product changes
	- Analyze data from across the product to uncover the root causes of metric movements
	- Communicate results to cross-functional stakeholders to inform product decisions
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Improve the work of other data scientists through mentorship and by bringing industry best practices to the team
	- Ability to be available for meetings and impromptu communication during Quora's “coordination hours" (Mon-Fri: 9am-3pm Pacific Time)
	- 3+ years work experience in an analytical or quantitative role as a Data Scientist or similar title
	- 2+ years experience working on product analytics
	- Extensive experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
	- Active Quora user with curiosity about the product
	- At Quora, we value diversity and inclusivity and welcome individuals from all backgrounds, including marginalized or underrepresented groups in tech, to apply for our job openings. We encourage all candidates who share a passion for growing the world’s knowledge, even those who may not strictly meet all the preferred requirements, to apply, as we know that a diverse range of perspectives can have a significant impact on our products and our culture.
	- US candidates only: For Colorado based applicants, the minimum base salary range is $122,000 - $171,000 USD + equity + benefits. For California, New Jersey, New York, and Washington based applicants, the minimum salary range is $122,000 - $201,000 USD + equity + benefits.
	- British Columbia Candidates Only: For British Columbia based applicants, the minimum base salary range is $122,000 - $201,000 CAD + equity + benefits.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












####	Data Visualization


+ data visualization with the following tools and/or libraries:
	- Plotly
	- Tableau
	- PowerBI
	- Qlik
	- Google Charts
	- d3.js
	- plotly.js
	- Angular.js
	- PowerBI
+ Producing effective and interactive data visualizations (Tableau, Shiny, D3)
+ Bonus points for experience building interactive data visualizations using libraries like D3, Highcharts, and Leaflet, and for experience working with big data systems like Hive, Hadoop, Scalding and Spark.
+ illustrate visualization ideas using storyboards, process flows, wireframes, and prototypes
+ [Plus] Familiarity with interactive data visualization using tools like D3.js
+ metrics systems:
	- ***Grafana.***



















####	Data Science for Social Media Companies



+ skill set:
	- The Consumer Data Science organization works closely with our cross-functional partners, and Twitter's leadership to understand user behavior, inform product decisions, safeguard the health and integrity of our services, and to influence company strategy. We are currently hiring for the following subteams on Consumer Data Science. These high-impact teams value creativity, critical thinking, and teamwork.
	- The Consumer Data Science organization is hiring Senior Data Scientists in the following areas:
		* Health (SF, Boulder) - The goal of this team is to improve the health of the public conversation, ensuring that users feel safe while using our platform. As part of our team, you'll help the Health Organization make strategic decisions that ensure that Twitter is a safe and informative experience for our customers. You will do this by performing and mentoring others through analyses, metrics, experimentation, research, and more.
		* Growth (SF) - Their mission is to increase Twitter's daily utility for new and returning users through impactful and creative applications of experimentation and data analysis. As a key member of Growth Data Science, your work will directly influence exciting new product areas and help grow Twitter usage around the globe.
		* Metrics (SF) - This team works to support company strategy by helping to define, maintain, and understand key success metrics to ensure that we continue to meet the demands of our customers.
		* Video (NY) - This team works with the Live Video, Video on Demand, Publishers, and Camera products. The team is involved in opportunity sizing, experiment setup and analysis, and metric design in order to influence video and media strategy at Twitter.
	- Support the entire product development lifecycle from product ideation to opportunity sizing to measurement design to experimentation and causal analysis to post-launch learning and iteration into the next development cycle.
	- "Design and implement experiments or other econometric methods to understand how changes to the platform affect user behavior."
	- Build novel metrics, identify the impact of product and policies, and study causal impact of our Product launches and Health initiatives.
	- Work in tandem with team members, applying advanced statistical methods; writing complex data flows using multiple languages/frameworks such as SQL, Scala (Scalding, Spark), Python; and using data visualization tools.
	- Communicate findings to executives and cross-functional stakeholders.
	- You are a self-starter who is capable of learning on the job, taking initiative, and thriving within a large team.
	- You are excited to learn and apply new data analysis techniques and tools. You are passionate about insights, not just data and methods. You are a strategic thinker and are able to synthesize technical concepts into actionable recommendations for a diverse audience.
	- You communicate your findings clearly and effectively to a wide audience of relevant partners and are capable of building meaningful presentations and analyses that tell a story.
	- You are rigorous, care about data quality, and strive to understand surprising results and underlying mechanisms in your analyses. You combine business insight with detailed data knowledge and statistical expertise to ensure an accurate interpretation of results.
	- You are a capable mentor. You enjoy knowledge sharing and working with junior teammates to up-level their skills and take the time to learn from them.
	- You value teamwork and teammates. You contribute positively and meaningfully to cultivate an inclusive team culture. You are personable, empathetic, and able to connect with each and every person on the team and throughout the company.
	- Experience using SQL, R, or Python for analysis, modeling, and data visualization.
	- 5+ years experience working with and analyzing large datasets to understand behavior, solve problems, and answer business questions.












###	Business Analytics



This subsubsection includes the following topics of business analytics, except financial analytics which is found in the following *Markdown* document for [Financial Engineering, Computational Finance, and Financial Analytics]().
+ [Marketing Analytics]()
+ [Human Resource Analytics]()
+ [Data Science for Logistics, Supply Chain Management, Industrial Distribution, & Retail Sales]()




Skill sets for business analytics:
+ skill set:
	- Minimum of 3 years' delivery experience in advanced modeling environment: strong understanding of statistical concepts and predictive modeling. (e.g., AI neural networks, multi-scalar dimensional models, logistic regression techniques, machine-based learning, big data platforms, SQL, etc.).
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
	- Possess a blend of marketing acumen, consulting expertise and analytical capabilities that can create value and insights for our clients.
+ You're familiar with business intelligence reporting platforms like OBIEE, Tableau, MicroStrategy, and Business Objects
+ skill set:
	- 2+ years SQL working experience (Redshift/PostgreSQL/MySQL)
	- Experience with BI & reporting dashboards (Periscope, Tableau, etc)
+ skill set:
	- enterprise resource planning, ERP
		* ***business intelligence***
			+ customer relationship management, CRM, customer services
			+ sales:
				- invoicing
				- order placement
				- order scheduling
				- shipping
		* e-commerce, electronic commerce
			+ product lifecycle management, PLM
				- planning
				- optimizing manufacturing capacity and material resources
				- manufacturing resource planning, MRP
					* material requirements planning, MRP
			+ supplier relationship management, SRM
				- maximize cost savings with support for the end-to-end procurement and logistic processes
		* enterprise asset management
			+ corporate performance and governance
			+ human resource
		* industrial distribution, logistics, supply chain management, SCM
		* accounting
			+ financial operations
			+ regulatory compliance
+ skill set:
	- SQL
	- Snowflake, Redshift, BigQuery
	- marketing platforms:
		* Facebook Ads
		* Google Ads
		* Appsflyer
		* SA360
		* CM360
		* TikTok ads
	- GeoXExperience
	- Looker, Mode, Tableau
+ PowerBI, Tableau, Qlikview






















####	Marketing Analytics




+ Use marketing technology such as Hubspot, Salesforce, WordPress, Sendoso, and Go-To-Webinar
	- While this list is provided for a marketing position, it indicates technologies people in marketing analytics can build for people on these platforms.







####	Human Resource Analytics









####	Data Science for Logistics, Supply Chain Management, Industrial Distribution, & Retail Sales


+ skill set:
	- At least 5 years demonstrated results in areas of Operations Research and/or Supply Chain Projects (inventory optimization, network design, and S&OP) in sophisticated and complex environments including the use of simulation and modeling tools (Llamasoft, CPLEX, Gurobi, or other similar)
	- At least 5 years performing data analytics and modeling with advanced languages (e.g. Python or R)






####	Data Science for Economics



+ skill set:
	- B.S. or M.S. in Economics, Statistics, or a similar field and 1+ year work experience in data science or analytics, or Ph.D. in a quantitative social/behavioral science (e.g. Economics, Sociology, Psychology, Statistics, or a similar field)
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Familiarity with Python or similar scripting language
	- Experience communicating technical statistical concepts clearly, for example, teaching or consulting
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework










###	Sports Analytics or Data Science for Sports









###	Data Science for Public Health










###	Data Science for Health Companies & Other Organizations


+ skill set:
	- At least 2 years designing and building healthcare data analysis solutions for the business payer or provider industry
	- At least 2 years using new developments in AI, machine learning, cognitive systems, and robotics to build amazing analytical tools
	- At least of 2 years working with tools like SAS, Python, SPSS, R, or SQL
	- At least 2 years working with data integration tools to streamline processes in platforms like Cerner EMR, Apache Spark, MapReduce, MongoDB and Couchbase
	- You can use data mining techniques to solve real world business problems
+ tech stack:
	- Hadoop ecosystem and its components.
	- Hadoop, Hive, HBase, and Pig
	- Working experience in HQL
	- Pig Latin Scripts and MapReduce jobs
	- Hands-on experience in backend programming, particularly Java, and Node.js
	- Analytical and problem-solving skills
+ skill set:
	- knowledge of:
		* clincial trials
		* digital health tools
		* employee's health management
		* health care system in other countries
	- management of health care real-world data, RWD
	- experience in defining/creating requirement documents (e.g., PRD) of health care product/service, launch them, and improve in an agile way 
















###	Data Science for Advocacy, Lobbying, Think Tanks












###	Data Science for Legal Informatics & Computational Law


This subsubsection includes skill sets for data science roles in legal services, including:
+ legal informatics
+ computational law











###	Data Science for Semiconductor Manufacturing





###	Data Science for Computational Science and Computational Engineering (except EDA)




Skill sets for data science roles in computational science (or scientific computing) and computational engineering (except EDA):
+ skill set:
	- Imagine a super-resolution microscope so easy to use that anyone on earth can take high-resolution images of bacteria, proteins, cells, and even genes. Imagine the same microscope accelerating cancer research, pharmaceutical development, and virus identification by empowering scientists with real-time, nanoscale imagery of cells and proteins. We’re building this technology at ONI!
	- Our aim is to make super-resolution imaging so easy and the insights so impactful that it becomes widely used by scientists and leads to radical discoveries and innovations. To achieve this goal, our platform will automate every stage of the workflow, integrating the Nanoimager, a microfluidics device (Roboflow) and an online analysis package (CODI) with next-gen super-resolution assays and application-specific microfluidic consumables. 
	- We will soon be launching our first kit which is designed for extracellular vesicles. With one click, the kit captures, images and analyses these tiny particles allowing researchers to characterise their biomarkers for the first time. We are also developing a revolutionary new super-resolution technology called Every Molecule Counts (EMC) that will give our customers a new level of confidence about the biology underlying their images. EMC technology will be integrated into all of our future consumables.
	- To drive all this innovation we have built a world-class R&D team with colleagues from disciplines including biology, computer science, mathematics and physics. A core principle behind our work is to be detailed in our thinking, but to make products that are simple and intuitive so they can be put in the hands of anyone irrespective of experience, background, or training. We are excited to welcome new team members who share these values and who are excited by our vision.
	- Currently employing a diverse team of 120+ people, representing over 40 nationalities, ONI is in a period of rapid growth. We closed our $75m Series B round at the start of 2022, led by ARCH Ventures and Casdin Capital, putting our post money valuation at c. $225M, to drive the development of our next generation of products. What we have achieved so far is just the beginning and we are always looking for passionate people to join us on this journey.
	- We are looking for a Senior Software Engineer who will support the team that develops and deploys leading edge data analysis tools and solutions to the users of our microscope. You will develop creative methods that can extract information from these results and use software engineering skills to consolidate these methods into usable tools. This role is a fantastic opportunity to work on ground breaking applications for super-resolution and single-molecule microscopy as well as to help shape this new diverse team.
	- As an experienced member of the team, you will be at the heart of developing new tools and techniques for super-resolution data analysis. You will take a leading role working closely with Data Scientists, Software Engineers, Application Development Scientists, and Hardware R&D, to create a bridge between the back-end and the data science team, mentoring, coaching and empowering data scientists, and helping provide infrastructure and system support to help supercharge the work of the data science team, and increase the delivery of code into the cloud platform.
	- Drive communications between data scientists and backend cloud developers to deliver fast and effective research tools
	- Be confident in applying domain specific knowledge to the development of algorithms.
	- Have a strong background in mathematics, probability, and statistics.
	- Be able to work independently or as part of  a team. 
	- Be comfortable with ambiguity and complexity, thrive in group discussions and be an able communicator with colleagues with diverse technical backgrounds.
	- Have a passion for learning, working in a team, teaching others & making an impact on the world.
	- Greater than 4 years experience as a software developer in a professional environment.
	- Have an academic background (BSc, MSc, or PhD) in Computer Science, or STEM related fields. 
	- Strong familiarity with the python ecosystem, and available tools, and best practices of python in the context of both Data Science and delivery of production code. 
	- Strength in packaging using pypi, and optionally conda, or alternatives, including compiled C++ accelerated bindings (Pybind11) across platforms.
	- Strength and familiarity using Cmake to build C++ projects across Linux and Windows platforms. 
	- Strong understanding of C++ ecosystem, standards, and comfortability navigating and learning unknown code-bases. 
	- Strength in maintenance and establishment of CI/CD workflows for python and C++, including integration testing and deployment, using common solutions such as Github Actions or CircleCI.
	- Experience utilizing Docker to enhance development, testing, and deployment.
	- Sound knowledge of basic mathematics and statistics concepts
	- Confidence communicating between scientists and backend cloud developers
	- GPU acceleration using CUDA.
	- MLOps; with deployment and or training in cloud infrastructure, or deployment of C++ compiled models in real-time.
	- Image processing experience. 
	- Strength in development and implementation of computationally intensive numerical algorithms. 
	- Experience with cloud and web-based solutions and toolings, including tools such as Django, Kubernetes, Celery, RabbitMQ, on platforms such as GCP, or AWS
+ skill set:
	- ONI is looking for a highly motivated and creative scientist to join our Applications Development team as a Data Scientist. The Applications Development team invents new, cutting-edge molecular biology techniques based on single-molecule microscopy, and converts them into integrated bioware products that enable our users to access the full potential of super-resolution imaging. This work spans a diverse range of biological and technological fields, firmly founded in advanced fluorescence microscopy.
	- The work of the Applications Development team demands creative, innovative ways to process and analyse imaging data of many formats - including localisation-based point clouds, single-particle tracking information, and pixel-based data. As the Data Scientist within the team, you will be responsible for identifying and developing the most effective and efficient ways to analyse data generated in the course of our developmental work. In the process you will bring new analysis tools into the company, and design bespoke tools that are unique to ONI. The insights that we need to extract from our data are highly dynamic and change according to the state of each project, and so this role is well-suited to a talented Data Scientist who likes solving problems creatively and rapidly. 
	- This role is open to candidates able to work in our Oxford, UK office and will involve interaction with scientists at both of our research centres. You will work across several research projects involving multiple development teams, and fulfil a vital function within the company by pushing the boundaries of the insights we can derive from our imaging technology.
	- Take a leading role in defining how data are analysed within Applications Development’s product- and technology-development process. This is through the development of new analytical methods to unlock previously unobtainable information within our data; advising on the most appropriate analytical approaches to take; and transferring the most cutting-edge analyses into the team
	- Directly handle the processing of some data collected by the wider team as part of our ongoing developmental work
	- Assist and support other members of the team in their data analysis 
	- Bridge ONI’s Applications Development and Software teams to ensure that requirements and strategies are fully understood by scientists with diverse biology and computing backgrounds
	- Contribute to ONI’s ongoing research projects as an integral and intellectually-invested team member
	- Contribute positively to ONI’s mission beyond your immediate work - e.g. providing technical advice/support to the wider company, promoting an exciting work culture etc.
	- Significant expertise in image analysis methods, particularly single-molecule methods
	- Experience of data analysis using Python
	- Understanding of super-resolution or single-particle imaging methods
	- Some laboratory experience in cell biology, molecular biology, or biochemistry
	- A passion and hunger for improvement, and inability to settle for average
	- Ability to be motivated by the success of a team, and derive fulfilment from enabling others
	- Strong cultural alignment to ONI and excellent people skills
	- Excellent record-keeping and data-handling skills
	- Ability to work to a high standard as part of a dynamic team
	- Ability to think independently and take initiative
	- Practical experience in sample preparation for fluorescence microscopy
	- Domain knowledge in a specific area of biology or biochemistry; preferably immunology, extracellular vesicle biology, oncology, virology, neurology, developmental biology, genetics/epigenetics, or pathology
	- Experience of other relevant data-handling resources; e.g. MatLab, R etc. 
	- Obtained a higher qualification (BSc, MSc, PhD, or equivalent) in a relevant area of science
	- Worked in an environment for with significant focus on imaging analysis methods (in either academia or industry)
	- Solved challenging data science problems in creative and innovative ways
	- Worked as part of a close team to deliver ambitious results together
	- Contributed creatively and intellectually to the success of your projects












###	Data Engineering



####	Notes about Data Engineering


Data engineering roles involve creating *Big Data* extract, transform, load (ETL) pipelines, and provide infrastructure support to help data scientists obtain insights from processing huge amounts of data.

They address:
+ readiness of data (sets)
+ format of data sets
+ resilience of infrastructure support for information systems
+ scaling of infrastructure support for information systems
+ security of infrastructure support for information systems

They support databases for:
+ operational data stores
+ data marts
+ data lakes
+ (enterprise) data warehouses, DW, DWH, or EDW




####	Skill Sets for Data Engineering

***Skill sets for data engineering***:
+ You've used several data storage technologies like Elasticsearch, Solr, PostgreSQL, MongoDB, or Cassandra and have some idea how they work and why they work that way. 
+ Experience with Scala, Scalding, Luigi, Hive, machine learning pipelines and model training is a plus
	- Luigi:
		* [The Enterprise-Ready Micro Frontend Framework](https://luigi-project.io/)
			+ "Luigi helps you to build modularizable, extensible, scalable and consistent UIs and Web Apps."
			+ "Create a unified user experience around your complex functionality in a distributed development environment."
			+ "Build administration and business User Interfaces using Luigi and discover its benefits."
		* [***Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.***](https://github.com/spotify/luigi)
			+ "Luigi is a Python (3.6, 3.7, 3.8, 3.9 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more."
			+ "The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else."
			+ "There are other software packages that focus on lower level aspects of data processing, like Hive, Pig, or Cascading. Luigi is not a framework to replace these. Instead it helps you stitch many tasks together, where each task can be a Hive query, a Hadoop job in Java, a Spark job in Scala or Python, a Python snippet, dumping a table from a database, or anything else. It's easy to build up long-running pipelines that comprise thousands of tasks and take days or weeks to complete. Luigi takes care of a lot of the workflow management so that you can focus on the tasks themselves and their dependencies."
			+ "You can build pretty much any task you want, but Luigi also comes with a toolbox of several common task templates that you use. It includes support for running Python mapreduce jobs in Hadoop, as well as Hive, and Pig, jobs. It also comes with file system abstractions for HDFS, and local files that ensures all file system operations are atomic. This is important because it means your data pipeline will not crash in a state containing partial data."
				- Hive:
					* ["The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive."](https://hive.apache.org/)
						+ "The Hive DDL operations are documented in Hive Data Definition Language."
						+ "The Hive DML operations are documented in Hive Data Manipulation Language."
						+ ["The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax."](https://cwiki.apache.org/confluence/display/Hive//Home)
							- "Tools to enable easy access to data via SQL, thus enabling data warehousing tasks such as extract/transform/load (ETL), reporting, and data analysis."
							- "Hive is not designed for online transaction processing (OLTP) workloads. It is best used for traditional data warehousing tasks."
							- "Hive is designed to maximize scalability (scale out with more machines added dynamically to the Hadoop cluster), performance, extensibility, fault-tolerance, and loose-coupling with its input formats."
				- Pig:
					* ["Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets."](https://pig.apache.org/)
						+ At the present time, Pig's infrastructure layer consists of a compiler that produces sequences of Map-Reduce programs, for which large-scale parallel implementations already exist (e.g., the Hadoop subproject).
						+ Pig's language layer currently consists of a textual language called Pig Latin, which has the following key properties:
							- Ease of programming. It is trivial to achieve parallel execution of simple, "embarrassingly parallel" data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.
							- Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.
							- Extensibility. Users can create their own functions to do special-purpose processing.
				- ["The ***Cascading Ecosystem*** is a collection of applications, languages, and APIs for developing data-intensive applications."](https://www.cascading.org/)
			+ ["Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more."](https://luigi.readthedocs.io/en/stable/index.html)
				- "The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else."
			+ From https://qconnewyork.com/ny2015/system/files/presentation-slides/MattWilliams%20-%20qcon2015%20-%20luigi.pdf:
				- Python module to help build complex pipelines
					* dependency resolution
					* workflow management
					* visualization
					* hadoop support built in
				- Doesn’t help you with the code, that’s what Scalding (scala), Pig, or anything else is good at.
				- It helps you with the plumbing of connecting lots of tasks into complicated pipelines, especially if those tasks run on Hadoop.
				- Luigi doesn’t replace Hadoop, Scalding, Pig, Hive, Redshift. It orchestrates them.
				- Core beliefs of Luigi:
					* Should remove all boilerplate code.
					* Be as general as possible.
					* Be easy to go from test to production code.
	- Scalding:
		* ["Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs."](https://twitter.github.io/scalding/)
			+ https://github.com/twitter/scalding
			+ "Scalding is built on top of Cascading, a Java library that abstracts away low-level Hadoop details."
			+ "Scalding is comparable to Pig, but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs."
+ skill set:
	- Investigate the feasibility of applying scientific principles and concepts to business problems.
	- Understand the ***Goodreads/Amazon data structures (MySQL/Data Lake/Redshift)***.
	- Acquire data by building the necessary ***SQL ETL queries***.
	- Import processes through various company specific interfaces for RedShift and Data Lake storage systems.
	- Analyze data for trends and input validity by inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies.
	- Build models using statistical modeling, mathematical modeling, econometric modeling, network modeling, social network modeling, natural language processing, machine learning algorithms, genetic algorithms, and neural networks.
	- Validate models against alternative approaches, expected and observed outcome, and other business defined key performance indicators.
	- Develop metrics to quantify the benefits of a solution and influence project resources. Partner with Engineering/Data Engineering to improve the quality of existing data and bring additional data sources in line. Audit metric data and measure project progress and success. Build/automate reports/dashboards (in Tableau) that allow the business leaders to get a clear snapshot of their operations. Design and analyze A/B tests to quantify impact of customer-facing changes. Develop innovative experimental design and measurement methodologies to understand customer growth and business efficacy. Participate in discussions, team planning, office hours, and metric reviews. Design and implement scalable and reliable approaches to support or automate decision-making throughout the business. Communicate insights to the business partners, Goodreads leadership, and Amazon stakeholders, with an emphasis on clarity, completeness, and actionability.
+ skill set:
	- Senior Data Engineer (US Remote Available)
	- The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.
	- The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible to build and scale the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineer and infrastructure teams). Strong technical experience within enterprise software is essential.
	- Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
	- Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
	- Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
	- Identify and document standard methodologies, standards, and architecture guidelines
	- Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
	- 7+ years of data architecture related experience such as data analysis, data modeling, and data integration.
	- Experience with GTM and customer success business processes and applications
	- ***Experience in custom ETL design, implementation, and maintenance***
	- Knowledge of programming languages (e.g. Python and Object Oriented Programming)
	- Hands-on experience with SQL database design
	- Experience working on CI/CD processes and source control tools such as Github and GitLab
	- Experience working Snowflake and relational databases
	- ***Extensive hands on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake***
	- Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
	- Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
	- Passionate about data solutions, technologies, and frameworks
+ skill set:
	- Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
	- 3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
	- Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
	- Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
	- Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
+ skill set:
	- You know how to work with data engineering technologies like Spark, no SQL DB or Lambda
	- You know everything there is to know about Robotic Process Automation
	- A minimum of 7 years experience in deep learning, machine learning or artificial intelligence applications like virtual agent, RPA, or video/image/text analytics
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift) is a plus
+ skill set:
	- Our data infrastructure team is responsible for all things data — our data warehouse, Hadoop, Redshift, Spark, Kafka, Airflow and so on.
	- Deep experience with MySQL, NoSQL data stores like HBase or similar.
	- Strong understanding of Unix/Linux variants, web network protocols, persistence solutions
+ skill set:
	- Python/Java Developer (Datagens)
	- LAB  SYSTEM & DATA ENGINEER
	- Are you interested in being part of a small, diverse team passionate about building technologies that change the way people learn?  Splunk Education is looking for a Lab and Data Engineer to join our Education Technologies Team. Our team is focused on building systems that help users learn to use our products in innovative ways.
	- Do you love tackling interesting problems and coming up with clean, stable solutions that delight users? We need to talk.
	- Opportunity to grow as an engineer. This role will provide a constant stream of new things to learn, providing the opportunity to expand your current knowledge and deep dive into new technologies.
	- Growth. We strongly believe in growing team members through ownership and leadership opportunities.
	- We pride ourselves on a collaborative, open and supported work environment.
	- The ability to work from home, or one of our many offices across the globe.
	- Participate in design and development of projects, either independently or in a team.
	- Be self-sufficient and take ownership of seeing projects through to successful conclusions.
	- Work with a diverse team of experts in education, video production, security and IT infrastructure.
	- Tasks include creating Terraform and Ansible playbooks to provision lab instances, along with building data generators that mock machine data across many technologies. 
	- B.S. degree in Computer Science or related field.
	- 8 years of experience
	- Deep knowledge of Python, Ansible, Terraform, Java, and AWS technologies.
	- Ability to read, understand and reproduce machine logs.
	- Experience with Docker, Kubernetes, GIT, and Splunk.
+ skill set:
	- Senior Software Engineer
	- We are seeking a passionate engineer to join our group, Data Platform. Our team designs distributed systems to collect and analyze high volumes of machine-generated data at scale. We are proud of owning what we build even after it's deployed to production. We ensure code hygiene, use open source libraries, employ continuous integration and delivery, and have a strong belief in automated testing at multiple levels (unit, integration, system). We are uniquely positioned as a globally distributed team with team members in a variety of locations. 
	- Develop and debug client-server system software written in C++ and/or Golang
	- Experience in distributed systems and large scale environments deployed at scale, both "on-premise" and in "cloud".
	- Experience with Linux deployments hosted by cloud service providers such as AWS and GCP.
	- Excellent problem solving, collaboration and communication skills, both verbal and written.
	- Mentored junior engineers in their development skills via code reviews and design discussions.
	- Owned features or sub-systems end-to-end from design to deployment and continuous improvement..
	- Develop server-side applications for data collection, indexing, clustering and other distributed systems.
	- Build robust, fault-tolerant distributed systems in a multi-threaded, multi-process environment.
	- Analyze, identify and resolve the bottlenecks of distributed systems, data pipeline, multi-threaded coherency and other complicated scenarios.
	- Analyze and improve the scalability of data collection, storage and retrieval.
	- Interact cross-functionally with other partners such as PMs, SREs, Devops, and support engineers.
	- Participate in rotating on-call duties to diagnose and fix customer issues.
+ skill set:
	- Principal Technical Program Manager - GDI (Remote - US)
	- We are looking for a Principal Technical Program Manager, who will be responsible for leading large-scale, complex programs from start to end within our GDI (getting data in) area. You will be working cross-functionally with a broad set of technical and business partners to drive programs that will further Splunk’s long range goals. We want a TPM who can thrive with a nebulous problem set, distill complex problems into concrete work, and guide multiple teams with their strong, critical thinking skills.
	- Strategically leading technical programs in the Platform Engineering organization, with a clear and constant understanding of priorities to drive expected outcomes, including New Product Introduction (NPI) and Go To Market (GTM) processes
	- Breaking down problems and operationalizing initiatives into coherent workstreams; defining program timelines and ensuring accountability of program goals; operating as the single source of truth in execution of highly complex programs
	- Proactively anticipating risks, developing mitigation plans, and driving problems to resolution
	- Aligning with organizational leadership and key business customers as a program lead across multiple programs
	- Understanding system interdependencies and facilitating technical discussions; communicating solutions and decisions with both engineers and non-technical audiences
	- Creating and maintaining detailed and easy-to-digest program documentation, managing dependencies, and tracking status across multiple teams and workstreams
	- 10+ years experience in software program management or closely related roles
	- Experience with enterprise software and cloud technologies, along with a deep understanding of the software development lifecycle
	- Understanding and experience with creating and consuming APIs
	- Experience working with or knowledge of Data Ingestion messaging or stream processing technologies, such as Flink, Kafka, Kinesis, Pulsar, Spark or similar
	- Extensive experience with large, cross-functional programs that have broad organizational impact
	- Demonstrated ability at juggling multiple, concurrent programs and prioritizing tasks based on criticality
	- Communication and risk management are your forte. You can communicate effectively with stakeholders of all levels and develop plans to act upon risks appropriately
	- Phenomenal at decision-making, consensus building, and problem solving; you don't shy away from challenges
	- Strong technical proficiency required to understand development tasks and identify and resolve issues as they arise
	- Understanding of key development tools and technologies such as Gitlab, AWS, Kubernetes, Terraform, etc. Experience with program management tools in the Atlassian stack (Jira and Confluence) are great
+ skill set:
	- Data Engineer, DevOps (US Remote Available)
	- The Analytics & Data Platform team (ADP) is responsible for Splunk’s data platform from ingestion to visualization. This platform enables our business partners to make the best data-driven decisions possible. We’re looking for a Data Engineer to join the team and contribute to the development and adoption of a true, self-serve, Data Mesh platform, with a focus on automation and observability. This position is responsible for the development, maintenance, and support of the data platform (including on-call rotations), partnering with all levels of customers in the course of support/adoption/migration activities, and actively participating in the growth and development of the ADP team and its capabilities and processes.
	- Operate: Perform the day to day updates, changes, and scheduled activities on the data platform. All the while, being attentive to opportunities for automation.
	- Support: Ensure SLAs are met by monitoring the data platform and responding to issues as they arise. Partner with other engineers or dependent teams as needed to resolve issues.
	- Develop: SQL and Python will be your go to tools, but by no means is that all. You’ll have a broad range of technologies that you’ll need to wrestle into submission to be successful: Terraform, Gitlab, Airflow, K8, Docker, DBT, and Linux, just to name a few.
	- Collaborate: Members of the ADP team are expected to work together to continuously improve the team’s processes, infrastructure, codebase, etc. Asking questions, challenging ideas, and recommending alternatives are all necessary for the team to continue to grow and improve. Passionate opinions are welcome.
	- Innovate: Develop creative solutions to edge cases that are causing unnecessary complications, with a focus on maintainability and scalability.
	- 4+ years of work experience in a relevant field (Data Engineer, DevOps Engineer, Backend Engineer, etc.)
	- Experience with data warehousing technologies (Redshift, Snowflake, BigQuery, or similar)
	- Proficient SQL skills and strong experience working with relational databases
	- Proficiency in a major programming language (ideally Python)
	- Excellent code and repo hygiene
	- Past experience as a user of Splunk
	- Experience with or a strong interest in containerization and/or Kubernetes
	- Experience with declarative configuration tools such as Terraform
	- Past experience contributing to an open source project
	- Passionate about technology with an insatiable curiosity for learning new things
	- B.S. degree in computer science, mathematics, statistics or a similar quantitative field, or sufficient relevant experience
+ skill set:
	- Director, Engineering
	- This Director for Engineering will lead critical services within our Data Processing umbrella. Success will be measured by your ability to build and lead your team as part of our overall engineering strategy. You will seamlessly integrate within our deeply innovative culture and contribute to our excellent record for delivering market leading solutions. You are a seasoned leader with a demonstrated history building and leading both established and emerging engineering talent, while encouraging an environment of inclusion, creativity, and innovation.
	- Our Data Processing team is a dynamic technology group with a mission to be the primary data processing path for any type of data transformation and routing activity in near real-time. If you possess a passion for extraordinary technology leadership and embrace the challenge of working at the frontier of what is possible in the industry today, then this position is for you. We are building state-of-the art capabilities, real-time messaging and streaming systems, support tools, and automation instrumentation that will greatly impact how our customers successfully use data to improve their businesses performance, scalability, profitability, and market strategies.
	- We are looking for a proven, seasoned engineering leader to lead streaming capabilities for the Data Processing team. You will build and lead an elite team of engineers who will be working on our streaming platform that will power the next generation of Splunk and enable ever deeper customer insights. You are a force multiplier who looks for ways to gain efficiencies for greater impact. You will influence the technical strategy and the roadmap to best serve our customers.
	- Hire and grow a new team within the Data Processing Engineering organization, focused on making our industry leading technology even more valuable to our customers.
	- Be an advocate of scalable and extensible, recoverable, manageable architecture for Core products and services.
	- Work with senior leadership on business goals, resource requirements and influence technical strategy
	- Diagnose and resolve systemic obstacles that prevent your team from delivering high-quality software.
	- Champion an atmosphere of continuous improvement by serving as a coach, mentor, and technical advisor for senior managers, managers and engineers.
	- Plan and support career development.
	- Recruit and retain top talent.
	- Masters or PhD in Computer Science or Engineering with 15+ years of industry experience, of which 10+ years is in Engineering Management.
	- 3+ years of experience managing multiple managers/teams on working on solutions to deeply challenging problems in processing data at massive scale.
	- Experience hiring and cultivating teams.
	- Experience developing new products, either in small companies or within the context of larger organizations. A mix of start-up and bigger company experience is a plus.
	- Experience working within geographically distributed organizations.
	- Experience building and cultivating strong engineering practices and processes.
	- Consistent track record of delivering scalable, high performance, and high quality software systems.
	- Broad understanding of various cloud development technologies and trends for enterprise-scale, distributed systems.
	- Strong technical acumen, creativity, interpersonal skills, and emotional intelligence.
+ ***[Apache Airflow, Luigi](https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7), workflow management system (WMS), Azkaban, [Open Source Data Pipeline – Luigi vs Azkaban vs Oozie vs Airflow](https://www.bizety.com/2017/06/05/open-source-data-pipeline-luigi-vs-azkaban-vs-oozie-vs-airflow/), [Pinball](https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8), Airbnb Airflow vs Apache Nifi***
	- ***Jenkins vs Airflow. Jenkins is an open source continuous integration tool written in Java.***
+ Exposure to big data systems like Hadoop, Spark, Kafka, etc.
+ skill set:
	- Understanding and experience with NoSQL such as MongoDB or Neo4j
	- Experience with the Hadoop ecosystem (HBase, MapReduce, Hive/Pig) or Spark
+ skill set:
	- Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Python, Elastic Search, Storm, Kafka, Oozie WFs etc. in both an on premise and cloud deployment model to solve large scale processing problems
	- Design, build and maintain Big Data workflows/pipelines to process billions of records into and out of our data lake
	- Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security
	- Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
	- Keen understanding of big data and parallelization accompanied with a stellar record of delivery
	- Experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred), AWS Glue
	- Experience with on-premises to cloud migrations including re-hosting, re-platforming and re-factoring
	- Experience with orchestration template technologies such as AWS CloudFormation
+ skill set:
	- Architect and operate high quality, large scale, multi-geo data pipelines that drive business decisions.
	- Redesigned data pipelines using the applicable DBR features, and incorporating external tools where necessary to have better reliability and tighter SLAs.
	- Established conventions or new APIs for logging feature usage for PM use-cases.
	- Understandable SLAs for each of the production data pipelines.
	- Improved test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests.
	- CI and deployment processes and best practices for the production data pipelines.
	- Reduction in overall alert noise and increase responsiveness by rethinking the current alert categories and priorities.
	- Design schemas for financial, sales and support data in the data warehouse.
	- Experience building, shipping and operating multi-geo data pipelines at scale.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Excellent communication (writing, conversation, presentation) skills, consensus builder
	- Strong analytical and problem solving skills
	- Passion for data engineering and for enabling others by making their data easier to access.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- If you are looking for an unparalleled opportunity to build the next generation big data processing platform, and learn how to launch hundreds of thousands of VMs a day at scale while running thousands of Kubernetes clusters, you have come to the right place. The platform team builds and manages the core systems powering Databricks, allowing it to seamlessly scale and run across various geographic regions/clouds, and making Databricks the go-to product for big data processing in the cloud.
	- You will be a senior software engineer responsible for architecting scalable systems to power Databricks, making it the de-facto platform for running Big Data and AI workloads. You will build and extend the Databricks cloud platform, which is based on a micro service architecture and includes systems for managing thousands of Kubernetes clusters at scale, systems for streaming and consuming gigabytes of log data per minute, onboarding and managing thousands of data scientists on Databricks, scalable API gateway, rate limiting framework, network security and encryption, build infrastructure (we use Bazel), and scalable CI/CD framework among many others.
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python or Javascript and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with internal APIs.
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- To achieve this, we build data reporting pipelines that support the underlying pricing infrastructure supporting tens to hundreds of millions of DBUs (Databricks Units) across multiple clouds and regions, UIs that allow Databricks administrators to view and manage their bill, and APIs and integrations to downstream processors to handle payments for all customers.
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
	- Unified Analytics Platform
+ skill set:
	- Our team drives state-of-the-art, open source Delta Lake project bringing reliable, scalable, ACID transactions to Apache Spark and other Big Data engines. Our mission is to deliver a robust and performant engine that enables users to build reliable data pipelines that ingest massive data volumes, optimize data layout, generate metadata and evolve data schemas all while guaranteeing transactional correctness and high query performance.
	- Build the core features that make Delta Lake the world's best Big Data storage abstraction in terms of performance, stability, security and scalability.
+ [***Delta Lake Community***; Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.](https://delta.io/)
+ skill set:
	- You will build tools and features to make Databricks the best place for large-scale enterprise R workloads.
	- Improve state of distributed R computing through Apache Spark and R integration on Databricks
	- Implement new features on Databricks platform for R users (e.g., ACL)
	- Improve and extend Databricks R notebooks to satisfy R users' use cases and requirements
	- Implement new R-based APIs on Databricks platform (e.g., secret management API)
	- Expand Databricks workspace through integration with third-party tools such as RStudio and Shiny.
	- Integrate critical packages from the R ecosystem into Databricks Runtime
	- Provide engineering support and thought leadership to Databricks field engineering teams on R
	- Give talks and write blog posts about R on Databricks
+ skill set:
	- You revel in building features quickly and iterating in a data-driven fashion
	- You lay awake thinking about improving the design, implementation and maintenance of large software systems with millions of users
	- Passion to hack social commerce
	- Data & Relevancy engineers work on our massive semi-structured datasets. They have domain experience in data mining, information retrieval, or machine learning, and a strong system orientation. Key product initiatives include product feed relevance, ad targeting, information extraction, and recommendations.
	- Infrastructure engineers scale a massive, highly-available platform end-to-end. They design distributed systems, validate performance, factor in security, and proactively monitor every corner of our stack. When things do go wrong, they are on-hand to fight the fires.
+ skill set:
	- Experience with data processing frameworks and data warehouses such as Hadoop, Spark, Redshift
	- Experience with designing, implementing, and optimizing ETL in Pentaho
+ skill set:
	- Technical fluency in one language and tool such as Python, Java or Scala, AWS (S3/EMR/Athena/Glue) and SQL.
	- Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow.
	- Experience working with either a MapReduce system of any size/scale.
+ skill set:
	- Minimum 3 years of designing, building and operationalizing large scale enterprise data solutions and applications using one or more of Azure / AWS / GCP data and analytics services in combination with custom solutions -  Spark, Azure Data Lake, HDInsights, SQL DW, DocumentDB, Search, Elastic Pool etc.  
	- Minimum 3 years experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on AZURE.
+ skill set:
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
+ skill set:
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- Demonstrates knowledge of the data engineering domain with experience in building and supporting non-interactive (batch, distributed) or real-time, highly available data, data pipelines.
	- Able to build fault tolerant, self-healing, adaptive computational pipelines
	- Contribute to the decision-making process related to the selection of software solutions that make up the architecture
+ skill set:
	- Minimum 3+ years of architecting, implementing and successfully operationalizing large scale data solutions in production environments using Hadoop and NoSQL ecosystem on premise or on Cloud (AWS, Google or Azure) using  many of the relevant technologies such as  Nifi, Spark, Kafka, HBase, Hive, Cassandra, EMR, Kinesis, BigQuery, DataProc, Azure Data Lake etc.  
	- Minimum 2+ years of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale and others
	- Minimum 3+ years of architecting data and building performant data models at scale for Hadoop/NoSQL ecosystem of data stores to support different business consumption patterns off a centralized data platform  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
	- Minimum 3++ years of architecting and industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scale
	- Minimum 2+ years of experience implementing large scale BI/Visualization solutions on Big Data platforms
	- Minimum 3+ years of experience implementing large scale secure cloud data solutions using AWS data and analytics services e.g. S3, EMR, Redshift
	- Minimum 2+ years of experience implementing large scale secure cloud data solutions using Google data and analytics services e.g. BigQuery, DataProc
	- Minimum 2+ years of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for modern data platforms that use Hadoop and NoSQL on premise or on AWS, Google and Azure cloud
	- Minimum 2+ years of experience securing Hadoop/NoSQL based modern data platforms on-premise or on AWS, Google, Azure cloud
	- Minimum 2+ years of Re-architecting and rationalizing traditional data warehouses with Hadoop or NoSQL technologies on premise or transition to AWS, Google clouds
	- Experience implementing data wrangling and data blending solutions for enabling self-service solutions using tools such as Trifacta, Paxata
	- 4 years industry systems development and implementation experience OR Minimum of 3 years of data loading, acquisition, storage, transformation, and analysis
	- Minimum 2+ years of using Talend, Informatica like ETL tools within a Big Data environment to perform large scale metadata integrated data transformation
	- Minimum 1+ years of building Business Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies
	- Architect modern data solutions in a hybrid environment of traditional and modern data technologies such as Hadoop, NoSQL
	- Create technical and operational architectures for these solutions incorporating Hadoop, NoSQL and other modern data technologies
	- Implement and deploy custom solutions/applications using Hadoop/NoSQL
	- Lead and guide implementation teams and provide technical subject matter expertise in support of the following:
	- Designing, implementing and deploying ETL to load data into Hadoop/NoSQL
	- Security implementation of a Hadoop/NoSQL solutions
	- Managing data in Hadoop/NoSQL co-existing with traditional data technologies in a hybrid environment
	- Troubleshooting production issues with Hadoop/NoSQL  
	- Performance tuning of a Hadoop/NoSQL environment
	- Architecting and implementing metadata management solutions around Hadoop and NoSQL in a hybrid environment
+ skill set:
	- Experience with Apache Big Data technologies such as Hadoop, Spark, Hive, Flink, Kafka, Beam etc
		* Apache Hadoop: for distributed computing
		* Apache Spark:
			+ open-source unified analytics engine for large-scale data processing
			+ large-scale data analytics
		* Apache Hive:
			+ [data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage](https://hive.apache.org/)
				- [The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.](https://cwiki.apache.org/confluence/display/HIVE)
			+ [Apache Hive is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale.](https://aws.amazon.com/big-data/what-is-hive/)
			+ data warehouse software project built on top of Apache Hadoop for providing data query and analysis
		* Apache Flink:
			+ open-source unified stream-processing and batch-processing framework
			+ distributed streaming data-flow engine
			+ for dataflow programs, executed in a data-parallel and pipelined manner (or task parallel manner)
			+ pipelined runtime system that enables the execution of bulk/batch and stream processing programs
		* Apache Kafka:
			+ distributed event store and stream-processing platform
			+ provide a unified, high-throughput, low-latency platform for handling real-time data feeds
			+ provides the Kafka Streams libraries for stream processing applications
			+ [open-source distributed event streaming platform used by thousands of companies for](https://kafka.apache.org/):
				- high-performance data pipelines
				- streaming analytics
				- data integration
				- mission-critical applications
				- ["event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time."](https://kafka.apache.org/intro)
		* Apache Beam: for defining and executing (via distributed processing back-ends) data processing pipelines for:
			+ ETL
			+ batch processing
			+ stream processing = continuous processing
	- Experience with messaging systems such as ActiveMQ
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Backend development experience with a strong interest in work involving data pipelines, distributed systems, performance analysis, and/or large-scale data processing Experience with software engineering practices (e.g. unit testing, code reviews, design documentation)
	- Able to take on complex problems, learn quickly, and persist towards a good solution
	- Experience designing fault-tolerant distributed systems
	- Experience with data pipelines
	- Experience with Hadoop or other MapReduce-based architectures
	- Experience with Kafka, Druid or other Streaming Compute based technologies is a plus
	- Experience with ad tech is a plus
+ tech stack:
	- Netflix culture resonates with you.
	- You can communicate effectively with experts of all backgrounds.
	- You are an expert analyst and can pick up any tool (e.g. Tableau, D3) to get the job done.
	- You dream in SQL and Python (or other similar languages).
	- You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Expertise in SQL, programming (e.g. Python, Scala), ETL and data warehousing concepts at scale (TBs of data)
+ Expertise in broad technical skills spanning data access, data storage, data processing, and data visualization.  Skills include: SQL, logical / semantic data modeling, ETL and data warehousing concepts, programming languages (Python)
+ Build data tooling to enable data lake, data warehouse, and analytics workflows within the AWS cloud (S3, Redshift, DynamoDB, Spark, Kinesis, Kubernetes, etc.)
+ experience with API design
	- REST
	- OpenAPI Specifications
	- GraphQL
+ Spark and/or other big data architectures (Hadoop, MapReduce) in high-volume environments
+ ***Experience with large-scale, distributed data processing frameworks (e.g., Spark, Kafka, YARN, Tachyon, Mesos, etc.) is a plus***
	- Apache Spark:
		* open-source unified analytics engine for large-scale data processing
		* open-source unified engine for large-scale data analytics
	- Apache Kafka:
		* distributed event store and stream processing platform
		* open-source distributed event streaming platform, for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications
	- YARN, Apache Hadoop YARN:
		* split up functionalities of resource management and job scheduling/monitoring into separate daemons, via global resource manager (RM) and per-application application master (AM)
		* not yarn package manager
	- Tachyon
		* from Alluxio Inc., previously Tachyon Nexus
			+ improve Apache Spark performance
			+ reliable memory-centric distributed storage system
			+ Alluxio: open-source virtual distributed file system, VDFS
				- in the big data analytics stack, Alluxio is the data asbtraction layer in between the computation frameworks and (multiple) storage systems, which is accessible via a common interface
				- APIs for:
					* Hadoop HDFS
					* Amazon S3
					* FUSE
			+ open-source, distributed, fault-tolerant, in-memory file system to enable data sharing across frameworks and perform operations at memory speed
		* not open-source component library
		* not the parallel/multiprocessor ray tracing software
	- Mesos
+ Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
+ tech stack:
	- Well-versed in one or more of the following languages and functional programming in general: Scala, Java, Python, JavaScript
	- Expert in SQL and comfortable designing, writing and maintaining complex SQL based ETL.
	- Experience with building large-scale batch and real-time data pipelines; ETL design, implementation, and maintenance.
	- Experience with schema design and data modeling, and the analytical skills to QA data and identify gaps and inconsistencies.
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift)
+ ***Programming skills sufficient to extract, transform, and clean large (multi-TB) data sets in a Unix/Linux environment.***
+ big data tools and stream-processing systems: Hadoop, Spark, Storm, Spark-Streaming
+ Extensive experience manipulating and analyzing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
+ skill set:
	- As the Principal Data Engineer on the Platform team at Zignal Labs, you will get to use your Scala and Java experience to build a best-in-class distributed data and analytics infrastructure by leveraging open source technologies such as Apache Spark, Apache Storm, and Elasticsearch.  We use social media, news, blogs and other media sources to empower our users with key insights based on real-time analysis.
	- Solve complex real-time data collection & analysis problems with cutting edge technical solutions
	- Iterate on our high performance and scalable platform for massive data collection, real-time analytics, NLP, machine learning, and backend data services
	- Build high performance, scalable, real-time, server-side technologies
	- Write scalable code with extensive test coverage, working in a professional software engineering environment with source control, dev/stage/production release cycles, continuous integration, and deployment
	- Work closely with product management, design, quality assurance and operations teams to understand our customers’ needs and effectively translate them to technical specifications
	- Lead projects from translating product requirements into architecture to production
	- Tech Stack:
		* Scala, Java, Python
		* Apache Spark, Spark Streaming, Databricks/Delta Lake, Apache Storm, Elasticsearch, Apache Nifi
		* Kafka, MongoDB, Redis
		* AWS
	- Bachelor's degree (or higher) in Computer Science, Engineering, or similar and/or relevant work experience
	- Experience providing technical leadership at the enterprise level for the design of information technology systems
	- Crafted and implemented operational data stores, as well as data lakes in production environments
	- Ability to analyze, diagnose and resolve complex architectural problems using industry standard engineering principles
	- Design and build data ingestion pipelines and ETL processing, including stream processing, while factoring in performance and cost
	- Identify and solve issues concerning data management to improve data quality
	- Clean, prepare and optimize data for ingestion and consumption
	- Experience solving performance problems with Lucene based search solutions like Elasticsearch or Solr
	- 9+ years experience in server-side/back-end full cycle product development in a production environment
	- 4+ years developing with Apache Spark, including Structured Streaming.   Experience with Databricks is a big plus
	- Knowledge of Scala or Java with exposure to or interest in Scala
	- Leads and mentors other team members
	- Provides partners with coaching and feedback in order to build effective teams
	- Provides effective support to cross-functional teams
+ skill set:
	- OLAP datastores:
		* Druid
		* ClickHouse
		* Scio
		* Flink
		* Spark
		* BigQuery
		* Parquet
		* Databricks
+ skill set:
	- As Scale's Analytics Engineer, you will spearhead building Scale's analytical and business-intelligence infrastructure. Scale's customers process millions of tasks through our APIs, and we're looking for a talented Analytics Engineer to build scalable solutions to support this growth. You will have widespread purview, with responsibility for understanding, mining, aggregating, and exposing data across the entire business to support timely and efficient decision-making and data exploration. You will also implement Scale's data warehouse, data mart, and business intelligence reporting environments, and help users transition their workflows to these systems.
	- Work with operations, finance, and engineering to drive the development of pipelines that provide single-source-of-truth foundational accuracy
	- Continually improve ongoing data pipelines and simplify self-service support for business stakeholders
	- Perform regular system audits, and create data quality tests to ensure complete and accurate reporting of data/metrics
	- 3+ years of relevant work experience in a role requiring application of data modeling and analytic skills
	- Ability to create extensible and scalable data schema and pipelines that lay the foundation for downstream analysis using SQL and Python
	- Experience with ETL tools and building / maintaining a data warehouse and data pipelines using tools such as DBT
	- Partner with operations and sales teams to automate manual workflows
	- Experience in using highly scalable data engineering technologies such as DBT, Airflow 
	- Experience in best practices in table partitioning/data sharding strategies and query optimization
	- The base salary range for this full-time position in our hub locations of San Francisco, New York, or Seattle, is $148,800 – $178,560. Compensation packages at Scale include base salary, equity, and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Scale employees are also granted Stock Options that are awarded upon board of director approval. You’ll also receive benefits including, but not limited to: Comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous PTO. Additionally, this role may be eligible for additional benefits such as a commuter stipend.
+ skill set:
	- Foursquare’s flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK); data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Studio).
	- Foursquare’s Marketers Engineering team writes and operates the software which produces core data sets for our Marketers suite of products. These petabyte-scale pipelines process geospatial data for the purposes of marketing use cases, such as ad targeting and attribution. It’s critical to this team’s success that we have rich data sets to build our applications on top of, and this data is kept fresh, easy to explore, and simple to make changes. The engineers on this team work closely with application engineers to prove out variant approaches and introduce new functionality.
	- In the Data Software Engineer role, you will ship products with high visibility and strategic importance to Foursquare and contribute directly to the revenue. Our pipelines are written in a variety of programming languages and deployed to multiple orchestration platforms. The main technologies we work with are Spark, Amazon EMR, Ruby, Java, and Apache Airflow.
	- Write and operate the data pipelines which produce Foursquare’s core data sets for our Marketers suite of products – Attribution and Targeting.
	- Document the expected and actual behavior of these pipelines, along with expectations for inputs and outputs.
	- Monitor data quality and freshness, with a focus on proactively evaluating the business impact of changes. Report regularly on the state of the data sets and the software which produces them.
	- Maintain a prioritized list of data questions and bugs which require further investigation. Escalate as needed to call attention to problems with the input data.
	- Evaluate new sources of data, and build new pipelines that combine our data in creative ways that drive customer value.
	- Participate in on-call rotation duties to ensure that data is correct and produced on-time, and to restore service when the pipelines are experiencing an outage.
	- 2-4 years of software development experience.
	- Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines.
	- Strong algorithms and data structures knowledge.
	- Professional experience scripting with the Unix/Linux command line or Python.
	- Experience with cloud computing service providers, such as AWS.
	- Experience with ***containerization technologies, such as Docker, Mesos or Kubernetes***.
	- Excellent written communication skills.
	- ***Your own unique talents! If you don’t meet 100% of the qualifications outlined above, we encourage and welcome you to still apply!***
	- Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI.
	- Experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform.
	- Experience with geospatial data processing.
+ skill set:
	- Foursquare’s flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK);  data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Unfolded Studio).
	- Our Data Platform team provides the infrastructure, tools, libraries, and APIs that power our data processing infrastructure. The goal of the team is to make engineers at Foursquare more effective and happier by offloading common problems from the product and content experts, enabling them to apply their skills and knowledge to their specific domain, while the Data Platform team works on problems that affect the company as a whole.
	- In this role, you will ship products with high visibility and strategic importance to Foursquare and contribute directly to revenue. Help us build and collaborate with Product, Engineering, and Data Science teams to create tools and processes to bring research and machine learning models to production.
	- Build tools and APIs that would be used by other FSQ Engineers
	- Build and maintain Foursquare's event streaming platform, Framework, and applications for data ingestion
	- Build resilient services and tooling which drive all of our offline processing of petabytes of data
	- Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production
	- Participate in on-call rotation duties
	- What you’ll need: If you have more or less experience than listed, please apply anyways and we will see if another role aligns better with your experience.
	- BS/BA in a technical field such as computer science or equivalent experience
	- 3+ year of experience in software development working with production-level code
	- Experience in one or more of the programming languages we use
	- Excellent communication skills, including the ability to identify and communicate data-driven insights
	- Experience with working in the cloud, preferably AWS
	- Strong algorithms and data structures knowledge
	- Comfort with Unix/Linux and the command line
	- Experience with Hadoop, Kafka, MapReduce, and/or Spark
	- Experience with AWS data processing services (EMR, Glue, Athena, …)
	- Experience with relational or document-oriented database systems
	- Prior software internship experience
	- ***Languages: Java, Scala, Python, Clojure, Ruby***
	- ***Tools for pipeline orchestration: Airflow, Luigi***
	- ***Frameworks: Spark, MapReduce, Scalding, Spring Boot***
	- ***Infrastructure: AWS, Hadoop, Kafka, Kubernetes, Docker***
	- ***Other technologies: Postgres, Hive, HBase, MongoDB***
+ skill set:
	- Has advanced experience with data warehousing, data modeling, and data ETL
	- Has hands-on familiarity with ***message queues (preferably Kafka) and one or more cloud provider (preferably AWS)***.
	- Familiarity with ***Hadoop, MapReduce, and Snowflake analytics*** are a plus
+ skill set:
	- Experience building and maintaining development pipelines with ***Jenkins, XL Release, QuickBuild*** (or similar) required.
	- Experience with web app internationalization and translation technologies like ***Smartling*** is strongly desired.
	- Experience with ***MySQL and SOLR*** is strongly desired.
+ ***Snowflake, AWS, Java, Solr, PHP***
+ skill set:
	- Open Source Infrastructure Engineer
	- Our company has its roots in the Python data science community. Our leaders have had significant involvement in the creation and maintenance of NumPy, SciPy, Jupyter, Spyder, Dask, Conda, Numba, Anaconda and PyData NumFOCUS. Our mission is to connect companies to open-source communities to create sustainable solutions that benefit the whole ecosystem.
	- We accomplish this mission by providing various services ranging from open-source software development to training and consulting. We believe in a culture of do-ers, learners, and collaborators. We are looking for people who are motivated, humble, curious, and respectful of others.
	- We are seeking a fully remote, experienced Open Source Infrastructure Engineer to join our team at Quansight. In this role, you will support Quansight’s growing cloud and on-premises infrastructure and help make them more reliable, scalable, and efficient. You will also  address support issues from our clients and collaborators, explore emerging technologies in the Cloud and DevOps spaces, and design and implement cloud computing systems with the rest of our infrastructure team.
	- Contribute to nebari (https://nebari.dev), an open source Data Science platform built on JupyterHub, Dask, and other tools from the PyData ecosystem.
	- Participate in upstream open source communities we rely on (such as JupyterHub, BinderHub, Dask, etc.) in partnership with the established leaders of those communities.
	- Deploy and ensure the reliable operation of Quansight’s and clients’ infrastructure.
	- Collaborate with a fully distributed team - team members are expected to communicate and collaborate proactively to allocate effort and maximize the team’s impact.
	- Experience using some form of infrastructure-as-code tooling (i.e., Ansible, Salt, Puppet, Terraform, etc.)
	- Experience with at least one major cloud platform (AWS, Azure, GCP)
	- Experience developing tools in a general purpose programming language (eg. Python)..
	- Experience with Continuous Integration and Continuous Delivery services (e.g., Circle CI, GitHub Actions)
	- Familiarity with software engineering best practices – including unit tests, code review, version control, production monitoring, etc.
	- Experience deploying and developing with Linux container-based technologies, such as Docker and Kubernetes.
	- Comfortable working independently and reaching out for feedback and support as needed.
	- Experience collaborating and coordinating work via online platforms, such as GitHub, GitLab, or BitBucket, and distributed revision control.
	- Ability to provide and constructively receive feedback.
	- Experience working on geographically distributed open-source projects.
	- Exposure to the Python Data Science stack - Pandas, Numpy, Dask, etc.
	- Experience with the Jupyter ecosystem and other tools for interactive computing.
	- Experience building and maintaining continuous deployment pipelines.
	- Experience with common data science methods, platforms, workflows, and infrastructures; with data management systems, practices, and standards; and the capacity to gain familiarity with new related topics.
+ skill set:
	- Experience in distributed data processing systems (Hive, Redshift, Presto, Snowflake, etc)
	- Breadth of knowledge around statistical methods including A/B testing, causal inference, experimentation, cluster-randomized testing
	- Familiarity with Snowflake, dbt, and ETL/reverse ETL tools.
+ Build and maintain robust data pipelines that ingest over 30TB of data each day.
+ skill set:
	- Data Platform Engineer
	- SHANGHAI /PLATFORM ENGINEERING – PLATFORM ENGINEERING // REMOTE
	- CertiK is a pioneer in blockchain security, leveraging best-in-class AI technology to protect and monitor blockchain protocols and smart contracts. Founded in 2018 by professors from Yale University and Columbia University, CertiK’s mission is to secure the web3 world. CertiK applies cutting-edge innovations from academia to enterprise, enabling mission-critical applications to scale with safety and correctness.
	- Manage critical data platform services for the data lifecycle, including ingestion, transformation, management, delivery, and analytics. 
	- Build tools and libraries that ease new pipeline development and reduce delivery time.
	- Work towards increasing the robustness and fault tolerance on the platform processing large amounts of data.
	- Implement best practices and design patterns for different data solutions. 
	- Define tools and techniques to improve overall data observability and discoverability.
	- Troubleshoot and remediate issues with the data platform services and data pipelines.
	- Collaborate on cross-team Data Platform initiatives. 
	- Track and execute continuous improvements.
	- Experience with blockchain technologies.
	- Experience with data warehouse/data lake platforms such as Snowflake, Databricks, Redshift, or BigQuery.
	- Experience with database technologies such as Redis, Kafka, MongoDB, PostgresSQL, and MySQL.
	- Experience with data engineering tools such as airflow, dbt, etc. 
	- Familiarity working with data observability and data governance. 
	- Programming experience in Bash, Python, Golang, C++, or Java and in data query and manipulation with SQL. 
	- Excellent written and verbal communication skills in ENGLISH and CHINESE.
	- Experience with graph databases. 
	- Experience with data science tools and platforms such as Jupiter notebooks, Databricks, SageMaker, and Tensorflow. 
	- Understanding of Linux. 
	- Familiarity working with open source software community. 
	- Proclivity for automation and DevOps practices and tools such as Git and Terraform.
	- Broad exposure to at least one cloud platform: AWS, Google, Azure.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















###	DataOps


####	Notes about DataOps

DataOps is a set of practices, processes and technologies that combines an integrated and process-oriented perspective on data with automation and methods from agile software engineering to improve quality, speed, and collaboration and promote a culture of continuous improvement in the area of data analytics.

DataOps incorporates the Agile methodology to shorten the cycle time of analytics development in alignment with business goals.

DevOps focuses on continuous delivery by leveraging on-demand IT resources and by automating test and deployment of software. This merging of software development and IT operations has improved velocity, quality, predictability and scale of software engineering and deployment. Borrowing methods from DevOps, DataOps seeks to bring these same improvements to data analytics.

DataOps utilizes statistical process control (SPC) to monitor and control the data analytics pipeline. With SPC in place, the data flowing through an operational system is constantly monitored and verified to be working. If an anomaly occurs, the data analytics team can be notified through an automated alert.

DataOps is not tied to a particular technology, architecture, tool, language or framework.

Tools that support DataOps promote:
+ collaboration
+ orchestration
+ quality
+ security
+ access
+ ease of use



The volume of data is forecast to grow at a rate of 32% CAGR to 180 Zettabytes by the year 2025. 

DataOps seeks to provide the ***tools, processes, and organizational structures*** to cope with this significant increase in data. ***Automation streamlines the daily demands of managing large integrated databases,*** freeing the data team to develop new analytics in a more efficient and effective way. ***DataOps seeks to increase velocity, reliability, and quality of data analytics. It emphasizes communication, collaboration, integration, automation, measurement and cooperation between data scientists, analysts, data/ETL (extract, transform, load) engineers, information technology (IT), and quality assurance/governance.***




DataOps leadership principles:
+ Establish progress and performance measurements at every stage of the data flow. Where possible, benchmark data-flow cycle times.
+ Define rules for an abstracted semantic layer. Ensure everyone is "speaking the same language" and agrees upon what the data (and metadata) is and is not.
+ Validate with the "eyeball test":
	- Include continuous-improvement -oriented human feedback loops.
	- Consumers must be able to trust the data, and that can only come with incremental validation.
+ Automate as many stages of the data flow as possible, including:
	- BI
	- data science
	- data analytics
+ Using benchmarked performance information, identify bottlenecks and then optimize for them. This may require investment in commodity hardware, or automation of a formerly-human-delivered data-science step in the process.
+ Establish governance discipline, with a particular focus on two-way data control, data ownership, transparency, and comprehensive data lineage tracking through the entire workflow.
+ ***Design process for growth and extensibility. The data flow model must be designed to accommodate volume and variety of data.*** Ensure enabling technologies are priced affordably to scale with that enterprise data growth.”









####	Skill Sets about DataOps

Skill sets for DataOps:
+ Demonstrated track record working with data warehouse concepts.
+ skill set:
	- Experience with automation tools and configuration-as-code (CloudFormation, Ansible, Puppet, Chef, Vagrant, etc.)
	- Experience working with either AWS or GCP services such as compute, databases, VPCs, networking, permissioning and storage
+ skill set:
	- Big Data platforms e.g. Cloudera, Hortonworks MapR
	- Big Data Analytic frameworks and query tools such as: HDINsight, Spark, Storm, Hive, Impala
	- IoT protocols, gateways, queues, messaging hubs such as IoT Hub, MQTT, XMPP, CoAP, etc.
	- IoT development experience on at least one of the industry leading platforms (Azure IoT, AWS IoT, GE Predix, Siemens Mindsphere, PTC Thingworx, SAP Leonardo, GCP)
	- Streaming data tools and techniques such as Apache Kafka, Azure Streaming Analytics, AWS Kinesis
+ skill set:
	- Minimum 1 year of building and coding applications using at least two Hadoop components – MapReduce, HDFS, Hbase, Pig, Hive, Spark, Scoop, Flume, etc
	- Minimum 1 year coding one of the following: Python, Pig programming, Hadoop Streaming, HiveQL
	- Minimum 1 year understanding of data modelling & data pipeline design: iterative data pipeline development from raw, curated, integrated to published data, with fit for use data modelling on Hadoop and NoSQL platforms
	- Minimum 1 year of experience implementing large scale cloud data solutions using Cloud Service Providers:  AWS data services (e.g. EMR, Redshift, GLUE) or Azure (Data Lake Store/Analytics, SQL Data Warehouse) or Google Cloud Platform Google Cloud (Big Data:  Big Query, Big Insights)
	- Minimum 1 year of experience delivering an operational Big Data solution using one or more of the following technologies: Hadoop, HortonWorks, Cloudera, Cassandra
	- Minimum 1 year of experience throughout the SDLC of a Hadoop implementation technologies including HortonWorks, Cloudera, Hive, Pig, MapReduce 
	- Minimum 1 year of experience throughout the SDLC of a HortonWorks, Cloudera, Cassandra / Hbase implementation 
	- Minimum of a Bachelor's Degree or 3 years IT/Programming experience
	- Minimum 1 year of experience developing REST web services
	- Industry experience (financial services, resources, healthcare, government, products, communications, high tech)  
	- Experience leading teams
	- Machine Learning tools, interfaces & Libraries: R, R-Studio, Spark R, sparklyr, MLlib, H2O etc.  
	- Experience with other tools, databases and Apache projects: Google BigQuery, Presto, Drill, Kylin, OpenTSDB, Spark Streaming
	- Enterprise data integration, BI and analytics platforms: Informatica, Talend, InfoSphere, SAS, RevoR, QlikView, Qlik Sense, Tableau, Spotfire, D3.js
	- Processing frameworks & programming tools: Spark (Scala/Python/Java), Kafka, Flink
	- Client facing skills: ability to build trusted relationships with client stakeholders and act as a trusted adviser
+ skill set:
	- Partner with engineering leadership to buildout data driven roadmap items to address performance in critical areas
	- Established performance test environments and frameworks
	- Experience evangelizing performance engineering techniques within a data driven engineering culture
	- Deep hands on experience with JVM tuning techniques
	- Supported efforts in performance testing and improvement in common JavaScript frameworks (Angular, React, JQuery)
	- Experience with Ruby (JRuby) and JavaScript
	- Extremely well versed in solving data access performance challenges across SQL data stores
	- Experience in AWS and other cloud providers when exploring different approaches to performance engineering
	- Experience with distributed architectures
	- Passionate about driving a performance engineering culture
+ Expert knowledge working with Spark and other distributed data technologies (e.g. Hadoop, Presto, Flink, Druid) for building efficient & large scale data pipelines.
+ skill set:
	- Our leaders have had significant involvement in the creation and maintenance of NumPy, SciPy, Jupyter, Spyder, Dask, Conda, Numba, Anaconda and PyData NumFOCUS.
	- We are seeking a fully remote, experienced Open Source Infrastructure Engineer to join our team at Quansight. In this role, you will support Quansight’s growing cloud and on-premises infrastructure and help make them more reliable, scalable, and efficient. You will also  address support issues from our clients and collaborators, explore emerging technologies in the Cloud and DevOps spaces, and design and implement cloud computing systems with the rest of our infrastructure team.
	- Contribute to nebari (https://nebari.dev), an open source Data Science platform built on JupyterHub, Dask, and other tools from the PyData ecosystem.
	- Participate in upstream open source communities we rely on (such as JupyterHub, BinderHub, Dask, etc.) in partnership with the established leaders of those communities.
	- Deploy and ensure the reliable operation of Quansight’s and clients’ infrastructure.
	- Collaborate with a fully distributed team - team members are expected to communicate and collaborate proactively to allocate effort and maximize the team’s impact.
	- ***Experience using some form of infrastructure-as-code tooling (i.e., Ansible, Salt, Puppet, Terraform, etc.)***
	- Experience with at least one major cloud platform (AWS, Azure, GCP)
	- Experience developing tools in a general purpose programming language (eg. Python)..
	- ***Experience with Continuous Integration and Continuous Delivery services (e.g., Circle CI, GitHub Actions)***
	- Familiarity with software engineering best practices – including unit tests, code review, version control, production monitoring, etc.
	- ***Experience deploying and developing with Linux container-based technologies, such as Docker and Kubernetes.***
	- Comfortable working independently and reaching out for feedback and support as needed.
	- Experience collaborating and coordinating work via online platforms, such as GitHub, GitLab, or BitBucket, and distributed revision control.
	- Ability to provide and constructively receive feedback.
	- While this is a remote position, we are looking for candidates with significant time overlap with US Central and Eastern time zones due to the location of many of our infrastructure team members and collaborators
	- Experience working on geographically distributed open-source projects.
	- ***Exposure to the Python Data Science stack - Pandas, Numpy, Dask, etc.***
	- Experience with the Jupyter ecosystem and other tools for interactive computing.
	- Experience building and maintaining continuous deployment pipelines.
	- ***Experience with common data science methods, platforms, workflows, and infrastructures; with data management systems, practices, and standards; and the capacity to gain familiarity with new related topics.***
+ skill set:
	- The Patrick J McGovern Foundation (PJMF) is seeking an experienced, multi-faceted and self-driven Software Development Engineer (SDE). This role sits within the Data Solutions team and will be directly engaged with the development, deployment and maintenance of data and AI products that will drive positive social impact around the world. The charter of the Data Solutions team is to identify, develop and deploy strategic data and AI solutions, focused on unlocking opportunities and addressing broad challenges the social good sector is facing now, and in the near future. The SDE is responsible for full stack development of cloud solutions that are based around ML model predictions, using a rapid prototyping development approach. We are a small, high performance team and in many ways function as a startup. As such, the ideal candidate will be someone who can develop an end-to-end solution around which our ML products will be deployed.
	- Work closely with the Technical Product Manager, along with partners and end-users, to understand their challenges in order to build effective and safe products
	- Understand end-user needs to build applications that can solve their challenges
	- Prototype, develop, test and deploy highly reliable, safe and scalable full stack solutions
	- Ability to develop frontend and backend applications to support and integrate with our data and AI solutions
	- Monitor and maintain application performance, functionality, scalability and security
	- Develop secure, modularized, well-documented and reusable code
	- Employ rapid prototyping techniques to quickly develop and iterate on product deployment and fast integration of end-user feedback loop
	- Integrate necessary data technologies, platforms, tools, and 3rd party tools needed for product deployment and functionality
	- Ensure appropriate maintenance, bug fix and feature requests are resolved
	- Create appropriate system diagrams and documentation
	- Experience - Demonstrated experience in full stack development and deployment
	- Technologies - ***Strong programming experience with Python, Scala, Java, Javascript or similar languages. Strong production experience with using cloud services and infrastructure, containerization (Docker) and orchestration (Kubernetes) tools. Experience with CI/CD, software bug tracking, version control and feature request management, using tools like Jenkins, Jira, GitHub, etc. Experience with build systems (make, gradle, etc) and package management (pip, etc). Experience with SQL database systems such as MySQL, PostgreSQL, etc and NoSQL systems such as MongoDB, DynamoDB, HBase, etc***
	- Learning - This role will require an individual with an appetite for (quickly) learning and applying different technologies, programming languages, and techniques needed to build the required solutions
	- Communication - Ability to communicate effectively with cross-functional teams, engineers, partners and end-users. Excellent written and verbal communication skills to technical and non-technical audiences. Excellent technical writing and documentation skills
	- Teamwork - Effective team player who understands the responsibility every individual brings to the table and how to encourage and drive results from each team member
	- High Performance - Ability to go outside your comfort zone to learn new technologies and methodologies to ensure successful and timely completion of product development
	- Results-oriented - Highly organized and detail-oriented, self-driven and able to adapt to learning and implementing new technologies, and creative in solving issues as they arise
	- Cultural alignment - An advocate for social progress; interest in emerging technologies and their ability to advance societal outcomes
	- Work Eligibility - Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time
	- Extensive knowledge & experience with Git, Linux system administration, Kubernetes/Docker, ML data pipelines, databases (SQL, NoSQL, RDS, CloudSQL, DynamoDB, Redshift, etc.), cloud services (at least AWS and/or GCP), and distributed systems
	- Ability to code in multiple languages (Python and SQL; R, Java, or others as well)
	- Strong communication and problem-solving skills with the ability to discuss projects with colleagues who have limited knowledge of ML/DevOps techniques and tools
	- Ability to oversee and provide input regarding infrastructure design and maintenance
+ skill set:
	- Analytics Engineer
	- As Scale's Analytics Engineer, you will spearhead building Scale's analytical and business-intelligence infrastructure. Scale's customers process millions of tasks through our APIs, and we're looking for a talented Analytics Engineer to build scalable solutions to support this growth. You will have widespread purview, with responsibility for understanding, mining, aggregating, and exposing data across the entire business to support timely and efficient decision-making and data exploration. You will also implement Scale's data warehouse, data mart, and business intelligence reporting environments, and help users transition their workflows to these systems.
	- Work with operations, finance, and engineering to drive the development of pipelines that provide single-source-of-truth foundational accuracy
	- Continually improve ongoing data pipelines and simplify self-service support for business stakeholders
	- Perform regular system audits, and create data quality tests to ensure complete and accurate reporting of data/metrics
	- 3+ years of relevant work experience in a role requiring application of data modeling and analytic skills
	- Ability to create extensible and scalable data schema and pipelines that lay the foundation for downstream analysis using SQL and Python
	- Experience with ETL tools and building / maintaining a data warehouse and data pipelines using tools such as DBT
	- Partner with operations and sales teams to automate manual workflows
	- Experience in using highly scalable data engineering technologies such as DBT, Airflow 
	- Experience in best practices in table partitioning/data sharding strategies and query optimization
+ skill set:
	- Full Stack Engineer - Public Sector
	- Scale AI is seeking a highly skilled and motivated Full Stack Engineer to join our dynamic Federal Engineering team. As a part of this team, you will play a critical role in delivering high-impact AI-powered mission solutions for government customers. Our scalable and high-performance platform forms the foundation for these solutions, and your expertise will be instrumental in designing and implementing systems that can handle billions of data points with exceptional performance.
	- Design and implement scalable backend systems for Federal customers, leveraging Scale's modern and cloud-native AI infrastructure.
	- Collaborate with cross-functional teams to define and execute the vision for backend solutions, ensuring they meet the unique needs of government agencies operating in secure environments.
	- Develop distributed systems, data-intensive applications, and machine learning infrastructure to enable real impact for mission owners.
	- Build robust and reliable backend systems that can serve as standalone products, empowering customers to accelerate their own AI ambitions.
	- Participate actively in customer engagements, working closely with stakeholders to understand requirements and deliver innovative solutions.
	- Contribute to the platform roadmap and product strategy for Scale AI's Federal business, playing a key role in shaping the future direction of our offerings.
	- Design and implement scalable backend systems for Federal customers, leveraging Scale's modern and cloud-native AI infrastructure.
	- Collaborate with cross-functional teams to define and execute the vision for backend solutions, ensuring they meet the unique needs of government agencies operating in secure environments.
	- Develop distributed systems, data-intensive applications, and machine learning infrastructure to enable real impact for mission owners.
	- Build robust and reliable backend systems that can serve as standalone products, empowering customers to accelerate their own AI ambitions.
	- Participate actively in customer engagements, working closely with stakeholders to understand requirements and deliver innovative solutions.
	- Contribute to the platform roadmap and product strategy for Scale AI's Federal business, playing a key role in shaping the future direction of our offerings.
	- Collaboration and Communication: Excellent interpersonal and communication skills to effectively collaborate with cross-functional teams, stakeholders, and customers. Ability to clearly articulate technical concepts to non-technical audiences and foster a collaborative work environment.
	- Adaptability and Learning Agility: Willingness to embrace new technologies, learn new skills, and adapt to evolving project requirements. Ability to quickly grasp and apply new concepts and stay up-to-date with emerging trends in software engineering.
+ skill set:
	- Foundation Models Lead
	- Scale AI is the leading provider of expert data and human feedback to power RLHF for most of the Large Language Models available today. Scale also partners with many companies to help evaluate the LLMs and, in turn, drives model improvement by providing data to improve them - forming a virtuous cycle. This forms the foundation of an AI ecosystem and sets the bar for the data-centric AI movement. We are looking for the key Foundation Model Lead to seed and grow our team. Your core focus will be on advancement of LLMs - you will lead our efforts to partner with the largest companies building LLMs and help drive their improvement. If you are excited about shaping the future of the AI movement in a data-centric way, we would love to hear from you! 
	- Apply state of the art models developed internally and from the community, use them in production to solve problems for our customers and data labelers. 
	- Work with product and research teams with the largest LLM providers to identify opportunities for improvement in our current product line and for enabling upcoming product lines.
	- Work closely with customers - some of the most sophisticated ML organizations in the world - to quickly prototype and build new deep learning models targeted at language and multi-modal inputs.
	- 5+ years of model training, deployment and maintenance in a production environment.
	- Strong skills in computer vision, NLP, or deep learning.
	- Solid background in algorithms, data structures, and object-oriented programming.
	- Strong programming skills in Python, experience in Tensorflow or PyTorch.
	- Strong written and verbal communication skills. 
	- Rapidly prototype and test cutting edge techniques like prompt engineering, instruction tuning, chaining, DSP, RLHF and more.
	- Architect, train and deploy LLMs
	- Obsessed with foundation models
	- Experience with deep learning with large scale video processing. 
	- Published research in areas of machine learning at major conferences and/or journals. 
	- Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment
	- Experience working on recommendation engines 
+ skill set:
	- AIML - Senior Data Infrastructure Software Engineer, Machine Learning Platform and Technology
	- The Data Infrastructure group within the AI/ML organization powers the analytics, experimentation and ML feature engineering that powers the Machine Learning technologies we all love in our Apple devices. Our mission is to provide cutting edge, reliable and easy to use infrastructure for ingesting, storing, processing and interacting with data while keeping Apple’s users’ data private and secure.
	- Are you a passionate about building scalable, reliable, maintainable infrastructure and solving data problems at scale? Come join us and be part of the Data Infrastructure journey.
	- 12+ years of experience in software engineering with deep knowledge in computer science fundamentals.
	- Strong in data structures and algorithms. Must write good quality code with test cases and review PR's in fast faced environment.
	- Expert in one or more functional or object-oriented programming languages (Scala, Java)
	- Fluent in at least one scripting or systems programming language (Python, Bash and Go etc.)
	- Experience or knowledge in distributed data systems like Hadoop, Spark, Kafka or Flink.
	- Experience or knowledge in public cloud is a big plus, preferably AWS.
	- Strong collaboration and communication (verbal and written) skills to work with diff
	- The role involves managing petabytes of data for machine learning applications and designing and implementing new frameworks to build scalable and efficient data processing workflows and machine learning pipelines. The successful candidate will be responsible for ensuring complete data lineage and legal workflow integration while optimizing performance and scalability. You will also be responsible for monitoring the performance of the system, optimizing it for cost and efficiency, and solving any issues that arise. This is an exciting opportunity to work on cutting-edge technology and collaborate with cross-functional teams to deliver high-quality software solutions. The ideal candidate should have a strong background in software development, experience with public cloud platforms, and familiarity with distributed databases.
	- Familiarity with distributed databases, such as DynamoDB, MongoDB, or Cassandra.
	- Experience with containerization and orchestration technologies, such as Docker and Kubernetes
+ Experience with AWS Services such as Amazon S3 EC2 EKS / Kubernetes
+ skill set:
	- AIML - Machine Learning Data Infrastructure Engineer, Data & Machine Learning Innovation
	- As part of Apple's AI and Machine Learning org, we encourage and create groundbreaking technology for large-scale ML systems, computer vision, natural language processing, and multi-modal understanding. The Data and Machine Learning Innovation (DMLI) team is looking for a passionate Machine Learning Engineer to explore new methods, challenge existing metrics or protocols, and develop new insightful practices that will change how we understand data and overcome real-world ML challenges. Are you excited to work on some of the most ambitious technical challenges in the field? Your role will involve collaborating closely with machine learning researchers, engineers, and data scientists. Together, we will spearhead groundbreaking research initiatives and develop transformative products designed to build a significant impact for billions of users worldwide.
	- Demonstrated expertise in machine learning with a passion for data-centric machine learning.
	- Experience with natural language processing (NLP), and large language models, such as BERT, GPT, or Transformers.
	- Staying on top of emerging trends in LLMs.
	- Strong programming skills and hands-on experience using the following languages or deep learning frameworks: Python, PyTorch, or Jax.
	- Strong problem-solving and communication skills.
	- 5+ years of experience with developing and evaluating ML applications, and demonstrated experience in understanding and improving data quality.
	- Demonstrated publication record in relevant conferences (e.g. ***ACL, EMNLP, NeurIPS, ICML, ICLR***, etc) is a plus.
	- As a Machine Learning (ML) Engineer, you will be entrusted with the critical role of innovating and applying innovative research in ML to tackle complex data problems. The solutions you develop will significantly impact future Apple products and the broader ML development ecosystem. You will work with a multidisciplinary team to actively participate in the data-model co-design and co-development practice. Your responsibilities will extend to the design and development of a comprehensive data curation framework. You will also build robust model evaluation pipelines, integral to the continuous improvement and assessment of ML models. Additionally, your role will entail an in-depth analysis of collected data to underscore its influence on model performance. Furthermore, you will have the opportunity to showcase your groundbreaking research work by publishing and presenting at premier academic venues. Your work may span a variety of topics, including but not limited to: Designing and implementing semi-supervised, self-supervised representation learning techniques for growing the power of both limited labeled data and large-scale unlabeled data. Developing evaluation protocols centered on the end-to-end user experience, with a focus on anticipating potential failure modes, edge cases, and anomalies. Employing data selection techniques such as novelty detection, active learning, and core-set selection for diverse data types like images, 3D models, natural language, and audio. Uncovering patterns in data, setting performance targets, and using modern statistical and ML-based methods to model data distributions. This will aid in reducing redundancy and addressing out-of-distribution samples.
	- Ph.D/MS degree in Machine Learning, Natural Language Processing, Computer Vision, Data Science, Statistics or related areas.
	- The base pay range for this role is between $170,700 and $300,200, and your base pay will depend on your skills, qualifications, experience, and location.













































##	Applied Machine Learning & Data Science Roles in Other Domains




For the ***transportation industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***management consulting market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











For the ***K-12 (kindergarten to grade 12) education market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.














For the ***higher education market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- Experience using native APIs from higher ed core systems (SIS, ERP, LMS) a plus
		* MIS, such as: ***student information systems, SIS***, student management systems, school administration software, school administration system
		* enterprise resource planning, ERP
			+ business intelligence
				- customer relationship management, CRM, customer services
				- sales:
					* invoicing
					* order placement
					* order scheduling
					* shipping
			+ e-commerce, electronic commerce
				- product lifecycle management, PLM
					* planning
					* optimizing manufacturing capacity and material resources
					* manufacturing resource planning, MRP
						+ material requirements planning, MRP
				- supplier relationship management, SRM
					* maximize cost savings with support for the end-to-end procurement and logistic processes
			+ enterprise asset management
				- corporate performance and governance
				- human resource
			+ industrial distribution, logistics, supply chain management, SCM
			+ accounting
				- financial operations
				- regulatory compliance
		* learning management systems, LMS
	- 3+ years' experience with enterprise level data integration working with multiple systems simultaneously; Including extracting data utilizing API integration from a variety of platforms, performing data mapping, data transformation, and loading data to the target system
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








For the ***real estate market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***waste management market***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.











For the ***health care industry***, including health care informatics, health care analytics, health care management and hospital management, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







For the ***media industry***, including mass media and social media, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.








For the ***hospitality industry***, including the tourism market, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.






For the ***telecommunication industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***electric power industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
















For the ***construction industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.







For the ***fashion industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***entertainment industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***music industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.









For the ***manufacturing industry***, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










For the ***regulatory compliance market***, and regulatory enforcement and inspection/auditing market, these are the skill sets for applied machine learning and data science roles.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




























#	Corporate Research Labs



+ Toyota Research Institute, TRI





##	EDA Research







###	EDA + Related Research with Alibaba Group


+ Using HW/SW Mechanisms to Improve Performance of remote Heterogeneous Systems
	- Alibaba is an e-commerce and AI company. We generate enormous data and consume huge amount of computation and storage resources every day. It is critical for Alibaba to keep on improving data center design given the emerging of powerful accelerator computation clusters.
	- We would focus on:
		* 1. Analyze different AI workloads in distributed GPU clusters, study their computation and network requirement
		* 2. Based on current remote accelerator technique, improve its efficiency via hardware/software solutions
		* 3. Apply the technique to real workload
	- Requirement:
		* 1. PHD candidate, experienced with distributed heterogeneous systems
		* 2. It's a plus if candidate worked with deep learning algorithms
		* 3. it's a plus if candidate has top conference publications
+ Last mile of datacenter as a computer -- local protocol and semantics based ASIC/FPGA cloud
	- Developers and customers prefer to use heterogeneous compute resources with a set of local server access protocol and semantics. We need to find talents to do research and prototyping with a specific local API on an ASIC or FPGA chip.
+ Emerging Accelerator Architecture, Programming Model, and Optimizations
	- The emerging hardware accelerator architectures, such as process-in-memory (PIM) and neuromorphic computing,  have shown great potential to speed up AI/ML and data-heavy applications. This research aims to investigate these non-traditional architecture designs and their performance implications for domain-specific applications in Alibaba datacenters and ecosystem. It will study the emerging architecture's programming model for usability and explore the software-hardware co-design strategies (e.g. reinforcement learning based architecture space exploration, architecture-aware compression and sparsity exploitation) and optimization trade-offs to maximize the performance.
+ Execution engine optimization based on GPUs and other modern hardware
	- Targeting Maxcompute SQL engine, we'd like to import modern hardware technology (such as GPU, FPGA etc) to model and improve the core operators of the distributed execution engine, optimize the system performance on specific scenes at last.
+ Performance/Power/Area (PPA) Modeling & Analysis
	- The ***Computing Technology Lab of Alibaba Damo Academy*** focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Domain Specific Architecture
	- As the end of Dennard's scaling and Moore's Law running out of steam, the traditional architecture for general-purpose processors can no longer meet the requirements of high performance and low energy consumption for various emerging applications. To allow the computing to have higher performance/energy efficiency, Domain Specific Architecture (DSA) has become a popular solution. However, there are many challenges in the DSA design. For example, the definition of the scope of Domain, trade-off between specialization and general-purpose, instruction set design, compiler design and optimization, memory wall, ultra-low-power design, micro-structure design and optimization, etc. This internship Project is a thorough and detailed study of the DSA to address these challenges.
	- The Computing Technology Lab focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Cloud Server Architecture
	- Perform profiling/modeling and evaluation of workloads for our cloud server, design and optimize server architecture including but not limited to: CPU, cache/memory, storage and accelerators.
+ Research on algorithms/architectures of the next-generation AliNPU for training
	- AliNPU targeting for neural network training is a key component of Alibaba's AI Chip strategy. To design an architecture surpassing the best of the AI training chips, such as GPU and TPU, we must look into all aspects from algorithms to HW architecture, for the potential computational efficiency improvements.
	- The works may focus on one or a few of the following directions：
		* 1. Algorithm innovations that may improve the system efficiency, and the experiments.
		* 2. The analysis of the theoretic bounds and/or the proof with regards to the algorithm innovations.
		* 3. Experimental HW architecture designs, simulations and their PPA analysis.
+ Hyper-scale cloud datacenter's compute resource pool and management platform prototype
	- Compute pools will be widely deployed in hyper-scale cloud datacenter. Alibaba Infrastructure AI Ops Platform (TIANJI) team is now actively seeking talents to work on research in this area.
+ Research on optimization of AI accelerator
	- Nowadays high performance computing has become one of the hot topics of AI research. The research aims on optimizing power dissipation and energy efficiency of AI accelerators targeting various of AI applications, providing high quality computation support for AI applications.
	- The research topics include:
		* 1. Research on computation pattern of various AI applications to look for bottleneck
		* 2. Research on AI accelerator architectures, implementations to improve performance and energy efficiency
		* 3. Codesign AI accelerator (SW/HW) and application to maximize the performance of accelerator)
+ Accelerating Machine Learning Applications on Heterogeneous Computing Architectures
	- This research aims to optimize ML applications on heterogeneous accelerators such as GPUs, FPGAs, and/or ASICs. S/he will conduct analysis and exploration on various performance bottlenecks in the full software/hardware stack, including ML algorithm improvement, model level transformation (e.g. compression, sparsity, data parallelization), and domain-specific architecture innovations, in order to dramatically boost the ML application's performance.






##	Computer Architecture Research




***Resources***
+ [HiPEAC](https://www.hipeac.net/)
	- For computer architecture, compiler design, and related areas.
	- [HiPEAC Jobs](https://www.hipeac.net/jobs/#/)





































##	Hardware Security Research














##	Formal Verification Research & Logical AI Research


***Resources***:
+ [SAT Live!](https://www.satlive.org/)
+ [Formal Land (Arae SARL)](https://formal.land/)














##	Machine Learning Research & Deep Learning Research







###	Machine Learning Research with Alibaba Group

+ Building an innovative and systematic AI benchmarks platform
	- Currently in Alibaba Group, deep learning and related applications have been employed in various business departments. Tmall, Alitrip, Taobao, Ant Financial and other departments are making extensive use of emerging deep learning technologies to continuously improve application and algorithms and enhance the consumer experience. On the one hand, Alibaba's engineering teams design, experiment and deploy different deep learning algorithms and applications every day. On the other hand, deep learning requires a lot of computational power, which also puts higher requirements on the computational power of the hardware and their adaptability to the application. How to balance the demand and supply relationship between these two and integrate the solution into a systematic platform product? How to automatically and systematically evaluate the computional power of an AI hardware? How to evaluate the advantages and disadvantages of a hardware for usage in an application and give customer recommendations through a systematic platform? These are the challenges we are currently dealing with and we need to solve. Recently we have launched the AI Matrix product (through aimatrix.ai website), but it is still in the early stage of the product. In the future, we need more people who have the same understanding as us and are willing to involve in solving these problems. Let's contribute our own strength and make the AI Matrix as an effective systematic platform and an impactful technical brand.
	- [AI Matrix](https://aimatrix.ai/en-us/)
	- https://github.com/alibaba/ai-matrix









##	Computer Vision Research









##	Compiler Design & Program Analysis (or Software Analysis) Research














##	Quantum Computing Research





###	Quantum Computing Research with Alibaba Group

+ Quantum Algorithm for Near-term Quantum Devices
	- A general-purpose fault-tolerant quantum computer will require millions of physical qubits and millions of quantum gate operations. With quantum computers of significant size now on the horizon, we should understand how to best exploit the initially limited abilities and how to develop and run useful quantum algorithms within the limited circuit depth of intermediate size quantum devices with limited error correction.
+ Research in Practical Applications of Quantum-Safe Communication
	- Quantum communication may refer to quantum cryptography, quantum teleportation, and quantum entanglement. Among those, quantum key distribution (QKD) is one of the most practical applications in recent years. Quantum cryptography takes the advantage of the laws of quantum physics to protect data,
	- Currently, the most significant problems in practical quantum cryptography systems include: high-speed quantum random number generation, long-distance fiber quantum key distribution with high key generation speed, co-fiber transmission of classical and quantum optical signals, as well as practical commercialization and stabilization.
	- Our project aims to study these critical issues in quantum cryptography system for practical applications. Due to the transmission loss and dark count, the bottleneck for its practical application lies in the trade-off between high speed key generation rate and long transmission distance. In order to solve these problems, one potential solution is to design more efficient telecommunication protocol to exceed the theoretical up bound of the generation rate. Meanwhile, the project will also focus on the study and practical solutions for quantum random number generation, post-quantum cryptography algorithm, the migration of classical and quantum networks, etc












#	Academic/University Research Labs


##	U.S. Academic/University Research Labs




+ Johns Hopkins University
	- Johns Hopkins University Applied Physics Laboratory, Johns Hopkins APL












