#	DevOps & Site Reliability Engineering (SRE) & DevSecOps & ITOps & IT operations analytics & AIOps & CloudOps & NoOps


##	DevOps



###	Notes About DevOps


A DevOps toolchain is a set or combination of tools that aid in the delivery, development, and management of software applications throughout the systems development life cycle, as coordinated by an organisation that uses DevOps practices.

Generally, DevOps tools fit into one or more activities, which supports specific DevOps initiatives: Plan, Create, Verify, Package, Release, Configure, Monitor, and Version Control.


DevOps include:
+ Microservices
+ DevOps automation
+ Automation with version control
	- Many organizations use version control to power DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD. 




DevOps life cycle:
+ continuous development
+ continuous integration
+ continuous testing
+ continuous monitoring
+ continuous feedback
+ continuous deployment
+ continuous operations



Alternate DevOps life cycle, exploiting real-time communication:
+ continuous development/build
+ continuous integration
+ continuous deployment
+ continuous operations
+ continuous feedback
+ continuous planning







DevOps is at the intersection of:
+ software development
+ software quality assurance
+ ITOps



***BizDevOps is DevOps that account for business needs.***
+ Or, ***BizDevOps*** is ***DevOps 2.0***.
+ Collaborate with business units using agile methods
	- Business unites have to contribute to reduce the product/service backlog
	- DevOps teams have to be more responsible for the business aspects of what they do.














###	Skill Set for DevOps



Skill set for DevOps:
+ Kubernetes, the open source system for managing containerized applications across multiple hosts
+ Experience working with tools like Jenkins, Bamboo, Ansible, SonarQube, Artifactory.
+ Familiar with Docker, Conan, JFrog
+ Exposure to container technologies - container orchestrators (Kubernetes, Mesos, Docker Swarm Mode) is a plus
+ Knowledge of cloud provider API (programming with AWS IAM, S3, CloudFormation, Lambda, EC2, CodeBuild or the equivalent from Azure, or Google Cloud Platforms)
+ Experience with Redis, Postgres, Elasticsearch
+ Automate and integrate security into CI/CD pipelines, such as static code analysis and dynamic code analysis.
+ Docker, Heroku, Kubernetes
	- Heroku is a cloud platform as a service supporting several programming languages.
+ Working knowledge of industry leading configuration management tools and methods (Salt, Ansible, Chef, Puppet, etc)
+ skill set:
	- Software Engineer III, HPP
	- With more than 40 million users, Anaconda is the world’s most popular data science platform and the foundation of modern AI development. We pioneered the use of Python for data science, championed its vibrant community, and continue to steward open-source projects that make tomorrow’s innovations possible. Our enterprise-grade solutions enable corporate, research, and academic institutions around the world to harness the power of open source for competitive advantage, groundbreaking research, and a better world.
	- Anaconda is seeking people who want to be at the center of Artificial Intelligence. Candidates should be knowledgeable and capable, but always eager to learn more and to teach others. 
	- Here is why people love most about working here: We’re not just a company, we’re part of a movement. Our dedicated employees and user community are democratizing data science and creating and promoting open-source technologies for a better world, and our commercial offerings make it possible for enterprise users to leverage the most innovative output from open source in a secure, governed way.
	- Anaconda is seeking a talented Software Engineer III to join our High Performance Python team. This is an excellent opportunity for you to leverage your experience and skills and apply it to the world of data science and machine learning.
	- Build and maintain an automated system for running Python benchmarks on a variety of platforms.
	- Interacting with IT to provision and monitor benchmarking resources in the cloud or on-premise.
	- Building data analysis pipelines to process performance data and share it with stakeholders.
	- Write Python-based tools to setup benchmark environments, run benchmarks, and gather / analyze results
	- Python development and packaging experience
	- CI/CD experience
	- Devops tools, such as Terraform
	- Some data engineering tool experience: Examples include dbt, Airflow, Prefect
	- Team attitude: “I am not done, until WE are done”
	- Embody our core values:  
		* Great People
		* Great Product
		* Great Performance
	- Care deeply about fostering an environment where people of all backgrounds and experiences can flourish 
	- Experience with high-performance computing and program optimization
	- Knowledge of Python internals
	- Experience creating compiled Python extensions
	- Experience with GPU computing
+ skill set:
	- Knowledge or hands-on experience in Container technologies such as Docker is a plus.
	- Knowledge of Virtual machines, Hypervisors is a plus
+ skill set:
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Working experience and detailed knowledge in Java, JavaScript, or Python
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Agile development (Scrum) experience
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- monitoring tools including New Relic, Splunk, and SignalFX;
	- Apache Camel Messaging Services
	- JBoss
	- build tools, including ANT, Maven, and Gradle
	- UML
+ Buildroot
	- Buildroot is a set of Makefiles and patches that simplifies and automates the process of building a complete and bootable Linux environment for an embedded system, while using cross-compilation to allow building for multiple target platforms on a single Linux-based development system. Buildroot can automatically build the required cross-compilation toolchain, create a root file system, compile a Linux kernel image, and generate a boot loader for the targeted embedded system, or it can perform any independent combination of these steps.
+ Software Engineer, Developer Experience
	- San Francisco, California, United States — Research Acceleration
	- The Research Acceleration team builds high-quality research tools and frameworks to increase research productivity across OpenAI, with the goal of accelerating progress towards AGI. We frequently collaborate with other teams to speed up the development of new state-of-the-art capabilities.
	- As we scale up with more researchers and engineers joining OpenAI, we seek a pragmatic and passionate engineer with a strong focus on the researcher experience.
	- In this role, you will be responsible for building and maintaining systems that allow our research + engineering organization to iteratively develop, test, and deploy new features reliably, with high velocity, and with a frictionless and fast development cycle.
	- You will be tasked with profiling and optimizing continuous integration pipelines, building better testing infrastructure, and serving as the go-to expert for any build or test-related inquiries. Our current environment relies heavily on Python, Rust, and C++, which you will take ownership of and strive to transform into a state of the art development experience for research.
	- Ultimately, your role will be to provide the necessary tools and metrics to support our fast-paced culture and ensure a stable, scalable platform for growth, while also fostering a seamless and low friction experience for OpenAI’s research.
	- This role is based in our San Francisco HQ. We offer relocation assistance to new employees.
	- Are a proficient Python programmer, with experience using Rust in production
	- Have worked with and on build systems
	- Are proficient with Kubernetes
	- Annual Salary Range: $245,000—$370,000 USD
+ skill set:
	- You should be experienced working with streaming platforms such as Azure EventHub, Apache Kafka, Apache Flink or Amazon Kinesis. You’re comfortable with infrastructure tooling such as Terraform, worked with Kubernetes, and have the skillsets of a SRE. 
	- Maintain the health and operability of critical data infrastructure systems such as Kafka, Azure EventHub while ensuring scalability, reliability, and security 
	- Ensure our data platform can scale reliably to the next several orders of magnitude
	- Accelerate company productivity by empowering your fellow engineers & teammates with excellent data tooling and systems, providing a best in case experience
	- Bring new features and capabilities to the world by partnering with product engineers, trust & safety and other teams to build the technical foundations
	- Like all other teams, we are responsible for the reliability of the systems we build. This includes an on-call rotation to respond to critical incidents as needed
	- Have 4+ years in stream infrastructure engineering maintaining Kafka, Azure EventHub, AWS Kinesis OR
	- Have 4+ years in infrastructure engineering with a strong interest in streaming systems
	- Take pride in building and operating scalable, reliable, secure systems
	- Are comfortable with ambiguity and rapid change
	- Have a voracious and intrinsic desire to learn and fill in missing skills—and an equally strong talent for sharing learnings clearly and concisely with others
	- Annual Salary Range: $200,000—$385,000 USD
+ Software Engineer, Developer Productivity
	- The Platform Runtime team builds core components to enable productive research from small to state of the art scale across OpenAI, with the goal of accelerating progress towards AGI. We frequently collaborate with other teams to speed up the development of new state-of-the-art capabilities.
	- As we scale up with more researchers and engineers joining OpenAI, we seek a pragmatic and passionate engineer with a strong focus on the development experience for both engineers and scientists.
	- In this role, you will be responsible for building and maintaining systems that allow our research + engineering organization to iteratively develop, test, and deploy new features reliably, with high velocity, and with a frictionless and fast development cycle.
	- You will help oversee and drive to the vision of how we should build, test and deploy software. You will drive the design of our continuous integration pipelines, testing infrastructure, training and support around our build system. Our current environment relies heavily on Python, Rust, and C++, which you will take ownership of and strive to transform into a state of the art development experience for research.
	- Ultimately, your role will be to provide the necessary tools and metrics to support our fast-paced culture and ensure a stable, scalable platform for growth, while also fostering a seamless and low friction experience for OpenAI’s research.
	- Have supported large monorepo deployments before
	- Are a proficient Python programmer, with experience using Rust in production
	- Are proficient with Kubernetes
	- Annual Salary Range: $245,000—$385,000 USD
+ skill set:
	- Experience with system level monitoring tools such as Nagios or Zabbix and application performance monitoring tools such as NewRelic, AppDynamics or Dynatrace.
	- Understand configuration management tools such as Puppet, Chef or Ansible.
	- Strong understanding of the DevOps landscape from orchestration to instrumentation, from VCS to CI tools, and from APM to monitoring tools
+ skill set:
	- Working experience with AWS services (Aurora DB, Dynamo DB, Athena, EMR, Redshift, Data Catalog etc)
	- Experience with log analysis tools like Splunk
	- Experience with Issue trackers tools like Jira etc
+ DevOps Engineer
	- Hyderabad, Telengana
	- B.E /B.Tech /M.Tech/MCA degree from a recognized and accredited university/college.
	- 5+ yearsof strong experience as a DevOps Engineer.
	- Building and setting up new development tools and infrastructure.
	- Experience in Container and Container Orchestration like Docker, ECS Fargate, Kubernetes (k8 cluster) with Helm, Istio and in cloud (EKS, GKE, AKS).
	- Experience in deploying/configuring logging/monitoring/visualization dashboard tools like ELK, Grafana, Splunk, Prometheus, etc.
	- Experience working in large-scale datacenter/cloud/network environments.
	- Automate the development cycle using the CI/CD pipelines.
	- Programmingexperience in Groovy/Python/Shell.
	- Experience with Git and build tools such as Jenkins, Maven.
	- Knowledge of REST API development standards.
	- Experience with virtualized products/services/solutions.
	- Good Knowledge of tools like Jira, Confluence, Git, Jenkins.
+ The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
	- BuildBot:
		* job scheduling system
		* open-source framework for automating software build
		* continuous integration tool, in the context of CI/CD
	- Phabricator
		* suite of Web development collaboration tools
			+ code review tool
			+ change monitoring tool
		* supports post-commit auditing
		* customizable task management
+ skill set:
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
		* ILOM
			- Integrated Lights Out Manager, ILOM
		* BMC
			- process-centric AIOps
		* OCP, Open Compute
	- Good knowledge of common development tools such as yocto/git/gtest
		* gtest, GoogleTest: Google Testing and Mocking Framework
			+ Software test framework for writing and running C++ unit tests, and is based on the xUnit architecture for software unit testing
+ skill set:
	- SGE or other DRMS
		* SGE
			+ search generative experience, based on generative AI
				- for AI-powered overviews
			+ Sun grid engine
		* DRMS, distributed resource management systems
		* in the context of job scheduler, or batch scheduling for batch processing 
			+ also known as:
				- batch system
				- DRMS, distributed resource management systems
				- DRM, distributed resource manager
				- workload automation
	- XML and XPath/XSLT
		* XML
		* XPath/XSLT
			+ XPath: XML Path Language, used with extensible stylesheet language transformations, XSLT
				- extensible stylesheet language, XSL
+ skill set:
	- Experience with a Data Warehouse (Redshift, Snowflake, etc) and data analysis using SQL
	- Experience with ETL and Data Integration Platforms like Mulesoft, SnapLogic and Airflow
	- Experience in at least one programming language (e.g. R, Python, Java, Ruby, Scala/Spark, or Go)
	- Understanding of data modeling, performance analysis and production DB migrations
	- Familiar with SFDC, Netsuite and Workday data models
	- Develop pub/sub, streaming and batch data loads into our Data warehouse (Snowflake)
	- Design scalable integrations with our product management teams that move key business data between our big rocks (SFDC, Workday, Netsuite, Internal Systems)
	- Experience with Looker, Tableau or other business intelligence platform
	- Experience with platform data like SFDC, NetSuite, Workday
+ skill set:
	- Familiarity with ETL/ELT and related techniques
	- Exposure to CI / CD (with either Docker, Kubernetes, SaltStack or Jenkins)
	- Prior experience in adtech or martech
+ skill set:
	- Troubleshooting and resolving technical support requests created by our customers spanning a growing range of container products and services, including Managed Kubernetes and Container Registry
	- Experience troubleshooting basic and advanced Kubernetes issues ranging from pods and deployments to the control plane
	- Knowledge of kubectl, community projects such as helm, istio, linkerd, prometheus, NGINX ingress-controller, and similar software and utilities used to manage deployments
	- Bonus: Certifications such as CKA and/or CKAD
		* Certified Kubernetes Application Developer (CKAD)
			+ From Cloud Native Computing Foundation (CNCF).
		* Certified Kubernetes Administrator (CKA)
			+ From Cloud Native Computing Foundation (CNCF).
	- Bonus: Experience with one or more database engines (MySQL, MariaDB, PostgreSQL, Redis, MongoDB)
+ skill set:
	- Experience deploying, scaling, and troubleshooting NodeJS, Python, and PHP applications in production
	- Knowledge of the CI/CD pipeline and GitHub repositories, actions, and deployment workflows
	- Growth mindset, with an unrelenting focus on improving your knowledge and skill set
	- Highly self-motivated with the ability to work independently and collaboratively with a high degree of autonomy in a remote work environment
	- Bonus: Experience troubleshooting basic and advanced Kubernetes issues, from pods and deployments to the control plane
	- Bonus: Experience troubleshooting one or more database engines: MySQL, MariaDB, PostgreSQL, Redis, MongoDB
+ skill set for DevOps:
	- Remote hardware administration with IPMI
		* IPMI, intelligent platform management interface
			+ standardized message-based hardware management interface
				- control the:
					* baseboard management controller, BMC
						+ or, management controller, MC
	- Configuration and management of
		* SGE/Univa, Slurm, LSF or other DRMS
		* Jenkins CI
		* Phabricator
		* FlexLM licensing
	- Puppet, Ansible, Nagios
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
	- Web programming – HTML/DOM, JavaScript, SQL
+ skill set:
	- Passion for process automation
	- Build system experience like Maven, Bazel, or Gradle
	- Continuous integration and testing experience like Jenkins
	- [Preferred] Kubernetes and Docker
	- [Preferred] Experience on working with services provided by AWS, Azure, or GCP
+ skill set:
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- One or more of the following languages/technologies: Electron, React, three.js, react-three-fiber, Typescript, Javascript, C++ or C#
	- The following languages/technologies: GraphQL, PostgreSQL, TypeORM, Apollo Server
	- Modern Development Environment - React, React-three-fiber, Flux, Electron, ZeroMQ, Protobuf, Typescript, Javascript, C#, C++, VSCode, Github, NUnit
	- Continuous Integration - TeamCity, Chef, Terraform, Kubernetes, Docker, Jenkins
	- Scrum - JIRA, Google Docs, Metabase, integrated QA
+ tech stack:
	- Docker (Kubernetes)
	- Spark (on Hadoop)
	- Kafka
	- Cassandra (or other NoSQL DBs)
	- AWS and some of its services
	- Azure and some of its services
+ Knowledge of container (docker or others) and orchestration (K8S or others) technology is a plus
+ Familiarity with automated build systems such as Jenkins or buildbot.
+ skill set:
	- Experience with the [Common Workflow Language (CWL)](https://www.commonwl.org/), [Arvados](https://arvados.org/), PERL, C++, AWS, Docker
	- [Dockstore, developed by the Cancer Genome Collaboratory, is an open platform used by the GA4GH for sharing Docker-based tools described with the Common Workflow Language (CWL), the Workflow Description Language (WDL), or Nextflow (NFL)](https://dockstore.org/)
	- [Nextflow](https://www.nextflow.io/)
	- [Workflow Description Language (WDL)](https://software.broadinstitute.org/wdl/)
	- https://fairsharing.org/bsg-s000606/
	- https://dnastack.com/#/
	- https://curoverse.com/about
+ Strong understanding of the Hadoop ecosystems (especially Kafka, Flume, Avro, Parquet, HBase, Hive, and Hue) and related technologies with +2 years of experience
+ skill set:
	- Reviewing the ETL Design and Data Roadmap, and continuously improving the processes.
	- Ensuring the ETL Codes are running and constantly improving/following the game features and Business Requirements.
	- Analyzing, developing and defining data integration solutions related to big data storage;
	- Applying data modeling, data design and implementation to support business requirements;
	- Participating in all agile development lifecycle activities: estimating, planning, designing, developing, documenting and testing;
	- A minimum of 3 years of professional experience in Data Transformation (ETL) and systems integration processes.
	- A deep understanding of Data flow, Database and data transformation principles (Big data and object storage concepts knowledge considered a major asset).
	- Knowledge of big data environment is an asset (Kafka, Databricks, Vertica, Others)
+ skill set:
	- Assisting developers to apply best practices to ensure fully working test, training and production environments using Gitlab-CI, Docker, Ansible.
	- Designing, building and maintaining CI/CD, testing, and operations infrastructure for our systems.
	- 5 years of DevOps experience, both building end-to-end automated CI/CD pipelines, as well as application and operations support. This experience should include hard-core hands-on Linux admin, networking, security, and AWS experience in a dev-through-production environment.
	- A proven track record of excellent customer service delivery, including working with developers, ops, and users to troubleshoot and resolve challenging problems in a timely manner, and being an embedded DevOps member of an application scrum team, as well as requirements gathering, design, project planning, and implementation of DevOps process and tooling.
	- Strong architectural level of understanding of software, networks, security, and operations, with the knowledge and know-how to influence software and operations design and process.
	- Strong hands-on familiarity with infrastructure automation tools such as Jenkins, GoCD, Terraform, Artifactory/Nexus, Ansible, Puppet, Chef, InSpec, etc.
+ Experience in the development, test, deployment and administration of one of the following types of systems: Ngnix, Kubernetes, Docker, OpenStack, Hadoop, Spark, Flink, etc. is preferred
+ Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)
+ skill set:
	- Proxmox, KVM virtualization, LXC and Docker containers
	- large scale object storage (Ceph, cloud-based object storages)
	- Puppet
		* Puppet is a software configuration management tool which includes its own declarative language to describe system configuration.
	- PostgreSQL
	- Distributed architecture (RabbitMQ, Kafka)
	- Icinga/Prometheus/ELK monitoring
		* Icinga
			+ Icinga is an open-source computer system and network monitoring application.
				- Fork of Nagios
					* Nagios Core /ˈnɑːɡiːoʊs/, formerly known as Nagios, is a free and open-source computer-software application that monitors systems, networks and infrastructure. Nagios offers monitoring and alerting services for servers, switches, applications and services. It alerts users when things go wrong and alerts them a second time when the problem has been resolved.
				- Monitor Your Entire Infrastructure
				- The Icinga stack spans six core strengths that cover all aspects of monitoring. Level up with valuable insights and on-time notifications, eye-opening visuals and analytics. Icinga easily integrates within your systems, and gives you the power to automate your tasks. Get going and stay in form!
		* Prometheus
			+ Prometheus (software), a monitoring system with a time series database
			+ Prometheus is a free software application used for event monitoring and alerting. It records metrics in a time series database (allowing for high dimensionality) built using an HTTP pull model, with flexible queries and real-time alerting.
			+ Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. 
			+ Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.
			+ The Prometheus monitoring system and time series database.
			+ Prometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed.
+ skill set:
	- Familiarity with containerization technologies (docker, lxc, rkt, etc.).
	- Familiarity with container orchestration technologies (Kubernetes, Marathon, etc.).
	- Experience working with cloud computing services providers (AWS, Google cloud platform, Azure, etc.).
	- Experience with Elasticsearch /Apache Solr and Logstash
	- Experience working with Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ, Celery, Kafka, etc.
	- Scalable data processing techniques: Kafka, Spark, ElasticSearch, Celery
	- Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ
	- Have proven experience with ORMs (e.g. Django) and RDBMS (e.g. MySQL) including development of complex SQL queries.
+ skill set:
	- Experience with Devops tools such as Jenkins, Nagios/icinga, Ansible, hashicorp vault.
	- Experience with Docker, Kubernetes is required.
+ Orchestration platforms for containers (i.e. Docker, Kubernetes, Swarm or ECS)
+ skill set:
	- What You’ll Do:
		* Be a key member of the Anaconda Enterprise team
		* Attend daily SCRUM meetings, sprint demos, etc.
		* Work with other teams to coordinate joint deliverables 
		* Work with QA team to ensure highest level of product quality
		* Create and maintain software documentation
	- What You Need:
		* 5 years minimum in a System Engineering or Backend Engineering role
		* Deep Linux system administrator knowledge
		* Deep understanding of core AWS services (IAM, EC2, VPC, EKS)
			+ Experience with IaC tools (Terraform, Ansible, Packer)
		* GitOps mindset with a desire to automate everything
		* Experience with IaC tools (Terraform, Ansible)
			+ Experience with IaC tools (Terraform, Ansible, Packer)
		* Experience with Kubernetes templating (Helm, Kustomize)
			+ Kubernetes experience (EKS preferred)
			+ Familiarity with CNCF ecosystem and its projects
		* Production Kubernetes experience (GKE, EKS, OpenShift)
		* Experience building and managing CI/CD pipelines (Jenkins, GitHub Actions)
		* Production experience with Envoy and Kubernetes service meshes (Istio)
	- What Will Make You Stand Out:
		* Relevant programming experience with Python or Go
		* Experience working in a fast-paced startup environment
		* Experience running Hashicorp Vault in production
		* Experience with GitOps tools like ArgoCD
		* Experience working in a open source or data science-oriented company
+ skill set:
	- Develop production-quality software, including testing, documentation, static analysis, and CI/CD/CT.
	- Experience with RTOSes such as FreeRTOS.
+ skill set:
	- Manage Kubernetes clusters across internal and cloud (AWS, Alicloud, etc.) environments.
	- Build and improve internal developer tools and drive the best practice of Kubernetes to increase productivity across the engineering organization.
	- Automate the Kubernetes deployment and maintain a consistent process across cloud and on-premise environments.
	- Performance tune, load test, and optimize scalability and availability of Kubernetes clusters.
	- Conduct capacity planning and resource optimization to support fast-growing data processing and machine learning requirements.
	- Bachelor of Science in Computer Science or equivalent.
	- 3+ years of hands-on technology experience in large enterprise environments.
	- Deep knowledge of containerization and Kubernetes.
	- Strong programming skills in python, java, or C++.
	- Familiarity with open-source configuration, orchestration, and CI/CD tools.
	- Excellent communications skills, capable of working with cross-functional technical and business teams and varying levels of management, in a professional manner
	- Strong TCP/IP networking knowledge
	- Experience operating and developing infrastructure and services in public cloud environments (AWS, GCP, or Azure)
	- Experience working in a 24/7 production engineering organization
+ skill set for Frameworks Integration Engineer:
	- Build and maintain SiFive Software CI/CD/CT flow pipelines using build and release orchestration tools (Jenkins, Travis CI, etc.)
	- Conduct SiFive Parallel Compute Frameworks testing and integration for product quality qualification and assurance
	- Build the required automation tools on the basis of past experience in scripting (BASH, Perl, Powershell, Python)
	- Closely collaborate with geographically distributed software and engineering teams.
	- 3+ year experience on large scale software integration
	- 1+ year experience on CI/CD/CT flow development
	- 3+ years Software design and programming experience in C/C++/Python for testing, debugging and problem solving
	- Experience with building tool/system like Make, CMake, Yocto, and Bazel
	- Familiar with gtest, python unittest or other testing frameworks.
	- Familiarity with version control tool with GIT and GitHub
	- Familiarity with software release management tools is plus
	- Strong system administration (Linux/Unix or Windows) at the command-line level is a plus
	- Good understanding of Deep learning, Computer Vision, NLP is plus
	- Familiarity with ML framework (Tensorflow/Tensorflow-Lite/Pytorch) is plus
	- In addition to the above, if you have rich experience in software automation and testing, and good system debugging and integration capability in large scale software systems, you are the professional we are looking for!
+ skill set:
	- Use your understanding of the Hadoop eco-system to build reliable and scalable automated systems for validating Cloudera products.
	- Work with our world-class development team to define new tools and tooling features that assist in triaging and debugging test failures.
	- Work with cross component teams and be an effective team player.
	- Experience developing in a containerized, Kubernetes environment
	- Experience with Kibana, Elastic Search, Ansible, and Helm a plus
	- Prefer familiarity with large-scale distributed systems and/or data management systems
	- Experience working with open source automation tools and familiarity with Git, Maven, Gerrit a plus
	- Experience with Apache Hadoop and its related technologies a big plus
+ skill set:
	- configuration management and orchestration tools:
		* Ansible
		* Chef
		* Docker
		* Terraform
	- using a Linux-based software development environment, and implementing DevOps best practices:
		* Airflow
		* Ansible
		* Docker
		* Kubernetes
		* Terraform
	- areas:
		* machine learning
		* natural language processing
		* machine vision
		* recommendation systems
		* data pipeline architecture
	- building tools for application performance, continuous integration, continuous deployment, or developer environments
	- Experience with:
		* Docker
		* Jenkins
		* Kubernetes
		* Terraform
		* Travis CI
		* other infrastructure/CI tools
+ Expertise with several continuous integration technologies (Jenkins, Ansible, CloudFormation, Terraform, etc.)
+ Demonstrated proficiency with Docker and container orchestration technologies (Kubernetes, ECS, etc.)
+ skill set:
	- As a Software Engineer on the CI/CD team, you will own the build/release pipeline operation of our production, staging and development systems. You will own and develop our CI/CD tooling and infrastructure, including high-compliance services on AWS/Azure, and help us roll out new services using Kubernetes on AWS/Azure or on-premise. You will lead the way in building tooling and automation to close the feedback gaps in the CI/CD pipeline, develop new integrations with third-party services and monitor the health of our build/deploy infrastructure. You will be responsible for the performance of our test suite and will be instrumental in creating a comfortable development environment for a growing team of software engineers. You will work in close cooperation with the Cloud team to achieve these goals. You will also get the opportunity to learn the fundamentals of distributed computing. You will get to work with infrastructure that handles massive amount of scale running tens of millions of tests per day across thousands of machines.
	- Requirements:
		* Bachelor’s degree in Computer Science, Electrical Engineering, or related field.
		* Software Engineer passionate about infrastructure and scale.
		* 3+ years of experience in a DevOps/SRE/Build-Release role, preferably using Kubernetes and Jenkins.
		* Experience with CI/CD principles, architecture and operations.
		* Experience setting up and working with Jenkins in a containerized environment.
		* Experience with Docker and container orchestration tools like Kubernetes, AWS ECS/EKS and (Azure) AKS.
		* Proficiency with a scripting language to develop integrations (Python, Javascript, Groovy).
		* Rigor in high code quality, automated testing, and other engineering best practices.
		* Thrive in a fast-paced, dynamic environment and value end-to-end ownership of projects.
		* Intellectually curious and open to challenges.
	- Preferred
		* Advanced degree in engineering, sciences or related field.
		* 2+ years of experience working with distributed systems.
		* 2+ years of experience with using and developing technologies like Cassandra, PostgreSQL, RedShift, DynamoDb, Elastic Beanstalk, Docker, and Amazon EMR.
		* Strong experience with Python, Bash, Jscript and automation tools such as Chef, Puppet, Ansible, etc.
		* Strong experience with container orchestration technologies such as Kubernetes, Docker Swarm, Mesos.
		* Proficiency in Linux administration, configuration, and automation tools.
		* Working knowledge of public Cloud platforms (AWS, Azure, Google Cloud Platform).
		* Knowledge of performance benchmarking and diagnostic tools.
		* Solid understanding and usage of observability solutions such as cAdvisor, Prometheus, Dynatrace, Splunk etc.
		* Experience with Java, Scala.
+ skill set:
	- worked within standard GitOps workflow
		* branch and merge
		* pull requests or PRs
		* CI/CD systems
	- DevOps via CI/CD and Docker+K8S
	- scalable data pipelines
	- infrastructure configuration
		* IaC, such as Terraform
		* cluster parameter tuning
		* service parameter tuning
	- data discovery tooling
		* Amundsen
		* Atlas
	- determine optimal caching strategies and eviction policies
	- approximation algorithms for high-use statistics of interest, such as approximate nearest neighbor
	- determine approximate relaxations to deterministic compute where appropriate and leverage probabilistics data structures, such as bloom filters and count min sketch
+ skill set:
	- Your Responsibilities:
		* Design and develop the core authentication and authorization system components of the C3 AI Suite
		* Research and apply identity and certificate management solutions to secure a distributed system deployed on Kubernetes (K8s)
		* Ensure that our core foundational system components including distributed stream processing engine, distributed queuing, distributed batch processing and cluster management are secure
		* Enable customers, partners and system integrators to securely integrate our platform into their infrastructure
		* Shrink the overall attack surface and keep it minimal throughout continuous integration and delivery
		* Work closely with our information security and operations teams to investigate and mitigate threats
	- Requirements:
		* Bachelor’s degree in Computer Science, Electrical Engineering, or related field.
		* 2+ years of relevant experience
		* Strong communication and interpersonal skills
		* Systematic problem-solving approach coupled with a strong sense of ownership and independence
		* Strong understanding of Computer Science fundamentals
		* Experience with security in Kubernetes (K8s) and the concept of a service mesh (Istio)
		* Proficiency with Java Cryptography infrastructure
		* Expert knowledge of SAML and OAuth authentication / authorization protocols
		* Proficiency in networking protocols, naming services and network trust
		* Demonstrated understanding of TLS certificate management
		* Demonstrated understanding of http protocol security (CORS, XSRF, etc.)
	- Preferred:
		* Master’s degree in Computer Science, Electrical Engineering, or related field.
		* Proficient in Python and JavaScript
+ DevOps Engineer
	- Speedata, an innovative fabless semiconductor company, is looking for an experienced and skilled DevOps Engineer to join our team. We are seeking a candidate with a Python programming background, hands-on lab experience, and a team player with excellent problem-solving skills. 
	- Working at least four days a week in the Netanya office.
	- Collaborate closely with development teams to maximize efficiency.
	- Create, build, and maintain automation workflows, scripts, and CI/CD pipelines across multiple environments.
	- Maintain custom container-based infrastructure and orchestration.
	- Oversee both on-premises servers and cloud infrastructure.
	- Supporting basic lab operations like PCIe card installations.
	- Create and implement design strategies to ensure the reliability, observability, performance, and security of both infrastructure and applications.
	- 3+ years of experience as a DevOps Engineer.
	- Strong programming skills in Python.
	- Deep understanding of software development, build, and deployment workflows for C/C++, Python, and Java.
	- Experience in designing and implementing CI/CD pipelines, preferably in GitLab.
	- Extensive hands-on experience with Docker usage and container orchestration tools.
	- Extensive hands-on experience with Linux systems and scripts.
	- Basic hands-on experience with lab operations like PCIe card installations.
	- Experience with managing cloud infrastructure, preferably AWS.
	- Strong problem-solving and troubleshooting skills.
	- Excellent communication and collaboration skills.
	- Knowledge of SQL and data analytics.
	- Knowledge in HW development using FPGA and Emulators.
	- Experience with infrastructure-as-code tools like Terraform.
+ skill set:
	- production level Kubernetes, Helm, and Terraform experience
		* Helm
			+ package manager for the Kubernetes platform, or "software built for Kubernetes"
			+ "Helm is a package manager that helps developers "easily manage and deploy applications onto the Kubernetes cluster." 
		* Terraform
	- experience with GitOps workflows for changes to environments
		* GitOps workflows
			+ "GitOps is not a single product, plugin, or platform"
			+ GitOps components include:
				- infrastructure as code, IaC
				- merge requests (MRs), and pull requests (PRs)
				- CI/CD, continuous integration and continuous delivery
	- experience with observability tools such as Prometheus, Grafana, ELK, Tracing, etc.
		* Prometheus
			+ Prometheus (software), a monitoring system with a time series database
			+ Prometheus is a free software application used for event monitoring and alerting. It records metrics in a time series database (allowing for high dimensionality) built using an HTTP pull model, with flexible queries and real-time alerting.
		* Grafana
			+ Grafana is a multi-platform open source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources.
		* ELK
			+ ELK stack, a technology stack composed of Elasticsearch, Logstash, and Kibana, now called Elastic Stack (probably this technology)
			+ Extension Language Kit, an implementation of the Scheme programming language
		* Tracing
			+ https://en.wikipedia.org/wiki/Tracing_(software)
+ Automate continuous delivery pipelines for fast and reliable updates
+ skill set:
	- Implementing infrastructure automation, metric collection, and impactful reporting to make data driven decisions.
	- Modern monitoring, metrics, and alerting using open source tools.
	- Improving and developing processes and best practices, maintaining documentation of the development environments.
+ Maintaining awareness of internal GitHub initiatives, the direction of the Source Control Management (SCM) industry, and trends in global developer collaboration tools and techniques.
+ Knowledge of Source Control Management (SCM) tools and workflows
+ skill set:
	- Systems Engineer / DevOps
	- Hive is the leading provider of cloud-based AI solutions for content understanding, trusted by the world’s largest, fastest growing, and most innovative organizations. The company empowers developers with a portfolio of best-in-class, pre-trained AI models, serving billions of customer API requests every month. Hive also offers turnkey software applications powered by proprietary AI models and datasets, enabling breakthrough use cases across industries. Together, Hive’s solutions are transforming content moderation, brand protection, sponsorship measurement, context-based ad targeting, and more.
	- DevOps and Systems Team
		* Our unique machine learning needs led us to open our own data centers, with an emphasis on GPU resources. Even with these data centers, we maintain a hybrid infrastructure into AWS to power some parts of our consumer apps. As we continue to commercialize our machine learning models, we also need to grow our DevOps and Systems team to maintain the reliability of a SaaS offering for our customers. Our ideal candidate is someone who is able to thrive in an unstructured environment and takes automation seriously. You believe there is no task that can’t be automated and no server scale too large. You take pride in ensuring developers can deploy their servers without worrying about downtime.
	- Automate manual operational processes
	- Improve workflows of developer, data, and machine learning teams
	- Manage integration and deployment tooling
	- Create, maintain, monitor, secure and audit infrastructure
	- Manage a diverse array of technology platforms, following best practices and procedures
	- Participate in on-call rotation and root cause analysis
	- Utilize OWASP top 10 techniques to secure code from vulnerabilities
	- Maintain awareness of industry best practices for data maintenance handling as it relates to your role
	- Adhere to policies, guidelines and procedures pertaining to the protection of information assets
	- Report actual or suspected security and/or policy violations/breaches to an appropriate authority
	- Minimum 1 - 2 years of previous experience in development, operations, IT, or a related field
	- Comfortable working on Linux infrastructures (Debian) via the CLIAble to learn quickly in a fast-paced environment
	- Able to multitask, prioritize, and manage time efficiently independently
	- Able to physically lift equipment at least 30 pounds
	- Can communicate effectively across teams and management levels
	- Degree in computer science, or similar, is an added plus!
	- Technology Stack
		* Operating Systems - Linux/Debian Family/Ubuntu
		* Configuration Management - Chef/Ansible/Puppet/Salt
		* Containerization - Docker
		* Container Orchestrators - Mesosphere/Kubernetes
		* Scripting Languages - Python/Ruby/Node/Bash
		* CI/CD Tools - Jenkins
		* Network hardware - Arista/Cisco/Fortinet
		* Hardware - HP/SuperMicro
		* Storage - Ceph, S3
		* Database - Scylla, Postgres, Pivotal GreenPlum
		* Message Brokers: RabbitMQ
		* Logging/Search - ELK Stack
		* AWS: VPC/EC2/IAM/S3
		* Networking: TCP / IP, ICMP, SSH, DNS, HTTP, SSL / TLS, Storage systems, RAID, distributed file systems, NFS / iSCSI / CIFS
+ skill set:
	- Monitor stack health for issues with reliability, performance, durability, and security, maintaining our zero-downtime standard
	- Implement tooling across our AWS infrastructure and services to identify and remove bottlenecks
	- Develop our infrastructure-as-code initiative to ensure all environmental changes are tested, audited, and reproducible across environments and stacksImplement
	- Docker containers and container management to support out-of-the-box deployment of applications across environments
	- Manage CI and CD tools in coordination with the software engineers
	- Research and promote new DevOps tools to simplify processes and identify opportunities for improving existing processes or implementing new process automation
	- Handle production and non-production support issues as they arise
	- Participate in a 24/7 on-call rotation
	- Maintain and test Disaster Recovery procedures
	- Manage penetration testing, vulnerability testing, and web application scanning
	- Represent DevOps during design and development of software or extensive revisions to existing applications
	- Support the sales team in completion of security & architecture questionnaires
	- Organize, conduct and own follow-up action items for Root Cause Analysis meetings
	- Deep knowledge of AWS services, including ALB, CloudFormation, CodeBuild, CodePipeline, ECS, IAM, Lambda, RDS, Route53, S3, and SSM
	- Expertise with an automation framework such as CloudFormation, Ansible, Chef, Salt, or Puppet
	- Strong background in administering Linux, including Apache/Nginx web services
	- Strong scripting skills (Bash, Python) with the ability to develop ad hoc tools
	- Knowledge of networking concepts (Firewalls, Network ACLs, HTTP, DNS)
	- Experience with Infrastructure
	- Monitoring using tools such as Datadog, NewRelic, etc
	- Experience working with AWS CodeBuild, CodePipeline, Jenkins, Concourse or other build automation tools
	- 5+ years of relevant experience
	- Experience with Git and GitHub for version control
	- Strong interest in higher education, startups, and/or SaaS technology
+ skill set:
	- Cerebras is developing a radically new chip and system to dramatically accelerate deep learning applications. Our system runs training and inference workloads orders of magnitude faster than contemporary machines, fundamentally changing the way ML researchers work and pursue AI innovation.
	- We are innovating at every level of the stack – from chip, to microcode, to power delivery and cooling, to new algorithms and network architectures at the cutting edge of ML research. Our fully-integrated system delivers unprecedented performance because it is built from the ground up for the deep learning workload.
	- You will develop and maintain the automation infrastructure for functional and performance test of Cerebras product. As an Automation Framework Developer, you will be responsible for designing and developing scalable automation framework to run in cloud and in our datacenter. You will work closely with the development and QA teams to support various test flows and to improve engineering productivity for software that runs on the Wafer Scale Engine (WSE), the world’s largest and fastest AI computer.
	- Minimum 10+ years of experience in software development environments
	- Experience building tools, libraries and automation framework for internal customers
	- Proficient in Python and shell scripting
	- Proficient developing fixtures, hooks and plugins in pytest framework
	- Experience running jobs on server clusters and workload managers like SLURM
	- Strong end-to-end triage, debug, and troubleshooting skills in CI/CD and regressions
	- Experience with setting up Jenkins and writing Jenkins pipelines
	- Expertise with GitHub Actions and GitHub webhooks
	- Linux system administration experience desirable
	- Familiarity with Docker, Kubernetes and container technology
	- Knowledge of building services on top of AWS or other cloud platforms at scale a plus
+ skil set:
	- Familiarity with Go, Kubernetes, Docker, React, AWS/GCP
	- Experience with containers and resource managers (e.g., Kubernetes, Mesos, YARN, Linux package managers)
+ skill set:
	- hands-on experience in configuring, maintaining, and building upon deployments of industry-standard tools (e.g., Jenkins, Docker, CMake, GitLab, Jira, etc.)
	- experience in software shipping cycles (dev, deploy/CD, release, CI,) and open-source software development.
+ CI/CD flows:
	- Bitbucket pipelines
	- CircleCI
	- Azure DevOps
	- Bitrise
	- Jenkins
	- Travis CI
	- GitLabs
	- Bamboo
	- AWS CodeBuild
	- Vercel
+ skill set:
	- You will define and own the future of developer experience at Ashby. You’ll spend most of your time making our engineering team more productive. You should have a strong desire to identify pain points in our current engineering workflows (e.g. CI takes a long time to run due to a very large test suite) and drastically improve them.
	- You’ll also be responsible for helping our team build reliable & scalable products. We invest heavily in observability and streamlined operations of our products and you will be the point person leading these efforts.
	- Our technology stack is: TypeScript (frontend & backend), Node.js, React, Apollo GraphQL, Postgres, Redis.
	- One formality, we require at least 1.5 years of experience as a full-time software engineer for this role.
	- You Could Be a Great Fit If
		* You love diving deep into complex technical issues and finding elegant solutions
		* Terms like runloop latency and predicate pushdown get you excited
		* You are persistent in pursuing optimal solutions, even if they are challenging
		* Big bonus if you have contributed to open source projects and/or have written or spoken about relevant tooling issues in the past
		* Beyond the above, we believe that this series of tweets is a great summary of what makes an amazing software engineer. We know this kind of talent is rare and we're excited to pay top of market to have you join us!
	- Examples of Things You’ll Work On
		* Design and implement a generalized worker infrastructure and API so developers can easily run and monitor async tasks immediately, at a later time, or on a schedule.
		* Work with the business and the engineering team to define SLOs, and work on implementing the corresponding SLIs.
		* Build a CLI tool that runs our development environment and allows engineers to register required or optional services without modifying the CLI source code.
		* Reduce our CI build time by parallelizing tests and other processes using services like GitHub Actions.
+ skill set:
	- We are seeking an experienced and highly motivated DevOps Engineer who will play a key role in the setup, support, and maintenance of the deployment strategies for our services. In this role, you will be responsible for delivery by optimizing practices, improving communications and collaborations, and creating automation.
	- responsibilities:
		* Own the stability and automation of our deployment.
		* Setup, support, and maintain cloud infrastructure
		* Setup, support and maintain code management and CI/CD pipelines
		* Implement monitoring and security best practices.
		* Own the stability and automation of our deployment.
		* Guide and potentially assist API and ML engineers developing in Go and Python.
	- experience and qualifications
		* 3+ years of DevOps experience, preferably in a lead role
		* Experience with Go (Strongly Preferred), or JVM languages like Java or Scala, or C/C++
		* Proficient with HashiCorp Terraform, Vault, and Nomad
		* Advanced ability to craft clear and concise documentation
		* Understanding of deployment orchestration using Kubernetes.
		* Ability to design and manage CI / CD pipelines
+ skill set:
	- Our core cultural values are manifested in our practices and processes every day. We highly value transparency and fairness in everything we do. We look for people who like to move quickly, are ambitious yet humble and have a great sense of humour. If you have a mischievous spark of fun, that’s even better.
	- What you will be doing
		* We’re looking for an experienced DevOps Engineer to join a team of talented engineers to develop high quality products which are scalable, testable, extensible, and provide high value to our customers.
		* As a key member of the server and infrastructure team, you will work closely with the various development teams to provide operational support. This support will help them gain insight into their systems, improve reliability, and increase developer productivity.
		* This is an individual contributor role where you will be reporting to a Team Lead. This role can be based in Vancouver or Toronto, with benefits from a flexible hybrid work model of remote + in office collaboration.
	- Responsibilities
		* The ultimate tasks for a DevOps Engineer can vary from company to company, but the base responsibilities are usually similar. Monitoring and improving infrastructure and processes will be part of the job at Later just as it would be anywhere else. That being said, there are some things that we are particularly keen on at Later.
		* Ability to work with different engineering teams, back end, web front end, and mobile teams.
		* Bringing your wealth of knowledge and experiences (good and bad) to Later and improving our processes and technology.
		* Educating product and stakeholders on the trade-offs of different paths to a given milestone to enable them to make educated decisions for our roadmap.
		* Writing documentation by hand or via automation.
		* Holding other developers, in particular other Senior Developers, to high coding standards through PR reviews.
		* Constantly learning and staying up-to-date through use of dedicated unstructured dev time for you to: making open-source contributions, writing technical blog articles, creating internal tools for non-feature work, as well as self-education on relevant topics.
		* Knowing when good is good enough.
		* Knowing when good just isn’t going to cut it.
		* Joining the on call rotation.
		* Writing documentation, did we mention that?
		* Taking ownership of pieces of code, infrastructure, and processes and then caring enough to improve them.
		* Knowing how to scale workloads while managing the various resources used by them.
		* Owning and improving deployment processes for the different apps and services
		* Supporting CI/CD pipelines for the various development teams
		* Maintaining documentation and runbooks for the services and infrastructure
		* Owning and scheduling maintenance for dependencies, i.e., databases and cache stores
		* Owning and improving infrastructure monitoring and alarming
		* Improving and optimizing the use of our AWS infrastructure
		* Automation of common task that allow you to be removed from the process
	- Supported Tech... in our tech stack:
		* A moderately sized Ruby on Rails monolith
		* Dockerized services written Node, Ruby, and Elixir
		* Node/Express/Typescript API
		* Postgres, Redis, and DynamoDB datastores
		* Deployments in Heroku and AWS (ECS, EKS, and Lambda)
		* CircleCI based CI pipeline
	- What we are looking for
		* 3+ years of experience working on a team and shipping code and services in production environments
		* Minimum 2+ years devops experience
		* Strong experience managing AWS based deployments, K8s preferred
		* Experience working with Dockerized services (micro or other)
		* Experience with infrastructure-as-code
		* Experience CI/CD pipelines across multiple domains
		* Familiarity with Postgres (or other relational DBs)
		* Familiarity with cloud based security best practices
		* Familiarity with Agile process
		* We believe that good DevOps people have the ability to pick up new processes and technologies quickly, but some of the following will help you hit the ground running in one area or another.
		* Experience working on Ruby on Rails applications
		* Experience working with Typescript/Express applications
		* Experience with AWS based DevSecOps
		* Experience with processing images at scale
	- Culture/Benefits
		* We are passionate about learning and development, providing opportunities through lunch and learns, training and workshops. We also provide each employee with a $3000 per year Education & Conference budget.
		* We provide our employees with a monthly Wellness Spending Account, to help cover costs related to fitness equipment, personal training, gym memberships or health and wellness practitioners.
		* We provide our team with a generous technology bonus and provide the tools you need to succeed in your role.
		* We offer a comprehensive benefits package including health, dental, vision, and an Employee and Family Assistance Program to support the wellbeing of you and your family.
		* We offer flexible working hours & schedules so you can work around school and home commitments.
		* We offer parental leave top-ups, family forming support and a life-transitions program to ensure you and your family are well supported when returning to work
		* We provide a variety of workshops, meditation and yoga at our monthly Wellness Wednesday events to help our team perform at their best.
		* All departments have quarterly department team building activities
		* We fly all employees in or out to join us at our fun filled annual company retreat.
		* Later values diversity of thought; we are committed to creating a diverse environment and are proud to be an equal opportunity employer. All applications will receive consideration for employment without regard to race, colour, religion, gender, gender identity or expression, national origin, disability, or age.
+ skill set:
	- YAML, or JSON, for customizing configuration files.
		* JSON files are valid YAML files.
			+ JSON is a subset of YAML.
		* YAML = Yet Another Markup Language, YAML Ain't Markup Language
		* YAML CRD
			+ For Kubernetes, CRD = custom resource definition.
+ Experience with Hadoop, Vertica, HBase, Spark.
+ Experience with Docker, AWS, and CircleCI is a bonus.
+ skill set:
	- Experience with Docker, Kubernetes, ceph, AWS a plus
	- Comfortable around data stores, MySQL, PostgreSQL and Redis a plus
	- You appreciate a well-designed REST API
	- You calmly triage production issues across microservices and approach the creation of software with a DevOps mindset
+ skill set:
	- Experience with build infrastructure (e.g. Jenkins), and build tools (e.g. Maven, Ant, Make, CMake, etc)
	- In-depth knowledge of version control systems
	- Infrastructure as code mindset
	- Experience with infrastructure design and operations (monitoring, alerting, BDR)
	- Experience with provisioning clusters on Amazon EC2, and/or Google Compute
	- Experience with private/public clouds, virtualization technologies (VMWare, KVM, or similar) and container technologies (Docker, Kubernetes)
	- Familiarity with concepts of InfoSec/AppSec
+ Comfortable with most of the following: linux, python, docker, kubernetes
+ skill set:
	- Experience with Kubernetes and Docker.
	- Experience with Elasticsearch, Redis and/or Memcached.
+ skill set:
	- An understanding of several of these methodologies and tools:
		* Software development methods such as Agile, Scrum, Lean, Waterfall
		* Software project tools like JIRA, Pivotal Tracker, Trello, Asana, and MS Project
		* Continuous integration and build automation with Jenkins, TeamCity, TFS, TravisCI, CircleCI
	- Experience configuring the following technologies:
		* LDAP, ActiveDirectory, and other SAML/Single-Sign-on services
		* VMware vSphere, ESXi, AWS, Azure, GCP, and other virtual infrastructure tools
+ Familiarity with configuration/orchestration management software such as Puppet, Chef, Ansible, or Salt.
+ developer experience (DX) lead
	- current tech stack:
		* Golang
		* TypeScript
		* GCP
		* Docker
		* Terraform
		* Serverless
		* K8s
		* VueJS
		* gRPC, gRPC Remote Procedure Calls, general-purpose RPC infrastructure, Stubby
	- Gopher communication protocol
+ skill set:
	- maintain and improve Torq's automation infrastructure and CI/CD pipelines
	- improve test's infrastructure to support scale, reduce delivery times and improve the overall quality of all product aspects
	- develop E2E (UI and API) tests for Torq's management app and microservices (Python)
	- contribute to integration test framework (TypeScript) - cross platform and cross browsers tests
	- develop performance tests over k6
	- experience with UI automation frameworks
		* Playwright or Puppeteer (Node.js libraries, headless browser and usage), CDP based
		* CDP, content delivery platform - SaaS content service
		* CMS, content management system
		* CDP, continuous data protection, continuous backup, or real-time backup
		* CDP, customer data protection
	- experience writing Python-based frameworks for test automation, Python, unittest
	- experience with writing JavaScript/TypeScript -based automation framework, Jest, Mocha
	- hands-on experience working with Linux and modern apps using Docker containers and k8s
	- working knowledge of CI/CD, and cloud deployment and testing
	- experience with testing gRPC microservices
+ skill set:
	- scalable ***configs as code***
	- developer-facing tooling
	- drive, implement, support, and maintain infrastructure services with:
		* Kubernetes
		* Envoy
		* Kafka
		* Cassandra
	- collaborate and evangelize the right cloud solutions throughout the business by creating a visionary direction and road map for infrastructure-as-a-service, IaaS
	- implement automation to perform the day to day operations/functions of the cloud platform, working across all teams
	- develop, monitor, and build alerts around error conditions and performance
	- work in a fast-paced environment while participating in conceptualizing and building CI/CD pipelines
	- be on-call as needed to support the infratructure and our systems, and drive philosophies around site reliability
	- experience in infrastructure engineering
	- experience with public cloud computing services, such as AWS and GCP
	- experience with container technologies
	- experience working with high availability and scalable SaaS (or consumer technologies)
	- experience deploying highly available and scalable, secure and reliable services with automatic failover using containers and container orchestration tools like K8s
	- use of service meshes
	- cloud formation via Terraform and Ansible
	- experience deploying container applications with helm charts
	- experience with:
		* monitoring tools:
			+ Datalog
		* on-call tooling
			+ PagerDuty
	- experience architecting, implementing, and managing environments in AWS
	- experience implementing AWS services, such as:
		* EC2 Load Balancing
		* VPC
		* Route 53
		* Direct Connect
		* NAT Gateway
		* VPN
		* EC2 Networking
		* Transit Gateway
		* Global Accelerator
+ skill set:
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
+ Specific technologies, like Spring, docker, Kubernetes, etc. are, of course, also a great help
+ skill set:
	- Excellent solid understanding of Apache Pulsar, RabbitMQ, or Apache Kafka.
	- experience in stream processing platform, such as Flink, Storm, Spark or equivalent
+ skill set:
	- Senior DevOps Engineer, Cloud IaaS (US Remote Available)
	- Splunk's IT Operations team's exciting and meaningful mission: Build, scale and maintain Splunk’s Infrastructure for all Splunkers. While various Engineering groups focus on building our products, IT Ops serves as the backbone operational support for Splunkers across the globe.
	- We are actively seeking DevOps Engineers with a real passion for automation to help build scalable tools to run our distributed systems. You will be responsible for expanding and supporting the infrastructure platform services we provide to Splunk, as well as engaging with other teams to help improve efficiency and optimize our infrastructure. You're also an individual who’s motivated by technology and enjoys automation and problem-solving. We work hard, we like to challenge the status quo, and we enjoy having fun!
	- Support and maintain IT Public/Private Cloud Infrastructure, including our virtualization and container platforms
	- Be a part of the On-Call for production issues during shift or as required.
	- Take on performance and stability issues using a wide variety of tools, including Splunk
	- Ensure that day-to-day operational requirements and SLAs are met
	- Work with key business partners to understand their requirements and recommend potential solutions, and secure resources to deliver
	- Maintain critical services and provide visibility to internal teams
	- Seek opportunities to improve or optimize processes through automation
	- 5 + years of experience as a DevOps Engineer administering/managing an AWS Public Cloud platform
	- 3+ years of experience providing automation with a major scripting language such as shell, python or go.
	- 5 + years of experience providing Linux systems administration/engineering
	- 1+ years of experience with Configuration Management tools like Ansible, Puppet or Chef
	- CI/CD pipeline tool experience (e.g. Jenkins, GitLab, etc)
	- Experience with Hashicorp toolsets Terraform, Vault, and Packer.
	- A deep understanding of networking concepts and internet protocols
	- Familiarity with Observability concepts and tools
	- Experience with Kubernetes and containers
	- Good understanding of cloud infrastructure security concepts
	- Ability to provide reliable technical support and mentorship on complex issues in a high velocity, dynamic environment
	- Ability to communicate complex technical concepts clearly to customers and upper management
+ skill set:
	- Senior Site Reliability Engineer (remote Spain)
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via Terraform as well as Kubernetes, Helm and Istio
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of Terraform, Ansible, Puppet or Salt.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio
	- Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Software Engineer - Developer Platform Infrastructure
	- We are looking for a Senior Software Engineer to help lead, design and build the next generation of our CI/CD and tools offerings. You will be working on the core infrastructure platform enabling the next generation of Splunk’s offerings.
	- CI/CD and tooling expertise. Cloud, container and virtualization experience. Innovating and scaling secure services on-prem and different cloud providers is a plus. You will use Kubernetes, Docker, UCP, AWS/GCP, Jenkins, Gitlab, Ansible.
	- Data structures and algorithms. A solid grasp of data structures, algorithms, and RESTful APIs.
	- Observability infrastructure expertise to ensure 24/7 operational excellence and data driven decision making.
	- Ability to work with multiple programming languages. We have code in several languages, ranging from Go to Python.
	- BS/MS degree in EE or CS or a related technical field or 3+ years of progressive experience.
	- Desire to learn and adapt. You will constantly be learning new areas and new technologies.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. We want you to be excited and have utmost ownership of your projects.
	- Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and observability.
	- Opportunities to develop and grow as an engineer. We are always expanding into new areas, working with open-source projects and contributing back, and exploring new technologies.
	- A team of incredibly capable and dedicated peers, all the way from engineering to product management and customer support.
	- Breadth and depth. You are interested to work on an area that dynamically scales to meet the needs of Splunk’s offerings.
	- You want to go deep into optimizing how we automate every manual process and tedious task we encounter.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe that mentors help both sides of the equation.
	- Fun. We have frequent group outings and team building events. We are committed to having every employee want to give it their all, be respectful and a part of the family, and have a smile on their face while doing it.
+ skill set:
	- Software Engineer- Tooling and Infrastructure
	- Splunk is looking for a seasoned professional engineer to join the effort to define and build the future of Splunk. Splunk is rapidly expanding their presence in the cloud, and we are looking for engineers who are interested in being founding members of the Deployment Tooling team (D4S)  that defines and builds tools to optimize how Splunk services are deployed to the public cloud.  This is a great opportunity to both lead and to learn.
	- In this role you will help Splunk to orchestrate deployments of its multi-tenant cloud platform across multiple regions, and to manage continuous deployment to these regions via Argo CD to provide for automated rolling deployments.  We are looking for candidates who have experience transitioning from operationalized deploys to automated deploys.  This is a position with broad impact–what you build will be used across all of Splunk cloud.  You need to be able to build robust solutions that are easy to use and provide exponential impact to an organization.
	- Opportunities to develop and grow as an engineer. We are at the forefront of our industry, always expanding into new areas, and working with open source & new technologies.
	- A set of talented and dedicated peers, all the way from engineering and QA to product management and customer support.
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both mentor and mentee alike.
	- An open, collaborative and supportive work environment. We embody the scrum values.  We also have a number of Employee Resource Groups for employees of all backgrounds.
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Experience in systems-level programming and distributed systems. You have knowledge of operating systems, networking and network protocols, messaging, consensus, failure modes, and parallel programming.
	- Demonstrated ability to advocate for simple and clear APIs for complex functionality. You have an API-first mentality, with the ability to build straightforward APIs to help services configure the service mesh routing, maintaining API contracts, etc.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. You actively own your work and be excited about your projects.
	- Ability to work with Golang and Python. Most of our services are written in Golang and tools are written in Python; if you are an expert at another language we can consider you, but you will be expected to program in Python and Golang.
	- Requires 3-5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience
	- 3-5 years experience in programming languages: Python, GoLang, Java, C, 
	- 2 years building and deploying cloud-native applications/services on AWS or other cloud services like GCP, Azure, etc.
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
+ skill set:
	- Principal Software Engineer - Analytics Platform (Remote)
	- We are seeking a Principal Software Engineer to help lead, design, develop and deliver Splunk's User Behavior Analytics (UBA) security analytics solution that detects known and unknown security threats at scale using big data and machine learning techniques. UBA helps security analysts quickly identify and resolve threats; delivered on customer managed resources using Kubernetes and Spark to run innovative stream processing and machine learning algorithms in near real-time.
	- We are a passionate team who care deeply about our customers and our teammates. In this role, you will work directly with Product Management, our Design Team, our Customers and other engineering teams to help derive the best experience for the customer. We have a lean process that focuses on empowering and serving our engineers as opposed to just directing them.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real world use-cases in order to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality.
	- Champion, coach and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 12+ years of Software Development experience.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience using languages such as Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, Hadoop, Cassandra.
	- Exposure to working with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Knowledge of distributed computing architectures and principles that solve for scalability, performance, redundancy and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
	- Background in developing machine learning products for the Security market a plus.
+ skill set:
	- Comfortable with Linux, Docker, AWS, GIT, Artifactory in terms of both tools and systems administration
	- Previous experience in design and implementation of solutions to evaluate and improve performance: availability, reliability, interoperability, scalability of SaaS / Cloud Native / Bigdata Platform and application with microservice architecture
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Design, implement, and maintain platform metadata features
	- Designing APIs and Platform Information Architecture
	- Serve as primary point of contact in one or more platform metadata areas
	- Collaborate with various Confluent Engineering groups to provide strong technical guidance and leadership related to managing metadata
	- In depth experience with concepts of distributed systems and big data such as - Kafka, Hadoop, Spark, Big Table, HBase etc
	- Full stack experience
	- Experience with Lineage, Governance and Auditing
	- Experience in ML/Data Engineering
	- Experience working with Docker or Kubernetes is a plus
	- Experience working with AWS, Azure, and/or GCP
+ skill set (Platform DevOps Engineer):
	- As a Platform DevOps, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience with Go, Java, C++ or Python required
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- As a Platform DevOps Engineer, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- Solid understanding of container orchestration systems such as Kubernetes, Mesos, etc.
	- Experience with C, C++, Java or Python required
	- Experience with container orchestration tools such as Docker, CoreOS, etc.
	- Experience with configuration management or provisioning tools such as chef, puppet, Ansible, etc
	- Experience building and operating large-scale systems
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Hands-on experience with Kubernetes operator, Helm, or StatefulSets is a plus
	- Proficiency in Go is a plus
	- Knowledge of Apache Kafka is a plus
+ ***Experience building and scaling automation frameworks***
+ skill set:
	- Build and maintain data foundations, metrics and dashboards to monitor the business performance and extract actionable insights
	- Apply quantitative analysis, data mining, and presentation of data to fuel business growth and drive customer success
	- Design and analyze experiments to test new product ideas, go to market strategies and/or funnel optimization; Convert the results into actionable recommendations
	- Build data products to improve operational efficiencies organizationally to scale with a hyper growth start-up
	- Inform, influence, support, and execute business decisions with senior leadership and business partners
	- Build robust, automated data pipelines to enable team effectiveness
	- 2+ years industry experience working with SQL (Teradata, Oracle, MySQL, BigQuery, etc.) and R (or Python)
	- Proficiency in applying statistical modeling and/or machine learning
	- Proficiency in data visualization (eg. Tableau, Looker, Matlab, etc.)
	- Bachelor or advanced degrees in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc
	- The ability to communicate cross-functionally, derive requirements and deliver insightful analysis and/or models; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience building data warehousing and ETL pipelines
	- Experience with Unix/Linux environment
	- Experience in developing data apps with Python/Java, high charts etc
	- Excellent communications skills, with the ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience working in the B2B growth/marketing or sales domains: CRM, sales effectiveness, branding, segmentation, web analytics, attribution, funnel optimization, etc.
	- Experience working with Marketo, Google Analytics and SFDC
+ skill set:
	- The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.
	- We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
	- Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
	- Implementing a solid, robust, extensible data warehousing design that supports key business flows
	- Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
	- Developing strong subject matter expertise and manage the SLAs for those data pipelines
	- Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
	- Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
	- Building and growing  partnership with cross functional teams, and evangelize data-driven culture
	- Contributing to innovations that fuel Confluent's vision and mission
	- 4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
	- Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
	- Highly proficient in Python and SQL coding
	- Highly proficient with tuning and optimizing data models and pipelines
	- Experience in developing data apps with Python, Javascript, high charts etc
	- The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience with Apache Kafka
	- Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
	- Experience in developing data apps with Python, Javascript, high charts, etc
+ skill set:
	- Deep knowledge of a configuration management tool (i.e. Puppet, Chef, Ansible, Salt, CFEngine). Experience with containers is a plus
	- Familiarity with distributed systems including service discovery, pub/sub, search indexing, storage, and caching. We use Zookeeper, Kafka, Elasticsearch, MySQL, Hbase, and Memcache respectively.
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Improve observability with distributed tracing for all requests from client to CDN to load balancer to cluster and back again
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Build observability into every aspect of our production infrastructure
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- Help developers smoke-test better by bringing canary analysis and automated scale testing into their world
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Ansible
	- Kafka/Cassandra
	- Linux
	- Git (github)
	- Vi / Vim
	- Elastic Search Stack
	- Graphite/Grafana
	- Data visualization
	- Python, Bash, Golang
	- Familiarity with JSON
+ skill set:
	- Expertise with 12 Factor application principles
	- Containers (Docker, Kubernetes...)
	- Streaming/logging technologies (ElasticSearch, fluentd, LogStash, Kafka)
	- Message Queueing (Kafka, SQS...)
	- Coding and scripting languages (Perl, Bash, Python, Go...)
	- AWS Ecosystem (EC2, VPC, S3, DynamoDB, RDS...)
	- You have deployed and configured a wide range of AWS services including databases, networking, and security. In this role you will work with such paradigms and technologies as: ***12 factor app design principles***, Docker, Kubernetes, and ElasticSearch ecosystem
		* [Twelve-Factor App methodology](https://en.wikipedia.org/wiki/Twelve-Factor_App_methodology)
			+ 1. Codebase: There should be exactly one codebase for a deployed service with the codebase being used for many deployments.
			+ 2. Dependencies: All dependencies should be declared, with no implicit reliance on system tools or libraries.
			+ 3. Config: Configuration that varies between deployments should be stored in the environment.
			+ 4. Backing services: All backing services are treated as attached resources and attached and detached by the execution environment.
			+ 5. Build, release, run: The delivery pipeline should strictly consist of build, release, run.
			+ 6. Processes: Applications should be deployed as one or more stateless processes with persisted data stored on a backing service.
			+ 7. Port binding: Self-contained services should make themselves available to other services by specified ports.
			+ 8. Concurrency: Concurrency is advocated by scaling individual processes.
			+ 9. Disposability: Fast startup and shutdown are advocated for a more robust and resilient system.
			+ 10. Dev/Prod parity: All environments should be as similar as possible.
			+ 11. Logs: Applications should produce logs as event streams and leave the execution environment to aggregate.
			+ 12. Admin Processes: Any needed admin tasks should be kept in source control and packaged with the application.
	- Support build/deployment processes with eye towards improving our CI/CD pipeline
	- Help troubleshoot production issues and perform root cause analyses that create effective mitigation strategies
	- Design, implement, monitor, and scale self-service oriented infrastructure
+ skill set:
	- Confluence
	- Jira
+ skill set:
	- Infrastructure Engineer.
	- In this role you will be responsible of monitoring the constant quality of the infrastructure and corporate networks, by performing diagnostic tests and debugging procedures to optimize computing systems.
	- Analyze and suggest changes to test, dev and prod infrastructure
	- Ensure utmost uptime for entire network services and servers
	- Implement infrastructure security policies
	- Provide consultancy and support for teams with various/different levels of IT knowledge
	- Make policy recommendations for further implementations and development
	- Develop and maintain an IT sourcing strategy and ensure the correct provisioning of IT equipment. Solve problems using in-depth understanding of information systems and computing solutions.
	- Monitor constantly the quality of the infrastructure and support the management of network infrastructures.
	- Ensure data is stored securely and backed up regularly
	- Work on process documentation as well as backup and recovery procedures, data retrieval, network and infrastructure diagrams.
	- Research, design and recommend new approaches to improve the networked computer system
	- Expert about networks and their protocols
	- Proficient with both network hardware and technologies and also with shared storage technologies
	- Skilled in backup procedures for data storage and disaster recovery
	- Expert about cloud environments and infrastructures, in particular Microsoft Azure and Amazon AWS
	- You have a good knowledge about firewalls and virtualization systems like VMware, Hyper-V
	- Fluent in English written and spoken
	- You have a good attitude towards problem solving and team working
	- You have at least 5 years of experience in this role and a degree in Computer Science or Engineering
	- It would be nice if you would have knowledge of container systems like Docker and Kubernetes, and CloudFlare.
+ skill set:
	- Cloud & DevOps Engineer – Italy.
	- Define and monitor the deployment process
	- Deploy both on-premises by the customer and on our infrastructure
	- Offer first-level support and triage of issues reported by customers
	- Directly manage deployment and infrastructure issues
	- Coordinate second-level support, engaging with developers and other teams
	- A graduate in Computer Science, Computer Engineering or an equivalent title
	- Expert of Windows/Linux operating systems with basic systemic knowledge
	- Good in speaking and writing in English
	- DevOps methodology
	- Cloud Native Architectures (Docker, Kubernetes)
	- Monitoring/logging systems (ELK Stack, CloudWatch)
	- CI/CD tools (Gitlab, Jenkins)
	- IaC tools (Terraform, Ansible)
	- AWS ecosystem
	- Precision, dedication and diligence
	- Priority management while ensuring the output quality
	- Ability to organize your tasks according to the company’s processes and procedures
	- Curiosity towards new technology and willingness to learn
	- Team working will be essential since you will also support other colleagues
+ skill set:
	- Monitoring/logging systems, such as the ELK Stack (or Elastic stack) [WikipediaContributors2022b].
		* visualize application monitoring
		* visualize infrastructure monitoring
		* faiciliate troubleshooting
		* for security analytics
		* ***ELK Stack*** (or ***Elastic stack***):
			+ ***Elasticsearch***
				- distributed search and analytics engine
				- use schema-free JSON documents
			+ ***Logstash***
				- ETL (extract, transform, load)
					* ***data ingestion tool***
					* collect data from different sources
					* transform data
					* send data to desired destination
			+ ***Kibana***
				- data visualization of logs and events, exploration of new maintenance solutions
				- tool for creating interactive charts
+ skill set:
	- Experience with CI systems (Jenkins, TeamCity);
	- AWS - EC2, RDS, ECS etc;
	- Docker and orchestration: Swarm, Kubernetes;
	- Elastic Search, RabbitMQ;
	- Bash / Powershell scripting experience;
	- Windows / Linux - admin level;
	- Experience with SVN / GIT - as a user and as an infrastructure owner;
	- Experience with high loaded distributed multi-tenanted cloud systems. Including: Disaster recovering mechanisms; Monitoring and logging; Redundancy (data, network, apps);
	- Comfortable working with distributed teams
+ skill set:
	- As a DevOps Engineer at Simbe Robotics you will be part of a talented team ensuring quality in our software as well deploying & managing our cloud services and world-wide fleet of autonomous robots.
	- Has experience with automated build and continuous integration systems (e.g. Jenkins, TravisCI)
	- Has knowledge of application/system level monitoring (Nagios, CloudWatch, Munin, Splunk)
	- Experience with configuration management (Chef, Puppet, Ansible) tools
	- Has experience with various application packaging and deployment technologies (Debian packages, Docker/Linux containers)
	- Experience configuring web servers (e.g. Apache/Tomcat, nginix)
+ ***Infrastructure as code*** experience (we use terraform)
+ Familiar with orchestration components (Chef-Puppet-Ansible-Kubernetes-VSTS)
+ view DevOps as ***configuration as code***, ***CaC***:
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef.
+ skill set:
	- Exposure to containers or orchestration services:  Kubernetes, Mesos, or Docker Swarm
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef
+ skill set:
	- Talend ETL, SQL, Postgres, AWS
		* Talend ETL: ETL tool for data integration.
	- Design, develop, and implement advanced ETL pipelines that bring together data from disparate sources, making it available to users using a variety of ETL tools.
	- Facilitate cross-functional data-integration efforts upstream and downstream
	- Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues
	- Extract data from multiple sources, and integrate them into a target database, application, or file using efficient programming processes.
	- Implement and deploy solutions in a CI/CD pipeline
	- Write and refine code to ensure performance and reliability of data extraction and processing.
	- Communicate with all levels of stakeholders as appropriate, including product managers, application developers, business users.
	- Participate in requirements gathering sessions with product managers and technical staff to distill technical requirements from business requests.
	- Recommend process improvements to increase efficiency and reliability in ETL development.
	- Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
	- Some of our technologies might include: Talend as well as various data stores such as Postgres SQL, S3, Aurora and AWS services.
		* Amazon S3, Amazon Simple Storage Service
			+ Minio
			+ Azure Blob Storage
			+ OLTP/OLAP database management
				- OLTP, online transactional processing
				- OLAP, online analytical processing
		* ability to configure and use database instances, their availability, and access roles.
		* Amazon Aurora, high-performance database
			+ relational database service
			+ offered by Amazon Web Service, AWS
			+ part of Amazon Relational Database Service, RDS
				- Postgres SQL
			+ Amazon DynamoDB, proprietary NoSQL database service.
			+ Amazon DocumentDB, managed proprietary NoSQL database service.
				- MongoDB
			+ Amazon SimpleDB, distributed database
		* data warehouse, DWH
			+ data warehouse architecture
				- enterprise data warehouse, EDW
				- operational data store, ODS
				- data mart
	- 2+ years of experience on Data Warehousing and building data pipelines.
+ Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
	- Hadoop: for distributed computing
	- K8s, Kubernetes: container orchestration system for automating software deployment, scaling, and management 
	- Terraform: software tool for ***infrastructure as code***
+ skill set:
	- Knowledge of Big Data, SAP ERP, Docker, Kubernetes, CXF or another ETL product is a plus;
		* Apache CXF: open-source Web services framework, for developing Web services using front-end Web development APIs, such as JAX-WS and JAX-RS, in the context of service-oriented architecture (SOA).
	- AWS, Azure, Google cloud, Apache Beam, NoSQL
		* ***Apache Beam***: open-source unified programming model, and set of software development kits (SDKs), to define and execute data processing pipelines, including ETL, batch and stream (continuous) processing. The data pipelines (Beam Pipelines) are executed in one of the supported distributed processing back-ends (Beam supported runners).
	- Experience of working with JDBC, XML, Junits, Maven, Avro and JSON;
		* JDBC, Java Database Connectivity: API for how client applications can connect to a database.
		* ***Apache Maven***:
			+ build automation tools, primarily for Java, and for:
				- Ruby
				- Scala
				- C\#
		* ***Apache Avro***:
			+ data serialization system
			+ row-oriented remote procedure call (RPC), for distributed computing, and data serialization framework
	- Good understanding of Web Services (SOAP/REST), knowledge of CXF is a plus.
+ skill set:
	- Experienced in data sanitization, data import and export (ETL).
	- Familiar with SQL Server products, i.e. SQL Integration Services and Reporting Services.
	- Work with the application team to create and maintain effective database-coupled application logic stored procedures, triggers and user-defined functions (UDFs); these are programs that are under the control of the DBMS (SQL, MySQL, Postgres, MongoDB)
+ tech stack:
	- Remote hardware administration with IPMI, intelligent platform management interface
		* IPMI, intelligent platform management interface: set of computer interface specifications for autonomous computer subsystem that provide management and monitoring capabilities independently of host system's processor, firmware (e.g., BIOS or UEFI), and operating system.
	- Configuration and management of:
		* SGE/Univa: Sun Grid Engine/Univa Grid Engine, batch-queueing system for scheduling resources in a data center applying user-configurable policies to help improve resource sharing and throughput by maximizing resource utilization.
		* Slurm, workload manager
		* LSF, load sharing facility
		* other DRMS
	- Jenkins CI
		* automation server
	- Phabricator, Web-based collaboration tools for Web development.
	- FlexLM licensing, for license management
	- Puppet, Ansible, Nagios
		* Puppet, for software configuration to specify system configuration
		* Ansible, automation tool that enables infrastructure as code.
		* Nagios Core, or Nagios, for monitoring systems, networks, and infrastructure
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
		* XPath, XML Path Language, an expression language for supporting the query or transformation of XML documents
		* XSLT, extensible stylesheet language transformation, XML-based language used by processing software to transform XML documents
	- Web programming – HTML/DOM, JavaScript, SQL
		* DOM, Document Object Model, programming API for HTML and XML documents
	- A solid knowledge about how orchestration tools (Kubernetes, Swarm, OpenStack, etc) can be used to deploy, scale, and operate virtualized entities
		* Kubernetes, for container orchestration
		* Swarm, for container orchestration in Docker
	- Understand CPU virtualization and container technology from the inside out (hypervisors, Xen, LXC, Docker)
		* hypervisors, for virtual machine monitors (VMMs) or virtualizers
		* Xen, a hypervisor tool.
		* LXC, OS-level virtualization for running multiple isolated Linux systems/containers on control host using a single Linux kernel.
	- The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
		* BuildBot, for job scheduling of the software build process and software tests to facilitate continuous integration (of new additions or changes to the code base).
	- Good knowledge of management and security frameworks (SNMP/MIB agents, CLI, RESTful API, OpenBMC) is very useful
		* SNMP/MIB,
			+ SNMP, Simple Network Management Protocol, Internet Standard protocol for collecting and organizing information about managed devices on IP networks
		* OpenBMC, management controllers for servers, rack switches, and RAID appliances
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
		* ILOM, Integrated Lights Out Manager, for managing and moitoring servers
		* BMC, baseboard management controller for IPMI, intelligent platform management interface
			+ Or, products from BMC Software.
		* OCP (Open Compute)
	- Test automation experience (Some exposure to CTest is desirable)
+ Chef/Puppet/Ansible/Terraform experience is nice to have
	- [Chef](https://www.chef.io/)
		* "Automation Software for Continuous Delivery of Secure Applications and Infrastructure"
		* ["Chef Infra, a powerful automation platform that transforms infrastructure into code automating how infrastructure is configured, deployed and managed across any environment, at any scale"](https://github.com/chef/chef)
			+ "Chef Infra is a configuration management tool designed to bring automation to your entire infrastructure."
	- [Puppet](https://puppet.com/)
		* infrastructure automation
		* "Puppet is redefining what’s possible for continuous operations. Easily automate your environment to deliver at cloud speed and cloud-scale with products that are responsive and predictive by design."
		* ["Puppet is a software configuration management tool which includes its own declarative language to describe system configuration. It is a model-driven solution that requires limited programming knowledge to use."](https://en.wikipedia.org/wiki/Puppet_(software))
		* ["Puppet, an automated administrative engine for your Linux, Unix, and Windows systems, performs administrative tasks (such as adding users, installing packages, and updating server configurations) based on a centralized specification."](https://github.com/puppetlabs/puppet)
	- [Ansible](https://www.ansible.com/)
		* "Ansible Automation Platform has grown over the past years to provide powerful automation solutions that work for operators, administrators and IT decision makers across a variety of technology domains. It’s a leading enterprise automation solution from Red Hat®, a thriving open source community, and the de facto standard technology of IT automation."
		* "The open source projects that power the Ansible Automation Platform are created with contributions from an active community and built for the people who use it every day. The Ansible product and supporting open source communities were made to help more people experience the power of automation so they could work better and faster together."
		* ["Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems."](https://github.com/ansible/ansible)
			+ "Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy."
			+ https://docs.ansible.com
				- "An enterprise automation platform for the entire IT organization, no matter where you are in your automation journey"
		* ["Red Hat® Ansible® Automation Platform is a foundation for building and operating automation across an organization. The platform includes all the tools needed to implement enterprise-wide automation."](https://www.redhat.com/en/technologies/management/ansible)
			+ Ansible Automation Platform provides an enterprise framework for building and operating IT automation at scale, from hybrid cloud to the edge. Ansible Automation Platform enables users across an organization to create, share, and manage automation—from development and operations to security and network teams.
			+ IT managers can provide guidelines on how automation is applied to individual teams, and automation creators can write tasks that use existing knowledge. Ansible Automation Platform provides a more secure and stable foundation for deploying end-to-end automation.
		* ["Ansible is a suite of software tools that enables infrastructure as code. It is open-source and the suite includes software provisioning, configuration management, and application deployment functionality"](https://en.wikipedia.org/wiki/Ansible_(software))
			+ [Windows Subsystem for Linux, WSL](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)
				- "Windows Subsystem for Linux (WSL) is a compatibility layer for running Linux binary executables (in ELF format) natively on Windows 10, Windows 11,[3] and Windows Server 2019."
		* [Ansible](https://opensource.com/resources/what-ansible)
			+ Ansible is a software tool that provides simple but powerful automation for cross-platform computer support. It is primarily intended for IT professionals, who use it for application deployment, updates on workstations and servers, cloud provisioning, configuration management, intra-service orchestration, and nearly anything a systems administrator does on a weekly or daily basis. Ansible doesn't depend on agent software and has no additional security infrastructure, so it's easy to deploy.
			+ Because Ansible is all about automation, it requires instructions to accomplish each job. With everything written down in simple script form, it's easy to do version control. The practical result of this is a major contribution to the "infrastructure as code" movement in IT: the idea that the maintenance of server and client infrastructure can and should be treated the same as software development, with repositories of self-documenting, proven, and executable solutions capable of running an organization regardless of staff changes.
			+ While Ansible may be at the forefront of automation, systems administration, and DevOps, it's also useful to everyday users. Ansible allows you to configure not just one computer, but potentially a whole network of computers at once, and using it requires no programming skills. Instructions written for Ansible are human-readable. Whether you're entirely new to computers or an expert, Ansible files are easy to understand.
	- [Terraform](https://www.terraform.io/)
		* "Automate Infrastructure on Any Cloud"
		* "Deliver infrastructure as code"
		* "Terraform codifies cloud APIs into declarative configuration files."
		* [Terraform@Wikipedia](https://en.wikipedia.org/wiki/Terraform_(software))
			+ "Terraform is an open-source, infrastructure as code, software tool created by HashiCorp. Users define and provide data center infrastructure using a declarative configuration language known as HashiCorp Configuration Language (HCL), or optionally JSON."
		* [Platforms to learn Terraform for](https://learn.hashicorp.com/terraform?track=gcp)
			+ AWS
			+ Microsoft Azure
			+ Terraform Cloud
			+ Docker
			+ Google Cloud Platform
			+ Oracle Cloud Infrastructure, OCI
		* ["Terraform enables you to safely and predictably create, change, and improve infrastructure. It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned."](https://github.com/hashicorp/terraform)
			+ "Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions."
			+ Infrastructure as Code: Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.
			+ "Execution Plans: Terraform has a 'planning' step where it generates an execution plan. The execution plan shows what Terraform will do when you call apply. This lets you avoid any surprises when Terraform manipulates infrastructure."
			+ "Resource Graph: Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure."
			+ "Change Automation: Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, avoiding many possible human errors."
		* [Terraform documentation](https://terraform-docs.io/)
+ skill set:
	- improve deployment pipeline
	- implement continuous integration (CI) pipeline
	- architect public cloud enviroments for multiple engineering teams using infrastructure-as-code tools with industry best practices
	- maintain and upgrade instructure with zero downtime
	- performance tuning and optimization across the CI stack
	- knowledge of microservice architecture pattern and cloud-native application
+ understanding of:
	- containerization and container orchestration tools, such as K8s.
+ Familiarity with Docker (and Kubernetes/Mesos Marathon)
	- Mesos Marathon:
		* for operating system -level virtualization, via container orchestration
		* Apache Mesos
			+ abstracts the following from machines (physical or virtual):
				- CPU
				- memory
				- storage
				- other compute resources
			+ enabling fault-tolerant and elastic distributed systems to easily be built and run effectively
			+ distributed systems kernel
				- provides applications, such as:
					* Hadoop
					* Spark
					* Kafka
					* Elasticsearch
			+ open-source project to manage computer clusters
		* Marathon is an industry-proven Apache Mesos framework for container orchestration.
		* Marathon is a production-grade container orchestration platform for Mesosphere's Datacenter Operating System (DC/OS) Apache Mesos.
			+ runs on DC/OS
				- Distributed Cloud Operating System (DC/OS)
					* open-source, distributed operating system based on Apache Mesos distributed systems kernel
					* provides (or automates the following):
						+ scheduling
						+ resource allocation
						+ service discovery
						+ workload co-location
						+ automatic recovery from failure
						+ load balancing
						+ software defined networking
						+ unified APIs for metric and log collection
						+ facilitates inter-process communication
						+ install and manage distributed services
					* manages multiple machines in the cloud or on-premises from a single interface
						+ Web interface and CLI facilitates remote management and monitoring of the cluster and its services
					* deploy containers, distributed services, and legacy applications into those machines
					* provide networking, service discovery, and resource management to keep the services running and communicating with each other
					* service catalog included
						+ installing distributed data services, databases, CI/CD tools, and monitoring on DC/OS takes a few clicks or commands
					* Mesos's 2-level scheduling increases resource utilization, and DC/OS lowers the barrier of entry to Mesos cluster administration
					* containers and fast data
						+ deploy and run stateful or stateless distributed workloads, including:
							- Docker containers
							- Big Data pipelines tools
							- traditional apps
					* containerized workloads
						+ deploy jobs, services, and containerized apps, and let DC/OS handle the rest
					* infrastructure portability
						+ develop locally on DC/OS and deploy to production on DC/OS in the cloud or on-premises
						+ support for multiple cloud providers prevents infrastructure lock-in
					* includes 2 built-in task schedule:
						+ Marathon
						+ DC/OS Jobs (Metronome)
					* includes 2 container runtimes:
						+ Docker
						+ Mesos
					* Mesophere Distributed Cloud Operating System (DC/OS) is a platform for running distributed containerized software:
						+ apps
						+ jobs
						+ services
					* at the software layer, DC/OS provides packaged management and a package repository to install and maage several types of services, as:
						+ packaged apps and services
							- databases
							- message queues
							- stream processors
							- artifact repositories
							- monitoring solutions
							- continuous integration tools
							- source control management
							- log aggegators
						+ custom apps, services, and scheduled jobs
					* at the platform layer, the categories of components (shared among master nodes, private agent nodes, and public agent nodes) are:
						+ cluster management
						+ container orchestration
						+ container runtimes
						+ logging and metrics
						+ networking
						+ package management
						+ IAM and security
						+ storage
					* to use a service discovery mechanism (or service discovery behavior), do not put dots in your application names
			+ virtual IP routing
			+ authorization, for DC/OS enterprise edition only
				- true multitenancy with each user or group having access to their own applications and groups
			+ can be used to run other Mesos frameworks, such as:
				- Chronos
			+ run other application containers, based on Docker or Mesos:
				- JBoss servers
				- Jetty
				- Sinatra
				- Rails, or Ruby on Rails
					* full-stack framework
					* Web-application framework that spans:
						+ creating database-backed Web applications according to the Model-View-Controller (MVC) pattern
							- Model-View-Controller (MVC) pattern is a software architectural pattern, a type of software design patterns.
							- splits Web applications into 3 layers:
								* Model
								* View
								* Controller
		* Marathon provides a REST API for starting, stopping, and scaling applications.
		* speed of infrastructure provisioning can match the speed of application development, application containerization, and application release
			+ enable the benefits of Docker to be realized in production
			+ enable rapid application spin-up time of a Docker container or cluster of Docker services
			+ makes it easier to Dockerize software applications, so that their compute, memory, and CPU resources in traditional hypervisor-centric model
			+ enables management of data center as if it is a single pool of resources
		* Apache Mesos is a distributed systems kernel, is designed natively for the cloud, and has built-in support for:
			+ Docker containers
			+ HTTP API
			+ Web UI
			+ true high availability
			+ fault tolerance
		* ***Marathon is a meta framework***
			+ Consequently, this meta framework allows people to ***"start other Mesos frameworks, such as Chronos and Storm,"*** which can be run with Mesos ***"to ensure that they survive machine failures"***.
			+ **"It can launch anything that can be launched in a standard shell."**
				- E.g., "you can ... start \[other\] Marathon instances via Marathon".
		* "cluster manager that handles workloads in a distributed environment through dynamic resource sharing and isolation"
		* "can deploy and manage applications in large-scale clustered environments"
		* "Mesos brings together the existing resources of the machines/nodes in a cluster into a single pool from which a variety of workloads may utilize."
		* Mesos is a data center kernel that is between the operating system and the software application layer.
		* "Mesos isolates the processes running in" the following resources, so that they can be kept "from interfering with each other":
			* a cluster
			* memory
			* CPU
			* file system
			* rack locality
			* I/O
		* can be used the following:
			* Apache Spark
			* Chronos
			* Apache Aurora
			* Mesosphere Marathon
			* Apache Zookeeper
+ VM embeddings in other systems (e.g., DBMSs, Big Data frameworks, Microservices, etc.)
+ skill set:
	- interact with developers to understand their workflows and gather requirements
	- design, create, and support CI pipelines for varied projects in a dynamic, fast-paced, and team-oriented environment
	- move the current CI system into a containerized, flexible, and scalable system 
	- keep up to date on the newest and bets practices in DevOps, and try them with the development team
	- improve task efficiency and document CI approaches
	- continuous build and delivery systems
		* Jenkins
		* Gitlab
	- knowledge of:
		* containers
			+ Docker
		* container orchestration system
			+ ECS
			+ Kubernetes
			+ Docker Swarm/Data Center
	- experience with ***Triplestores***
+ Experience working with a CI system is preferred (ex. TeamCity, Concourse, Jenkins, etc.)
+ Prior experience with infrastructure automation frameworks (Ansible, Terraform, Chef or Puppet, etc.)
+ Virtualization and containerization (Xen, LXC, cgroups, Docker, Kubernetes)
+ Knowledge of source control tools (Git, CodeCommit, SVN, and TFS), build/release tools (Jenkins, CodeBuild, CodeDeploy, CodePipeline), and infrastructure as code tools (Terraform, CloudFormation)
+ Experience working with Atlassian products (JIRA, Confluence)
+ Docker orchestration systems and cluster managers (Kubernetes, Mesos/Marathon, ECS)
+ tech stack:
	- Ability to configure and maintain webservers (e.g. apache & nginx), DNS servers, Firewalls, LDAP servers, Tomcat servers
	- Ability to back up the Data infrastructure
	- Ability to manage/configure  Git, Maven and Jenkins
	- Managing QA/production release and deployment
	- Ability to Install/Configure/Manage VM servers using OpenStack
	- Ability to install configure or manage Monitoring servers using Opensource softwares
	- Experience with Amazon Web Services:
	- autoscaling, & use of Netflix Asgard
	- ELB management,
	- EBS storage management
	- S3
	- RDS
	- Manage configuration using Puppet
	- Familiar with Cloud Computing in genera
+ stream pipelines and all sorts of data stores (SQL, NoSQL, triplestores, wide column, graph)
+ infrastructure-as-code and automation tools (e.g. Terraform, Ansible/Chef, Cloud Formation)
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ skill set:
	- big data pipeline systems (Elasticsearch, Spark, Kafka, NiFi)
	- 2+ years of AWS experience across a range of services including EC2, Auto Scaling, Lambda, RDS
	- DevOps and automation mindset
	- Experience deploying and managing big data pipeline systems (Elasticsearch, Spark, Kafka, NiFi)
	- Familiarity working with multiple programming languages and runtimes for application containerization and deployment (Java, Scala, NodeJS, Python)
	- Experience administering CI/CD and build tools like CircleCI or AWS CodeBuild, as well as working with artifact storage platforms
	- Experience with container orchestration platforms (Elastic Container Service, Kubernetes) is highly desirable
	- Expertise in infrastructure capacity planning, AWS cost management using tools like Cloudhealth
	- Familiarity with monitoring toolsets like DataDog or Dynatrace
+ skill set:
	- Workload managers: Slurm, LSF, Altair
	- Web development: nginx, javascript, html, css
	- Database: mysql, apache derby, ingress/egress
	- Deep \*nix skills
+ skill set:
	- Senior DevOps/SysOps Engineer
	- Come and be a part of our close-knit distributed team, working remotely from different locations while enjoying a good cup of tea. We thrive on interesting technical challenges and building magical products that improve people’s lives. As our expert DevOps/Systems engineer,  you will be responsible for evolving our production and developer infrastructure, by maintaining and improving the backend infrastructure used by Sched.com.
	- You will be making an immediate impact on transitioning Sched from a monolithic legacy architecture to a scalable application! You will be ensuring site reliability, monitoring and enhancing the observability of key systems, security, reliable deployment of services, disaster recovery planning, and CI. Additionally, you will bring valuable knowledge to work closely with the development team and help improve our processes and our PHP, Go and JS applications and services to best serve our customers!
	- Your valuable contributions to uptime, stability, scalability, and reliability – are paramount to the success of Sched while opening the opportunity to learn and apply new technologies to scale and grow!
Projects you will be expected to complete in your the first 6 months
	– Automating CI and deployment of Golang daemons across multiple machines
	– Planning out and executing a rolling upgrade of all systems to latest Ubuntu release
	– Dockerization of major system components to allow easier management and updating
	– Planning out cost-effective “on-demand” peak-traffic scaling
	– Prioritize and resolve live issues 
	– Manage, plan and execute system and software updates and upgrades when needed
	– Manage application deployments in coordination with the development team
	– Maintain and improve our monitoring systems to pre-empt issues that may affect our live environments
	– Investigate and implement system improvements
	– Maintain and improve system documentation and runbooks
	– Maintain and improve disaster recovery and backup plans
	– An expert in Linux system administration – covering security, maintenance, backups, disaster recovery, storage management, monitoring (Ubuntu)
	– Comfortable with AWS infrastructure management
	– Well-versed in command line / bash scripting
	– Skillful in database administration (MySQL, Percona)
	– Knowledgeable in infrastructure configuration management tools (Ansible)
	– Capable of monitoring, logging, & observability tools (Sentry, Munin, Grafana)
	– Experience in networking: DNS, SSL, SMTP, SSH, VPN
	– Competent in HA and web tech: HAProxy, ProxySQL, Apache, PHP-fpm, Redis and Redis Sentinel, Percona Cluster, Corosync
	– Skillful with Docker, containers, CI/CD, automation
	– Proficient with PHP and Golang code
	- Our tech stack:
		* Apps: Javascript, PHP, Swift, Kotlin
		* Backend: PHP, Golang, Percona Cluster, Redis, Memcache, HAproxy, ProxySQL, Apache
		* Infrastructure: Docker, Ansible, Munin, Sentry, Corosync, DRBD, KVM
+ skill set:
	- Python Developer Tools Engineer
	- Python is a vital part of Jane Street’s research and trading work, acting as the go-to language for data analysis, visualization, and machine learning. Our Python developer tools team is responsible for the tools that support our work in Python, including:
		* CI for running tests and static analysis
		* Automation around the deployment of Python environments and applications
		* Libraries for building hybrid Python/OCaml systems
	- We’re looking for engineers with experience building Python tooling at large scale who want to leverage those skills in a new environment. 
	- The job is wide-ranging, involving both technical and product design challenges — from finding ways to speed up our static analysis and test harnesses to working with users to understand their workflows and requirements and designing solutions that fit their needs. 
	- We’re also looking for someone who can contribute to guiding our relatively young Python ecosystem, helping us form and communicate best practices, and make good choices about the tools and libraries we use.
	- Lots of our automation and Python tooling is written in OCaml, but we’re happy to teach you that, and we don’t expect any previous experience with OCaml or any other functional programming language.
+ Experience with Kubernetes, Terraform, Docker, Jenkins, Prometheus, Elasticsearch, MySQL, Postgresql, Mongodb, and Ansible.
+ skill set:
	- Leverage tools/languages such as Kubernetes, Terraform (IaC), Helm, Python, and others!
	- Have production experience with IaC (Terraform, ARM, Bicep)
+ skill set:
	- Senior DevOps Engineering Consultant
	- Searching for that company that will appreciate your coding skills, your character, and your input? If yes, you're more than welcome to join our ITL Connect network. Here, your capabilities will always be appreciated, your code will find its way into international projects, and you'll have the chance to rub shoulders and learn from some of the best pros in the industry.
	- Develop and refine CI/CD tools that help engineering to release faster, with high confidence, and with the highest quality possible
	- Improve workflow automation across the engineering
	- Partner with the rest of the engineering with the shaping and engineering a high-performing secure payment platform, ensuring it is scalable, stable, and performant, as well as easy to use and maintain
	- Providing metrics, documentation, and self-service infrastructure to help our users work at pace and get the most out of the platform
	- Developing automation libraries and tooling
	- Implement and maintain CI/CD pipelines
	- Using advanced troubleshooting skills to diagnose and fix problems
	- Experience in working with Linux and Windows operating systems
	- Experience with cloud providers as AWS or Azure (Azure being the preferred one)
	- Experience with monitoring tools such as ***Prometheus and Grafana*** or others
	- Previous work experience with CI/CD which includes tools like SVN, GIT, Octopus Deploy and TeamCity
	- Experience with tools like ELK Stack, Nginx, CloudFront, RabbitMQ, Ansible, Terraform, CloudFormation
	- Experience with Agile/Scrum development methodologies
	- Willingness to learn new tools, programming languages, technologies
	- Excellent written and spoken English
	- Well organized with excellent attention to detail.
	- Team player and able to work with teams that are in multiple locations.
	- Personal initiative and drive with the ability to work in a fast-paced work environment.
	- Scripting with Python or Java programming language
	- Certification in Azure or AWS
	- Microservice Architecture knowledge
+ skill set:
	- Infrastructure experience: Maven/SBT, GIT/GERRIT, Jenkins, CI/CD processes, Linux Environment
	- Hand-on experience in REST, GIT
	- Experience with Apache Kafka/Spark/Flink, Amazon Web Services and AWS EMR is considered a plus
	- This position requires fluency in the latest Java capabilities and Spring frameworks.
	- Experience working with continuous integration and deployment tools like GitLab, Docker, Kubernetes etc.
	- Knowledge of AWS and similar cloud-based technologies
	- Knowledge of location-based services such as mapping, navigation, GPS
	- Knowledge of computational geometry or graph/routing algorithms
	- Experience with Linux / WSL
+ skill set:
	- Apply and extended CI/CD tools and execute end-2-end ownership of your developed software stack, including DevOps and testing aspects
	- We use modern concepts, implementing them in Java and Rust to build our robust, scalable distributed REST services. Our software stack also includes map data compilation and automated testing suites for quality assurance. We use latest CI/CD toolchains to build and deploy our services in a Kubernetes-based environment.
	- Experience working with continuous integration and deployment tools like GitLab, Docker, Kubernetes etc.
	- Knowledge of computational geometry or graph/routing algorithms
	- Experience with Linux / WSL
	- Maintain and enhance our map data compilation framework.
	- Strong and applied experience in REST based web service technologies, JSON and XML, SOA and micro services architectures
	- Strong and applied DBMS & SQL skills, with a strong preference on PostgreSQL / PostGIS
	- Strong and applied AWS skills (EC2, S3, RDS,), as well as experience with virtualization tools like Docker
	- Strong experience CI/CD tools, with a preference on Jenkins
	- Good knowledge of using issue tracking tools (e.g., JIRA), code collaboration tools (e.g., Git/GitLab) and team collaboration tools (e.g., Confluence/Wiki).
+ skill set:
	- Excellent experience running production services on public cloud (AWS preferred)
	- Experience on creating software architecture.
	- Advanced experience in continuous integration & continuous delivery using GitLab (++)
	- Hands-on Design patterns and SOA
	- Experience on Performance optimization
	- Experience of persistence with relational and NoSQL data stores
	- Experience in large scale data processing
	- Proven ability to work independently on milestone with limited supervision.
	- Excellent team player with the ability to work within a collaborative environment
	- Working with mapdata e.g. NDS, GIS, RDF, GDF is considered a plus
	- Working with data transformation scripting / ETL is considered a plus 
+ skill set:
	- Everything from Network Design, Computer Virtualization, Operating Systems, Storage, Security, Computer Architecture, Product Architecture etc. are pulled together to develop low maintenance, 0-downtime solution architecture, covering operability, monitoring and management.
	- Automate as much as possible to eliminate human errors.  Work closely with key stakeholders, including Product Development Teams, Network and Technology Strategy Architecture in ensuring the technologies adhere to 99.9% or higher uptime.
	- Participate and drive technical deliverable with or without assistance from a designated Project Manager. Establish/reiterate the ways of working with larger teams/organization
	- Performance, Monitoring and Availability: build and maintain a set of Performance benchmarks, and test each new deployments against these benchmarks. Develop and utilize performance and capacity management KPI's for proactive management of large distributed infrastructure environment.
	- Work as part of a service specific engineering team to design, deploy and support operations
	- Work as part of a virtual team of engineers responsible for defining and driving various improvement efforts for development workflows
	- Consolidates system data to formulate reports for business managers
	- Continue to build and maintain positive customer relationship with teams you support
	- Present service/solution architecture to larger audience and provide service operations status to the stakeholder
	- Bachelor's or Master's degree in Computer Science/Engineering, IT or equivalent
	- 8 to 12  years in software development/testing/DevOps 
	- 3+ years experiences in DevOps related topics
	- Very good knowledge of tools for CI/CD (e.g. GitLab CI)
	- Proven proficiency in scripting language like Python, Shell for automation purposes
	- Knowledge on Infrastructure as Code (IaC) with proficiency in terraform, ansible, AWS CloudFormation
	- Automation and management of Release and Delivery processes
	- Experience in maintenance and support of production environments
	- Experience with cloud technologies: AWS (EC2, VPC, EKS, RDS, S3)
+ skill set:
	- Proven expertise in Infrastructure as Code (IaC) &/or Software as a service (SaaS) combined with 5+ years proven proficiency in the scripting technologies Python (++), groovy(+);
	- Container Orchestration: Kubernetes, Mesosphere, Docker Swarm
	- Proven track record of having designed and deployed large scale & low latency cloud based systems
	- Experience with Microservices Architecture
	- Experience with Amazon Web Services, Spark, Splunk, Kubernetes, Docker, Helm, iPass, Grafana, Prometheus, NGNIX
	- Proven expertise in GitLab advanced usage & administrative skills, Artifactory & maven
	- Advanced experience in continuous integration & continuous delivery using GitLab (++)
	- Experience in configuring and using Open-Source monitoring and trending systems: Prometheus, Grafana, Kibana, Nagios
	- Experience as Oracle DBA/ SQL, PL/SQL, Oracle 11 – 19
	- Experience in Spark, Hadoop & Scala
	- Experience in key-value store
+ skill set:
	- Experience one or more of Presto, Apache Flink, Apache Spark and Apache Parquet for high volume data processing
	- Experience with Identity and Access Management (IAM) concepts like OAuth, OpenID, OIDC, JWT
+ skill set:
	- Experience in Kubernetes and Docker technology
	- Experience with cloud provider offerings, preferably AWS
+ skill set:
	- Expert in high throughput data processing in multitenant cloud systems using technologies like AWS ElastiCache, AWS S3, Splunk, AWS RDS
	- Experience in Kubernetes and/or Docker technology, understanding of microservices architectural style
	- Experience with Spark and/or Flink
+ skill set:
	- Experience with Containerization and Container Orchestration like Docker, Kubernetes, AWS Fargate.
	- Experience with Messaging or Streaming Systems like Kafka, AWS Kinesis, AWS SQS, RabbitMQ, ActiveMQ is a plus
	- Experience in using Big Data processing Frameworks like Spark, Flink, Presto is a plus.
+ Work with development teams to enable robust CI/CD pipelines and mentor team-mates on standard practices
+ skill set:
	- Good knowledge of multi-tiered application stacks (primarily Java based) with a mix of technologies; including Apache, Tomcat and JBoss
	- Familiar with Eclipse, Maven, GIT and CI servers (e.g. Jenkins/Hudson, GO, Bamboo).
	- Experience with relational databases.
	- Working knowledge of RESTful APIs, XML, JSON, network programming and message exchange systems using tools like SoapUI, etc.
	- Experience configuring and using monitoring tools for internal datacenter infrastructure, cloud infrastructure, & containers (Nagios, DataDog, & CloudWatch).
+ skill set:
	- Exposure with Amazon Web Services and associated technologies
	- Experience with Kubernetes / EKS-based container orchestration is a strong plus.
	- Worked with Gitlab Pipelines.
+ Experience with the following technologies is plus: GitLab, AWS, Docker, Artifactory, Jenkins, and Ansible.
+ skill set:
	- Familiarity with HTTP, the REST principles, and TCP/IP networking, good understanding of OAuth and other authentication and authorization standards
	- Solid experience in Java, and understanding of Microservices Architecture.
	- Experience with Amazon Web Services, Spark, Splunk, Kubernetes, Docker, Helm, iPass, Grafana, Prometheus, NGNIX, and Envoy.
	- Experience in scripting languages Python, Bash, and Lua is a plus.
	- Strong experience with Web Application Servers and Web Application Architecture, exposure to Amazon API Gateway, or similar products is a plus.
+ skill set:
	- Design, develop, test, deploy, and oncall support HERE API Gateway components.
	- Evaluate and choose technologies suitable, and communicate effectively about decisions, direction, and progress, both inside and outside the team.
	- Provide coaching and technical guidance for less experienced team members.
	- Develop components and services using agile methods and tools
	- Build and deploy microservices using Kubernetes, and Docker and understand deployment and monitoring tools like Helm, iPass, Grafana, Prometheus
	- Must have a BS/MS Degree with 5+ years of experience.
	- Excellent analytical skills, communication skills with good written and spoken English
	- Knowledge of basic algorithms and data structures, In-depth experience with writing unit-, acceptance- and performance tests
	- Familiarity with HTTP, the REST principles, and TCP/IP networking, good understanding of OAuth and other authentication and authorization standards
	- Solid experience in Java, and understanding of Microservices Architecture.
	- Experience with Amazon Web Services, Spark, Splunk, Kubernetes, Docker, Helm, iPass, Grafana, Prometheus, NGNIX, and Envoy.
	- Experience in scripting languages Python, Bash, and Lua is a plus.
	- Strong experience with Web Application Servers and Web Application Architecture, exposure to Amazon API Gateway, or similar products is a plus.
+ skill set:
	- As a Software Engineer, you'll have the opportunity to work on this amazing product. Within the cross-functional and self-organized team, you’ll have fun working on systems under huge load and storing a vast amount of data in the very heart of our platform - the Delivery Platform. These services are mission-critical and meet demanding compliance, security, performance, and uptime requirements. You will help run it on production with full control and responsibility. Your role encompasses defining and working on features and technical tasks in a self-directed fashion, supporting your colleagues with technical challenges, working with product management to analyse requirements, talking to stakeholders, designing, implementing, and documenting solutions as well as testing and reviewing your colleagues' work.
	- architecture, design, develop, test the backend components
	- evaluate and choose technologies suitable for work
	- elaboration and refinement of user stories
	- development infrastructure (build, testing, continuous integration, delivery)
	- run, monitor, and support in production, ensure our customers receive world-class service by periodically participating in 24x7 on-call shifts
	- team ceremonies (planning, demo, triage, retrospective)
	- communicate effectively about decisions, direction, and progress, both inside and outside the team
	- An ambitious team-player who has experience developing and deploying production-grade services. You have experience with the production software lifecycle, as well as deployment of software to production ready, stable environments. You have 8+ years of professional software development and deployment experience including knowledge of continuous integration, continuous delivery, and automated testing disciplines. You also have good interpersonal skills and feel comfortable coordinating across teams in multiple design centres. In addition you have:
	- Bachelor/Masters in a technical degree like C.S. or technical management
	- Strong programming skills with Python.
	- Experience with cloud-based technologies (e.g. AWS, Azure, GCP, OCI)
	- Experience with Kubernetes
	- Experience with deployment automation systems (e.g. Jenkins, Gitlab, etc.)
	- Interest in high throughput, low latency, highly reliable systems at scale
	- Interest in DevOps methodologies to drive throughput and stability
	- Strong analytical skills
+ skill set:
	- Experience with cloud-based technologies (e.g. AWS, Azure, GCP, OCI)
	- Experience with Kubernetes
	- Knowledge with distributed systems; Docker, Terraform, Puppet are desired
	- Experience with deployment automation systems (e.g. Jenkins, Gitlab, etc.)
	- Interest in high throughput, low latency, highly reliable systems at scale
	- Interest in DevOps methodologies to drive throughput and stability
+ skill set:
	- Strong in Apache Kafka
	- Strong in Microservices Architecture 
	- Strong and applied DBMS & SQL skills, with a strong preference for PostgreSQL / PostGIS
	- Strong and applied AWS skills (EC2, S3, RDS,), as well as experience with virtualization tools like Docker
	- Strong experience with CI/CD tools, with a preference for Jenkins
+ skill set:
	- Experience working in an Agile team
	- Strong level in algorithm engineering
	- Mastery of all aspects of the C++03 programming language
	- Solid knowledge of C++11/14/17/20 improvements
	- Effective practice of the Standard Template Library
	- Strong practice of object-oriented programming
	- Comfort with threading and IPC
	- Excellent collaborative and communication skills
	- Comfortable with agile development methodologies like Scrum or Kanban
	- Knowledge of iOS programming with Swift
	- Knowledge of Android programming with Java or Kotlin
	- Knowledge of Boost C++ libraries
	- Knowledge of OpenGL
	- Knowledge of location based services such as mapping, navigation, GPS
	- Experience in using git as a version control system
+ skill set:
	- Apply and extended CI/CD tools and execute end-2-end ownership of your developed software stack, including DevOps and testing aspects
	- Work closely with other full stack and frontend engineers on the team to de/-refine APIs and system integrations
	- Work with other engineering teams and internal customers to identify new opportunities, address critical needs, and solve complex problems using your backend development expertise
	- Become an expert at leveraging internal platform resources and APIs
	- Drive initiatives that contribute in development of software that is efficient, re-usable and reliable (e.g. agile, coding, verification)
	- Be part of an agile team, share knowledge, carry out code reviews.
+ skill set:
	- Strong in Data Structures & Algorithms
	- Strong in Java 8 and above, Spring Boot
	- Strong and applied experience in REST-based web service technologies, JSON and XML, SOA, and microservices architectures
	- Strong and applied AWS skills (EC2, S3, RDS,), as well as experience with virtualization tools like Docker
	- Able to translate business and architectural features into quality, consistent software design
	- Solid knowledge of programming practices, strive to write great code that is reusable, flexible, and reliable.
	- Strong quality mindset is considered a must: unit testing, performance testing, writing testable
+ skill set:
	- Hands-on experience in Jenkins, Python, Shell scripts, JSON/XML/YAML, and such
	- Experience with HCI, virtualization and/or containers. Experience in working (configuring, deploying, managing, and monitoring) with AWS, GCP, and/or Microsoft Azure
	- Ability to use common DevOps practices and tools for agile development, with security in mind
	- Experience with Go, Ansible, Terraform, Cloud Formation is a plus
	- Certifications are a plus:  AWS Solutions Architect, Cloud Security Certification, OpenStack Certification
+ skill set:
	- Infrastructure Software Engineer
	- We are changing as the world changes and have evolved into a Geo-agnostic company meaning you work where you are. Exceptional candidates will thrive in asynchronous partnerships and remote collaboration methods. Some roles may require being located near our primary sites, which will be indicated in the job description. We offer a competitive salary & benefits package, numerous quality-of-life perks such as a home office stipend, flexible learning allowance, optional professional coaching and a schedule of fun team activities. 
	- Groq's Software Infrastructure team maintains a wide array of services and tools cutting across all of Groq Software Engineering (assembler, compiler, systems/runtime, user libraries, and machine learning applications).  Our goal is to accelerate teams with reliable software that is pleasant to use.  We use a myriad of tools and techniques, leaning towards functional programming (FP) languages and paradigms where appropriate.  Constraints and invariants are important tools for us.
	- Maintain and improve Groq’s custom Nix-based build infrastructure
	- Develop and maintain management systems for long-lived jobs
	- Integrate various services, jobs, and builds with industry-standard tools as appropriate
	- Detect and resolve problems proactively with dashboards, metrics, and alerts
	- Improve CI and test infrastructure, from unit tests to hardware testing
	- Maintain and improve the company's cloud infrastructure
	- Improve developer experience with tools such as static analyzers, linters, documentation editing/generation tools
	- Strong programming skills covering multiple languages and paradigms, from statically type-checked functional programming to dynamic imperative "scripting"
	- Relevant experience supporting company infrastructure with quality software:
		* Polyglot build systems (we use Nix heavily)
		* Resource management
		* Configuration management (cloud and collocated)
		* Job control
		* Version management
		* Performance tuning
		* Logs, analytics, and visualization
	- Address infrastructure with correct, maintainable, and timely software
	- Assess and manage risk when employing progressive approaches
	- Have strong situational awareness of business needs
	- Communicate effectively for both active collaboration and solo work
	- Architect, plan, and document strategically to successfully drive longer-term initiatives
	- Build up the expertise of your peers as you continue building your own
	- Have empathy for everyone involved with your work, both internal and external to Groq
	- Below are some of the open source tools we currently build solutions upon.  We’re open to adapt, and we don’t expect you to know everything we use.
		* ***General purpose build systems: Nix, Make, CMake***
		* ***Programming languages: Nix, HaskdsCloud services: Google Cloud Platform
		* Container-based systems: Docker
		* The national pay range for our technical roles are $100,000-$500,000.
+ skill set:
	- Infrastructure Software Engineer
	- Why join Groq? You want to be a part of something groundbreaking, where every day you can see the impact of your work on Groq’s technology and customer solutions.  As a Groqstar, you will join a talent-rich group of problem solvers and doers; in a culture that focuses on team, growth, innovation, and creativity. Simply put, at Groq, we defy gravity. 
	- We are changing as the world changes and have evolved into a Geo-agnostic company meaning you work where you are. Exceptional candidates will thrive in asynchronous partnerships and remote collaboration methods. Some roles may require being located near our primary sites, which will be indicated in the job description. We offer a competitive salary & benefits package, numerous quality-of-life perks such as a home office stipend, flexible learning allowance, optional professional coaching and a schedule of fun team activities. 
	- Are you ready to join our crew and help us reimagine machine learning and AI at scale? 
	- Groq's Software Infrastructure team maintains a wide array of services and tools cutting across all of Groq Software Engineering (assembler, compiler, systems/runtime, user libraries, and machine learning applications).  Our goal is to accelerate teams with reliable software that is pleasant to use.  We use a myriad of tools and techniques, leaning towards functional programming (FP) languages and paradigms where appropriate.  Constraints and invariants are important tools for us.
	- Maintain and improve Groq’s custom Nix-based build infrastructure
	- Develop and maintain management systems for long-lived jobs
	- Integrate various services, jobs, and builds with industry-standard tools as appropriate
	- Detect and resolve problems proactively with dashboards, metrics, and alerts
	- Improve CI and test infrastructure, from unit tests to hardware testing
	- Maintain and improve the company's cloud infrastructure
	- Improve developer experience with tools such as static analyzers, linters, documentation editing/generation tools
	- Strong programming skills covering multiple languages and paradigms, from statically type-checked functional programming to dynamic imperative "scripting"
	- Relevant experience supporting company infrastructure with quality software:
		* Polyglot build systems (we use Nix heavily)
		* Resource management
		* Configuration management (cloud and collocated)
		* Version management
		* Performance tuning
		* Logs, analytics, and visualization
	- Address infrastructure with correct, maintainable, and timely software
	- Assess and manage risk when employing progressive approaches
	- Have strong situational awareness of business needs
	- Communicate effectively for both active collaboration and solo work
	- Architect, plan, and document strategically to successfully drive longer-term initiatives
	- Build up the expertise of your peers as you continue building your own
	- Have empathy for everyone involved with your work, both internal and external to Groq
	- Below are some of the open source tools we currently build solutions upon.  We’re open to adapt, and we don’t expect you to know everything we use.
		* General purpose build systems: Nix, Make, CMake
		* Programming languages: Nix, Haskell, Python, C/C++
		* Software packaging and distribution: Deb, RPM, Docker
		* Continuous integration: GitLab
		* Metrics and visualization: Grafana
		* Configuration management: Salt
		* Cloud services: Google Cloud Platform
		* Container-based systems: Docker
	- The national pay range for our technical roles are $100,000-$500,000. The national pay range for our non-technical roles are $90,000-$470,000. Individual compensation will be commensurate with the candidate’s experience aligned with Groq’s internal leveling guidelines and benchmarks.
+ skill set:
	- Hands-on experience with container orchestration and automation in cloud architectures (Kubernetes, Helm)
	- Experience with modern observability principles, stacks and tools like Grafana, Prometheus, Thanos, Alertmanager
	- Knowledge in GitOps principles and CI/CD tooling
	- Experience with infrastructure-as-code tools such as Terraform
	- Worked with at least one public cloud provider (Azure, GCP, AWS, ...) already
	- Knowledge of Linux system administration, networking concepts and major Internet protocols (TCP/IP, IPsec, SSL, SSH, SMTP, HTTPS, DNS)
	- Experience with services meshes (e.g. Istio)
	- Knowledgeable in different database technologies (SQL, time series, NoSQL) 
	- Practical knowledge of modern programming languages (Java, Golang)
	- Scripting and automation skills using languages such as Python, Perl, Bash
+ skill set:
	- Practical experience with cloud and containerisation environments (Kubernetes, Helm, Argocd, Terraform)
	- Knowledge of networking technologies and protocols (e.g. dynamic routing, firewalling, TCP/IP, ...)
	- Experience with the Linux Netfilter framework
+ Familiarity with GitOps, Terraform, Kubernetes, Prometheus, as well as major clouds (Azure, AWS, GCP...)
+ skill set:
	- Experience with orchestration and build systems, such as Kubernetes, OpenShift, or ***Bazel***.
	- Join a team of passionate developers working with modern web development technologies: Java, Spring, Typescript, React, Cypress, next.js, Kubernetes, Bazel.
+ Experience with at least one of the job schedulers such as LSF, SLURM, Mesos/Marathon, Kubernetes, Docker Swarm
+ Experience with containers – Docker, Clear Containers, rkt
+ Some experience working with and implementing DevOps solutions including but not limited to Python, Ansible, Docker/Containers, Kubernetes and datacenter deployments
+ skill set:
	- Platform Engineer III
	- Be at the center of AI
	- With more than 45 million users, Anaconda is the most popular operating system for AI providing access to the foundational open-source Python packages used in modern AI, data science, and machine learning through a seamless platform. We pioneered the use of Python for data science, championed its vibrant community, and continue to steward open-source projects that make tomorrow’s innovations possible. Our enterprise-grade solutions enable corporate, research, and academic institutions around the world to harness the power of open source for competitive advantage, groundbreaking research, and a better world. To learn more visit https://www.anaconda.com.
	- Here is why people love most about working here: We’re not just a company, we’re part of a movement. Our dedicated employees and user community are democratizing data science and creating and promoting open-source technologies for a better world, and our commercial offerings make it possible for enterprise users to leverage the most innovative output from open source in a secure, governed way.
	- Anaconda is seeking a talented Platform Engineer III to join our rapidly growing company. This is an excellent opportunity for you to leverage your experience and skills and apply it to the world of data science, artificial intelligence, and machine learning.
	- Be a member of our Core Platform team helping make sure Anaconda apps and services run efficiently as container workloads, and stay up to date
	- Improve our existing infrastructure with new ideas, debugging, performance tuning, cost-savings, better integration with external services, and more
	- Enable developers to work as autonomously as possible while ensuring the stability of our platform through technical solutions, guidance, and education
	- Work with teams to troubleshoot, advise, and implement software and system changes
	- Work with other teams to find the right tool. Our stack is primarily AWS and Cloudflare, when requirements fall outside, you’ll help with design, integration, and maintenance
	- Participate in on-call rotations monitoring and supporting teams and systems
	- Participate on Slack and in team meetings, reviews, and demos
	- Work from backlogs of planned opportunities and on ad-hocs asks
	- GitOps mindset with a desire to reduce toil and automate everything
	- Production experience with Kubernetes, GitHub Actions, ArgoCD, or Helm
	- Experience provisioning, tuning, securing, scaling, networking, and monitoring containerized applications
	- Experience using IaC tools (e.g. Terraform, Helm, ArgoCD) 
	- Experience working in a fast-paced agile or remote environment
	- Results-driven, “self-starter” approach to your work
	- Team attitude: “I am not done until WE are done”
	- Embody our core values:     
		* Great People
		* Great Product
		* Great Performance
	- Care deeply about fostering an environment where people of all backgrounds and experiences can flourish 
	- Anaconda, Python, or OSS community experience
	- AWS storage and data transfer optimization
	- Examples of test automation, linting, and refactoring in your prior work
	- Great peer recommendations and appreciation
	- Experience working in a fast-paced startup environment
	- Experience working in an open-source, AI, or data science-oriented company
+ skill set for Senior DevOps Engineer:
	- Experience Level:  Recent College Grad
	- Job Type:  Full-Time
	- Location:  France - Paris, FR
	- Requisition ID:  8062
	- Qorvo (Nasdaq: QRVO) supplies innovative semiconductor solutions that make a better world possible. We combine product and technology leadership, systems-level expertise and global manufacturing scale to quickly solve our customers' most complex technical challenges. Qorvo serves diverse high-growth segments of large global markets, including consumer electronics, smart home/IoT, automotive, EVs, battery-powered appliances, network infrastructure, healthcare and aerospace/defense. Visit www.qorvo.com to learn how our diverse and innovative team is helping connect, protect and power our planet.
	- QSWG is looking for a DevOps to work on cloud product development and CI/CD infrastructure.
	- In Qorvo Infrastructure as a Code environment, the mission of DevOps is to understand Qorvo products and to create and maintain the appropriate CI pipeline and underlying infrastructure to support internal and external customers. The DevOps needs a good understanding of the software development process to train and review the developer teams on CI best practices and setup. To succeed in this role, a background in software development, system administration, automation and security is preferred.
	- CI/CD pipelines design:
		* Design complex pipeline interactions across multiple repositories
		* Optimize pipeline run time (builds system, pipeline definition)
		* Design GitLab runner architecture
		* Design deployment process and automation (including hardware deployment)
		* Design data collection platform
		* Design security from the start
		* Act as evangelist of the CI/CD platform
	- Development:
		* Design and develop tools in Python
		* Implement compliance and security tools
		* Data Engineering
	- Maintenance:
		* Monitor pipeline implementation
		* Develop and improve Supervision
		* CI/CD infrastructure management (GitLab runner on hybrid cloud environment)
		* Enhance security tools and setup
	- Technologies:
		* GitLab CI
		* AWS
		* Docker
		* Kubernetes
		* Terraform
		* Saltstack
		* Git
		* GitLab
		* Python
		* Prometheus
		* Grafana
		* ELK Stack
	- QUALIFICATIONS
		* Bachelor or higher degree in Computer Science or related discipline, or equivalent
		* industry experience
		* 4-5+ years of relevant work experience
		* Education: Master/Engineering Degree
		* Experience in C development
		* Deep understanding of build systems
		* Deep knowledge of Git and Git workflows
		* Experience in writing complex CI Pipelines across multiple projects
		* Experience and expertise in Python development
		* Familiarity with DevOps tools (Terraform, Saltstack/Ansible)
		* Ability to understand complex development environment
		* Good summary capabilities
		* Good communication skills in English
		* Autonomy and ability to learn new tech and tools
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










###	Skill Set for Compiler DevOps


Skill set for Compiler DevOps:
+ skill set for Compiler DevOps, Software Intern:
	- Do you want to help drive the progress of compiler development for cutting-edge technologies? Are you excited to learn new tools and develop infrastructure applications to assist developers? Are you passionate about improving software development workflows?
	- We are building the next generation of compiler technologies to accelerate graphics, compute and deep learning applications. We are looking for an intern to work on infrastructure projects to accelerate compiler development & testing.
	- In this role you will work closely with our build, release and productivity engineering teams to develop full-stack applications.
	- Pursuing a BS in Computer Science, Computer/Electrical Engineering or related field.
	- You are familiar with modern web UI frameworks such as React, Angular, or others.
	- Experience with Docker containers, Jenkins, Artifactory and Kubernetes.
	- You are skilled in wide variety of programming languages including Python and JavaScript.
	- Background with MySQL or equivalent and exposure to NoSQL databases.
	- Experience working across teams and converting designs to visual elements.
	- Excellent communication, problem solving and analytical skills to decompose complex issues and present them clearly and simply.
	- Background with agile software development methodology and practices.
	- Experience designing and deploying large-scale and distributed system software in cloud environments.
	- Experience with one or more version control systems, such as git or Perforce.
	- Experience with Linux configuration and installation of apps. Networking is a plus.
	- Interest in ground breaking technologies and the ability to take initiatives and drive them across multiple functional teams.
+ Software Engineer, SDLC
	- We believe that AI is a net positive force in the world. Our vision and mission are to help rebuild AI infrastructure to advance humanity and our environment. We will do whatever it takes to empower our customers, team, and company to benefit from that pursuit. You can read about our culture and careers here to understand how we work and what we value.
	- We are owners and advocates for the underlying technologies, developer platforms, product components, and infrastructure. These essential building blocks form the high-quality and coherent experiences our users expect. We aim to drive the pace of innovation for every AI/ML developer.
	- Within the Software Development Life Cycle Team, we seek to understand and apply the capabilities that drive software delivery performance at Modular. We are heavily data driven and make use of the dora metrics to track progress as we run experiments to determine the impact of our team’s initiatives and investments.
	- We are looking for “T” shaped individuals to help accelerate the Software Development Life Cycle. We focus on compounding improvements of customer value delivery and exponential gains in velocity and quality. The goal is to bring together diverse, collaborative teams for intensive project work. We are looking for candidates based on both their breadth and depth of experience.
	- The vertical stroke of the “T” is a depth of skill that allows candidates to contribute to the creative process. That can be from any number of different specialities: software engineering, IAAS and PAAS, test, build, compiler, kernel, hardware, machine learning and more.
	- The horizontal stroke of the “T” is the disposition for collaboration across disciplines. It is composed of two things. First, empathy. It’s important because it allows people to imagine the problem from another perspective – to stand in somebody else’s shoes. Second, ideal candidates tend to get very enthusiastic about other people’s disciplines, to the point that they may actually start to practice them. T-shaped people have both depth and breadth in their skills.
	- Come and be part of our team and architect our software development life cycle at Modular. Join our world-leading AI infrastructure team and help drive our AI infrastructure stack forward!
	- Build, optimize, and contribute to machine learning frameworks such as TensorFlow, PyTorch, and ONNX/ONNXRuntime.
	- Ensure that existing and up-and-coming accelerators are high performing and work with ML frameworks inside the Modular runtime.
	- Work with compiler, performance, and kernel teams to provide a foundation that fully utilizes today’s complex server, mobile, and other production systems.
	- Collaborate with open source engineers to help facilitate the future of ML software stacks.
	- Develop highly reliable and scalable CI/CD pipelines for builds, tests and benchmarking across diverse OS and hardware targets.
	- Monitoring, alerting and observability for Software Development Life Cycle services.
	- 4+ years of systems design experience such as HPC, kernel development, low-level threading, high-performance concurrency, etc.
	- In-depth knowledge of modern C++ is required.
	- Experience with one or more cloud application programming languages (C, C++, Java, C#, Go etc.) and modern scripting languages (Bash, Python, etc). 
	- Ability to utilize and work within traditional GitHub workflows such as pull requests.
	- Strong collaboration skills, ability to share information with internal and external engineering teams.
	- Experience, or deep interest, in machine learning technologies and use cases.
	- Creativity and curiosity for solving complex problems, a team-oriented attitude that enables you to work well with others, and alignment with our culture.
	- Experience using open source software components in production.
	- Devotion to quality and engineering excellence.
	- Strong written and verbal communication skills.
	- Strongly identifies with our core company cultural values.
	- LLVM, MLIR, or Python experience is a bonus.
	- Experience with working on or contributing to a major machine learning framework such as TensorFlow, PyTorch, and/or ONNX/ONNXRuntime is a bonus.
	- The estimated base salary range for this role to be performed in the US, regardless of the state, is $166,500.00 - $286,000.00 USD. The salary for the successful applicant will depend on a variety of permissible, non-discriminatory job-related factors, which include but are not limited to education, training, work experience, business needs, or market demands. This range may be modified in the future. The total compensation for a candidate will also include annual target bonus, equity, and benefits, with equity making up a significant portion of your total compensation.














##	Site Reliability Engineering (SRE)





Layers of a pyramid for Site Reliability Engineering (SRE).
+ product (design and management)
+ (product/software) development
+ (infrastructure) capacity planning
+ (software) testing and release management
+ postmortem analysis, or root-cause analysis (RCA)
+ incident response
+ (cloud computing or online Web) service monitoring






Skill sets for Site Reliability Engineering, SRE:
+ skill set:
	- ArgoCD
	- AlloyDB / PostgreSQL
	- Github Actions
	- Google Cloud Platform
	- Fluentbit
	- Kubernetes, Helm & Docker
	- Meillisearch
	- Node.js
	- OpsGenie
	- Playwright
	- Redis
	- Temporal
	- Traefik
	- Turbo repo
	- TypeScript
	- Vite
	- Vitest
+ skill set:
	- Experience monitoring cloud environments using tools like Splunk, VictorOps and Nagios
	- Knowledge of best practices related to security, performance, and disaster recovery.
	- Skilled in identifying performance bottlenecks, spotting anomalous system behavior, and determining the root cause of incidents.
+ skill set:
	- Senior Site Reliability Engineer
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via ***Terraform*** as well as ***Kubernetes, Helm and Istio***
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of ***Terraform, Ansible, Puppet or Salt***.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including ***Kubernetes, Helm, and Istio***
	- Experience operating one or more OSS technologies like ***Kafka, Cassandra, Zookeeper***; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Support and improve our customer Insights Platform by operating and extending the Open Source software we depend on (Prometheus, Thanos, Grafana).
	- Work closely with other product teams to enhance product offerings and improve our customer's observability.
	- Automate as much of the operational work as possible to allow time for innovation.
	- Working with exciting technologies such as Kubernetes, Prometheus/Thanos, Go, Docker, Kafka, ScyllaDB, and more.
	- Proficiency in at least one of the following languages: Go, C/C++, Java, Python, Ruby. We primarily use Go, but sometimes we need to extend systems written in other languages such as Java.
	- A willingness to dive into configuration management, deployment automation, and instrumentation.
	- An appreciation of SRE principles, along with utilizing data and automation to improve systems.
	- Excellent communication skills. We work intimately with product teams to ensure our customers have the observability they need to be successful on our platform.
	- We value development. You will work with some of the smartest and most interesting people in the industry. We are a high-performance organization that is always challenging ourselves to continuously grow. We maintain a growth mindset in everything we do and invest deeply in employee development through formalized mentorship, LinkedIn Learning tracks, and other internal programs. We also provide all employees with reimbursement for relevant conferences, training, and education.
+ skill set:
	- Principal Site Reliability Engineer, Splunk Observability - remote Spain
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Principal Site Reliability Engineer, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- ***Responsible for automating & operationalizing cloud provider infrastructure via Terraform, Kubernetes, Helm and Istio***
	- ***Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.***
	- ***Own & operate critical back-end open-source services like Cassandra, Kafka, Elasticsearch, MongoDB, and Zookeeper.***
	- Build tools and design processes that help improve observability and system resiliency.
	- ***Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.***
	- Implement service level metrics & service level objectives that act as service-level health indicators.
	- ***Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.***
	- Strong coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code experience within one or more of Terraform, Ansible, Puppet or Salt.***
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- ***Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)***
	- ***Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio***
	- ***Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus***
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 12+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- GitHub is seeking software engineering professionals to join its new SRE team. As a valued member of our close-knit team, you will bring your passion for building fault tolerant systems and reliable software to help us steward reliability as a feature throughout the organization. Your work will help us scale the world's largest code hosting platform.
	- Our charter is broad but our focus is to improve the availability, resilience, and sustainability of GitHub's products. We do this through architecture, technology, process, and partnerships with product teams.
	- Our SRE team is highly distributed; our work environment is one of remote work, asynchronous communication, trust, and respect. Through your strong written communication and software skills, you will develop meaningful working relationships with coworkers from around the globe.
	- The SRE role at GitHub is an opportunity to blend your system design, empathy, and software engineering skills on an ever-changing set of novel reliability challenges. Join us on this journey and have a meaningful impact on how the world builds software.
	- Responsibilities:
		* Exert technical influence to improve the reliability of our products and systems
		* Develop and maintain infrastructure products and software automation
		* Integrate with third-party solutions where it makes the most sense.
		* Work closely with our observability and chaos engineering teams.
		* Cultivate GitHub's open source projects and build things you are proud to share.
		* Steward reliability as a feature across the organization through concepts such as SLOs and service maturity.
	- Minimum Qualifications:
		* Comfort with the GNU/Linux operating system.
		* Experience with distributed systems with high availability requirements.
		* Exposure to system-level languages such as Go or C/C++.
		* Familiarity with configuration management software such as Puppet, Ansible, or Salt.
		* Familiarity with infrastructure services and sidecar patterns.
		* Experience balancing the service reliability, sustainability, and technical debt for services running at scale.
	- Preferred Qualifications:
		* Experience with highly available systems at scale.
		* Experience building infrastructure and automation.
		* Experience negotiating SLIs, SLOs, and SLAs with product owners.
		* Success in a remote work environment.
		* Incident response and/or incident management experience.
		* Exposure to CNCF projects such as Kubernetes or Prometheus.
+ skill set:
	- Site Reliability Engineer (Stack Automation Service Team)
	- Splunk's Cloud group is looking for an experienced Site Reliability Engineer to join a team that is responsible for Cloud’s operational infrastructure and delivery. As a member of the Stack Automation Service team, you will be responsible for maintaining and troubleshooting Splunk's SaaS system, monitoring system stability and performance, troubleshooting complex problems, performing Amazon instance maintenance and system upgrades, and managing Amazon server/storage deployments, all while collaborating with various other Splunk Cloud teams. This is a fantastic opportunity to work with an exceptional team, grow your cloud experience, and help drive the growth of Splunk Cloud.
	- Puppet experience. You have at least 2 years of Puppet experience, including writing Puppet code and configuration management.
	- Python or Bash scripting experience. You will develop scripts and tools in Python/Bash.
	- AWS experience. Knowledge of Amazon EC2 including machine image management and storage, as well as an understanding of Amazon EC2 regional centers, availability zones, and HA strategies
	- Unix/Linux. You will use a command line terminal frequently.
	- Multi-tenant infrastructure experience. Experience supporting customer facing multi-tenant infrastructure (SaaS) or similar cloud related services
	- Software Development and Data Structures/Algorithms. We code primarily in Golang and Ruby, and work with RESTful APIs.
	- Cloud and container experience. Building and scaling secure services on different cloud providers.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and disaster recovery.
	- Problem Solving. You are able to fix a product outage, skilled in identifying performance bottlenecks, spotting anomalous system behavior, and figuring out the root cause of incidents.
	- Desire to learn and adapt. Our team has many projects going on at once, and you'll have the opportunity to learn to navigate new code and features.
	- Passion. We want you to actively own your work and be excited about your projects.
	- ***Kubernetes experience. Working in Kubernetes systems with experience in kubectl and docker containers.***
	- Terraform experience. Any prior work with Terraform is a plus.
	- ***Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.***
+ skill set:
	- In our Core C/ Chef Product Group, we develop the world's best products for managing applications and infrastructure at scale, and we deploy them to solve real problems in all kinds of industries. We get to work with the latest in cloud and container technologies. We have the opportunity not just to follow but to shape best practices. Our platform is used to enable billions of people around the world to chat, fly, present, bank, game, shop, and learn. Chances are the applications and devices you use every day to have infrastructure built, deployed, secured, and run with our code.
	- We are seeking a highly motivated, results-oriented individual with strong Site Reliability Engineering skills and experience in cloud technologies to join our platform engineering team. As a Site Reliability Engineer, you will play a lead role in designing, implementing, and supporting the platform for Chef Cloud services. You will also have a key influence on our future processes and platform design.
	- Build, operate, and maintain a platform for Chef Cloud services. This will include technologies such as AWS services (ECS, EKS, S3, and more), Kubernetes, service mesh (Linkerd or Envoy), Postgres/RDS, Graph databases, API gateways, authentication services, 3rd party integrations, and more.
	- Collaborate on achieving the best design/architecture for our systems and infrastructure.
	- Collaborate with other Engineering teams to support services before they go live through activities such as system design consulting, developing software platforms and frameworks, capacity planning, and launch reviews.
	- Maintain services once they are live by measuring and monitoring availability, latency, and overall system health.
	- Implement modern systems observability solutions including monitoring, alerting, metrics, logging, and APM & distributed tracing.
	- Scale systems sustainably through automation and evolve systems by pushing for changes that improve reliability and velocity.
	- Be on-call for services that the SRE team owns.
	- Practice sustainable incident response and post-incident analysis by acting as an incident manager. You’ll follow our existing incident management process and recommend improvements to that process.
	- Mentor the team.
	- You have a Bachelor's degree in Computer Science or related field and 10+ years of relevant experience (or equivalent combination of education and experience).
	- You have a solid understanding of and experience with configuration management and compliance automation.
	- You have an expert-level understanding of and at least 2 years of working experience with containerization using Docker and Kubernetes in a production environment.
	- You’re comfortable deploying and operating services using AWS technologies and have an expert understanding of the various offerings available.
	- You’ve built and supported systems using cloud-native (CNCF) technologies at scale.
	- Working knowledge on terraform or similar tools
	- You are interested in designing, analyzing, and troubleshooting large-scale distributed systems.
	- You understand what it means to operate infrastructure as code, and have experience developing services and automation to do so. Chef knowledge would be a plus
	- You have a great ability to debug and optimize code and automate routine tasks to eliminate toil.
	- You have a systematic problem-solving approach, coupled with strong communication skills and a sense of ownership, initiative, grit, and drive.
	- You have designed and implemented applications and systems that scale, are resilient to failure, and are observable.
+ skill set:
	- distributed systems for customer-facing environments
	- navigate and scale multi-tier cloud environments on AWS and GCP
	- container-centric architectures, using:
		* Docker
		* Kubernetes
			+ EKS
			+ GKE
			+ AKS
			+ OpenShift
		* ECS
		* Docker Swarm
		* Mesos
	- infrastructure-as-code tools:
		* Terraform
		* Ansible
		* Puppet
		* Chef
	- full-stack Web development using:
		* React
		* Node
		* MongoDB
	- experience with OpenResty, Pulumi
		* OpenResty: full-fledged Web platform based on nginx, which runs Lua scripts on LuaJIT engine
		* Pulumi: universal infrastructure-as-code SDK, for:
			* creating cloud infrastructure
			* deploying cloud infrastructure
			* managing cloud infrastructure
+ skill set:
	- RELIABILITY ENGINEER
	- Our Global Reliability Engineering group consists of multiple teams of versatile full stack engineers who drive the expansion and maintenance of Two Sigma’s many and varied systems. The team exists in the space between traditional systems engineering and development, and seeks to merge the capabilities from both disciplines.
	- Improving all aspects of software reliability, including better monitoring, alerting and documentation
	- Facilitating the adoption of reliability engineering best practices by our software engineering partner teams
	- Serving on a rotating business-hours on call rotation investigating and remediating alerts using service specific runbooks
	- Recommending, developing, and automating production and research data pipelines using industry best practices
	- Creating acceptance and quality criteria to guarantee the integrity of production and research data sets
	- Automating validation processes for our data sets
	- Collaborating with business partners, software engineers and quantitative modelers, to understand and improve business processes
	- Dynamic resource management frameworks (ex: Kubernetes, Docker)
	- Large-scale batch processing using public cloud technologies (AWS, GCP, or Azure)
	- Building resilient data pipelines using open source technologies (ex. Airflow, Prefect)
	- Observability in distributed systems (ex. Elasticsearch, Logstash, Kibana, Datadog, Prometheus, Grafana)
	- BS/BA in computer science or another highly technical, scientific discipline
	- Minimum 1 year of experience required; preferred 1-10 year(s) of experience with automating technical processes
	- Experience with Bash, Python, or other similar languages
	- Familiarity with the UNIX command line
	- Prior experience in a similar Site Reliability Engineering (SRE), DevOps, distributed computing, systems engineering/administration, or related function.
+ skill set:
	- Site Reliability Engineer
	- Responsible for building, tuning and operating the entire infrastructure that powers Abacus.AI's multi-cloud SaaS products. We have a modern technology stack built on Kubernetes, Spark, TensorFlow, Python, Go, Mysql & Redis
	- Requirements:
		* BS or MS from a top notch CS programs
		* 2+ years professional experience in hands-on engineering roles including operating production environments in public clouds: AWS, GCP, Azure
		* Strong Linux/Unix systems fundamentals
		* Python programming experience in production environments
		* Experience with modern cloud environments: containerization, infrastructure-as-code, devops, CI/CD pipelines and automation
	- Preferred:
		* Operating Kubernetes clusters
		* Experience with ML Ops: Spark, TensorFlow, GPUs
		* Experience with Terraform
		* Hands-on experience with network security, databases systems
+ skill set:
	- Reliability Engineer
	- Join the engineering teams that bring OpenAI’s ideas safely to the world!!
	- The Applied Engineering team works across research, engineering, product, and design to bring OpenAI’s technology to consumers and businesses. We seek to learn from deployment and distribute the benefits of AI, while ensuring that this powerful tool is used responsibly and safely. Safety is more important to us than unfettered growth.
	- As OpenAI continues to grow, we are looking for experienced, problem-solving engineers to ensure our systems scale. Our success depends on our ability to quickly iterate on products while also ensuring that they are performant and reliable. You will work in a deeply iterative, collaborative, fast-paced environment to bring our technology to millions of users around the world, and ensure it’s delivered with safety and reliability in mind. Successful candidates will play a crucial role in ensuring the reliability, scalability, and performance of our systems as we continue to expand. As a Reliability Engineer, you will be at the forefront of maintaining and enhancing the stability, scalability, and performance of our rapidly evolving infrastructure. You will work closely with cross-functional teams, including software engineers, product managers, and data scientists, to build and maintain resilient systems that can handle our growing user base and workload.
	- Design and implement solutions to ensure the scalability of our infrastructure to meet rapidly increasing demands.
	- Collaborate with development teams to make the systems they design and operate more reliable.
	- Implement and manage monitoring systems to proactively identify issues and anomalies in our production environment.
	- Develop and maintain service level objectives (SLOs) and service level indicators (SLIs) to measure and ensure system reliability.
	- Implement fault-tolerant and resilient design patterns to minimize service disruptions.
	- Build and maintain automation tools to streamline repetitive tasks and improve system reliability.
	- Partner with researchers, engineers, product managers, and designers to bring new features and research capabilities to the world.
	- Participate in an on-call rotation to respond to critical incidents and ensure 24/7 system availability.
	- Enjoy seeking out and addressing bottlenecks and areas for performance improvement in our systems.
	- Utilize Infrastructure as Code (IaC) principles to automate infrastructure provisioning and configuration management.
	- Are experienced in collaborating with cross-functional teams to ensure that reliability and scalability are considered in the design and development of new features and services.
	- Have a track record of accelerating engineering reliability by empowering your fellow engineers with excellent tooling and systems.
	- Help create a diverse, equitable, and inclusive culture that makes all feel welcome while enabling radical candor and the challenging of group think.
	- Have a humble attitude, an eagerness to help your colleagues, and a desire to do whatever it takes to make the team succeed.
	- Own problems end-to-end, and are willing to pick up whatever knowledge you're missing to get the job done.
	- Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).
	- Proven experience as an reliability engineer or a similar role in a fast-paced, rapidly scaling company.
	- Strong proficiency in cloud infrastructure.
	- Proficiency in programming/scripting languages.
	- Experience with containerization technologies and container orchestration platforms like Kubernetes.
	- Knowledge of IaC tools such as Terraform or CloudFormation.
	- Excellent problem-solving and troubleshooting skills.
	- Strong communication and collaboration skills.
	- Experience with observability tools such as DataDog, Prometheus, Grafana, Splunk and ELK stack.
	- Experience with microservices architecture and service mesh technologies.
	- Knowledge of security best practices in cloud environments.
	- Annual Salary Range: $245,000—$385,000 USD
+ skill set:
	- Site Reliability Engineer (SRE)
	- London, UK or Palo Alto, CA
	- The xAI London team is a team of software engineers with a focus on large-scale, highly-reliable distributed systems. We work on many different levels of the stack ranging from build systems, to production backend infrastructure, and frontend development. For example, we built large parts of the Grok production stack. We focus on building high-quality software and aren’t afraid to delve into technically complex topics to solve problems the right way.
	- We’re looking for an experienced site reliability engineer (SRE) who can thrive in a dynamic start-up environment. The main responsibilities for this role are:
	- Improving our observability by adding/adjusting metrics,
	- Building easily parsable dashboards,
	- Building reliable alerts,
	- Designing and overseeing our on-call rotations,
	- Improving our deployment process to increase reliability.
	- Expert in at least one programming language that compiles to machine code such as Rust, C++, or Go. Rust or C++ experience is preferred,
	- Expert knowledge of monitoring technologies such as Prometheus, Grafana, and PagerDuty,
	- Expert knowledge of deployment technologies such as Pulumi or Terraform,
	- Expert knowledge of Kubernetes.
	- The role is based in our London office close to Piccadilly Circus underground station. We usually work from the office 5 days a week but allow for work-from-home days when required. Candidates must be willing to attend late meetings at least twice a week to coordinate with the rest of our team, which is based in California. This role includes semi-regular business trips to California.
	- Interview process
		* After submitting your application, the team reviews your CV and statement of exceptional work. If your application passes this stage, you will be invited to a 15 minute interview (“phone interview”) during which a member of our team will ask some basic questions. If you clear the initial phone interview, you will enter the main process, which consists of four technical interviews:
			+ Coding interview in Rust, C++ or Go,
			+ Monitoring & deployment design interview,
			+ Distributed systems design interview,
			+ Meet the wider team and give a 20 minute presentation about the most difficult technical problems you have solved.
		* Our goal is to finish the process within one week. We don’t rely on recruiters for assessments. Every application is reviewed by a member of our technical team. All interviews will be conducted via Google Meet.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















##	DevSecOps




DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term "shift left" can be used. Security is tested in three main areas: static, software composition, and dynamic.









+ Skill Set for Senior DevSecOps Engineer:
	- Globus (www.globus.org) is a sustainable, non-profit unit within The University of Chicago delivering solutions to the research community worldwide. Globus develops and provides critical services that support scientific research for governmental, academic, and commercial organizations in a wide range of disciplines including life sciences, physics, and astronomy.  We develop and operate commercial-quality, cloud-based software application and platform services used by 10s of thousands of researchers to manage their large–and growing–data management challenges. We have offices located at the NBC Tower in the heart of downtown Chicago and remote employees who work-from-home. Globus, together with Globus Labs, a research group within the University of Chicago, and part of the Data Science and Learning Division at Argonne National Labs, develop and deploy cutting edge technologies to solve new challenges facing the scientific community and enable break-through scientific discoveries.
	- As a senior member of the Globus Operations team, reporting to the System and Security Operations Manager, the focus of this position is to ensure security is comprehensively and effectively integrated into both our software services and AWS cloud infrastructure.  The Globus platform is a hybrid solution that combines AWS-hosted orchestration services with installable applications, enabling seamless integration into customer environments. It offers a range of capabilities, including identity and access management, data transfer and sharing, and task automation, delivered as both software-as-a-service (SaaS) and platform-as-a-service (PaaS).
	- This position will architect, implement, and operate a resilient Cloud infrastructure to ensure high-security, high-performance, high-availability and scalability for Globus platform services. We’re looking for a talented senior DevSecOps engineer who can effectively work across teams to integrate security into new and existing software and infrastructure services as well as be an evangelist and educator of security best practices.
	- Security Operations: lead the implementation and monitoring of Globus’ security procedures and controls, including creating audit reports to meet target security compliance standards. Participate in incident response, as required. 
	- Security Integration: Integrate security features into the software development life cycle, as well as existing services and infrastructure. Identify potential threats and vulnerabilities early in the development process through techniques like STRIDE and implement mitigating strategies.
	- Architecture and Design: lead the definition and documentation of Cloud infrastructure architecture, including security, monitoring, logging, and deployment strategies, for the production and development environments.
	- SRE/Operations: Deploy, operate, monitor, and maintain production Globus services for high availability.
	- Support: As a technical consultant and resource for other team members, including the engineering and user support team, assist in addressing operational issues and troubleshooting.
	- Designs new systems, features, and tools. Solves complex problems and identifies opportunities for technical improvement and performance optimization. Reviews and tests solutions to ensure appropriate standards are met.
	- Utilizes technical knowledge of existing and emerging technologies, including public cloud offerings from Amazon Web Services, Microsoft Azure, and Google Cloud.
	- Performs other related work as needed.
	- Strong understanding of security concepts, including threat modeling, risk assessment, and vulnerability management.
	- Knowledge of the SDLC and experience integrating security best practices at every process stage.
	- Understanding cloud security principles, including secure architecture design and configuration management.
	- Knowledge of container security principles relating to Docker and AWS ECS.
	- Good analytical problem-solving skills to scrutinize and solve very intricate security problems with effective solutions.
	- System Administration, Operations Monitoring, Application Performance Monitoring, Logging, Incident and Problem Management.
	- Use operational tools and monitoring platforms to gain in-depth knowledge, understanding, and ongoing system availability, performance, and capacity.
	- Define requirements and develop tools and reporting as needed by projects and operations.
	- Experience with threat modeling techniques and tools (e.g. STRIDE, Veracode, SonarCube, OWASP suite), as well as security frameworks and standards such as NIST 800-53, OWASP, and CIS Benchmarks.
	- Experience developing in one or more scripting languages, preferably including Python and Bash, to automate, monitor and maintain infrastructure.
	- Experience with Linux administration, troubleshooting, and management tools.
	- Demonstrated experience with relevant AWS services including AWS IAM, VPC, WAF, KMS, EC2, RDS, DynamoDB, ElasticSearch, SQS, S3, ECS.
	- A solid understanding of networking, firewalls, and DNS.
	- Experience with monitoring solutions (e.g. AWS CloudWatch, Nagios).
	- Expertise in common relational database administration tasks (e.g., PostgreSQL, MySQL, SQLite).
	- Experience with CI/CD orchestration tools (e.g. GitHub Actions, Jenkins).
	- Expertise with designing and building Infrastructure as Code (IaC) (Terraform, CloudFormation).
	- Operational experience with web-servers (e.g. Apache, Nginx) and web-based technologies (e.g. RESTful APIs, SSL certificate management, TLS).
	- Security Operations experience with public sector compliance regulations (e.g. FedRAMP, HIPAA, FISMA), as well as working with compliance teams and auditors to produce compliance records and artifacts.
	- Proficient in uniting cross-functional teams and communicating clearly, while fervently pursuing knowledge of the latest trends and technologies in security.
	- Exposure to fundamental concepts, practices, and procedures of software development.
	- Expertise in integrating development and deployment framework with the monitoring, operations, and orchestration required for running applications securely and at-scale on public cloud platforms.
	- Problem solving skills.
	- Ability to prioritize and manage workload to meet critical project milestones and deadlines.
	- Ability to work in a collaborative team environment.
	- Confidentiality related to sensitive University matters such as, strategic initiatives, trade secrets, quiet periods, and scientific discoveries yet to be put in the public domain.
	- Passionate about continued learning and being aware of current security trends and technologies.
	- Working Conditions: This job requires occasional evening or weekend hours.
		* This job is a remote position with occasional attendance at in-person meetings required. 
	- Pay Range: $145,000.00 - $170,000.00
		* The included pay rate or range represents the University’s good faith estimate of the possible compensation offer for this role at the time of posting.
+ skill set:
	- Dev Sec Ops Manager (IT)
	- At Edge Impulse we are building the future of data-driven engineering.
	- This is a unique opportunity to join us as we usher in the future of embedded machine learning by empowering developers to create and optimize solutions with real-world data. Our teams make the process of building, deploying, and scaling embedded ML applications easier and faster than ever, unlocking massive value across every industry, with millions of developers making billions of devices smarter.
	- We are growing! This is an opportunity to lead and directly contribute to the development and evolution of IT and Security operations (a 1st-of-its-kind role at Edge Impulse).
	- Provision and enable users with appropriate access and accelerate their timeline to full productivity with key IT and Security tools and processes
	- Maintain company systems and tools estate
	- Manage and maintain company compliance and security processes including SOC 2 Type I/II working proactively and closely with stakeholders across the company
	- Be 1st point of contact for completing customer, partner and vendor security inquiries
	- Proactively identify processes and/or tools that enhance user experience and productivity across the organization
	- A minimum of three years of experience managing system uptime and performance in a cloud-native enterprise company (more experience preferred)
	- Experience managing and working across the organization to maintain SOC 2 Type I/II compliance
	- Demonstrated experience building and maintaining relationships with key stakeholders across an organization
	- Scripting and system automation necessary - - some interest in SRE is a plus
	- Experience implementing and/or maintaining OneTrust, Drata and/or Vanta a plus
	- Deep experience with enterprise tools such as: 
	- Mac laptop shop
	- MDM solutions - e.g. Kandji, Jamf
	- Anti-virus tooling - e.g. Cortex XDR, Carbon Black 
	- Logging tools - e.g. Splunk, Datadog
	- Google Workspace, Slack and other enterprise-wide systems
	- SSO - e.g. OKTA, JumpCloud, OneLogin
	- Engineering tools - eg. Github, AWS
	- Bachelor degree in computer science, management information systems or a related field preferred
+ skill set:
	- Senior Client Platform Engineer
	- Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
	- Are you the kind of systems engineer that has a passion for administering enterprise software using best of breed technologies? Are you self-motivated and require minimal supervision? Do you put together a rolling 12-month roadmap to execute against? If so, then this is the dream job you've been looking for. 
	- Splunk is looking for a highly skilled Senior Systems Engineer focusing on endpoint security, and configuration management. You should be comfortable delivering at a high level in a fast paced and growing environment. You will drive standardization and management for our endpoints along with a number of enterprise applications and services. This role provides high visibility and impact to both the CIO and CISO organizations.
	- Architecture, design, integration, implementation, operation, and support of enterprise-wide applications and services for our Windows fleet.
	- Assisting in developing long-term strategies and capacity planning for meeting future end user needs
	- Configuration Management for Windows using industry standard tools to meet Security requirements and comply with CIS benchmarks
	- Managing configuration of our endpoint security software such as endpoint detection and response, application allow/block lists, and host-based intrusion detection software
	- Partnering with the Security Engineering leads to coordinate efforts, initiatives, and roadmaps
	- Administer enterprise software including deployment and package management
	- Write scripts/policies that automate application and settings distribution using internal tools
	- Manage transition plans for major upgrades or patches
	- Integrate with other internal systems and tools
	- Manage and report on application performance against KPI’s
	- Work as the escalation point between various support teams for issues on the client platform
	- Work as a liaison from the Splunker Technology Success org to other IT Service organizations to deliver feature enhancements and best in class solutions through shared products and goals
	- Drive client security models and best practices in an enterprise environment
	- Drive business decisions through data using tools like Splunk
	- Diagnose and investigate unique and complex systemic problems
	- Develop solutions that meet the business needs to complex customer requirements
	- 10+ years of overall IT experience; 5+ years experience of providing application support and engineering
	- Experience with implementing security standards and compliance across a huge enterprise organization
	- Knowledge of bash/python scripting
	- Experience with Endpoint Management platforms such as WorkspaceONE/InTune/LANdesk/Kace/etc.
	- Experience with DevOps platforms such as Puppet/Salt/Chef
	- Ability to work in high pressure, highly flexible environment against both short and long term requirements
	- Passionate about technology and solving IT operations-focused problems
+ skill set:
	- Software Engineer - Analytics Platform (Remote)
	- Are you passionate about working on products that make a difference for your customers? Do you enjoy building large scale applications that are powered by huge data sets? Do you value working in an environment where you're empowered to make key technical decisions across a full stack of technologies? If so, a role on the Splunk Security Analytics team might be a great fit for you.
	- As a Software Engineer on the Security Content Engineering team, your primary focus is content distribution, content improvement, and content assurance. You will contribute to build and maintain the content distribution system and support content improvements.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify, and improve designs. Help put together a plan for how to make it happen using Agile Methodologies.
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality while continuously deployed in the cloud to our customers.
	- Participate in the software development lifecycle by writing code, tests, documentation; support the sprint management process; and communicate effectively with peers and managers.
	- 2+ years of Software Engineering experience.
	- Bachelor's degree in Computer Science or equivalent training and work experience.
	- Proficient with Java or Python programming.
	- Engagement with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Exposure to an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Versed with CI/CD frameworks and experience with automation.
	- Strong oral and written communication skills, including a demonstrated ability to prepare documentation and presentations for technical and non-technical audiences.
	- Background in developing products for the Security market, a plus.
+ skill set:
	- Senior Software Engineer - Analytics Platform (Remote)
	- Enterprise Security behavioral analytics service (Advanced Analytics) is Splunk’s next-generation, cloud-native, multi-tenant analytics solution that detects known and unknown security threats at petabyte scale. Advanced Analytics detects cybersecurity threats by using stream and batch processing, and building large scale analytics infrastructure for petabyte scale data ingestion, processing, storage, and analysis. Advanced Analytics will power large and medium scale enterprises to combat security threats, protect brand reputation and protect intellectual property.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep aware of security trends in the industry and bring that knowledge back to the team.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best software development practices.
	- Champion, coach, and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 8+ years of experience in Enterprise Software Engineering.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience with Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, HBase, Cassandra, MongoDB.
	- Exposure to working with cloud environments (AWS, Azure, GCP) and container ecosystems (Docker and Kubernetes).
	- Knowledge of distributed computing architectures and principles that solve for scalability, consistency, availability, performance, and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












##	ITOps





ITOps (for data centers):
+ maintain core infrastructure and associated services needed to support AI and machine learning models running in business applications
+ enterprise AI role:
	- provide highly available, secure infrastructure to operate performant models in enterprise application to scale, 24x7
	- deliver performance metrics for deployed models and applications
	- plan and execute infrastructure evolution to support AI technologies
+ for ModelOps
	- centralized catalog of all:
		* models
		* model runtime requirements
		* lineage
		* operational history
	- regardless of tools to create AI and ML models, support standardized models on any infrastructure (prem, cloud, or hybrid) that can be consistently:
		* deployed
		* monitored
		* controlled
	- provide automated alerts regarding model performance and behavior
	- automated processes and approvals for:
		* deployment
		* testing
		* refresh
		* monitoring
	- real-time visibility to performance of deployed models
+ Information Technologies Operations (ITOps) is the process responsible for acquiring, designing, deploying, configuring, and maintaining the physical and virtual components that comprise your IT infrastructure
+ While ITOps takes a broad view of the entire technology landscape that your organization relies on to conduct its business mission, DevOps focuses on the task at hand.
	- DevOps teams don’t always have the visibility or awareness of the downstream implications on the enterprise stack.
	- And this is where can modern ITOps team can help.
+ ITOps (or) Tech Ops is the most traditional Ops that refers to managing all the physical and software components of an organization’s IT environment.
	- It is responsible for the smooth running of a business by handling applications, delivery, maintaining services, and the underlying technologies administrated by a company's IT members to its internal or external clients.
	- It includes:
		* maintaining networks
		* ensuring security
		* managing data center
		* system administrators
		* regulatory compliance
		* licensing
		* managing software
		* managing help desk
		* supports IT infrastructure and systems for daily business operations
	- IT department can effortlessly manage all the IT operations and make the job easier by using IT Operations.
		* It guides the business to be more secure, swift, and productive.
+ includes:
	- networking operations
	- deploying, maintaining, and configuring applications
	- overseeing both physical and virtual components of a company's IT environment
+ includes:
	- administration and maintenance,
	- network management,
	- systems management,
	- technical support (help desk)



ITOps processes consists of 5 main aspects/phase:
+ Run Solutions
	- The primary purpose of having ITOps teams is to run solutions such as data backups, configurations, handling servers, and restoring systems after an outage or update.
	- The aim is to optimize the performance and allocate the proper resource for the most effective delivery.
+ Infrastructure
	- Managing the IT infrastructure includes maintaining, provisioning, setting up, and updating all the software and hardware applications and cloud resources of the company's IT infrastructure.
	- These components include:
		* operating systems
		* hypervisors
		* network infrastructure
		* platforms
		* container environments
		* physical servers
		* application software
	- The maintaining oversight of IT infrastructure is made on-premises data center or in the cloud.
+ Network
	- Network management is responsible for managing all network functions for internal or external IT communications.
	- Network management is also involved in configuring and managing telecommunication lines.
	- In addition, it allows authorized customers to secure remote access to the company's network.
+ Security
	- Security management is an integral function of IT service management (ITSM).
		* It deals with:
			+ securing the hardware and software assets
			+ implementing security within development operations
			+ managing access control
			+ ensuring that security standards have reached across the IT environment of the organization
+ Problem Solving
	- Event management or incident management is also known as Problem-solving, it can be divided into 2 types – preventive measures and reactive measures.
		* Preventive Measures
			+ Preventive measures reduce the possibility of disasters and find the solution to anticipate and avoid any negative impacts on the IT environment.
		* Reactive Measures
			+ This concept refers to cyber-attacks, critical situations, and other issue when implementing disaster recovery plans and help desk management services.




ITOps tasks:
+ Network infrastructure:
	- Configuring and managing all networking functions for internal and external IT communications
	- Configuring and managing telecommunication lines
	- Managing firewall ports to allow the network to communicate with outside servers
	- Providing authorized users secure remote access to the organization’s network
	- Monitoring network health and performance, detecting anomalies, and preventing or quickly resolving issues, which may include building and managing a network operations center (NOC, pronounced “knock”), a centralized physical location from which ITOps teams can continuously monitor a network
+ Server and device management:
	- Configuring, maintaining and managing servers for infrastructure and applications
	- Managing network and individual storage to ensure they meet application requirements
	- Setting up and authorizing email and file servers
	- Provisioning and managing company-approved PCs
	- Provisioning and managing cell phones and other mobile devices
	- Managing licensing and desktop, laptop and mobile device software
+ Computer operations and help desk:
	- Managing data center locations and equipment
	- Operating the help desk
	- Creating, authorizing and managing all user profiles on organizational systems
	- Providing network configuration auditing information to regulatory agencies, business partners and other outside entities
	- Ensuring high availability of the network and disaster recovery plans
	- Alerting users when a major incident impacts network services
	- Instituting regular backups to facilitate data recovery when needed
	- Maintaining the ITIL for the organization








ITOps addresses:
+ infrastructure capacity
+ infrastructure availability
+ infrastructure security


KPIs for ITOps are based on:
+ application performance
+ infrastructure availability
+ infrastructure security
+ infrastructure cost




*Information Technology Infrastructure Library, ITIL*:
+ functions of *IT Operations Management framework*
	- ITOps, or IT operations, refers to the processes and services administered by an organization's IT staff to its internal or external clients.
	- applications management
	- technical management
	- service desk







+ skill set:
	- 4+ years of experience in developing, planning, and and administering VMware ESXI and vCenter deployments
	- Experience with vSphere, vSAN, and other VMware products and platforms
	- Experience with troubleshooting for virtualized platform
	- Ability to provide and manage virtual machine configurations, including networking, storage and security settings and deploy it to hosts upon request
	- Knowledge of data center networking, including TCP/IP, switching/routing, ports and protocols, firewall concepts, or load-balancing
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Knowledge of data center architecture: power, cooling, and networking.
	- Significant experience working with data center hardware and writing software to make that easier, faster, and less manual effort
	- Familiarity with best practices for hardware acceptance testing
+ Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
+ Software Engineer, Hardware Health
	- San Francisco, California, United States — Scaling
	- OpenAI’s Hardware Health team oversees all hardware health related aspects of our custom-built hyperscale supercomputers. The team is responsible for maximizing the available supercomputing capacity for research and ensuring that our researchers are minimally impacted by hardware faults.
	- The hardware health team is being incubated inside OpenAI’s Scaling team, which operates at the far edge of all available innovations in AI — doing the engineering and research required to train large-scale AI models of unprecedented capability.
	- As a SWE in Hardware Health, you will work to maintain a sophisticated and comprehensive suite of hardware health tests and collaborate with researchers and our Supercomputing team on root-causing and reliably reproducing newly discovered problems.
	- The team moves at a fast pace and provides individuals with a high degree of autonomy and a strong ability to affect change.
	- A balance of building and operational skills
	- Excellent abilities developing in python and shell scripting
	- A high degree of comfort digging into noisy data with SQL, PromQL, and Pandas
	- Experience developing reproducible analyses / building dashboards and visualizations
	- A high level of detail orientation and a good intuition for when results are “too good/bad to be true”
	- A strong sense of ownership causing them to very carefully monitor outcome of deployed updates
	- Prior TL experience as this is a 0-1 effort with team growth on the horizon
	- Bonus Points if you have expertise and interest in low level details of hardware components, protocols, and associated Linux tooling (PCIe, networking, power management, kernel perf tuning).
	- Annual Salary Range: $200,000—$370,000 USD
+ DevOps Engineer - Supercomputing
	- Bay Area (San Francisco and Palo Alto)
	- xAI’s mission is to create AI systems that can accurately understand the universe and aid humanity in its pursuit of knowledge.
	- Our team is small, highly motivated, and focused on engineering excellence. This organization is for individuals who appreciate challenging themselves and thrive on curiosity. Engineers are encouraged to work across multiple areas of the company, and as a result, all engineers and researchers share the title "Member of Technical Staff."
	- We operate with a flat organizational structure. All employees are expected to be hands-on and to contribute directly to the company’s mission. Leadership is given to those who show initiative and consistently deliver excellence. Work ethic and strong prioritization skills are important.
	- All engineers and researchers are expected to have strong communication skills. They should be able to concisely and accurately share knowledge with their teammates.
	- xAI does not have recruiters. Every application is reviewed directly by a technical member of the team.
	- Tech Stack
		* Kubernetes
		* Pulumi
		* Rust and Go
		* Flux / ArgoCD
	- The role is based in the Bay Area [San Francisco and Palo Alto]. Candidates are expected to be located near the Bay Area or open to relocation.
	- Operating some of the world’s largest GPU supercomputing clusters for both AI training and serving production models.
	- Implement IaC best practices, enhancing deployment pipelines, and ensuring robust, secure service delivery across our production environments.
	- Working with both on-premise clusters and cloud providers.
	- Help with security best practices for internal researchers and live external traffic.
	- Writing scalable and highly available containerized applications in Rust.
	- Managing compute fleets with Pulumi, Terraform, Ansible, or other stateful automation libraries.
	- Interview Process
		* After submitting your application, the team reviews your CV and statement of exceptional work. If your application passes this stage, you will be invited to an initial interview (45 minutes - 1 hour) during which a member of our team will ask some basic questions. If you clear the initial phone interview, you will enter the main process, which consists of four technical interviews:
			+ Coding assessment in a language of your choice.
			+ Systems design: Translate high-level requirements into a scalable, fault-tolerant service.
			+ Systems hands-on: Demonstrate practical skills in a live problem-solving session.
			+ Project deep-dive: Present your past exceptional work to a small audience.
			+ Meet and greet with the wider team.
		* Our goal is to finish the main process within one week. We don’t rely on recruiters for assessments. Every application is reviewed by a member of our technical team. All interviews will be conducted via Google Meet.
		* Annual Salary Range: $180,000 - $370,000 USD






















##	 IT operations analytics, ITOA


IT operations analytics (ITOA) is an approach or method to retrieve, analyze, and report data for IT operations.
+ Or, known as:
	- advanced operational analytics
	- IT data analytics
+ ITOA may apply big data analytics to large datasets to produce business insights.
+ ***ITOA is different than AIOps, which focuses on applying artificial intelligence and machine learning to the applications of ITOA.***
+ The use of mathematical algorithms and other innovations to extract meaningful information from the sea of raw data collected by management and monitoring technologies.



Context:
+ (IT) systems management
	- Fault, Configuration, Accounting, Performance, Security (FCAPS)
	- Distributed Management Task Force (DMTF)





applications of ITOA systems:
+ Root cause analysis:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored can help users pinpoint fine-grained and previously unknown root causes of overall system behavior pathologies.
+ Proactive control of service performance and availability:
	- Predicts future system states and the impact of those states on performance.
+ Problem assignment:
	- Determines how problems may be resolved or, at least, direct the results of inferences to the most appropriate individuals or communities in the enterprise for problem resolution.
+ Service impact analysis:
	- When multiple root causes are known, the analytics system's output is used to determine and rank the relative impact, so that resources can be devoted to correcting the fault in the most timely and cost-effective way possible.
+ Complement best-of-breed technology:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored are used to correct or extend the outputs of other discovery-oriented tools to improve the fidelity of information used in operational tasks (e.g., service dependency maps, application runtime architecture topologies, network topologies).
+ Real time application behavior learning:
	- Learns & correlates the behavior of Application based on user pattern and underlying Infrastructure on various application patterns, create metrics of such correlated patterns and store it for further analysis.
+ Dynamically baselines threshold:
	- Learns behavior of Infrastructure on various application user patterns and determines the Optimal behavior of the Infra and technological components, bench marks and baselines the low and high water mark for the specific environments and dynamically changes the bench mark baselines with the changing infra and user patterns without any manual intervention




Types of ITOA:
+ Log analysis
+ Unstructured text indexing, search and inference (UTISI)
+ Topological analysis (TA)
+ Multidimensional database search and analysis (MDSA)
+ Complex operations event processing (COEP)
+ Statistical pattern discovery and recognition (SPDR)

















+ skill set:
	- experience with large-scale distributed storage and database systems
		* SQL
		* NoSQL
		* MySQL
		* Cassandra
	- data processing experience with building and maintaining large-scale and/or real-time complex data processing pipelines using:
		* Kafka
		* Hadoop
		* Hive
		* Storm
		* Zookeeper
	- experience with developing complex software systems scaling to substantial data volumes or millions of users with production quality deployment, monitoring, and reliability
	- experience running scalable (thousands of RPS) and reliable (three 9's) systems
+ Experience with monitoring and tracking tools such as Splunk, NewRelic, Adobe/Google Analytics
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




##	 Artificial Intelligence for IT Operations, AIOps



###	 Notes about Artificial Intelligence for IT Operations, AIOps


Artificial Intelligence for IT Operations (AIOps) is an industry category for machine learning analytics technology that enhances IT operations analytics.


Goals of AIOps:
+ enable IT transformation
+ receive continuous insights that provide continuous fixes and improvements via automation



AIOps can be viewed as CI/CD for core IT functions.
+ Given the inherent nature of IT operations, which is closely tied to cloud deployment and the management of distributed applications, AIOps has increasingly led to the coalescence of machine learning and cloud research.



main aspects of an AIOps platform:
+ machine learning
+ big data

In order to collect observational data and engagement data that can be found inside a big data platform, which requires a shift away from sectionally segregated IT data, a holistic machine learning and analytics strategy is implemented with the combined IT data.





AIOps tasks include:
+ automation
+ performance monitoring
+ event correlations



AIOps process:
+ The *normalized data* is suitable to be processed through machine learning algorithms to *automatically reduce noise* and *identify the probable root cause of incidents*.
	- The main output of such stage is the ***detection of any abnormal behavior from users, devices or applications***.
+ Noise reduction can be done by various methods, but most of the research in the field points to the following actions:
	- Analysis of all incoming alerts;
	- Remove duplicates;
	- Identify the false positives;
	- Early *anomaly, fault and failure* (AFF) detection and analysis.[13]
+ ***Anomaly detection*** - another step in any AIOps process is based on the analysis of past behavior of users, equipment and applications. Anything that strays from that behavior baseline is considered unusual and flagged as abnormal.
+ ***Root cause determination*** is usually done by passing incoming alerts through algorithms that take into consideration correlated events as well as topology dependencies. The algorithms on which AI are basing their functioning can be influenced directly, essentially by "training" them.





AIOps platforms enabling IT operations management (ITOM)
+ inputs:
	- historic data
	- real-time streaming data
	- vendor-agnostic data ingestion
+ input types:
	- logs
	- metrics
	- wire data
	- document text
+ observe (monitoring)
+ act (IT Service Management process, ITSM)
+ engage (monitoring)
+ machine learning with *Big Data*
+ outputs:
	- historical analysis
	- anomaly detection
	- performance analysis
	- correlation and contextualization


Factors of IT maturity:
+ organizational structures
+ processes and practices
+ skills and knowledge,
+ tools and policies
+ systems and data
+ documents and agreements



cause-and-effect sequence
+ outcomes
+ capabilities
+ IT maturity
+ IT excellence
+ business maturity, innovation, and productivity




Role of IT suppert services:
+ aware
	- The organization is aware of its chaotic stage and needs.
	- IT capabilities are unstable and success depends on the individuals' effort and technical knowledge.
+ committed
	- IT operations are more process-oriented, thus more reputable.
	- Success depends on process adherence and point collaboration.
	- This level is a stable plateau where IT can "keep the lights on."
+ proactive
	- IT organization is recognized as "mature."
	- At this level, IT has reached a tipping point from which a path to high IT maturity is accessible.
+ aligned
	- IT is a highly efficient internal service provider, offering a stable portfolio of optimized services.
+ business partner
	- IT is a trusted partner and innovator for the business












 










Applications of AIOps:
+ analysis of large and unconnected datasets, or large number of un-normalized databases
	- including aggregated data
+ Automation of tasks (DevOps)
+ Machine learning platforms
+ Augmented reality
+ Agent-based simulations
+ Internet of things (IoT)
+ AI Optimized Hardware
+ Natural language generation
+ Streaming data platforms
+ Conversational BI and analytics









###	 Skill Sets for using Artificial Intelligence for IT Operations, AIOps



+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










##	CloudOps



CloudOps include:
+ cloud-specific flexible provisioning and scalability of environments,
+ built-in task automation.













##	NoOps



NoOps (or) No Operation is the new idea that completely automates a software environment from the underlying system infrastructure through technologies including machine learning (ML), and Artificial Intelligence (AI). As a result, there is no need for any operation team to manage software in-house.

With NoOps, developers can concentrate solely on writing and improving the software product’s code that improves the resources like management, security, infrastructure, product, and operations part of the lifecycle. Additionally, the service providers offer developers to develop software like resources, backups, patches, and the right cloud infrastructure to work independently without any interference.

***Serverless architecture*** is the best example for NoOps software. The developer’s team aims to create their application and deploy them in ***serverless computing*** without interfering any operational or infrastructure considerations.

Operating the right tools in NoOps can achieve a faster deployment process than DevOps by running Platform as a Service (PaaS) or Function as a Service (FaaS) in the cloud. Moreover, NoOps can easily be adaptable for Product as a Service companies, small-scale applications, and start-ups.





##	Software Intelligence


Software intelligence:
+ [get] ["insight into the structural condition of software assets produced by software designed to analyze database structure, software framework and source code to better understand and control complex software systems in Information Technology environments"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["set of software tools and techniques for the mining of data and software inner-structure"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["End results are information used by business and software stakeholders to make informed decisions,[citation needed] measure the efficiency of software development organizations, communicate about software health, prevent software catastrophes."](https://en.wikipedia.org/wiki/Software_intelligence)



["Software intelligence is derived from"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Software composition is the construction of software application components. Components result from software coding, as well as the integration of the source code from external components: Open source, 3rd party components, or frameworks. Other components can be integrated using application programming interface call to libraries or services."
+ "Software architecture refers to the structure and organization of elements of a system, relations, and properties among them."
+ "Software flaws designate problems that can cause security, stability, resiliency, and unexpected results. There is no standard definition of software flaws but the most accepted is from The MITRE Corporation where common flaws are cataloged as Common Weakness Enumeration."
+ "Software grades assess attributes of the software. Historically, the classification and terminology of attributes have been derived from the ISO 9126-3 and the subsequent ISO 25000:2005 quality model."
+ "Software economics refers to the resource evaluation of software in past, present, or future to make decisions and to govern."





["The capabilities of Software intelligence platforms include an increasing number of components"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Code analyzer to serve as an information basis for other Software Intelligence components identifying objects created by the programming language, external objects from Open source, third parties objects, frameworks, API, or services"
+ "Graphical visualization and blueprinting of the inner structure of the software product or application considered including dependencies, from data acquisition (automated and real-time data capture, end-user entries) up to data storage, the different layers within the software, and the coupling between all elements."
+ "Navigation capabilities within components and impact analysis features"
+ "List of flaws, architectural and coding violations, against standardized best practices, cloud blocker preventing migration to a Cloud environment, and rogue data-call entailing the security and integrity of software"
+ "Grades or scores of the structural and software quality aligned with industry-standard like OMG, CISQ or SEI assessing the reliability, security, efficiency, maintainability, and scalability to cloud or other systems."
+ "Metrics quantifying and estimating software economics including work effort, sizing, and technical debt"
+ "Industry references and benchmarking allowing comparisons between outputs of analysis and industry standards"



It is used as part of ["Application Portfolio Analysis (APA)"](https://en.wikipedia.org/wiki/Software_intelligence).

















#	References




	@misc{WikipediaContributors2022b,
		Address = {San Francisco, {CA}},
		Author = {{Wikipedia contributors}},
		Howpublished = {Available online from {\it Wikipedia, The Free Encyclopedia: Web frameworks} at: \url{https://en.wikipedia.org/wiki/Solution_stack}; July 8, 2022 was the last accessed date},
		Month = {June 21},
		Publisher = {Wikimedia Foundation},
		Title = {Solution stack},
		Url = {https://en.wikipedia.org/wiki/Solution_stack},
		Year = {2022}}
