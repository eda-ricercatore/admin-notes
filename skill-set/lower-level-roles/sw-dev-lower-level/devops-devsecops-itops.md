#	DevOps & Site Reliability Engineering (SRE) & DevSecOps & ITOps & IT operations analytics & AIOps & CloudOps & NoOps


##	DevOps



###	Notes About DevOps


A DevOps toolchain is a set or combination of tools that aid in the delivery, development, and management of software applications throughout the systems development life cycle, as coordinated by an organisation that uses DevOps practices.

Generally, DevOps tools fit into one or more activities, which supports specific DevOps initiatives: Plan, Create, Verify, Package, Release, Configure, Monitor, and Version Control.


DevOps include:
+ Microservices
+ DevOps automation
+ Automation with version control
	- Many organizations use version control to power DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD. 




DevOps life cycle:
+ continuous development
+ continuous integration
+ continuous testing
+ continuous monitoring
+ continuous feedback
+ continuous deployment
+ continuous operations



Alternate DevOps life cycle, exploiting real-time communication:
+ continuous development/build
+ continuous integration
+ continuous deployment
+ continuous operations
+ continuous feedback
+ continuous planning







DevOps is at the intersection of:
+ software development
+ software quality assurance
+ ITOps



***BizDevOps is DevOps that account for business needs.***
+ Or, ***BizDevOps*** is ***DevOps 2.0***.
+ Collaborate with business units using agile methods
	- Business unites have to contribute to reduce the product/service backlog
	- DevOps teams have to be more responsible for the business aspects of what they do.



###	Skill Set for DevOps


Skill set for DevOps:
+ skill set:
	- YAML, or JSON, for customizing configuration files.
		* JSON files are valid YAML files.
			+ JSON is a subset of YAML.
		* YAML = Yet Another Markup Language, YAML Ain't Markup Language
		* YAML CRD
			+ For Kubernetes, CRD = custom resource definition.
+ skill set:
	- Experience with Kubernetes and Docker.
	- Experience with Elasticsearch, Redis and/or Memcached.
+ skill set:
	- An understanding of several of these methodologies and tools:
		* Software development methods such as Agile, Scrum, Lean, Waterfall
		* Software project tools like JIRA, Pivotal Tracker, Trello, Asana, and MS Project
		* Continuous integration and build automation with Jenkins, TeamCity, TFS, TravisCI, CircleCI
	- Experience configuring the following technologies:
		* LDAP, ActiveDirectory, and other SAML/Single-Sign-on services
		* VMware vSphere, ESXi, AWS, Azure, GCP, and other virtual infrastructure tools
+ Familiarity with configuration/orchestration management software such as Puppet, Chef, Ansible, or Salt.
+ developer experience (DX) lead
	- current tech stack:
		* Golang
		* TypeScript
		* GCP
		* Docker
		* Terraform
		* Serverless
		* K8s
		* VueJS
		* gRPC, gRPC Remote Procedure Calls, general-purpose RPC infrastructure, Stubby
	- Gopher communication protocol
+ skill set:
	- maintain and improve Torq's automation infrastructure and CI/CD pipelines
	- improve test's infrastructure to support scale, reduce delivery times and improve the overall quality of all product aspects
	- develop E2E (UI and API) tests for Torq's management app and microservices (Python)
	- contribute to integration test framework (TypeScript) - cross platform and cross browsers tests
	- develop performance tests over k6
	- experience with UI automation frameworks
		* Playwright or Puppeteer (Node.js libraries, headless browser and usage), CDP based
		* CDP, content delivery platform - SaaS content service
		* CMS, content management system
		* CDP, continuous data protection, continuous backup, or real-time backup
		* CDP, customer data protection
	- experience writing Python-based frameworks for test automation, Python, unittest
	- experience with writing JavaScript/TypeScript -based automation framework, Jest, Mocha
	- hands-on experience working with Linux and modern apps using Docker containers and k8s
	- working knowledge of CI/CD, and cloud deployment and testing
	- experience with testing gRPC microservices
+ skill set:
	- scalable ***configs as code***
	- developer-facing tooling
	- drive, implement, support, and maintain infrastructure services with:
		* Kubernetes
		* Envoy
		* Kafka
		* Cassandra
	- collaborate and evangelize the right cloud solutions throughout the business by creating a visionary direction and road map for infrastructure-as-a-service, IaaS
	- implement automation to perform the day to day operations/functions of the cloud platform, working across all teams
	- develop, monitor, and build alerts around error conditions and performance
	- work in a fast-paced environment while participating in conceptualizing and building CI/CD pipelines
	- be on-call as needed to support the infratructure and our systems, and drive philosophies around site reliability
	- experience in infrastructure engineering
	- experience with public cloud computing services, such as AWS and GCP
	- experience with container technologies
	- experience working with high availability and scalable SaaS (or consumer technologies)
	- experience deploying highly available and scalable, secure and reliable services with automatic failover using containers and container orchestration tools like K8s
	- use of service meshes
	- cloud formation via Terraform and Ansible
	- experience deploying container applications with helm charts
	- experience with:
		* monitoring tools:
			+ Datalog
		* on-call tooling
			+ PagerDuty
	- experience architecting, implementing, and managing environments in AWS
	- experience implementing AWS services, such as:
		* EC2 Load Balancing
		* VPC
		* Route 53
		* Direct Connect
		* NAT Gateway
		* VPN
		* EC2 Networking
		* Transit Gateway
		* Global Accelerator
+ skill set:
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
+ Specific technologies, like Spring, docker, Kubernetes, etc. are, of course, also a great help
+ skill set:
	- Excellent solid understanding of Apache Pulsar, RabbitMQ, or Apache Kafka.
	- experience in stream processing platform, such as Flink, Storm, Spark or equivalent
+ skill set:
	- Senior DevOps Engineer, Cloud IaaS (US Remote Available)
	- Splunk's IT Operations team's exciting and meaningful mission: Build, scale and maintain Splunk’s Infrastructure for all Splunkers. While various Engineering groups focus on building our products, IT Ops serves as the backbone operational support for Splunkers across the globe.
	- We are actively seeking DevOps Engineers with a real passion for automation to help build scalable tools to run our distributed systems. You will be responsible for expanding and supporting the infrastructure platform services we provide to Splunk, as well as engaging with other teams to help improve efficiency and optimize our infrastructure. You're also an individual who’s motivated by technology and enjoys automation and problem-solving. We work hard, we like to challenge the status quo, and we enjoy having fun!
	- Support and maintain IT Public/Private Cloud Infrastructure, including our virtualization and container platforms
	- Be a part of the On-Call for production issues during shift or as required.
	- Take on performance and stability issues using a wide variety of tools, including Splunk
	- Ensure that day-to-day operational requirements and SLAs are met
	- Work with key business partners to understand their requirements and recommend potential solutions, and secure resources to deliver
	- Maintain critical services and provide visibility to internal teams
	- Seek opportunities to improve or optimize processes through automation
	- 5 + years of experience as a DevOps Engineer administering/managing an AWS Public Cloud platform
	- 3+ years of experience providing automation with a major scripting language such as shell, python or go.
	- 5 + years of experience providing Linux systems administration/engineering
	- 1+ years of experience with Configuration Management tools like Ansible, Puppet or Chef
	- CI/CD pipeline tool experience (e.g. Jenkins, GitLab, etc)
	- Experience with Hashicorp toolsets Terraform, Vault, and Packer.
	- A deep understanding of networking concepts and internet protocols
	- Familiarity with Observability concepts and tools
	- Experience with Kubernetes and containers
	- Good understanding of cloud infrastructure security concepts
	- Ability to provide reliable technical support and mentorship on complex issues in a high velocity, dynamic environment
	- Ability to communicate complex technical concepts clearly to customers and upper management
+ skill set:
	- Senior Site Reliability Engineer (remote Spain)
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via Terraform as well as Kubernetes, Helm and Istio
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of Terraform, Ansible, Puppet or Salt.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio
	- Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Software Engineer - Developer Platform Infrastructure
	- We are looking for a Senior Software Engineer to help lead, design and build the next generation of our CI/CD and tools offerings. You will be working on the core infrastructure platform enabling the next generation of Splunk’s offerings.
	- CI/CD and tooling expertise. Cloud, container and virtualization experience. Innovating and scaling secure services on-prem and different cloud providers is a plus. You will use Kubernetes, Docker, UCP, AWS/GCP, Jenkins, Gitlab, Ansible.
	- Data structures and algorithms. A solid grasp of data structures, algorithms, and RESTful APIs.
	- Observability infrastructure expertise to ensure 24/7 operational excellence and data driven decision making.
	- Ability to work with multiple programming languages. We have code in several languages, ranging from Go to Python.
	- BS/MS degree in EE or CS or a related technical field or 3+ years of progressive experience.
	- Desire to learn and adapt. You will constantly be learning new areas and new technologies.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. We want you to be excited and have utmost ownership of your projects.
	- Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and observability.
	- Opportunities to develop and grow as an engineer. We are always expanding into new areas, working with open-source projects and contributing back, and exploring new technologies.
	- A team of incredibly capable and dedicated peers, all the way from engineering to product management and customer support.
	- Breadth and depth. You are interested to work on an area that dynamically scales to meet the needs of Splunk’s offerings.
	- You want to go deep into optimizing how we automate every manual process and tedious task we encounter.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe that mentors help both sides of the equation.
	- Fun. We have frequent group outings and team building events. We are committed to having every employee want to give it their all, be respectful and a part of the family, and have a smile on their face while doing it.
+ skill set:
	- Software Engineer- Tooling and Infrastructure
	- Splunk is looking for a seasoned professional engineer to join the effort to define and build the future of Splunk. Splunk is rapidly expanding their presence in the cloud, and we are looking for engineers who are interested in being founding members of the Deployment Tooling team (D4S)  that defines and builds tools to optimize how Splunk services are deployed to the public cloud.  This is a great opportunity to both lead and to learn.
	- In this role you will help Splunk to orchestrate deployments of its multi-tenant cloud platform across multiple regions, and to manage continuous deployment to these regions via Argo CD to provide for automated rolling deployments.  We are looking for candidates who have experience transitioning from operationalized deploys to automated deploys.  This is a position with broad impact–what you build will be used across all of Splunk cloud.  You need to be able to build robust solutions that are easy to use and provide exponential impact to an organization.
	- Opportunities to develop and grow as an engineer. We are at the forefront of our industry, always expanding into new areas, and working with open source & new technologies.
	- A set of talented and dedicated peers, all the way from engineering and QA to product management and customer support.
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both mentor and mentee alike.
	- An open, collaborative and supportive work environment. We embody the scrum values.  We also have a number of Employee Resource Groups for employees of all backgrounds.
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Experience in systems-level programming and distributed systems. You have knowledge of operating systems, networking and network protocols, messaging, consensus, failure modes, and parallel programming.
	- Demonstrated ability to advocate for simple and clear APIs for complex functionality. You have an API-first mentality, with the ability to build straightforward APIs to help services configure the service mesh routing, maintaining API contracts, etc.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. You actively own your work and be excited about your projects.
	- Ability to work with Golang and Python. Most of our services are written in Golang and tools are written in Python; if you are an expert at another language we can consider you, but you will be expected to program in Python and Golang.
	- Requires 3-5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience
	- 3-5 years experience in programming languages: Python, GoLang, Java, C, 
	- 2 years building and deploying cloud-native applications/services on AWS or other cloud services like GCP, Azure, etc.
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
+ skill set:
	- Principal Software Engineer - Analytics Platform (Remote)
	- We are seeking a Principal Software Engineer to help lead, design, develop and deliver Splunk's User Behavior Analytics (UBA) security analytics solution that detects known and unknown security threats at scale using big data and machine learning techniques. UBA helps security analysts quickly identify and resolve threats; delivered on customer managed resources using Kubernetes and Spark to run innovative stream processing and machine learning algorithms in near real-time.
	- We are a passionate team who care deeply about our customers and our teammates. In this role, you will work directly with Product Management, our Design Team, our Customers and other engineering teams to help derive the best experience for the customer. We have a lean process that focuses on empowering and serving our engineers as opposed to just directing them.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real world use-cases in order to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality.
	- Champion, coach and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 12+ years of Software Development experience.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience using languages such as Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, Hadoop, Cassandra.
	- Exposure to working with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Knowledge of distributed computing architectures and principles that solve for scalability, performance, redundancy and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
	- Background in developing machine learning products for the Security market a plus.
+ skill set:
	- Comfortable with Linux, Docker, AWS, GIT, Artifactory in terms of both tools and systems administration
	- Previous experience in design and implementation of solutions to evaluate and improve performance: availability, reliability, interoperability, scalability of SaaS / Cloud Native / Bigdata Platform and application with microservice architecture
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Design, implement, and maintain platform metadata features
	- Designing APIs and Platform Information Architecture
	- Serve as primary point of contact in one or more platform metadata areas
	- Collaborate with various Confluent Engineering groups to provide strong technical guidance and leadership related to managing metadata
	- In depth experience with concepts of distributed systems and big data such as - Kafka, Hadoop, Spark, Big Table, HBase etc
	- Full stack experience
	- Experience with Lineage, Governance and Auditing
	- Experience in ML/Data Engineering
	- Experience working with Docker or Kubernetes is a plus
	- Experience working with AWS, Azure, and/or GCP
+ skill set (Platform DevOps Engineer):
	- As a Platform DevOps, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience with Go, Java, C++ or Python required
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- As a Platform DevOps Engineer, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- Solid understanding of container orchestration systems such as Kubernetes, Mesos, etc.
	- Experience with C, C++, Java or Python required
	- Experience with container orchestration tools such as Docker, CoreOS, etc.
	- Experience with configuration management or provisioning tools such as chef, puppet, Ansible, etc
	- Experience building and operating large-scale systems
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Hands-on experience with Kubernetes operator, Helm, or StatefulSets is a plus
	- Proficiency in Go is a plus
	- Knowledge of Apache Kafka is a plus
+ ***Experience building and scaling automation frameworks***
+ skill set:
	- Build and maintain data foundations, metrics and dashboards to monitor the business performance and extract actionable insights
	- Apply quantitative analysis, data mining, and presentation of data to fuel business growth and drive customer success
	- Design and analyze experiments to test new product ideas, go to market strategies and/or funnel optimization; Convert the results into actionable recommendations
	- Build data products to improve operational efficiencies organizationally to scale with a hyper growth start-up
	- Inform, influence, support, and execute business decisions with senior leadership and business partners
	- Build robust, automated data pipelines to enable team effectiveness
	- 2+ years industry experience working with SQL (Teradata, Oracle, MySQL, BigQuery, etc.) and R (or Python)
	- Proficiency in applying statistical modeling and/or machine learning
	- Proficiency in data visualization (eg. Tableau, Looker, Matlab, etc.)
	- Bachelor or advanced degrees in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc
	- The ability to communicate cross-functionally, derive requirements and deliver insightful analysis and/or models; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience building data warehousing and ETL pipelines
	- Experience with Unix/Linux environment
	- Experience in developing data apps with Python/Java, high charts etc
	- Excellent communications skills, with the ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience working in the B2B growth/marketing or sales domains: CRM, sales effectiveness, branding, segmentation, web analytics, attribution, funnel optimization, etc.
	- Experience working with Marketo, Google Analytics and SFDC
+ skill set:
	- The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.
	- We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
	- Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
	- Implementing a solid, robust, extensible data warehousing design that supports key business flows
	- Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
	- Developing strong subject matter expertise and manage the SLAs for those data pipelines
	- Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
	- Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
	- Building and growing  partnership with cross functional teams, and evangelize data-driven culture
	- Contributing to innovations that fuel Confluent's vision and mission
	- 4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
	- Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
	- Highly proficient in Python and SQL coding
	- Highly proficient with tuning and optimizing data models and pipelines
	- Experience in developing data apps with Python, Javascript, high charts etc
	- The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience with Apache Kafka
	- Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
	- Experience in developing data apps with Python, Javascript, high charts, etc
+ skill set:
	- Deep knowledge of a configuration management tool (i.e. Puppet, Chef, Ansible, Salt, CFEngine). Experience with containers is a plus
	- Familiarity with distributed systems including service discovery, pub/sub, search indexing, storage, and caching. We use Zookeeper, Kafka, Elasticsearch, MySQL, Hbase, and Memcache respectively.
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Improve observability with distributed tracing for all requests from client to CDN to load balancer to cluster and back again
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Build observability into every aspect of our production infrastructure
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- Help developers smoke-test better by bringing canary analysis and automated scale testing into their world
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Ansible
	- Kafka/Cassandra
	- Linux
	- Git (github)
	- Vi / Vim
	- Elastic Search Stack
	- Graphite/Grafana
	- Data visualization
	- Python, Bash, Golang
	- Familiarity with JSON
+ skill set:
	- Expertise with 12 Factor application principles
	- Containers (Docker, Kubernetes...)
	- Streaming/logging technologies (ElasticSearch, fluentd, LogStash, Kafka)
	- Message Queueing (Kafka, SQS...)
	- Coding and scripting languages (Perl, Bash, Python, Go...)
	- AWS Ecosystem (EC2, VPC, S3, DynamoDB, RDS...)
	- You have deployed and configured a wide range of AWS services including databases, networking, and security. In this role you will work with such paradigms and technologies as: ***12 factor app design principles***, Docker, Kubernetes, and ElasticSearch ecosystem
		* [Twelve-Factor App methodology](https://en.wikipedia.org/wiki/Twelve-Factor_App_methodology)
			+ 1. Codebase: There should be exactly one codebase for a deployed service with the codebase being used for many deployments.
			+ 2. Dependencies: All dependencies should be declared, with no implicit reliance on system tools or libraries.
			+ 3. Config: Configuration that varies between deployments should be stored in the environment.
			+ 4. Backing services: All backing services are treated as attached resources and attached and detached by the execution environment.
			+ 5. Build, release, run: The delivery pipeline should strictly consist of build, release, run.
			+ 6. Processes: Applications should be deployed as one or more stateless processes with persisted data stored on a backing service.
			+ 7. Port binding: Self-contained services should make themselves available to other services by specified ports.
			+ 8. Concurrency: Concurrency is advocated by scaling individual processes.
			+ 9. Disposability: Fast startup and shutdown are advocated for a more robust and resilient system.
			+ 10. Dev/Prod parity: All environments should be as similar as possible.
			+ 11. Logs: Applications should produce logs as event streams and leave the execution environment to aggregate.
			+ 12. Admin Processes: Any needed admin tasks should be kept in source control and packaged with the application.
	- Support build/deployment processes with eye towards improving our CI/CD pipeline
	- Help troubleshoot production issues and perform root cause analyses that create effective mitigation strategies
	- Design, implement, monitor, and scale self-service oriented infrastructure
+ skill set:
	- Confluence
	- Jira
+ skill set:
	- Infrastructure Engineer.
	- In this role you will be responsible of monitoring the constant quality of the infrastructure and corporate networks, by performing diagnostic tests and debugging procedures to optimize computing systems.
	- Analyze and suggest changes to test, dev and prod infrastructure
	- Ensure utmost uptime for entire network services and servers
	- Implement infrastructure security policies
	- Provide consultancy and support for teams with various/different levels of IT knowledge
	- Make policy recommendations for further implementations and development
	- Develop and maintain an IT sourcing strategy and ensure the correct provisioning of IT equipment. Solve problems using in-depth understanding of information systems and computing solutions.
	- Monitor constantly the quality of the infrastructure and support the management of network infrastructures.
	- Ensure data is stored securely and backed up regularly
	- Work on process documentation as well as backup and recovery procedures, data retrieval, network and infrastructure diagrams.
	- Research, design and recommend new approaches to improve the networked computer system
	- Expert about networks and their protocols
	- Proficient with both network hardware and technologies and also with shared storage technologies
	- Skilled in backup procedures for data storage and disaster recovery
	- Expert about cloud environments and infrastructures, in particular Microsoft Azure and Amazon AWS
	- You have a good knowledge about firewalls and virtualization systems like VMware, Hyper-V
	- Fluent in English written and spoken
	- You have a good attitude towards problem solving and team working
	- You have at least 5 years of experience in this role and a degree in Computer Science or Engineering
	- It would be nice if you would have knowledge of container systems like Docker and Kubernetes, and CloudFlare.
+ skill set:
	- Cloud & DevOps Engineer – Italy.
	- Define and monitor the deployment process
	- Deploy both on-premises by the customer and on our infrastructure
	- Offer first-level support and triage of issues reported by customers
	- Directly manage deployment and infrastructure issues
	- Coordinate second-level support, engaging with developers and other teams
	- A graduate in Computer Science, Computer Engineering or an equivalent title
	- Expert of Windows/Linux operating systems with basic systemic knowledge
	- Good in speaking and writing in English
	- DevOps methodology
	- Cloud Native Architectures (Docker, Kubernetes)
	- Monitoring/logging systems (ELK Stack, CloudWatch)
	- CI/CD tools (Gitlab, Jenkins)
	- IaC tools (Terraform, Ansible)
	- AWS ecosystem
	- Precision, dedication and diligence
	- Priority management while ensuring the output quality
	- Ability to organize your tasks according to the company’s processes and procedures
	- Curiosity towards new technology and willingness to learn
	- Team working will be essential since you will also support other colleagues
+ skill set:
	- Monitoring/logging systems, such as the ELK Stack (or Elastic stack) [WikipediaContributors2022b].
		* visualize application monitoring
		* visualize infrastructure monitoring
		* faiciliate troubleshooting
		* for security analytics
		* ***ELK Stack*** (or ***Elastic stack***):
			+ ***Elasticsearch***
				- distributed search and analytics engine
				- use schema-free JSON documents
			+ ***Logstash***
				- ETL (extract, transform, load)
					* ***data ingestion tool***
					* collect data from different sources
					* transform data
					* send data to desired destination
			+ ***Kibana***
				- data visualization of logs and events, exploration of new maintenance solutions
				- tool for creating interactive charts
+ skill set:
	- Experience with CI systems (Jenkins, TeamCity);
	- AWS - EC2, RDS, ECS etc;
	- Docker and orchestration: Swarm, Kubernetes;
	- Elastic Search, RabbitMQ;
	- Bash / Powershell scripting experience;
	- Windows / Linux - admin level;
	- Experience with SVN / GIT - as a user and as an infrastructure owner;
	- Experience with high loaded distributed multi-tenanted cloud systems. Including: Disaster recovering mechanisms; Monitoring and logging; Redundancy (data, network, apps);
	- Comfortable working with distributed teams
+ skill set:
	- As a DevOps Engineer at Simbe Robotics you will be part of a talented team ensuring quality in our software as well deploying & managing our cloud services and world-wide fleet of autonomous robots.
	- Has experience with automated build and continuous integration systems (e.g. Jenkins, TravisCI)
	- Has knowledge of application/system level monitoring (Nagios, CloudWatch, Munin, Splunk)
	- Experience with configuration management (Chef, Puppet, Ansible) tools
	- Has experience with various application packaging and deployment technologies (Debian packages, Docker/Linux containers)
	- Experience configuring web servers (e.g. Apache/Tomcat, nginix)
+ ***Infrastructure as code*** experience (we use terraform)
+ Familiar with orchestration components (Chef-Puppet-Ansible-Kubernetes-VSTS)
+ view DevOps as ***configuration as code***:
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef.
+ skill set:
	- Exposure to containers or orchestration services:  Kubernetes, Mesos, or Docker Swarm
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef
+ skill set:
	- Talend ETL, SQL, Postgres, AWS
		* Talend ETL: ETL tool for data integration.
	- Design, develop, and implement advanced ETL pipelines that bring together data from disparate sources, making it available to users using a variety of ETL tools.
	- Facilitate cross-functional data-integration efforts upstream and downstream
	- Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues
	- Extract data from multiple sources, and integrate them into a target database, application, or file using efficient programming processes.
	- Implement and deploy solutions in a CI/CD pipeline
	- Write and refine code to ensure performance and reliability of data extraction and processing.
	- Communicate with all levels of stakeholders as appropriate, including product managers, application developers, business users.
	- Participate in requirements gathering sessions with product managers and technical staff to distill technical requirements from business requests.
	- Recommend process improvements to increase efficiency and reliability in ETL development.
	- Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
	- Some of our technologies might include: Talend as well as various data stores such as Postgres SQL, S3, Aurora and AWS services.
		* Amazon S3, Amazon Simple Storage Service
			+ Minio
			+ Azure Blob Storage
			+ OLTP/OLAP database management
				- OLTP, online transactional processing
				- OLAP, online analytical processing
		* ability to configure and use database instances, their availability, and access roles.
		* Amazon Aurora, high-performance database
			+ relational database service
			+ offered by Amazon Web Service, AWS
			+ part of Amazon Relational Database Service, RDS
				- Postgres SQL
			+ Amazon DynamoDB, proprietary NoSQL database service.
			+ Amazon DocumentDB, managed proprietary NoSQL database service.
				- MongoDB
			+ Amazon SimpleDB, distributed database
		* data warehouse, DWH
			+ data warehouse architecture
				- enterprise data warehouse, EDW
				- operational data store, ODS
				- data mart
	- 2+ years of experience on Data Warehousing and building data pipelines.
+ Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
	- Hadoop: for distributed computing
	- K8s, Kubernetes: container orchestration system for automating software deployment, scaling, and management 
	- Terraform: software tool for ***infrastructure as code***
+ skill set:
	- Knowledge of Big Data, SAP ERP, Docker, Kubernetes, CXF or another ETL product is a plus;
		* Apache CXF: open-source Web services framework, for developing Web services using front-end Web development APIs, such as JAX-WS and JAX-RS, in the context of service-oriented architecture (SOA).
	- AWS, Azure, Google cloud, Apache Beam, NoSQL
		* ***Apache Beam***: open-source unified programming model, and set of software development kits (SDKs), to define and execute data processing pipelines, including ETL, batch and stream (continuous) processing. The data pipelines (Beam Pipelines) are executed in one of the supported distributed processing back-ends (Beam supported runners).
	- Experience of working with JDBC, XML, Junits, Maven, Avro and JSON;
		* JDBC, Java Database Connectivity: API for how client applications can connect to a database.
		* ***Apache Maven***:
			+ build automation tools, primarily for Java, and for:
				- Ruby
				- Scala
				- C\#
		* ***Apache Avro***:
			+ data serialization system
			+ row-oriented remote procedure call (RPC), for distributed computing, and data serialization framework
	- Good understanding of Web Services (SOAP/REST), knowledge of CXF is a plus.
+ skill set:
	- Experienced in data sanitization, data import and export (ETL).
	- Familiar with SQL Server products, i.e. SQL Integration Services and Reporting Services.
	- Work with the application team to create and maintain effective database-coupled application logic stored procedures, triggers and user-defined functions (UDFs); these are programs that are under the control of the DBMS (SQL, MySQL, Postgres, MongoDB)
+ tech stack:
	- Remote hardware administration with IPMI, intelligent platform management interface
		* IPMI, intelligent platform management interface: set of computer interface specifications for autonomous computer subsystem that provide management and monitoring capabilities independently of host system's processor, firmware (e.g., BIOS or UEFI), and operating system.
	- Configuration and management of:
		* SGE/Univa: Sun Grid Engine/Univa Grid Engine, batch-queueing system for scheduling resources in a data center applying user-configurable policies to help improve resource sharing and throughput by maximizing resource utilization.
		* Slurm, workload manager
		* LSF, load sharing facility
		* other DRMS
	- Jenkins CI
		* automation server
	- Phabricator, Web-based collaboration tools for Web development.
	- FlexLM licensing, for license management
	- Puppet, Ansible, Nagios
		* Puppet, for software configuration to specify system configuration
		* Ansible, automation tool that enables infrastructure as code.
		* Nagios Core, or Nagios, for monitoring systems, networks, and infrastructure
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
		* XPath, XML Path Language, an expression language for supporting the query or transformation of XML documents
		* XSLT, extensible stylesheet language transformation, XML-based language used by processing software to transform XML documents
	- Web programming – HTML/DOM, JavaScript, SQL
		* DOM, Document Object Model, programming API for HTML and XML documents
	- A solid knowledge about how orchestration tools (Kubernetes, Swarm, OpenStack, etc) can be used to deploy, scale, and operate virtualized entities
		* Kubernetes, for container orchestration
		* Swarm, for container orchestration in Docker
	- Understand CPU virtualization and container technology from the inside out (hypervisors, Xen, LXC, Docker)
		* hypervisors, for virtual machine monitors (VMMs) or virtualizers
		* Xen, a hypervisor tool.
		* LXC, OS-level virtualization for running multiple isolated Linux systems/containers on control host using a single Linux kernel.
	- The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
		* BuildBot, for job scheduling of the software build process and software tests to facilitate continuous integration (of new additions or changes to the code base).
	- Good knowledge of management and security frameworks (SNMP/MIB agents, CLI, RESTful API, OpenBMC) is very useful
		* SNMP/MIB,
			+ SNMP, Simple Network Management Protocol, Internet Standard protocol for collecting and organizing information about managed devices on IP networks
		* OpenBMC, management controllers for servers, rack switches, and RAID appliances
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
		* ILOM, Integrated Lights Out Manager, for managing and moitoring servers
		* BMC, baseboard management controller for IPMI, intelligent platform management interface
			+ Or, products from BMC Software.
		* OCP (Open Compute)
	- Test automation experience (Some exposure to CTest is desirable)
+ Chef/Puppet/Ansible/Terraform experience is nice to have
	- [Chef](https://www.chef.io/)
		* "Automation Software for Continuous Delivery of Secure Applications and Infrastructure"
		* ["Chef Infra, a powerful automation platform that transforms infrastructure into code automating how infrastructure is configured, deployed and managed across any environment, at any scale"](https://github.com/chef/chef)
			+ "Chef Infra is a configuration management tool designed to bring automation to your entire infrastructure."
	- [Puppet](https://puppet.com/)
		* infrastructure automation
		* "Puppet is redefining what’s possible for continuous operations. Easily automate your environment to deliver at cloud speed and cloud-scale with products that are responsive and predictive by design."
		* ["Puppet is a software configuration management tool which includes its own declarative language to describe system configuration. It is a model-driven solution that requires limited programming knowledge to use."](https://en.wikipedia.org/wiki/Puppet_(software))
		* ["Puppet, an automated administrative engine for your Linux, Unix, and Windows systems, performs administrative tasks (such as adding users, installing packages, and updating server configurations) based on a centralized specification."](https://github.com/puppetlabs/puppet)
	- [Ansible](https://www.ansible.com/)
		* "Ansible Automation Platform has grown over the past years to provide powerful automation solutions that work for operators, administrators and IT decision makers across a variety of technology domains. It’s a leading enterprise automation solution from Red Hat®, a thriving open source community, and the de facto standard technology of IT automation."
		* "The open source projects that power the Ansible Automation Platform are created with contributions from an active community and built for the people who use it every day. The Ansible product and supporting open source communities were made to help more people experience the power of automation so they could work better and faster together."
		* ["Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems."](https://github.com/ansible/ansible)
			+ "Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy."
			+ https://docs.ansible.com
				- "An enterprise automation platform for the entire IT organization, no matter where you are in your automation journey"
		* ["Red Hat® Ansible® Automation Platform is a foundation for building and operating automation across an organization. The platform includes all the tools needed to implement enterprise-wide automation."](https://www.redhat.com/en/technologies/management/ansible)
			+ Ansible Automation Platform provides an enterprise framework for building and operating IT automation at scale, from hybrid cloud to the edge. Ansible Automation Platform enables users across an organization to create, share, and manage automation—from development and operations to security and network teams.
			+ IT managers can provide guidelines on how automation is applied to individual teams, and automation creators can write tasks that use existing knowledge. Ansible Automation Platform provides a more secure and stable foundation for deploying end-to-end automation.
		* ["Ansible is a suite of software tools that enables infrastructure as code. It is open-source and the suite includes software provisioning, configuration management, and application deployment functionality"](https://en.wikipedia.org/wiki/Ansible_(software))
			+ [Windows Subsystem for Linux, WSL](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)
				- "Windows Subsystem for Linux (WSL) is a compatibility layer for running Linux binary executables (in ELF format) natively on Windows 10, Windows 11,[3] and Windows Server 2019."
		* [Ansible](https://opensource.com/resources/what-ansible)
			+ Ansible is a software tool that provides simple but powerful automation for cross-platform computer support. It is primarily intended for IT professionals, who use it for application deployment, updates on workstations and servers, cloud provisioning, configuration management, intra-service orchestration, and nearly anything a systems administrator does on a weekly or daily basis. Ansible doesn't depend on agent software and has no additional security infrastructure, so it's easy to deploy.
			+ Because Ansible is all about automation, it requires instructions to accomplish each job. With everything written down in simple script form, it's easy to do version control. The practical result of this is a major contribution to the "infrastructure as code" movement in IT: the idea that the maintenance of server and client infrastructure can and should be treated the same as software development, with repositories of self-documenting, proven, and executable solutions capable of running an organization regardless of staff changes.
			+ While Ansible may be at the forefront of automation, systems administration, and DevOps, it's also useful to everyday users. Ansible allows you to configure not just one computer, but potentially a whole network of computers at once, and using it requires no programming skills. Instructions written for Ansible are human-readable. Whether you're entirely new to computers or an expert, Ansible files are easy to understand.
	- [Terraform](https://www.terraform.io/)
		* "Automate Infrastructure on Any Cloud"
		* "Deliver infrastructure as code"
		* "Terraform codifies cloud APIs into declarative configuration files."
		* [Terraform@Wikipedia](https://en.wikipedia.org/wiki/Terraform_(software))
			+ "Terraform is an open-source, infrastructure as code, software tool created by HashiCorp. Users define and provide data center infrastructure using a declarative configuration language known as HashiCorp Configuration Language (HCL), or optionally JSON."
		* [Platforms to learn Terraform for](https://learn.hashicorp.com/terraform?track=gcp)
			+ AWS
			+ Microsoft Azure
			+ Terraform Cloud
			+ Docker
			+ Google Cloud Platform
			+ Oracle Cloud Infrastructure, OCI
		* ["Terraform enables you to safely and predictably create, change, and improve infrastructure. It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned."](https://github.com/hashicorp/terraform)
			+ "Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions."
			+ Infrastructure as Code: Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.
			+ "Execution Plans: Terraform has a 'planning' step where it generates an execution plan. The execution plan shows what Terraform will do when you call apply. This lets you avoid any surprises when Terraform manipulates infrastructure."
			+ "Resource Graph: Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure."
			+ "Change Automation: Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, avoiding many possible human errors."
		* [Terraform documentation](https://terraform-docs.io/)
+ skill set:
	- improve deployment pipeline
	- implement continuous integration (CI) pipeline
	- architect public cloud enviroments for multiple engineering teams using infrastructure-as-code tools with industry best practices
	- maintain and upgrade instructure with zero downtime
	- performance tuning and optimization across the CI stack
	- knowledge of microservice architecture pattern and cloud-native application
+ understanding of:
	- containerization and container orchestration tools, such as K8s.
+ Familiarity with Docker (and Kubernetes/Mesos Marathon)
	- Mesos Marathon:
		* for operating system -level virtualization, via container orchestration
		* Apache Mesos
			+ abstracts the following from machines (physical or virtual):
				- CPU
				- memory
				- storage
				- other compute resources
			+ enabling fault-tolerant and elastic distributed systems to easily be built and run effectively
			+ distributed systems kernel
				- provides applications, such as:
					* Hadoop
					* Spark
					* Kafka
					* Elasticsearch
			+ open-source project to manage computer clusters
		* Marathon is an industry-proven Apache Mesos framework for container orchestration.
		* Marathon is a production-grade container orchestration platform for Mesosphere's Datacenter Operating System (DC/OS) Apache Mesos.
			+ runs on DC/OS
				- Distributed Cloud Operating System (DC/OS)
					* open-source, distributed operating system based on Apache Mesos distributed systems kernel
					* provides (or automates the following):
						+ scheduling
						+ resource allocation
						+ service discovery
						+ workload co-location
						+ automatic recovery from failure
						+ load balancing
						+ software defined networking
						+ unified APIs for metric and log collection
						+ facilitates inter-process communication
						+ install and manage distributed services
					* manages multiple machines in the cloud or on-premises from a single interface
						+ Web interface and CLI facilitates remote management and monitoring of the cluster and its services
					* deploy containers, distributed services, and legacy applications into those machines
					* provide networking, service discovery, and resource management to keep the services running and communicating with each other
					* service catalog included
						+ installing distributed data services, databases, CI/CD tools, and monitoring on DC/OS takes a few clicks or commands
					* Mesos's 2-level scheduling increases resource utilization, and DC/OS lowers the barrier of entry to Mesos cluster administration
					* containers and fast data
						+ deploy and run stateful or stateless distributed workloads, including:
							- Docker containers
							- Big Data pipelines tools
							- traditional apps
					* containerized workloads
						+ deploy jobs, services, and containerized apps, and let DC/OS handle the rest
					* infrastructure portability
						+ develop locally on DC/OS and deploy to production on DC/OS in the cloud or on-premises
						+ support for multiple cloud providers prevents infrastructure lock-in
					* includes 2 built-in task schedule:
						+ Marathon
						+ DC/OS Jobs (Metronome)
					* includes 2 container runtimes:
						+ Docker
						+ Mesos
					* Mesophere Distributed Cloud Operating System (DC/OS) is a platform for running distributed containerized software:
						+ apps
						+ jobs
						+ services
					* at the software layer, DC/OS provides packaged management and a package repository to install and maage several types of services, as:
						+ packaged apps and services
							- databases
							- message queues
							- stream processors
							- artifact repositories
							- monitoring solutions
							- continuous integration tools
							- source control management
							- log aggegators
						+ custom apps, services, and scheduled jobs
					* at the platform layer, the categories of components (shared among master nodes, private agent nodes, and public agent nodes) are:
						+ cluster management
						+ container orchestration
						+ container runtimes
						+ logging and metrics
						+ networking
						+ package management
						+ IAM and security
						+ storage
					* to use a service discovery mechanism (or service discovery behavior), do not put dots in your application names
			+ virtual IP routing
			+ authorization, for DC/OS enterprise edition only
				- true multitenancy with each user or group having access to their own applications and groups
			+ can be used to run other Mesos frameworks, such as:
				- Chronos
			+ run other application containers, based on Docker or Mesos:
				- JBoss servers
				- Jetty
				- Sinatra
				- Rails, or Ruby on Rails
					* full-stack framework
					* Web-application framework that spans:
						+ creating database-backed Web applications according to the Model-View-Controller (MVC) pattern
							- Model-View-Controller (MVC) pattern is a software architectural pattern, a type of software design patterns.
							- splits Web applications into 3 layers:
								* Model
								* View
								* Controller
		* Marathon provides a REST API for starting, stopping, and scaling applications.
		* speed of infrastructure provisioning can match the speed of application development, application containerization, and application release
			+ enable the benefits of Docker to be realized in production
			+ enable rapid application spin-up time of a Docker container or cluster of Docker services
			+ makes it easier to Dockerize software applications, so that their compute, memory, and CPU resources in traditional hypervisor-centric model
			+ enables management of data center as if it is a single pool of resources
		* Apache Mesos is a distributed systems kernel, is designed natively for the cloud, and has built-in support for:
			+ Docker containers
			+ HTTP API
			+ Web UI
			+ true high availability
			+ fault tolerance
		* ***Marathon is a meta framework***
			+ Consequently, this meta framework allows people to ***"start other Mesos frameworks, such as Chronos and Storm,"*** which can be run with Mesos ***"to ensure that they survive machine failures"***.
			+ **"It can launch anything that can be launched in a standard shell."**
				- E.g., "you can ... start \[other\] Marathon instances via Marathon".
		* "cluster manager that handles workloads in a distributed environment through dynamic resource sharing and isolation"
		* "can deploy and manage applications in large-scale clustered environments"
		* "Mesos brings together the existing resources of the machines/nodes in a cluster into a single pool from which a variety of workloads may utilize."
		* Mesos is a data center kernel that is between the operating system and the software application layer.
		* "Mesos isolates the processes running in" the following resources, so that they can be kept "from interfering with each other":
			* a cluster
			* memory
			* CPU
			* file system
			* rack locality
			* I/O
		* can be used the following:
			* Apache Spark
			* Chronos
			* Apache Aurora
			* Mesosphere Marathon
			* Apache Zookeeper
+ VM embeddings in other systems (e.g., DBMSs, Big Data frameworks, Microservices, etc.)
+ skill set:
	- interact with developers to understand their workflows and gather requirements
	- design, create, and support CI pipelines for varied projects in a dynamic, fast-paced, and team-oriented environment
	- move the current CI system into a containerized, flexible, and scalable system 
	- keep up to date on the newest and bets practices in DevOps, and try them with the development team
	- improve task efficiency and document CI approaches
	- continuous build and delivery systems
		* Jenkins
		* Gitlab
	- knowledge of:
		* containers
			+ Docker
		* container orchestration system
			+ ECS
			+ Kubernetes
			+ Docker Swarm/Data Center
	- experience with ***Triplestores***
+ Experience working with a CI system is preferred (ex. TeamCity, Concourse, Jenkins, etc.)
+ Prior experience with infrastructure automation frameworks (Ansible, Terraform, Chef or Puppet, etc.)
+ Virtualization and containerization (Xen, LXC, cgroups, Docker, Kubernetes)
+ Knowledge of source control tools (Git, CodeCommit, SVN, and TFS), build/release tools (Jenkins, CodeBuild, CodeDeploy, CodePipeline), and infrastructure as code tools (Terraform, CloudFormation)
+ Experience working with Atlassian products (JIRA, Confluence)
+ Docker orchestration systems and cluster managers (Kubernetes, Mesos/Marathon, ECS)
+ tech stack:
	- Ability to configure and maintain webservers (e.g. apache & nginx), DNS servers, Firewalls, LDAP servers, Tomcat servers
	- Ability to back up the Data infrastructure
	- Ability to manage/configure  Git, Maven and Jenkins
	- Managing QA/production release and deployment
	- Ability to Install/Configure/Manage VM servers using OpenStack
	- Ability to install configure or manage Monitoring servers using Opensource softwares
	- Experience with Amazon Web Services:
	- autoscaling, & use of Netflix Asgard
	- ELB management,
	- EBS storage management
	- S3
	- RDS
	- Manage configuration using Puppet
	- Familiar with Cloud Computing in genera
+ stream pipelines and all sorts of data stores (SQL, NoSQL, triplestores, wide column, graph)
+ infrastructure-as-code and automation tools (e.g. Terraform, Ansible/Chef, Cloud Formation)
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ skill set:
	- big data pipeline systems (Elasticsearch, Spark, Kafka, NiFi)
	- 2+ years of AWS experience across a range of services including EC2, Auto Scaling, Lambda, RDS
	- DevOps and automation mindset
	- Experience deploying and managing big data pipeline systems (Elasticsearch, Spark, Kafka, NiFi)
	- Familiarity working with multiple programming languages and runtimes for application containerization and deployment (Java, Scala, NodeJS, Python)
	- Experience administering CI/CD and build tools like CircleCI or AWS CodeBuild, as well as working with artifact storage platforms
	- Experience with container orchestration platforms (Elastic Container Service, Kubernetes) is highly desirable
	- Expertise in infrastructure capacity planning, AWS cost management using tools like Cloudhealth
	- Familiarity with monitoring toolsets like DataDog or Dynatrace
+ skill set:
	- Workload managers: Slurm, LSF, Altair
	- Web development: nginx, javascript, html, css
	- Database: mysql, apache derby, ingress/egress
	- Deep \*nix skills
+ skill set:
	- Senior DevOps/SysOps Engineer
	- Come and be a part of our close-knit distributed team, working remotely from different locations while enjoying a good cup of tea. We thrive on interesting technical challenges and building magical products that improve people’s lives. As our expert DevOps/Systems engineer,  you will be responsible for evolving our production and developer infrastructure, by maintaining and improving the backend infrastructure used by Sched.com.
	- You will be making an immediate impact on transitioning Sched from a monolithic legacy architecture to a scalable application! You will be ensuring site reliability, monitoring and enhancing the observability of key systems, security, reliable deployment of services, disaster recovery planning, and CI. Additionally, you will bring valuable knowledge to work closely with the development team and help improve our processes and our PHP, Go and JS applications and services to best serve our customers!
	- Your valuable contributions to uptime, stability, scalability, and reliability – are paramount to the success of Sched while opening the opportunity to learn and apply new technologies to scale and grow!
Projects you will be expected to complete in your the first 6 months
	– Automating CI and deployment of Golang daemons across multiple machines
	– Planning out and executing a rolling upgrade of all systems to latest Ubuntu release
	– Dockerization of major system components to allow easier management and updating
	– Planning out cost-effective “on-demand” peak-traffic scaling
	– Prioritize and resolve live issues 
	– Manage, plan and execute system and software updates and upgrades when needed
	– Manage application deployments in coordination with the development team
	– Maintain and improve our monitoring systems to pre-empt issues that may affect our live environments
	– Investigate and implement system improvements
	– Maintain and improve system documentation and runbooks
	– Maintain and improve disaster recovery and backup plans
	– An expert in Linux system administration – covering security, maintenance, backups, disaster recovery, storage management, monitoring (Ubuntu)
	– Comfortable with AWS infrastructure management
	– Well-versed in command line / bash scripting
	– Skillful in database administration (MySQL, Percona)
	– Knowledgeable in infrastructure configuration management tools (Ansible)
	– Capable of monitoring, logging, & observability tools (Sentry, Munin, Grafana)
	– Experience in networking: DNS, SSL, SMTP, SSH, VPN
	– Competent in HA and web tech: HAProxy, ProxySQL, Apache, PHP-fpm, Redis and Redis Sentinel, Percona Cluster, Corosync
	– Skillful with Docker, containers, CI/CD, automation
	– Proficient with PHP and Golang code
	- Our tech stack:
		* Apps: Javascript, PHP, Swift, Kotlin
		* Backend: PHP, Golang, Percona Cluster, Redis, Memcache, HAproxy, ProxySQL, Apache
		* Infrastructure: Docker, Ansible, Munin, Sentry, Corosync, DRBD, KVM
+ skill set:
	- Python Developer Tools Engineer
	- Python is a vital part of Jane Street’s research and trading work, acting as the go-to language for data analysis, visualization, and machine learning. Our Python developer tools team is responsible for the tools that support our work in Python, including:
		* CI for running tests and static analysis
		* Automation around the deployment of Python environments and applications
		* Libraries for building hybrid Python/OCaml systems
	- We’re looking for engineers with experience building Python tooling at large scale who want to leverage those skills in a new environment. 
	- The job is wide-ranging, involving both technical and product design challenges — from finding ways to speed up our static analysis and test harnesses to working with users to understand their workflows and requirements and designing solutions that fit their needs. 
	- We’re also looking for someone who can contribute to guiding our relatively young Python ecosystem, helping us form and communicate best practices, and make good choices about the tools and libraries we use.
	- Lots of our automation and Python tooling is written in OCaml, but we’re happy to teach you that, and we don’t expect any previous experience with OCaml or any other functional programming language.
+ Experience with Kubernetes, Terraform, Docker, Jenkins, Prometheus, Elasticsearch, MySQL, Postgresql, Mongodb, and Ansible.
+ skill set:
	- Leverage tools/languages such as Kubernetes, Terraform (IaC), Helm, Python, and others!
	- Have production experience with IaC (Terraform, ARM, Bicep)
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




















##	Site Reliability Engineering (SRE)





Layers of a pyramid for Site Reliability Engineering (SRE).
+ product (design and management)
+ (product/software) development
+ (infrastructure) capacity planning
+ (software) testing and release management
+ postmortem analysis, or root-cause analysis (RCA)
+ incident response
+ (cloud computing or online Web) service monitoring






+ skill set:
	- Experience monitoring cloud environments using tools like Splunk, VictorOps and Nagios
	- Knowledge of best practices related to security, performance, and disaster recovery.
	- Skilled in identifying performance bottlenecks, spotting anomalous system behavior, and determining the root cause of incidents.
+ skill set:
	- Senior Site Reliability Engineer
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via ***Terraform*** as well as ***Kubernetes, Helm and Istio***
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of ***Terraform, Ansible, Puppet or Salt***.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including ***Kubernetes, Helm, and Istio***
	- Experience operating one or more OSS technologies like ***Kafka, Cassandra, Zookeeper***; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Principal Site Reliability Engineer, Splunk Observability - remote Spain
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Principal Site Reliability Engineer, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- ***Responsible for automating & operationalizing cloud provider infrastructure via Terraform, Kubernetes, Helm and Istio***
	- ***Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.***
	- ***Own & operate critical back-end open-source services like Cassandra, Kafka, Elasticsearch, MongoDB, and Zookeeper.***
	- Build tools and design processes that help improve observability and system resiliency.
	- ***Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.***
	- Implement service level metrics & service level objectives that act as service-level health indicators.
	- ***Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.***
	- Strong coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code experience within one or more of Terraform, Ansible, Puppet or Salt.***
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- ***Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)***
	- ***Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio***
	- ***Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus***
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 12+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Site Reliability Engineer (Stack Automation Service Team)
	- Splunk's Cloud group is looking for an experienced Site Reliability Engineer to join a team that is responsible for Cloud’s operational infrastructure and delivery. As a member of the Stack Automation Service team, you will be responsible for maintaining and troubleshooting Splunk's SaaS system, monitoring system stability and performance, troubleshooting complex problems, performing Amazon instance maintenance and system upgrades, and managing Amazon server/storage deployments, all while collaborating with various other Splunk Cloud teams. This is a fantastic opportunity to work with an exceptional team, grow your cloud experience, and help drive the growth of Splunk Cloud.
	- Puppet experience. You have at least 2 years of Puppet experience, including writing Puppet code and configuration management.
	- Python or Bash scripting experience. You will develop scripts and tools in Python/Bash.
	- AWS experience. Knowledge of Amazon EC2 including machine image management and storage, as well as an understanding of Amazon EC2 regional centers, availability zones, and HA strategies
	- Unix/Linux. You will use a command line terminal frequently.
	- Multi-tenant infrastructure experience. Experience supporting customer facing multi-tenant infrastructure (SaaS) or similar cloud related services
	- Software Development and Data Structures/Algorithms. We code primarily in Golang and Ruby, and work with RESTful APIs.
	- Cloud and container experience. Building and scaling secure services on different cloud providers.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and disaster recovery.
	- Problem Solving. You are able to fix a product outage, skilled in identifying performance bottlenecks, spotting anomalous system behavior, and figuring out the root cause of incidents.
	- Desire to learn and adapt. Our team has many projects going on at once, and you'll have the opportunity to learn to navigate new code and features.
	- Passion. We want you to actively own your work and be excited about your projects.
	- ***Kubernetes experience. Working in Kubernetes systems with experience in kubectl and docker containers.***
	- Terraform experience. Any prior work with Terraform is a plus.
	- ***Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.***
+ skill set:
	- In our Core C/ Chef Product Group, we develop the world's best products for managing applications and infrastructure at scale, and we deploy them to solve real problems in all kinds of industries. We get to work with the latest in cloud and container technologies. We have the opportunity not just to follow but to shape best practices. Our platform is used to enable billions of people around the world to chat, fly, present, bank, game, shop, and learn. Chances are the applications and devices you use every day to have infrastructure built, deployed, secured, and run with our code.
	- We are seeking a highly motivated, results-oriented individual with strong Site Reliability Engineering skills and experience in cloud technologies to join our platform engineering team. As a Site Reliability Engineer, you will play a lead role in designing, implementing, and supporting the platform for Chef Cloud services. You will also have a key influence on our future processes and platform design.
	- Build, operate, and maintain a platform for Chef Cloud services. This will include technologies such as AWS services (ECS, EKS, S3, and more), Kubernetes, service mesh (Linkerd or Envoy), Postgres/RDS, Graph databases, API gateways, authentication services, 3rd party integrations, and more.
	- Collaborate on achieving the best design/architecture for our systems and infrastructure.
	- Collaborate with other Engineering teams to support services before they go live through activities such as system design consulting, developing software platforms and frameworks, capacity planning, and launch reviews.
	- Maintain services once they are live by measuring and monitoring availability, latency, and overall system health.
	- Implement modern systems observability solutions including monitoring, alerting, metrics, logging, and APM & distributed tracing.
	- Scale systems sustainably through automation and evolve systems by pushing for changes that improve reliability and velocity.
	- Be on-call for services that the SRE team owns.
	- Practice sustainable incident response and post-incident analysis by acting as an incident manager. You’ll follow our existing incident management process and recommend improvements to that process.
	- Mentor the team.
	- You have a Bachelor's degree in Computer Science or related field and 10+ years of relevant experience (or equivalent combination of education and experience).
	- You have a solid understanding of and experience with configuration management and compliance automation.
	- You have an expert-level understanding of and at least 2 years of working experience with containerization using Docker and Kubernetes in a production environment.
	- You’re comfortable deploying and operating services using AWS technologies and have an expert understanding of the various offerings available.
	- You’ve built and supported systems using cloud-native (CNCF) technologies at scale.
	- Working knowledge on terraform or similar tools
	- You are interested in designing, analyzing, and troubleshooting large-scale distributed systems.
	- You understand what it means to operate infrastructure as code, and have experience developing services and automation to do so. Chef knowledge would be a plus
	- You have a great ability to debug and optimize code and automate routine tasks to eliminate toil.
	- You have a systematic problem-solving approach, coupled with strong communication skills and a sense of ownership, initiative, grit, and drive.
	- You have designed and implemented applications and systems that scale, are resilient to failure, and are observable.
+ skill set:
	- distributed systems for customer-facing environments
	- navigate and scale multi-tier cloud environments on AWS and GCP
	- container-centric architectures, using:
		* Docker
		* Kubernetes
			+ EKS
			+ GKE
			+ AKS
			+ OpenShift
		* ECS
		* Docker Swarm
		* Mesos
	- infrastructure-as-code tools:
		* Terraform
		* Ansible
		* Puppet
		* Chef
	- full-stack Web development using:
		* React
		* Node
		* MongoDB
	- experience with OpenResty, Pulumi
		* OpenResty: full-fledged Web platform based on nginx, which runs Lua scripts on LuaJIT engine
		* Pulumi: universal infrastructure-as-code SDK, for:
			* creating cloud infrastructure
			* deploying cloud infrastructure
			* managing cloud infrastructure
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















##	DevSecOps




DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term "shift left" can be used. Security is tested in three main areas: static, software composition, and dynamic.











+ skill set:
	- Senior Client Platform Engineer
	- Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
	- Are you the kind of systems engineer that has a passion for administering enterprise software using best of breed technologies? Are you self-motivated and require minimal supervision? Do you put together a rolling 12-month roadmap to execute against? If so, then this is the dream job you've been looking for. 
	- Splunk is looking for a highly skilled Senior Systems Engineer focusing on endpoint security, and configuration management. You should be comfortable delivering at a high level in a fast paced and growing environment. You will drive standardization and management for our endpoints along with a number of enterprise applications and services. This role provides high visibility and impact to both the CIO and CISO organizations.
	- Architecture, design, integration, implementation, operation, and support of enterprise-wide applications and services for our Windows fleet.
	- Assisting in developing long-term strategies and capacity planning for meeting future end user needs
	- Configuration Management for Windows using industry standard tools to meet Security requirements and comply with CIS benchmarks
	- Managing configuration of our endpoint security software such as endpoint detection and response, application allow/block lists, and host-based intrusion detection software
	- Partnering with the Security Engineering leads to coordinate efforts, initiatives, and roadmaps
	- Administer enterprise software including deployment and package management
	- Write scripts/policies that automate application and settings distribution using internal tools
	- Manage transition plans for major upgrades or patches
	- Integrate with other internal systems and tools
	- Manage and report on application performance against KPI’s
	- Work as the escalation point between various support teams for issues on the client platform
	- Work as a liaison from the Splunker Technology Success org to other IT Service organizations to deliver feature enhancements and best in class solutions through shared products and goals
	- Drive client security models and best practices in an enterprise environment
	- Drive business decisions through data using tools like Splunk
	- Diagnose and investigate unique and complex systemic problems
	- Develop solutions that meet the business needs to complex customer requirements
	- 10+ years of overall IT experience; 5+ years experience of providing application support and engineering
	- Experience with implementing security standards and compliance across a huge enterprise organization
	- Knowledge of bash/python scripting
	- Experience with Endpoint Management platforms such as WorkspaceONE/InTune/LANdesk/Kace/etc.
	- Experience with DevOps platforms such as Puppet/Salt/Chef
	- Ability to work in high pressure, highly flexible environment against both short and long term requirements
	- Passionate about technology and solving IT operations-focused problems
+ skill set:
	- Software Engineer - Analytics Platform (Remote)
	- Are you passionate about working on products that make a difference for your customers? Do you enjoy building large scale applications that are powered by huge data sets? Do you value working in an environment where you're empowered to make key technical decisions across a full stack of technologies? If so, a role on the Splunk Security Analytics team might be a great fit for you.
	- As a Software Engineer on the Security Content Engineering team, your primary focus is content distribution, content improvement, and content assurance. You will contribute to build and maintain the content distribution system and support content improvements.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify, and improve designs. Help put together a plan for how to make it happen using Agile Methodologies.
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality while continuously deployed in the cloud to our customers.
	- Participate in the software development lifecycle by writing code, tests, documentation; support the sprint management process; and communicate effectively with peers and managers.
	- 2+ years of Software Engineering experience.
	- Bachelor's degree in Computer Science or equivalent training and work experience.
	- Proficient with Java or Python programming.
	- Engagement with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Exposure to an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Versed with CI/CD frameworks and experience with automation.
	- Strong oral and written communication skills, including a demonstrated ability to prepare documentation and presentations for technical and non-technical audiences.
	- Background in developing products for the Security market, a plus.
+ skill set:
	- Senior Software Engineer - Analytics Platform (Remote)
	- Enterprise Security behavioral analytics service (Advanced Analytics) is Splunk’s next-generation, cloud-native, multi-tenant analytics solution that detects known and unknown security threats at petabyte scale. Advanced Analytics detects cybersecurity threats by using stream and batch processing, and building large scale analytics infrastructure for petabyte scale data ingestion, processing, storage, and analysis. Advanced Analytics will power large and medium scale enterprises to combat security threats, protect brand reputation and protect intellectual property.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep aware of security trends in the industry and bring that knowledge back to the team.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best software development practices.
	- Champion, coach, and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 8+ years of experience in Enterprise Software Engineering.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience with Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, HBase, Cassandra, MongoDB.
	- Exposure to working with cloud environments (AWS, Azure, GCP) and container ecosystems (Docker and Kubernetes).
	- Knowledge of distributed computing architectures and principles that solve for scalability, consistency, availability, performance, and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












##	ITOps





ITOps (for data centers):
+ maintain core infrastructure and associated services needed to support AI and machine learning models running in business applications
+ enterprise AI role:
	- provide highly available, secure infrastructure to operate performant models in enterprise application to scale, 24x7
	- deliver performance metrics for deployed models and applications
	- plan and execute infrastructure evolution to support AI technologies
+ for ModelOps
	- centralized catalog of all:
		* models
		* model runtime requirements
		* lineage
		* operational history
	- regardless of tools to create AI and ML models, support standardized models on any infrastructure (prem, cloud, or hybrid) that can be consistently:
		* deployed
		* monitored
		* controlled
	- provide automated alerts regarding model performance and behavior
	- automated processes and approvals for:
		* deployment
		* testing
		* refresh
		* monitoring
	- real-time visibility to performance of deployed models
+ Information Technologies Operations (ITOps) is the process responsible for acquiring, designing, deploying, configuring, and maintaining the physical and virtual components that comprise your IT infrastructure
+ While ITOps takes a broad view of the entire technology landscape that your organization relies on to conduct its business mission, DevOps focuses on the task at hand.
	- DevOps teams don’t always have the visibility or awareness of the downstream implications on the enterprise stack.
	- And this is where can modern ITOps team can help.
+ ITOps (or) Tech Ops is the most traditional Ops that refers to managing all the physical and software components of an organization’s IT environment.
	- It is responsible for the smooth running of a business by handling applications, delivery, maintaining services, and the underlying technologies administrated by a company's IT members to its internal or external clients.
	- It includes:
		* maintaining networks
		* ensuring security
		* managing data center
		* system administrators
		* regulatory compliance
		* licensing
		* managing software
		* managing help desk
		* supports IT infrastructure and systems for daily business operations
	- IT department can effortlessly manage all the IT operations and make the job easier by using IT Operations.
		* It guides the business to be more secure, swift, and productive.
+ includes:
	- networking operations
	- deploying, maintaining, and configuring applications
	- overseeing both physical and virtual components of a company's IT environment
+ includes:
	- administration and maintenance,
	- network management,
	- systems management,
	- technical support (help desk)



ITOps processes consists of 5 main aspects/phase:
+ Run Solutions
	- The primary purpose of having ITOps teams is to run solutions such as data backups, configurations, handling servers, and restoring systems after an outage or update.
	- The aim is to optimize the performance and allocate the proper resource for the most effective delivery.
+ Infrastructure
	- Managing the IT infrastructure includes maintaining, provisioning, setting up, and updating all the software and hardware applications and cloud resources of the company's IT infrastructure.
	- These components include:
		* operating systems
		* hypervisors
		* network infrastructure
		* platforms
		* container environments
		* physical servers
		* application software
	- The maintaining oversight of IT infrastructure is made on-premises data center or in the cloud.
+ Network
	- Network management is responsible for managing all network functions for internal or external IT communications.
	- Network management is also involved in configuring and managing telecommunication lines.
	- In addition, it allows authorized customers to secure remote access to the company's network.
+ Security
	- Security management is an integral function of IT service management (ITSM).
		* It deals with:
			+ securing the hardware and software assets
			+ implementing security within development operations
			+ managing access control
			+ ensuring that security standards have reached across the IT environment of the organization
+ Problem Solving
	- Event management or incident management is also known as Problem-solving, it can be divided into 2 types – preventive measures and reactive measures.
		* Preventive Measures
			+ Preventive measures reduce the possibility of disasters and find the solution to anticipate and avoid any negative impacts on the IT environment.
		* Reactive Measures
			+ This concept refers to cyber-attacks, critical situations, and other issue when implementing disaster recovery plans and help desk management services.




ITOps tasks:
+ Network infrastructure:
	- Configuring and managing all networking functions for internal and external IT communications
	- Configuring and managing telecommunication lines
	- Managing firewall ports to allow the network to communicate with outside servers
	- Providing authorized users secure remote access to the organization’s network
	- Monitoring network health and performance, detecting anomalies, and preventing or quickly resolving issues, which may include building and managing a network operations center (NOC, pronounced “knock”), a centralized physical location from which ITOps teams can continuously monitor a network
+ Server and device management:
	- Configuring, maintaining and managing servers for infrastructure and applications
	- Managing network and individual storage to ensure they meet application requirements
	- Setting up and authorizing email and file servers
	- Provisioning and managing company-approved PCs
	- Provisioning and managing cell phones and other mobile devices
	- Managing licensing and desktop, laptop and mobile device software
+ Computer operations and help desk:
	- Managing data center locations and equipment
	- Operating the help desk
	- Creating, authorizing and managing all user profiles on organizational systems
	- Providing network configuration auditing information to regulatory agencies, business partners and other outside entities
	- Ensuring high availability of the network and disaster recovery plans
	- Alerting users when a major incident impacts network services
	- Instituting regular backups to facilitate data recovery when needed
	- Maintaining the ITIL for the organization








ITOps addresses:
+ infrastructure capacity
+ infrastructure availability
+ infrastructure security


KPIs for ITOps are based on:
+ application performance
+ infrastructure availability
+ infrastructure security
+ infrastructure cost




*Information Technology Infrastructure Library, ITIL*:
+ functions of *IT Operations Management framework*
	- ITOps, or IT operations, refers to the processes and services administered by an organization's IT staff to its internal or external clients.
	- applications management
	- technical management
	- service desk







+ skill set:
	- 4+ years of experience in developing, planning, and and administering VMware ESXI and vCenter deployments
	- Experience with vSphere, vSAN, and other VMware products and platforms
	- Experience with troubleshooting for virtualized platform
	- Ability to provide and manage virtual machine configurations, including networking, storage and security settings and deploy it to hosts upon request
	- Knowledge of data center networking, including TCP/IP, switching/routing, ports and protocols, firewall concepts, or load-balancing
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Knowledge of data center architecture: power, cooling, and networking.
	- Significant experience working with data center hardware and writing software to make that easier, faster, and less manual effort
	- Familiarity with best practices for hardware acceptance testing
+ Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.





















##	 IT operations analytics, ITOA


IT operations analytics (ITOA) is an approach or method to retrieve, analyze, and report data for IT operations.
+ Or, known as:
	- advanced operational analytics
	- IT data analytics
+ ITOA may apply big data analytics to large datasets to produce business insights.
+ ***ITOA is different than AIOps, which focuses on applying artificial intelligence and machine learning to the applications of ITOA.***
+ The use of mathematical algorithms and other innovations to extract meaningful information from the sea of raw data collected by management and monitoring technologies.



Context:
+ (IT) systems management
	- Fault, Configuration, Accounting, Performance, Security (FCAPS)
	- Distributed Management Task Force (DMTF)





applications of ITOA systems:
+ Root cause analysis:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored can help users pinpoint fine-grained and previously unknown root causes of overall system behavior pathologies.
+ Proactive control of service performance and availability:
	- Predicts future system states and the impact of those states on performance.
+ Problem assignment:
	- Determines how problems may be resolved or, at least, direct the results of inferences to the most appropriate individuals or communities in the enterprise for problem resolution.
+ Service impact analysis:
	- When multiple root causes are known, the analytics system's output is used to determine and rank the relative impact, so that resources can be devoted to correcting the fault in the most timely and cost-effective way possible.
+ Complement best-of-breed technology:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored are used to correct or extend the outputs of other discovery-oriented tools to improve the fidelity of information used in operational tasks (e.g., service dependency maps, application runtime architecture topologies, network topologies).
+ Real time application behavior learning:
	- Learns & correlates the behavior of Application based on user pattern and underlying Infrastructure on various application patterns, create metrics of such correlated patterns and store it for further analysis.
+ Dynamically baselines threshold:
	- Learns behavior of Infrastructure on various application user patterns and determines the Optimal behavior of the Infra and technological components, bench marks and baselines the low and high water mark for the specific environments and dynamically changes the bench mark baselines with the changing infra and user patterns without any manual intervention




Types of ITOA:
+ Log analysis
+ Unstructured text indexing, search and inference (UTISI)
+ Topological analysis (TA)
+ Multidimensional database search and analysis (MDSA)
+ Complex operations event processing (COEP)
+ Statistical pattern discovery and recognition (SPDR)

















+ skill set:
	- experience with large-scale distributed storage and database systems
		* SQL
		* NoSQL
		* MySQL
		* Cassandra
	- data processing experience with building and maintaining large-scale and/or real-time complex data processing pipelines using:
		* Kafka
		* Hadoop
		* Hive
		* Storm
		* Zookeeper
	- experience with developing complex software systems scaling to substantial data volumes or millions of users with production quality deployment, monitoring, and reliability
	- experience running scalable (thousands of RPS) and reliable (three 9's) systems
+ Experience with monitoring and tracking tools such as Splunk, NewRelic, Adobe/Google Analytics
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




##	 Artificial Intelligence for IT Operations, AIOps



###	 Notes about Artificial Intelligence for IT Operations, AIOps


Artificial Intelligence for IT Operations (AIOps) is an industry category for machine learning analytics technology that enhances IT operations analytics.


Goals of AIOps:
+ enable IT transformation
+ receive continuous insights that provide continuous fixes and improvements via automation



AIOps can be viewed as CI/CD for core IT functions.
+ Given the inherent nature of IT operations, which is closely tied to cloud deployment and the management of distributed applications, AIOps has increasingly led to the coalescence of machine learning and cloud research.



main aspects of an AIOps platform:
+ machine learning
+ big data

In order to collect observational data and engagement data that can be found inside a big data platform, which requires a shift away from sectionally segregated IT data, a holistic machine learning and analytics strategy is implemented with the combined IT data.





AIOps tasks include:
+ automation
+ performance monitoring
+ event correlations



AIOps process:
+ The *normalized data* is suitable to be processed through machine learning algorithms to *automatically reduce noise* and *identify the probable root cause of incidents*.
	- The main output of such stage is the ***detection of any abnormal behavior from users, devices or applications***.
+ Noise reduction can be done by various methods, but most of the research in the field points to the following actions:
	- Analysis of all incoming alerts;
	- Remove duplicates;
	- Identify the false positives;
	- Early *anomaly, fault and failure* (AFF) detection and analysis.[13]
+ ***Anomaly detection*** - another step in any AIOps process is based on the analysis of past behavior of users, equipment and applications. Anything that strays from that behavior baseline is considered unusual and flagged as abnormal.
+ ***Root cause determination*** is usually done by passing incoming alerts through algorithms that take into consideration correlated events as well as topology dependencies. The algorithms on which AI are basing their functioning can be influenced directly, essentially by "training" them.





AIOps platforms enabling IT operations management (ITOM)
+ inputs:
	- historic data
	- real-time streaming data
	- vendor-agnostic data ingestion
+ input types:
	- logs
	- metrics
	- wire data
	- document text
+ observe (monitoring)
+ act (IT Service Management process, ITSM)
+ engage (monitoring)
+ machine learning with *Big Data*
+ outputs:
	- historical analysis
	- anomaly detection
	- performance analysis
	- correlation and contextualization


Factors of IT maturity:
+ organizational structures
+ processes and practices
+ skills and knowledge,
+ tools and policies
+ systems and data
+ documents and agreements



cause-and-effect sequence
+ outcomes
+ capabilities
+ IT maturity
+ IT excellence
+ business maturity, innovation, and productivity




Role of IT suppert services:
+ aware
	- The organization is aware of its chaotic stage and needs.
	- IT capabilities are unstable and success depends on the individuals' effort and technical knowledge.
+ committed
	- IT operations are more process-oriented, thus more reputable.
	- Success depends on process adherence and point collaboration.
	- This level is a stable plateau where IT can "keep the lights on."
+ proactive
	- IT organization is recognized as "mature."
	- At this level, IT has reached a tipping point from which a path to high IT maturity is accessible.
+ aligned
	- IT is a highly efficient internal service provider, offering a stable portfolio of optimized services.
+ business partner
	- IT is a trusted partner and innovator for the business












 










Applications of AIOps:
+ analysis of large and unconnected datasets, or large number of un-normalized databases
	- including aggregated data
+ Automation of tasks (DevOps)
+ Machine learning platforms
+ Augmented reality
+ Agent-based simulations
+ Internet of things (IoT)
+ AI Optimized Hardware
+ Natural language generation
+ Streaming data platforms
+ Conversational BI and analytics









###	 Skill Sets for using Artificial Intelligence for IT Operations, AIOps



+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










##	CloudOps



CloudOps include:
+ cloud-specific flexible provisioning and scalability of environments,
+ built-in task automation.













##	NoOps



NoOps (or) No Operation is the new idea that completely automates a software environment from the underlying system infrastructure through technologies including machine learning (ML), and Artificial Intelligence (AI). As a result, there is no need for any operation team to manage software in-house.

With NoOps, developers can concentrate solely on writing and improving the software product’s code that improves the resources like management, security, infrastructure, product, and operations part of the lifecycle. Additionally, the service providers offer developers to develop software like resources, backups, patches, and the right cloud infrastructure to work independently without any interference.

***Serverless architecture*** is the best example for NoOps software. The developer’s team aims to create their application and deploy them in ***serverless computing*** without interfering any operational or infrastructure considerations.

Operating the right tools in NoOps can achieve a faster deployment process than DevOps by running Platform as a Service (PaaS) or Function as a Service (FaaS) in the cloud. Moreover, NoOps can easily be adaptable for Product as a Service companies, small-scale applications, and start-ups.





##	Software Intelligence


Software intelligence:
+ [get] ["insight into the structural condition of software assets produced by software designed to analyze database structure, software framework and source code to better understand and control complex software systems in Information Technology environments"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["set of software tools and techniques for the mining of data and software inner-structure"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["End results are information used by business and software stakeholders to make informed decisions,[citation needed] measure the efficiency of software development organizations, communicate about software health, prevent software catastrophes."](https://en.wikipedia.org/wiki/Software_intelligence)



["Software intelligence is derived from"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Software composition is the construction of software application components. Components result from software coding, as well as the integration of the source code from external components: Open source, 3rd party components, or frameworks. Other components can be integrated using application programming interface call to libraries or services."
+ "Software architecture refers to the structure and organization of elements of a system, relations, and properties among them."
+ "Software flaws designate problems that can cause security, stability, resiliency, and unexpected results. There is no standard definition of software flaws but the most accepted is from The MITRE Corporation where common flaws are cataloged as Common Weakness Enumeration."
+ "Software grades assess attributes of the software. Historically, the classification and terminology of attributes have been derived from the ISO 9126-3 and the subsequent ISO 25000:2005 quality model."
+ "Software economics refers to the resource evaluation of software in past, present, or future to make decisions and to govern."





["The capabilities of Software intelligence platforms include an increasing number of components"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Code analyzer to serve as an information basis for other Software Intelligence components identifying objects created by the programming language, external objects from Open source, third parties objects, frameworks, API, or services"
+ "Graphical visualization and blueprinting of the inner structure of the software product or application considered including dependencies, from data acquisition (automated and real-time data capture, end-user entries) up to data storage, the different layers within the software, and the coupling between all elements."
+ "Navigation capabilities within components and impact analysis features"
+ "List of flaws, architectural and coding violations, against standardized best practices, cloud blocker preventing migration to a Cloud environment, and rogue data-call entailing the security and integrity of software"
+ "Grades or scores of the structural and software quality aligned with industry-standard like OMG, CISQ or SEI assessing the reliability, security, efficiency, maintainability, and scalability to cloud or other systems."
+ "Metrics quantifying and estimating software economics including work effort, sizing, and technical debt"
+ "Industry references and benchmarking allowing comparisons between outputs of analysis and industry standards"



It is used as part of ["Application Portfolio Analysis (APA)"](https://en.wikipedia.org/wiki/Software_intelligence).

















#	References




	@misc{WikipediaContributors2022b,
		Address = {San Francisco, {CA}},
		Author = {{Wikipedia contributors}},
		Howpublished = {Available online from {\it Wikipedia, The Free Encyclopedia: Web frameworks} at: \url{https://en.wikipedia.org/wiki/Solution_stack}; July 8, 2022 was the last accessed date},
		Month = {June 21},
		Publisher = {Wikimedia Foundation},
		Title = {Solution stack},
		Url = {https://en.wikipedia.org/wiki/Solution_stack},
		Year = {2022}}