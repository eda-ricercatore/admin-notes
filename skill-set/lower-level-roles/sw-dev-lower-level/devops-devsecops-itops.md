#	DevOps & Site Reliability Engineering (SRE) & DevSecOps & ITOps & IT operations analytics & AIOps & CloudOps & NoOps


##	DevOps




A DevOps toolchain is a set or combination of tools that aid in the delivery, development, and management of software applications throughout the systems development life cycle, as coordinated by an organisation that uses DevOps practices.

Generally, DevOps tools fit into one or more activities, which supports specific DevOps initiatives: Plan, Create, Verify, Package, Release, Configure, Monitor, and Version Control.


DevOps include:
+ Microservices
+ DevOps automation
+ Automation with version control
	- Many organizations use version control to power DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD. 




DevOps life cycle:
+ continuous development
+ continuous integration
+ continuous testing
+ continuous monitoring
+ continuous feedback
+ continuous deployment
+ continuous operations



Alternate DevOps life cycle, exploiting real-time communication:
+ continuous development/build
+ continuous integration
+ continuous deployment
+ continuous operations
+ continuous feedback
+ continuous planning







DevOps is at the intersection of:
+ software development
+ software quality assurance
+ ITOps



***BizDevOps is DevOps that account for business needs.***
+ Or, ***BizDevOps*** is ***DevOps 2.0***.
+ Collaborate with business units using agile methods
	- Business unites have to contribute to reduce the product/service backlog
	- DevOps teams have to be more responsible for the business aspects of what they do.






Skill set for DevOps:
+ skill set:
	- Experience with Kubernetes and Docker.
	- Experience with Elasticsearch, Redis and/or Memcached.
+ skill set:
	- An understanding of several of these methodologies and tools:
		* Software development methods such as Agile, Scrum, Lean, Waterfall
		* Software project tools like JIRA, Pivotal Tracker, Trello, Asana, and MS Project
		* Continuous integration and build automation with Jenkins, TeamCity, TFS, TravisCI, CircleCI
	- Experience configuring the following technologies:
		* LDAP, ActiveDirectory, and other SAML/Single-Sign-on services
		* VMware vSphere, ESXi, AWS, Azure, GCP, and other virtual infrastructure tools
+ Familiarity with configuration/orchestration management software such as Puppet, Chef, Ansible, or Salt.
+ developer experience (DX) lead
	- current tech stack:
		* Golang
		* TypeScript
		* GCP
		* Docker
		* Terraform
		* Serverless
		* K8s
		* VueJS
		* gRPC, gRPC Remote Procedure Calls, general-purpose RPC infrastructure, Stubby
	- Gopher communication protocol
+ skill set:
	- maintain and improve Torq's automation infrastructure and CI/CD pipelines
	- improve test's infrastructure to support scale, reduce delivery times and improve the overall quality of all product aspects
	- develop E2E (UI and API) tests for Torq's management app and microservices (Python)
	- contribute to integration test framework (TypeScript) - cross platform and cross browsers tests
	- develop performance tests over k6
	- experience with UI automation frameworks
		* Playwright or Puppeteer (Node.js libraries, headless browser and usage), CDP based
		* CDP, content delivery platform - SaaS content service
		* CMS, content management system
		* CDP, continuous data protection, continuous backup, or real-time backup
		* CDP, customer data protection
	- experience writing Python-based frameworks for test automation, Python, unittest
	- experience with writing JavaScript/TypeScript -based automation framework, Jest, Mocha
	- hands-on experience working with Linux and modern apps using Docker containers and k8s
	- working knowledge of CI/CD, and cloud deployment and testing
	- experience with testing gRPC microservices
+ skill set:
	- scalable ***configs as code***
	- developer-facing tooling
	- drive, implement, support, and maintain infrastructure services with:
		* Kubernetes
		* Envoy
		* Kafka
		* Cassandra
	- collaborate and evangelize the right cloud solutions throughout the business by creating a visionary direction and road map for infrastructure-as-a-service, IaaS
	- implement automation to perform the day to day operations/functions of the cloud platform, working across all teams
	- develop, monitor, and build alerts around error conditions and performance
	- work in a fast-paced environment while participating in conceptualizing and building CI/CD pipelines
	- be on-call as needed to support the infratructure and our systems, and drive philosophies around site reliability
	- experience in infrastructure engineering
	- experience with public cloud computing services, such as AWS and GCP
	- experience with container technologies
	- experience working with high availability and scalable SaaS (or consumer technologies)
	- experience deploying highly available and scalable, secure and reliable services with automatic failover using containers and container orchestration tools like K8s
	- use of service meshes
	- cloud formation via Terraform and Ansible
	- experience deploying container applications with helm charts
	- experience with:
		* monitoring tools:
			+ Datalog
		* on-call tooling
			+ PagerDuty
	- experience architecting, implementing, and managing environments in AWS
	- experience implementing AWS services, such as:
		* EC2 Load Balancing
		* VPC
		* Route 53
		* Direct Connect
		* NAT Gateway
		* VPN
		* EC2 Networking
		* Transit Gateway
		* Global Accelerator
+ skill set:
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
+ Specific technologies, like Spring, docker, Kubernetes, etc. are, of course, also a great help
+ skill set:
	- Excellent solid understanding of Apache Pulsar, RabbitMQ, or Apache Kafka.
	- experience in stream processing platform, such as Flink, Storm, Spark or equivalent
+ skill set:
	- Senior DevOps Engineer, Cloud IaaS (US Remote Available)
	- Splunk's IT Operations team's exciting and meaningful mission: Build, scale and maintain Splunk’s Infrastructure for all Splunkers. While various Engineering groups focus on building our products, IT Ops serves as the backbone operational support for Splunkers across the globe.
	- We are actively seeking DevOps Engineers with a real passion for automation to help build scalable tools to run our distributed systems. You will be responsible for expanding and supporting the infrastructure platform services we provide to Splunk, as well as engaging with other teams to help improve efficiency and optimize our infrastructure. You're also an individual who’s motivated by technology and enjoys automation and problem-solving. We work hard, we like to challenge the status quo, and we enjoy having fun!
	- Support and maintain IT Public/Private Cloud Infrastructure, including our virtualization and container platforms
	- Be a part of the On-Call for production issues during shift or as required.
	- Take on performance and stability issues using a wide variety of tools, including Splunk
	- Ensure that day-to-day operational requirements and SLAs are met
	- Work with key business partners to understand their requirements and recommend potential solutions, and secure resources to deliver
	- Maintain critical services and provide visibility to internal teams
	- Seek opportunities to improve or optimize processes through automation
	- 5 + years of experience as a DevOps Engineer administering/managing an AWS Public Cloud platform
	- 3+ years of experience providing automation with a major scripting language such as shell, python or go.
	- 5 + years of experience providing Linux systems administration/engineering
	- 1+ years of experience with Configuration Management tools like Ansible, Puppet or Chef
	- CI/CD pipeline tool experience (e.g. Jenkins, GitLab, etc)
	- Experience with Hashicorp toolsets Terraform, Vault, and Packer.
	- A deep understanding of networking concepts and internet protocols
	- Familiarity with Observability concepts and tools
	- Experience with Kubernetes and containers
	- Good understanding of cloud infrastructure security concepts
	- Ability to provide reliable technical support and mentorship on complex issues in a high velocity, dynamic environment
	- Ability to communicate complex technical concepts clearly to customers and upper management
+ skill set:
	- Senior Site Reliability Engineer (remote Spain)
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via Terraform as well as Kubernetes, Helm and Istio
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of Terraform, Ansible, Puppet or Salt.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio
	- Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Software Engineer - Developer Platform Infrastructure
	- We are looking for a Senior Software Engineer to help lead, design and build the next generation of our CI/CD and tools offerings. You will be working on the core infrastructure platform enabling the next generation of Splunk’s offerings.
	- CI/CD and tooling expertise. Cloud, container and virtualization experience. Innovating and scaling secure services on-prem and different cloud providers is a plus. You will use Kubernetes, Docker, UCP, AWS/GCP, Jenkins, Gitlab, Ansible.
	- Data structures and algorithms. A solid grasp of data structures, algorithms, and RESTful APIs.
	- Observability infrastructure expertise to ensure 24/7 operational excellence and data driven decision making.
	- Ability to work with multiple programming languages. We have code in several languages, ranging from Go to Python.
	- BS/MS degree in EE or CS or a related technical field or 3+ years of progressive experience.
	- Desire to learn and adapt. You will constantly be learning new areas and new technologies.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. We want you to be excited and have utmost ownership of your projects.
	- Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and observability.
	- Opportunities to develop and grow as an engineer. We are always expanding into new areas, working with open-source projects and contributing back, and exploring new technologies.
	- A team of incredibly capable and dedicated peers, all the way from engineering to product management and customer support.
	- Breadth and depth. You are interested to work on an area that dynamically scales to meet the needs of Splunk’s offerings.
	- You want to go deep into optimizing how we automate every manual process and tedious task we encounter.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe that mentors help both sides of the equation.
	- Fun. We have frequent group outings and team building events. We are committed to having every employee want to give it their all, be respectful and a part of the family, and have a smile on their face while doing it.
+ skill set:
	- Software Engineer- Tooling and Infrastructure
	- Splunk is looking for a seasoned professional engineer to join the effort to define and build the future of Splunk. Splunk is rapidly expanding their presence in the cloud, and we are looking for engineers who are interested in being founding members of the Deployment Tooling team (D4S)  that defines and builds tools to optimize how Splunk services are deployed to the public cloud.  This is a great opportunity to both lead and to learn.
	- In this role you will help Splunk to orchestrate deployments of its multi-tenant cloud platform across multiple regions, and to manage continuous deployment to these regions via Argo CD to provide for automated rolling deployments.  We are looking for candidates who have experience transitioning from operationalized deploys to automated deploys.  This is a position with broad impact–what you build will be used across all of Splunk cloud.  You need to be able to build robust solutions that are easy to use and provide exponential impact to an organization.
	- Opportunities to develop and grow as an engineer. We are at the forefront of our industry, always expanding into new areas, and working with open source & new technologies.
	- A set of talented and dedicated peers, all the way from engineering and QA to product management and customer support.
	- Breadth and depth. You may be diving deep into CI/CD tools and best practices, but you’ll have the freedom to work on other areas you are passionate about.
	- Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both mentor and mentee alike.
	- An open, collaborative and supportive work environment. We embody the scrum values.  We also have a number of Employee Resource Groups for employees of all backgrounds.
	- Experience in building and deploying cloud-native applications/services. If you have built cloud-native microservices, that’s great!  Even better is if you have experience developing CI/CD systems integrating terraform, vault, Kubernetes, AWS.  In depth experience with CI/CD pipeline tools such as Gitlab, Argo, Spinnaker, Artifactory are a definite plus!
	- Experience in systems-level programming and distributed systems. You have knowledge of operating systems, networking and network protocols, messaging, consensus, failure modes, and parallel programming.
	- Demonstrated ability to advocate for simple and clear APIs for complex functionality. You have an API-first mentality, with the ability to build straightforward APIs to help services configure the service mesh routing, maintaining API contracts, etc.
	- Passion. Our customers are passionate about Splunk, and we want the same from our engineers. You actively own your work and be excited about your projects.
	- Ability to work with Golang and Python. Most of our services are written in Golang and tools are written in Python; if you are an expert at another language we can consider you, but you will be expected to program in Python and Golang.
	- Requires 3-5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience
	- 3-5 years experience in programming languages: Python, GoLang, Java, C, 
	- 2 years building and deploying cloud-native applications/services on AWS or other cloud services like GCP, Azure, etc.
	- Terraform 
	- Kubernetes 
	- Vault 
	- Artifactory 
	- Gitlab
	- Argo 
+ skill set:
	- Principal Software Engineer - Analytics Platform (Remote)
	- We are seeking a Principal Software Engineer to help lead, design, develop and deliver Splunk's User Behavior Analytics (UBA) security analytics solution that detects known and unknown security threats at scale using big data and machine learning techniques. UBA helps security analysts quickly identify and resolve threats; delivered on customer managed resources using Kubernetes and Spark to run innovative stream processing and machine learning algorithms in near real-time.
	- We are a passionate team who care deeply about our customers and our teammates. In this role, you will work directly with Product Management, our Design Team, our Customers and other engineering teams to help derive the best experience for the customer. We have a lean process that focuses on empowering and serving our engineers as opposed to just directing them.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real world use-cases in order to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality.
	- Champion, coach and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 12+ years of Software Development experience.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience using languages such as Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, Hadoop, Cassandra.
	- Exposure to working with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Knowledge of distributed computing architectures and principles that solve for scalability, performance, redundancy and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
	- Background in developing machine learning products for the Security market a plus.
+ skill set:
	- Comfortable with Linux, Docker, AWS, GIT, Artifactory in terms of both tools and systems administration
	- Previous experience in design and implementation of solutions to evaluate and improve performance: availability, reliability, interoperability, scalability of SaaS / Cloud Native / Bigdata Platform and application with microservice architecture
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Design, implement, and maintain platform metadata features
	- Designing APIs and Platform Information Architecture
	- Serve as primary point of contact in one or more platform metadata areas
	- Collaborate with various Confluent Engineering groups to provide strong technical guidance and leadership related to managing metadata
	- In depth experience with concepts of distributed systems and big data such as - Kafka, Hadoop, Spark, Big Table, HBase etc
	- Full stack experience
	- Experience with Lineage, Governance and Auditing
	- Experience in ML/Data Engineering
	- Experience working with Docker or Kubernetes is a plus
	- Experience working with AWS, Azure, and/or GCP
+ skill set (Platform DevOps Engineer):
	- As a Platform DevOps, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience with Go, Java, C++ or Python required
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- As a Platform DevOps Engineer, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- Solid understanding of container orchestration systems such as Kubernetes, Mesos, etc.
	- Experience with C, C++, Java or Python required
	- Experience with container orchestration tools such as Docker, CoreOS, etc.
	- Experience with configuration management or provisioning tools such as chef, puppet, Ansible, etc
	- Experience building and operating large-scale systems
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Hands-on experience with Kubernetes operator, Helm, or StatefulSets is a plus
	- Proficiency in Go is a plus
	- Knowledge of Apache Kafka is a plus
+ ***Experience building and scaling automation frameworks***
+ skill set:
	- Build and maintain data foundations, metrics and dashboards to monitor the business performance and extract actionable insights
	- Apply quantitative analysis, data mining, and presentation of data to fuel business growth and drive customer success
	- Design and analyze experiments to test new product ideas, go to market strategies and/or funnel optimization; Convert the results into actionable recommendations
	- Build data products to improve operational efficiencies organizationally to scale with a hyper growth start-up
	- Inform, influence, support, and execute business decisions with senior leadership and business partners
	- Build robust, automated data pipelines to enable team effectiveness
	- 2+ years industry experience working with SQL (Teradata, Oracle, MySQL, BigQuery, etc.) and R (or Python)
	- Proficiency in applying statistical modeling and/or machine learning
	- Proficiency in data visualization (eg. Tableau, Looker, Matlab, etc.)
	- Bachelor or advanced degrees in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc
	- The ability to communicate cross-functionally, derive requirements and deliver insightful analysis and/or models; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience building data warehousing and ETL pipelines
	- Experience with Unix/Linux environment
	- Experience in developing data apps with Python/Java, high charts etc
	- Excellent communications skills, with the ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience working in the B2B growth/marketing or sales domains: CRM, sales effectiveness, branding, segmentation, web analytics, attribution, funnel optimization, etc.
	- Experience working with Marketo, Google Analytics and SFDC
+ skill set:
	- The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.
	- We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
	- Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
	- Implementing a solid, robust, extensible data warehousing design that supports key business flows
	- Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
	- Developing strong subject matter expertise and manage the SLAs for those data pipelines
	- Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
	- Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
	- Building and growing  partnership with cross functional teams, and evangelize data-driven culture
	- Contributing to innovations that fuel Confluent's vision and mission
	- 4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
	- Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
	- Highly proficient in Python and SQL coding
	- Highly proficient with tuning and optimizing data models and pipelines
	- Experience in developing data apps with Python, Javascript, high charts etc
	- The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience with Apache Kafka
	- Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
	- Experience in developing data apps with Python, Javascript, high charts, etc
+ skill set:
	- Deep knowledge of a configuration management tool (i.e. Puppet, Chef, Ansible, Salt, CFEngine). Experience with containers is a plus
	- Familiarity with distributed systems including service discovery, pub/sub, search indexing, storage, and caching. We use Zookeeper, Kafka, Elasticsearch, MySQL, Hbase, and Memcache respectively.
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Improve observability with distributed tracing for all requests from client to CDN to load balancer to cluster and back again
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Build observability into every aspect of our production infrastructure
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- Help developers smoke-test better by bringing canary analysis and automated scale testing into their world
	- DevOps and systems experience is highly valued; If you've gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we'd love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- Ansible
	- Kafka/Cassandra
	- Linux
	- Git (github)
	- Vi / Vim
	- Elastic Search Stack
	- Graphite/Grafana
	- Data visualization
	- Python, Bash, Golang
	- Familiarity with JSON
+ skill set:
	- Expertise with 12 Factor application principles
	- Containers (Docker, Kubernetes...)
	- Streaming/logging technologies (ElasticSearch, fluentd, LogStash, Kafka)
	- Message Queueing (Kafka, SQS...)
	- Coding and scripting languages (Perl, Bash, Python, Go...)
	- AWS Ecosystem (EC2, VPC, S3, DynamoDB, RDS...)
	- You have deployed and configured a wide range of AWS services including databases, networking, and security. In this role you will work with such paradigms and technologies as: ***12 factor app design principles***, Docker, Kubernetes, and ElasticSearch ecosystem
		* [Twelve-Factor App methodology](https://en.wikipedia.org/wiki/Twelve-Factor_App_methodology)
			+ 1. Codebase: There should be exactly one codebase for a deployed service with the codebase being used for many deployments.
			+ 2. Dependencies: All dependencies should be declared, with no implicit reliance on system tools or libraries.
			+ 3. Config: Configuration that varies between deployments should be stored in the environment.
			+ 4. Backing services: All backing services are treated as attached resources and attached and detached by the execution environment.
			+ 5. Build, release, run: The delivery pipeline should strictly consist of build, release, run.
			+ 6. Processes: Applications should be deployed as one or more stateless processes with persisted data stored on a backing service.
			+ 7. Port binding: Self-contained services should make themselves available to other services by specified ports.
			+ 8. Concurrency: Concurrency is advocated by scaling individual processes.
			+ 9. Disposability: Fast startup and shutdown are advocated for a more robust and resilient system.
			+ 10. Dev/Prod parity: All environments should be as similar as possible.
			+ 11. Logs: Applications should produce logs as event streams and leave the execution environment to aggregate.
			+ 12. Admin Processes: Any needed admin tasks should be kept in source control and packaged with the application.
	- Support build/deployment processes with eye towards improving our CI/CD pipeline
	- Help troubleshoot production issues and perform root cause analyses that create effective mitigation strategies
	- Design, implement, monitor, and scale self-service oriented infrastructure
+ skill set:
	- Confluence
	- Jira
+ skill set:
	- Infrastructure Engineer.
	- In this role you will be responsible of monitoring the constant quality of the infrastructure and corporate networks, by performing diagnostic tests and debugging procedures to optimize computing systems.
	- Analyze and suggest changes to test, dev and prod infrastructure
	- Ensure utmost uptime for entire network services and servers
	- Implement infrastructure security policies
	- Provide consultancy and support for teams with various/different levels of IT knowledge
	- Make policy recommendations for further implementations and development
	- Develop and maintain an IT sourcing strategy and ensure the correct provisioning of IT equipment. Solve problems using in-depth understanding of information systems and computing solutions.
	- Monitor constantly the quality of the infrastructure and support the management of network infrastructures.
	- Ensure data is stored securely and backed up regularly
	- Work on process documentation as well as backup and recovery procedures, data retrieval, network and infrastructure diagrams.
	- Research, design and recommend new approaches to improve the networked computer system
	- Expert about networks and their protocols
	- Proficient with both network hardware and technologies and also with shared storage technologies
	- Skilled in backup procedures for data storage and disaster recovery
	- Expert about cloud environments and infrastructures, in particular Microsoft Azure and Amazon AWS
	- You have a good knowledge about firewalls and virtualization systems like VMware, Hyper-V
	- Fluent in English written and spoken
	- You have a good attitude towards problem solving and team working
	- You have at least 5 years of experience in this role and a degree in Computer Science or Engineering
	- It would be nice if you would have knowledge of container systems like Docker and Kubernetes, and CloudFlare.
+ skill set:
	- Cloud & DevOps Engineer – Italy.
	- Define and monitor the deployment process
	- Deploy both on-premises by the customer and on our infrastructure
	- Offer first-level support and triage of issues reported by customers
	- Directly manage deployment and infrastructure issues
	- Coordinate second-level support, engaging with developers and other teams
	- A graduate in Computer Science, Computer Engineering or an equivalent title
	- Expert of Windows/Linux operating systems with basic systemic knowledge
	- Good in speaking and writing in English
	- DevOps methodology
	- Cloud Native Architectures (Docker, Kubernetes)
	- Monitoring/logging systems (ELK Stack, CloudWatch)
	- CI/CD tools (Gitlab, Jenkins)
	- IaC tools (Terraform, Ansible)
	- AWS ecosystem
	- Precision, dedication and diligence
	- Priority management while ensuring the output quality
	- Ability to organize your tasks according to the company’s processes and procedures
	- Curiosity towards new technology and willingness to learn
	- Team working will be essential since you will also support other colleagues
+ skill set:
	- Monitoring/logging systems, such as the ELK Stack (or Elastic stack) [WikipediaContributors2022b].
		* visualize application monitoring
		* visualize infrastructure monitoring
		* faiciliate troubleshooting
		* for security analytics
		* ***ELK Stack*** (or ***Elastic stack***):
			+ ***Elasticsearch***
				- distributed search and analytics engine
				- use schema-free JSON documents
			+ ***Logstash***
				- ETL (extract, transform, load)
					* ***data ingestion tool***
					* collect data from different sources
					* transform data
					* send data to desired destination
			+ ***Kibana***
				- data visualization of logs and events, exploration of new maintenance solutions
				- tool for creating interactive charts
+ skill set:
	- Experience with CI systems (Jenkins, TeamCity);
	- AWS - EC2, RDS, ECS etc;
	- Docker and orchestration: Swarm, Kubernetes;
	- Elastic Search, RabbitMQ;
	- Bash / Powershell scripting experience;
	- Windows / Linux - admin level;
	- Experience with SVN / GIT - as a user and as an infrastructure owner;
	- Experience with high loaded distributed multi-tenanted cloud systems. Including: Disaster recovering mechanisms; Monitoring and logging; Redundancy (data, network, apps);
	- Comfortable working with distributed teams
+ skill set:
	- As a DevOps Engineer at Simbe Robotics you will be part of a talented team ensuring quality in our software as well deploying & managing our cloud services and world-wide fleet of autonomous robots.
	- Has experience with automated build and continuous integration systems (e.g. Jenkins, TravisCI)
	- Has knowledge of application/system level monitoring (Nagios, CloudWatch, Munin, Splunk)
	- Experience with configuration management (Chef, Puppet, Ansible) tools
	- Has experience with various application packaging and deployment technologies (Debian packages, Docker/Linux containers)
	- Experience configuring web servers (e.g. Apache/Tomcat, nginix)
+ ***Infrastructure as code*** experience (we use terraform)
+ Familiar with orchestration components (Chef-Puppet-Ansible-Kubernetes-VSTS)
+ view DevOps as ***configuration as code***:
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef.
+ skill set:
	- Exposure to containers or orchestration services:  Kubernetes, Mesos, or Docker Swarm
	- Experience with ***configuration as code***; Puppet, SaltStack, Ansible, or Chef
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




















##	Site Reliability Engineering (SRE)





Layers of a pyramid for Site Reliability Engineering (SRE).
+ product (design and management)
+ (product/software) development
+ (infrastructure) capacity planning
+ (software) testing and release management
+ postmortem analysis, or root-cause analysis (RCA)
+ incident response
+ (cloud computing or online Web) service monitoring






+ skill set:
	- Experience monitoring cloud environments using tools like Splunk, VictorOps and Nagios
	- Knowledge of best practices related to security, performance, and disaster recovery.
	- Skilled in identifying performance bottlenecks, spotting anomalous system behavior, and determining the root cause of incidents.
+ skill set:
	- Senior Site Reliability Engineer
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Software Engineer - Infrastructure, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- Responsible for automating & operationalizing cloud provider infrastructure via ***Terraform*** as well as ***Kubernetes, Helm and Istio***
	- Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.
	- Own & operate critical back-end open-source services like Cassandra, Kafka, and Zookeeper.
	- Build tools and design processes that help improve observability and system resiliency.
	- Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.
	- Partner with service owners to implement service level metrics & service level objectives that act as service-level health indicators.
	- Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.
	- Coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code*** experience with in one or more of ***Terraform, Ansible, Puppet or Salt***.
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)
	- Strong working knowledge of orchestration engines and package management including ***Kubernetes, Helm, and Istio***
	- Experience operating one or more OSS technologies like ***Kafka, Cassandra, Zookeeper***; other backends and streaming systems a plus
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 8+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Principal Site Reliability Engineer, Splunk Observability - remote Spain
	- The Splunk Observability Suite is a new generation of cloud applications for microservices and distributed applications. We work on new, world-class tools to monitor and observe microservice-based applications. Site Reliability Engineers at Splunk are hybrid Software/Systems Engineers whose overarching goal is to ensure that production services are always up and running reliably.
	- As a Principal Site Reliability Engineer, you will help us run one of the largest and most sophisticated cloud-scale, big data systems in the world. You will be responsible for improving operational efficiency, optimal utilization and system resiliency for a real-time streaming analytics platform. You are passionate about automation, infrastructure-as-code, and getting rid of tedious, manual tasks.
	- ***Responsible for automating & operationalizing cloud provider infrastructure via Terraform, Kubernetes, Helm and Istio***
	- ***Monitor capacity & utilization and work closely with the infrastructure team to orchestrate scale-up/down of backend services.***
	- ***Own & operate critical back-end open-source services like Cassandra, Kafka, Elasticsearch, MongoDB, and Zookeeper.***
	- Build tools and design processes that help improve observability and system resiliency.
	- ***Triage site availability incidents and proactively work towards reducing MTTR for customer-impacting incidents.***
	- Implement service level metrics & service level objectives that act as service-level health indicators.
	- ***Establish design patterns for monitoring, benchmarking and deploying new features for the backend services.***
	- Strong coding experience in one or more of Python, Go or Java.
	- ***Infrastructure as code experience within one or more of Terraform, Ansible, Puppet or Salt.***
	- Strong experience with modern application development workflows and version control systems like GitHub, Gitlab or Bitbucket
	- ***Strong working knowledge of Docker containers and cloud platforms (AWS, GCP and/or Azure)***
	- ***Strong working knowledge of orchestration engines and package management including Kubernetes, Helm, and Istio***
	- ***Experience operating one or more OSS technologies like Kafka, Cassandra, Zookeeper; other backends and streaming systems a plus***
	- Extensive understanding of Unix/Linux systems from kernel to shell and beyond (system libraries, file systems, and client-server protocols).
	- 12+ years of experience as a Site Reliability Engineer, Production Engineer or Backend Software Engineer for web-scale or similar platforms.
	- BS degrees in Computer Science or related technical field, or equivalent practical experience.
+ skill set:
	- Site Reliability Engineer (Stack Automation Service Team)
	- Splunk's Cloud group is looking for an experienced Site Reliability Engineer to join a team that is responsible for Cloud’s operational infrastructure and delivery. As a member of the Stack Automation Service team, you will be responsible for maintaining and troubleshooting Splunk's SaaS system, monitoring system stability and performance, troubleshooting complex problems, performing Amazon instance maintenance and system upgrades, and managing Amazon server/storage deployments, all while collaborating with various other Splunk Cloud teams. This is a fantastic opportunity to work with an exceptional team, grow your cloud experience, and help drive the growth of Splunk Cloud.
	- Puppet experience. You have at least 2 years of Puppet experience, including writing Puppet code and configuration management.
	- Python or Bash scripting experience. You will develop scripts and tools in Python/Bash.
	- AWS experience. Knowledge of Amazon EC2 including machine image management and storage, as well as an understanding of Amazon EC2 regional centers, availability zones, and HA strategies
	- Unix/Linux. You will use a command line terminal frequently.
	- Multi-tenant infrastructure experience. Experience supporting customer facing multi-tenant infrastructure (SaaS) or similar cloud related services
	- Software Development and Data Structures/Algorithms. We code primarily in Golang and Ruby, and work with RESTful APIs.
	- Cloud and container experience. Building and scaling secure services on different cloud providers.
	- Knowledge of technical excellence. You know continuous delivery, testing, security practices, performance, and disaster recovery.
	- Problem Solving. You are able to fix a product outage, skilled in identifying performance bottlenecks, spotting anomalous system behavior, and figuring out the root cause of incidents.
	- Desire to learn and adapt. Our team has many projects going on at once, and you'll have the opportunity to learn to navigate new code and features.
	- Passion. We want you to actively own your work and be excited about your projects.
	- ***Kubernetes experience. Working in Kubernetes systems with experience in kubectl and docker containers.***
	- Terraform experience. Any prior work with Terraform is a plus.
	- ***Distributed programming. Experience in working on distributed systems like databases, distributed file systems, distributed concurrency control, consistency models, CAP theorem is an added plus.***
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.

















##	DevSecOps




DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term "shift left" can be used. Security is tested in three main areas: static, software composition, and dynamic.











+ skill set:
	- Senior Client Platform Engineer
	- Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
	- Are you the kind of systems engineer that has a passion for administering enterprise software using best of breed technologies? Are you self-motivated and require minimal supervision? Do you put together a rolling 12-month roadmap to execute against? If so, then this is the dream job you've been looking for. 
	- Splunk is looking for a highly skilled Senior Systems Engineer focusing on endpoint security, and configuration management. You should be comfortable delivering at a high level in a fast paced and growing environment. You will drive standardization and management for our endpoints along with a number of enterprise applications and services. This role provides high visibility and impact to both the CIO and CISO organizations.
	- Architecture, design, integration, implementation, operation, and support of enterprise-wide applications and services for our Windows fleet.
	- Assisting in developing long-term strategies and capacity planning for meeting future end user needs
	- Configuration Management for Windows using industry standard tools to meet Security requirements and comply with CIS benchmarks
	- Managing configuration of our endpoint security software such as endpoint detection and response, application allow/block lists, and host-based intrusion detection software
	- Partnering with the Security Engineering leads to coordinate efforts, initiatives, and roadmaps
	- Administer enterprise software including deployment and package management
	- Write scripts/policies that automate application and settings distribution using internal tools
	- Manage transition plans for major upgrades or patches
	- Integrate with other internal systems and tools
	- Manage and report on application performance against KPI’s
	- Work as the escalation point between various support teams for issues on the client platform
	- Work as a liaison from the Splunker Technology Success org to other IT Service organizations to deliver feature enhancements and best in class solutions through shared products and goals
	- Drive client security models and best practices in an enterprise environment
	- Drive business decisions through data using tools like Splunk
	- Diagnose and investigate unique and complex systemic problems
	- Develop solutions that meet the business needs to complex customer requirements
	- 10+ years of overall IT experience; 5+ years experience of providing application support and engineering
	- Experience with implementing security standards and compliance across a huge enterprise organization
	- Knowledge of bash/python scripting
	- Experience with Endpoint Management platforms such as WorkspaceONE/InTune/LANdesk/Kace/etc.
	- Experience with DevOps platforms such as Puppet/Salt/Chef
	- Ability to work in high pressure, highly flexible environment against both short and long term requirements
	- Passionate about technology and solving IT operations-focused problems
+ skill set:
	- Software Engineer - Analytics Platform (Remote)
	- Are you passionate about working on products that make a difference for your customers? Do you enjoy building large scale applications that are powered by huge data sets? Do you value working in an environment where you're empowered to make key technical decisions across a full stack of technologies? If so, a role on the Splunk Security Analytics team might be a great fit for you.
	- As a Software Engineer on the Security Content Engineering team, your primary focus is content distribution, content improvement, and content assurance. You will contribute to build and maintain the content distribution system and support content improvements.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify, and improve designs. Help put together a plan for how to make it happen using Agile Methodologies.
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best practices to ensure that the product is of high quality while continuously deployed in the cloud to our customers.
	- Participate in the software development lifecycle by writing code, tests, documentation; support the sprint management process; and communicate effectively with peers and managers.
	- 2+ years of Software Engineering experience.
	- Bachelor's degree in Computer Science or equivalent training and work experience.
	- Proficient with Java or Python programming.
	- Engagement with container ecosystems (Docker, Kubernetes, Kubernetes Operator Framework).
	- Exposure to an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab, Bitbucket).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Versed with CI/CD frameworks and experience with automation.
	- Strong oral and written communication skills, including a demonstrated ability to prepare documentation and presentations for technical and non-technical audiences.
	- Background in developing products for the Security market, a plus.
+ skill set:
	- Senior Software Engineer - Analytics Platform (Remote)
	- Enterprise Security behavioral analytics service (Advanced Analytics) is Splunk’s next-generation, cloud-native, multi-tenant analytics solution that detects known and unknown security threats at petabyte scale. Advanced Analytics detects cybersecurity threats by using stream and batch processing, and building large scale analytics infrastructure for petabyte scale data ingestion, processing, storage, and analysis. Advanced Analytics will power large and medium scale enterprises to combat security threats, protect brand reputation and protect intellectual property.
	- Achieve a deep knowledge of our product architecture, usage patterns, and real-world use cases to better understand what solutions will bring value to our customers.
	- Develop new product features, clarify and improve designs, and help put together a plan for how to make it happen (using Agile Methodologies).
	- Collaborate closely with Product Management and members of our team to design and create comprehensive end-to-end user workflows.
	- Keep aware of security trends in the industry and bring that knowledge back to the team.
	- Keep product quality top of mind by extensively using Continuous Integration/Continuous Development (CI/CD), testing technologies (unit, functional, performance), and best software development practices.
	- Champion, coach, and mentor others to solve problems in new and creative ways with the goal to maintain team efficiency and morale.
	- 8+ years of experience in Enterprise Software Engineering.
	- Bachelor's degree in Computer Science or another quantitative field. Other education and/or experience will be considered.
	- Programming experience with Java, C/C++, or Go.
	- Familiarity with streaming and distributed computing technologies such as Kafka, Pulsar, Flink, Spark, HBase, Cassandra, MongoDB.
	- Exposure to working with cloud environments (AWS, Azure, GCP) and container ecosystems (Docker and Kubernetes).
	- Knowledge of distributed computing architectures and principles that solve for scalability, consistency, availability, performance, and reliability.
	- Experienced with an Agile DevOps engineering environment that effectively uses CI/CD pipelines (Jenkins, GitLab).
	- Ability to learn new technologies quickly and to understand a wide variety of technical challenges to be solved.
	- Strong collaborative and interpersonal skills, specifically a proven ability to effectively work with others within a dynamic environment.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.












##	ITOps





ITOps (for data centers):
+ maintain core infrastructure and associated services needed to support AI and machine learning models running in business applications
+ enterprise AI role:
	- provide highly available, secure infrastructure to operate performant models in enterprise application to scale, 24x7
	- deliver performance metrics for deployed models and applications
	- plan and execute infrastructure evolution to support AI technologies
+ for ModelOps
	- centralized catalog of all:
		* models
		* model runtime requirements
		* lineage
		* operational history
	- regardless of tools to create AI and ML models, support standardized models on any infrastructure (prem, cloud, or hybrid) that can be consistently:
		* deployed
		* monitored
		* controlled
	- provide automated alerts regarding model performance and behavior
	- automated processes and approvals for:
		* deployment
		* testing
		* refresh
		* monitoring
	- real-time visibility to performance of deployed models
+ Information Technologies Operations (ITOps) is the process responsible for acquiring, designing, deploying, configuring, and maintaining the physical and virtual components that comprise your IT infrastructure
+ While ITOps takes a broad view of the entire technology landscape that your organization relies on to conduct its business mission, DevOps focuses on the task at hand.
	- DevOps teams don’t always have the visibility or awareness of the downstream implications on the enterprise stack.
	- And this is where can modern ITOps team can help.
+ ITOps (or) Tech Ops is the most traditional Ops that refers to managing all the physical and software components of an organization’s IT environment.
	- It is responsible for the smooth running of a business by handling applications, delivery, maintaining services, and the underlying technologies administrated by a company's IT members to its internal or external clients.
	- It includes:
		* maintaining networks
		* ensuring security
		* managing data center
		* system administrators
		* regulatory compliance
		* licensing
		* managing software
		* managing help desk
		* supports IT infrastructure and systems for daily business operations
	- IT department can effortlessly manage all the IT operations and make the job easier by using IT Operations.
		* It guides the business to be more secure, swift, and productive.
+ includes:
	- networking operations
	- deploying, maintaining, and configuring applications
	- overseeing both physical and virtual components of a company's IT environment
+ includes:
	- administration and maintenance,
	- network management,
	- systems management,
	- technical support (help desk)



ITOps processes consists of 5 main aspects/phase:
+ Run Solutions
	- The primary purpose of having ITOps teams is to run solutions such as data backups, configurations, handling servers, and restoring systems after an outage or update.
	- The aim is to optimize the performance and allocate the proper resource for the most effective delivery.
+ Infrastructure
	- Managing the IT infrastructure includes maintaining, provisioning, setting up, and updating all the software and hardware applications and cloud resources of the company's IT infrastructure.
	- These components include:
		* operating systems
		* hypervisors
		* network infrastructure
		* platforms
		* container environments
		* physical servers
		* application software
	- The maintaining oversight of IT infrastructure is made on-premises data center or in the cloud.
+ Network
	- Network management is responsible for managing all network functions for internal or external IT communications.
	- Network management is also involved in configuring and managing telecommunication lines.
	- In addition, it allows authorized customers to secure remote access to the company's network.
+ Security
	- Security management is an integral function of IT service management (ITSM).
		* It deals with:
			+ securing the hardware and software assets
			+ implementing security within development operations
			+ managing access control
			+ ensuring that security standards have reached across the IT environment of the organization
+ Problem Solving
	- Event management or incident management is also known as Problem-solving, it can be divided into 2 types – preventive measures and reactive measures.
		* Preventive Measures
			+ Preventive measures reduce the possibility of disasters and find the solution to anticipate and avoid any negative impacts on the IT environment.
		* Reactive Measures
			+ This concept refers to cyber-attacks, critical situations, and other issue when implementing disaster recovery plans and help desk management services.




ITOps tasks:
+ Network infrastructure:
	- Configuring and managing all networking functions for internal and external IT communications
	- Configuring and managing telecommunication lines
	- Managing firewall ports to allow the network to communicate with outside servers
	- Providing authorized users secure remote access to the organization’s network
	- Monitoring network health and performance, detecting anomalies, and preventing or quickly resolving issues, which may include building and managing a network operations center (NOC, pronounced “knock”), a centralized physical location from which ITOps teams can continuously monitor a network
+ Server and device management:
	- Configuring, maintaining and managing servers for infrastructure and applications
	- Managing network and individual storage to ensure they meet application requirements
	- Setting up and authorizing email and file servers
	- Provisioning and managing company-approved PCs
	- Provisioning and managing cell phones and other mobile devices
	- Managing licensing and desktop, laptop and mobile device software
+ Computer operations and help desk:
	- Managing data center locations and equipment
	- Operating the help desk
	- Creating, authorizing and managing all user profiles on organizational systems
	- Providing network configuration auditing information to regulatory agencies, business partners and other outside entities
	- Ensuring high availability of the network and disaster recovery plans
	- Alerting users when a major incident impacts network services
	- Instituting regular backups to facilitate data recovery when needed
	- Maintaining the ITIL for the organization








ITOps addresses:
+ infrastructure capacity
+ infrastructure availability
+ infrastructure security


KPIs for ITOps are based on:
+ application performance
+ infrastructure availability
+ infrastructure security
+ infrastructure cost




*Information Technology Infrastructure Library, ITIL*:
+ functions of *IT Operations Management framework*
	- ITOps, or IT operations, refers to the processes and services administered by an organization's IT staff to its internal or external clients.
	- applications management
	- technical management
	- service desk







+ skill set:
	- 4+ years of experience in developing, planning, and and administering VMware ESXI and vCenter deployments
	- Experience with vSphere, vSAN, and other VMware products and platforms
	- Experience with troubleshooting for virtualized platform
	- Ability to provide and manage virtual machine configurations, including networking, storage and security settings and deploy it to hosts upon request
	- Knowledge of data center networking, including TCP/IP, switching/routing, ports and protocols, firewall concepts, or load-balancing
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Knowledge of data center architecture: power, cooling, and networking.
	- Significant experience working with data center hardware and writing software to make that easier, faster, and less manual effort
	- Familiarity with best practices for hardware acceptance testing






















##	 IT operations analytics, ITOA


IT operations analytics (ITOA) is an approach or method to retrieve, analyze, and report data for IT operations.
+ Or, known as:
	- advanced operational analytics
	- IT data analytics
+ ITOA may apply big data analytics to large datasets to produce business insights.
+ ***ITOA is different than AIOps, which focuses on applying artificial intelligence and machine learning to the applications of ITOA.***
+ The use of mathematical algorithms and other innovations to extract meaningful information from the sea of raw data collected by management and monitoring technologies.



Context:
+ (IT) systems management
	- Fault, Configuration, Accounting, Performance, Security (FCAPS)
	- Distributed Management Task Force (DMTF)





applications of ITOA systems:
+ Root cause analysis:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored can help users pinpoint fine-grained and previously unknown root causes of overall system behavior pathologies.
+ Proactive control of service performance and availability:
	- Predicts future system states and the impact of those states on performance.
+ Problem assignment:
	- Determines how problems may be resolved or, at least, direct the results of inferences to the most appropriate individuals or communities in the enterprise for problem resolution.
+ Service impact analysis:
	- When multiple root causes are known, the analytics system's output is used to determine and rank the relative impact, so that resources can be devoted to correcting the fault in the most timely and cost-effective way possible.
+ Complement best-of-breed technology:
	- The models, structures and pattern descriptions of IT infrastructure or application stack being monitored are used to correct or extend the outputs of other discovery-oriented tools to improve the fidelity of information used in operational tasks (e.g., service dependency maps, application runtime architecture topologies, network topologies).
+ Real time application behavior learning:
	- Learns & correlates the behavior of Application based on user pattern and underlying Infrastructure on various application patterns, create metrics of such correlated patterns and store it for further analysis.
+ Dynamically baselines threshold:
	- Learns behavior of Infrastructure on various application user patterns and determines the Optimal behavior of the Infra and technological components, bench marks and baselines the low and high water mark for the specific environments and dynamically changes the bench mark baselines with the changing infra and user patterns without any manual intervention




Types of ITOA:
+ Log analysis
+ Unstructured text indexing, search and inference (UTISI)
+ Topological analysis (TA)
+ Multidimensional database search and analysis (MDSA)
+ Complex operations event processing (COEP)
+ Statistical pattern discovery and recognition (SPDR)

















+ skill set:
	- experience with large-scale distributed storage and database systems
		* SQL
		* NoSQL
		* MySQL
		* Cassandra
	- data processing experience with building and maintaining large-scale and/or real-time complex data processing pipelines using:
		* Kafka
		* Hadoop
		* Hive
		* Storm
		* Zookeeper
	- experience with developing complex software systems scaling to substantial data volumes or millions of users with production quality deployment, monitoring, and reliability
	- experience running scalable (thousands of RPS) and reliable (three 9's) systems
+ Experience with monitoring and tracking tools such as Splunk, NewRelic, Adobe/Google Analytics
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.




##	 Artificial Intelligence for IT Operations, AIOps



Artificial Intelligence for IT Operations (AIOps) is an industry category for machine learning analytics technology that enhances IT operations analytics.


Goals of AIOps:
+ enable IT transformation
+ receive continuous insights that provide continuous fixes and improvements via automation



AIOps can be viewed as CI/CD for core IT functions.
+ Given the inherent nature of IT operations, which is closely tied to cloud deployment and the management of distributed applications, AIOps has increasingly led to the coalescence of machine learning and cloud research.



main aspects of an AIOps platform:
+ machine learning
+ big data

In order to collect observational data and engagement data that can be found inside a big data platform, which requires a shift away from sectionally segregated IT data, a holistic machine learning and analytics strategy is implemented with the combined IT data.





AIOps tasks include:
+ automation
+ performance monitoring
+ event correlations



AIOps process:
+ The *normalized data* is suitable to be processed through machine learning algorithms to *automatically reduce noise* and *identify the probable root cause of incidents*.
	- The main output of such stage is the ***detection of any abnormal behavior from users, devices or applications***.
+ Noise reduction can be done by various methods, but most of the research in the field points to the following actions:
	- Analysis of all incoming alerts;
	- Remove duplicates;
	- Identify the false positives;
	- Early *anomaly, fault and failure* (AFF) detection and analysis.[13]
+ ***Anomaly detection*** - another step in any AIOps process is based on the analysis of past behavior of users, equipment and applications. Anything that strays from that behavior baseline is considered unusual and flagged as abnormal.
+ ***Root cause determination*** is usually done by passing incoming alerts through algorithms that take into consideration correlated events as well as topology dependencies. The algorithms on which AI are basing their functioning can be influenced directly, essentially by "training" them.





AIOps platforms enabling IT operations management (ITOM)
+ inputs:
	- historic data
	- real-time streaming data
	- vendor-agnostic data ingestion
+ input types:
	- logs
	- metrics
	- wire data
	- document text
+ observe (monitoring)
+ act (IT Service Management process, ITSM)
+ engage (monitoring)
+ machine learning with *Big Data*
+ outputs:
	- historical analysis
	- anomaly detection
	- performance analysis
	- correlation and contextualization


Factors of IT maturity:
+ organizational structures
+ processes and practices
+ skills and knowledge,
+ tools and policies
+ systems and data
+ documents and agreements



cause-and-effect sequence
+ outcomes
+ capabilities
+ IT maturity
+ IT excellence
+ business maturity, innovation, and productivity




Role of IT suppert services:
+ aware
	- The organization is aware of its chaotic stage and needs.
	- IT capabilities are unstable and success depends on the individuals' effort and technical knowledge.
+ committed
	- IT operations are more process-oriented, thus more reputable.
	- Success depends on process adherence and point collaboration.
	- This level is a stable plateau where IT can "keep the lights on."
+ proactive
	- IT organization is recognized as "mature."
	- At this level, IT has reached a tipping point from which a path to high IT maturity is accessible.
+ aligned
	- IT is a highly efficient internal service provider, offering a stable portfolio of optimized services.
+ business partner
	- IT is a trusted partner and innovator for the business












 










Applications of AIOps:
+ analysis of large and unconnected datasets, or large number of un-normalized databases
	- including aggregated data
+ Automation of tasks (DevOps)
+ Machine learning platforms
+ Augmented reality
+ Agent-based simulations
+ Internet of things (IoT)
+ AI Optimized Hardware
+ Natural language generation
+ Streaming data platforms
+ Conversational BI and analytics













+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.










##	CloudOps



CloudOps include:
+ cloud-specific flexible provisioning and scalability of environments,
+ built-in task automation.













##	NoOps



NoOps (or) No Operation is the new idea that completely automates a software environment from the underlying system infrastructure through technologies including machine learning (ML), and Artificial Intelligence (AI). As a result, there is no need for any operation team to manage software in-house.

With NoOps, developers can concentrate solely on writing and improving the software product’s code that improves the resources like management, security, infrastructure, product, and operations part of the lifecycle. Additionally, the service providers offer developers to develop software like resources, backups, patches, and the right cloud infrastructure to work independently without any interference.

***Serverless architecture*** is the best example for NoOps software. The developer’s team aims to create their application and deploy them in ***serverless computing*** without interfering any operational or infrastructure considerations.

Operating the right tools in NoOps can achieve a faster deployment process than DevOps by running Platform as a Service (PaaS) or Function as a Service (FaaS) in the cloud. Moreover, NoOps can easily be adaptable for Product as a Service companies, small-scale applications, and start-ups.





##	Software Intelligence


Software intelligence:
+ [get] ["insight into the structural condition of software assets produced by software designed to analyze database structure, software framework and source code to better understand and control complex software systems in Information Technology environments"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["set of software tools and techniques for the mining of data and software inner-structure"](https://en.wikipedia.org/wiki/Software_intelligence)
+ ["End results are information used by business and software stakeholders to make informed decisions,[citation needed] measure the efficiency of software development organizations, communicate about software health, prevent software catastrophes."](https://en.wikipedia.org/wiki/Software_intelligence)



["Software intelligence is derived from"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Software composition is the construction of software application components. Components result from software coding, as well as the integration of the source code from external components: Open source, 3rd party components, or frameworks. Other components can be integrated using application programming interface call to libraries or services."
+ "Software architecture refers to the structure and organization of elements of a system, relations, and properties among them."
+ "Software flaws designate problems that can cause security, stability, resiliency, and unexpected results. There is no standard definition of software flaws but the most accepted is from The MITRE Corporation where common flaws are cataloged as Common Weakness Enumeration."
+ "Software grades assess attributes of the software. Historically, the classification and terminology of attributes have been derived from the ISO 9126-3 and the subsequent ISO 25000:2005 quality model."
+ "Software economics refers to the resource evaluation of software in past, present, or future to make decisions and to govern."





["The capabilities of Software intelligence platforms include an increasing number of components"](https://en.wikipedia.org/wiki/Software_intelligence):
+ "Code analyzer to serve as an information basis for other Software Intelligence components identifying objects created by the programming language, external objects from Open source, third parties objects, frameworks, API, or services"
+ "Graphical visualization and blueprinting of the inner structure of the software product or application considered including dependencies, from data acquisition (automated and real-time data capture, end-user entries) up to data storage, the different layers within the software, and the coupling between all elements."
+ "Navigation capabilities within components and impact analysis features"
+ "List of flaws, architectural and coding violations, against standardized best practices, cloud blocker preventing migration to a Cloud environment, and rogue data-call entailing the security and integrity of software"
+ "Grades or scores of the structural and software quality aligned with industry-standard like OMG, CISQ or SEI assessing the reliability, security, efficiency, maintainability, and scalability to cloud or other systems."
+ "Metrics quantifying and estimating software economics including work effort, sizing, and technical debt"
+ "Industry references and benchmarking allowing comparisons between outputs of analysis and industry standards"



It is used as part of ["Application Portfolio Analysis (APA)"](https://en.wikipedia.org/wiki/Software_intelligence).

















#	References




	@misc{WikipediaContributors2022b,
		Address = {San Francisco, {CA}},
		Author = {{Wikipedia contributors}},
		Howpublished = {Available online from {\it Wikipedia, The Free Encyclopedia: Web frameworks} at: \url{https://en.wikipedia.org/wiki/Solution_stack}; July 8, 2022 was the last accessed date},
		Month = {June 21},
		Publisher = {Wikimedia Foundation},
		Title = {Solution stack},
		Url = {https://en.wikipedia.org/wiki/Solution_stack},
		Year = {2022}}