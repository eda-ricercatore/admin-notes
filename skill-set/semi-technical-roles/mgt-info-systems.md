#	Management of Information Systems, MIS


##	Notes about MIS


Goals of MIS:
+ facilitate decision making
+ coordination, control, analysis, and visualization of information in an organization


Types of MIS include:
+ decision support systems, DSSs
+ executive information systems, EIS
+ marketing information systems
+ accounting information systems
+ human resource information systems
+ office automation systems, OAS
+ school information management systems, SIMS
+ enterprise resource planning, ERP
+ local databases
	- primal or base-level version of MIS




Enterprise applications of MIS:
+ customer relationship management, CRM
+ DLP, data loss prevention software
	- ILDP, information leak detection and prevention
	- ILP, information leak prevention
	- CMF, content monitoring and filtering
	- IPC, information protection and control
	- EPS, extrusion prevention system
	- IPS, intrusion prevention system
	- IDS, intrusion detection system
		* NIDS, network intrusion detection system
		* HIDS, host-based intrusion detection system
+ enterprise resource planning, ERP
	- or, enterprise systems
+ knowledge management system, KMS
+ supply chain management, SCM






Technologies and skills for MIS:
+ ITIL, Information Technology Infrastructure Library
	- Services: The principal concept of ITIL is that it implies that the customers will be provided with all the requisite levels of satisfaction and commitment without any loss or investment on the part of the customers.
	- Service Assets: The assets literally mean the resources. The resources imply the infrastructure and the facilities the providers can provide to both the business and the customers.
	- Service Management: Managing the service efficiently and delivering the most to the customers for the ultimate level of popularity is what service management implies in the concept of ITIL.
	- Utility, Warranty, and Value: Now comes the crucial part. The value-based integrity, which governs the principal concept of ITIL, is the central tenet that will help you go for quality service and eliminate all the bugs and constraints affecting the service's whole utility to the customers. With the assurance of the quality facility, provisions must also be made about the warranty that will ensure the reliability and guarantee of the functioning.
	- Processes: Coming to the next point, you need to see that every set of programs or entity requires efficient governing. These practical sets of rules are known as processes that are used to achieve a specific target or goal.  Processes can be used to deliver the desired outcome to both company stakeholders and clients. They can make a valid transition of the input to the targeted output. While achieving the target, you must always measure the process and see if it can be pulled out regardless of any particular events.
	- Functions: Functions are usually the medium by which the group of employees and the high-end programs are used to follow up the processes that are used to make up the operations efficiently.
	- Roles: The responsibilities that are entrusted to each set of entities are that specific people will play in the execution of the project.
	- Capabilities: Lastly, the whole system jots down the specified abilities and the achievement that a set of entities can achieve so that the role and responsibilities can be efficiently distributed according to that.
	- Service Strategy: The first and foremost step of the whole process of the ITIL framework is to chalk out a strategy. The development of the plan is the key to determining the functioning of the entire system, which includes all the facilities and provisions that the company can offer to the organization and client as a whole. It will also help you know about the errors and updates that need to be done to improve the function of the IT organization and go in an imperative manner.
	- Service Design: The whole plan is to go for the service and design the latest version of the Information Technology service.  This implies the architecture of the program, which traces up the entire new Information Technology function and how you can incorporate the improvisation in the more original service.
	- Service Transition: After planning and designing, you now need to finally configure, install, organize and install the Information Technology service that will ultimately help you to coordinate in an organized manner.
	- Service Operation: Now comes the testing and functioning part. The services that were developed in the previous processes will help you to know about the working of the process, that in turn will help you to see if it is working or not. The process consists of granting the request of the users, servicing the errors, and then carrying out the task that was meant to be completed.
	- CSI or Continual Service Improvement: Now, while the whole process is fully operational, you can find out the scope that you need to go for the continual improvement of the service that will do the service more and better. The stage aims to improve the service and deliver the performance with precision continuously. You need to go for continuous improvement by constant monitoring of the scenes mentioned above.
	- Delivering maximum value to customers
	- Optimizing resources and capabilities
	- Offering services that are useful and reliable
	- Planning processes with specific goals in mind
	- Defining roles clearly for each task.






















##	Skill Sets for MIS


Skill set for management of information systems, MIS, including data analyst roles:
+ ***Refactor SQL code based on our best-in-class, in-house style guide.***
+ GraphQL experience is a plus
+ Knowledge of integration of other enterprise systems (e.g., ERP, EAM, CRM, SCM)
+ database systems:
	- PostgreSQL database management system
+ Interact with MySQL data stores and NSQ messaging queues.
+ data analyst skills:
	- A working knowledge of SQL, specifically involving coding your own queries and running programs on platforms such as Qubole
	- Experience with marketing tools such as Tableau, Sendgrid, Wordpress, Instapages, and PyCharm preferred
+ Senior Data Integration Solution Architect
	- Join us as we pursue our disruptive new vision to make machine data accessible, usable, and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most importantly, to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!
	- As a Senior Solution Architect within the Enterprise Architecture group, you will play an important role in the success of our data integration strategy that directly impacts critical internal business processes that run our $3B+ and growing business. You will be responsible for defining the overall solution and technical architecture for our data integration platform that meets our vision for a Programmable Enterprise, while providing data integration solutions and expertise for a variety of complex internal business scenarios across the enterprise. You will need to apply your deep knowledge of ​data integration technologies, expertise with integration design patterns, and business application integration experience. To be successful you will have to balance priorities, collaborate with senior leaders, and present to executives while delivering within an agile delivery framework to meet key performance indicators.
	- This position requires you to be a self-starter with the ability to take ownership, prioritize, and handle various tasks simultaneously while maintaining a positive demeanor. In addition, strong verbal communication and written documentation skills are a must for this role for promoting ideas throughout the business to both technical and non-technical audiences.
	- Lead the overall technical strategy and solution design for a robust and scalable data integration and API platform
	- Facilitate the evaluation and selection of integration tools and technologies to meet business needs and demands
	- Define merger and acquisition data integration and migration strategies
	- Perform proof-of-concepts (POC) to prove out solutions and establish a core codebase ready for implementation
	- Deliver as-is and to-be application integration solution architecture for a variety of scenarios to meet business requirements and scale the company
	- Document data flows in relevant segments and ratify data ownership and retention policy with appropriate partners
	- Deliver high-quality deployment architecture diagrams that describe the solution and implementation
	- Participate in organization-wide project planning activities with project and product managers to guide on integration implementation strategies and cross-team dependencies
	- Partner closely with project managers, product managers, and engineering managers to define roadmaps and sprint planning objectives to meet solution designs
	- Represent the IT organization as the expert for data integration and API reference architecture and ensure alignment across partners
	- Collaborate with the broader architecture community, such as Architectural Review Board (ARB) and Enterprise Architects Council (EAC), to establish strategies and standard practices
	- Create executive presentations and present simple solutions to sophisticated problems to diverse partners across the company
	- 10+ years of demonstrable technical integration experience within IT application development environments
	- 8+ years of solution and technical architecture experience within the middleware industry to meet strict security and compliance requirements
	- Experience designing technical integration solutions for Salesforce, SAP S4/HANA, and Workday
	- Deep experience with a variety of middleware and data integration technology solutions, such as TIBCO, Boomi, Jitterbit, Oracle SOA, IBM WebSphere, Informatica, and/or Matillion
	- Familiarity with enterprise architecture frameworks such as Zachman and/or TOGAF
	- Demonstrated knowledge of Service Oriented Architecture (SOA) concepts and practices
	- Deep understanding of data integration and API concepts, patterns, and technologies
	- Demonstrable experience working in an Agile software development environment
	- Excellent both verbal and written communication skills to convey abstract and complex concepts
	- Travel up to 25% between Splunk offices locations
	- TOGAF 9 certification
	- Experience crafting high availability, publish and subscribe systems
	- Experience with AWS Application Integration and Compute products
	- BA, BS, or MS in Computer Science/Information Systems/Business or equivalent experience required
	- (Colorado only\*) Minimum base salary of $135,000.00. You may also be eligible for incentive pay + equity + benefits. \*Note: Disclosure per sb19-085 (8-5-201 et seq).
+ Experience in operations or cloud service environments and related partner platforms
	- EDR, endpoint detection and response, or endpoint threat detection and response
	- XDR, extended detection and response
		* NTA, network traffic analysis
	- CSPM
		* cloud security posture management
	- SEIM
		* security event and incident management
			+ Or, SIEM, security incident and event management
		* SEIM = security information management + security event management
			+ SEIM = SIM + SEM
		* log management
		* MSS, managed security service
		* MSSP, managed security service provider
		* SECaaS, security as a service
+ You have familiarity with any query language like SQL, SPL etc.
+ Exposure to ***MariaDB*** or other RDMS
+ Extensive experience with one or more of the follow frameworks. (Spark, Druid, Hadoop, HBase, Kafka)
+ Familiarity with GraphQL and Relay
	- [***Relay***](https://relay.dev/)
		* GraphQL client that scales with the database or data set.
		* Includes *incremental compiler* that performs *automatic optimizations*.
		* Supports data consistency.
+ skill set:
	- Experience building or maintaining databases (MySQL, Hive, etc.)
	- Experience building or maintaining Big data & streaming systems (Hadoop, HDFS, Kafka, etc.)
	- Cross-platform coding
	- Large-scale, large-user base website development experience
	- Data mining, machine learning, AI, statistics, information retrieval, linguistic analysis
+ [Sphinx](https://en.wikipedia.org/wiki/Sphinx_(search_engine))
	- search engine for MySQL family of database management systems (DBMS).
+ AWS DynamoD
+ Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (Redis, Cassandra or DynamoDB)
+ skill set:
	- Experience managing enterprise monitoring solution; System Center Operations Manager (SCOM), Solarwinds, and/or CA UIM preferred
	- Experience managing server automation tool and server patching tool; System Center Configuration Manager (SCCM) preferred
	- Experience creating or modifying scripts or automation, such as Perl, PowerShell, Python, TCL/TK, Ruby or similar for cloud orchestration required
+ skill set:
	- PostgreSQL
	- MongoDB
	- Prometheus
		* open-source monitoring system and time series database
		* dimensional data model
		* flexible query language
		* efficient time series database
		* modern alerting approach
	- Kubernetes
+ skill set:
	- This person will assist in developing data migrations, writing SQL and reports and mentoring other engineers in optimizing and writing efficient queries.
	- Develop and proactively review the monitoring of production PostgreSQL databases
	- Participate in system capacity planning
	- Participate in database design, data modeling and provide recommendations for improvement or optimizations
	- Provide query / index optimizations
	- Participate in an agile software development life cycle including providing testing guidelines for database related changes
	- Provide SQL development support and query tuning
	- Mentor other engineers in developing efficient SQL queries
	- Follow the Quality Management System for developing and deploying software
	- Write reports
	- 3+ years experience managing a production RDBMS including experience with PostgreSQL
	- Experience in SQL development and database design
	- Expertise in SQL DML and DDL
		* DML, Data Manipulation Language
			+ SELECT
			+ INSERT
			+ UPDATE
			+ DELETE
			+ MERGE
			+ CALL
			+ EXPLAIN PLAN
			+ LOCK TABLE
		* DDL, Data Definition Language
			+ CREATE
			+ ALTER
			+ DROP
			+ TRUNCATE
			+ COMMENT
			+ RENAME
		* DCL, Data Control Language
			+ GRANT
			+ REVOKE
		* DQL, Data Query Language
			+ SELECT
		* TCL, Transaction Control Language
			+ COMMIT
			+ SAVEPOINT
			+ ROLLBACK
			+ SET TRANSACTION
			+ SET CONSTRAINT
		* Procedural language (for certain SQL databases).
		* Debugging statements (for certain SQL databases).
	- Have a solid understanding of query planning
	- Understand PostgreSQL tuning and optimization parameters
	- Excellent interpersonal and communication skills in both oral and written English
	- Able to collaborate with cross functional team members
	- Familiarity with PostgreSQL replication techniques, data warehouse design, and Amazon RDS support beneficial
+ Automate DB (Oracle, Postgres, MongoDB, ...) configuration, deployment, backups, ...
+ Experience with at least one of: Oracle, Postgres, MongoDB, Solr
	- [Solr](https://solr.apache.org/)
		* "Solr is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene™."
		* "Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more."
+ skill set:
	- Familiarity with:
		* Apple Business Manager
		* Jamf
		* InTune
	- experience driving to SLA and SLO metrics, and improving results
	- vendor management experience both in purchasing realm and contractors
		* contractor management
+ skill set:
	- experience in HRIS/ERP/SAP administration/development
	- experience in:
		* CI/CD
		* GitHub Action
		* Jenkins administration/development
	- experience in endpoint management:
		* Jamf Pro
		* InTune
		* Puppet/Chef
	- experience with supporting/administration:
		* corporate applications
			+ Oracle Financials
			+ Concur
			+ SuccessFactor, Successfactors
		* endpoint management systems
			+ ServiceNow
			+ Jamf
			+ InTune
		* engineering platforms
			+ Slack
			+ Okata
			+ Azure AD
			+ Google Workspace
			+ Confluence/Jira
			+ GitHub
		* internal HR corporation system
			+ SuccessFactor
			+ Lever
			+ DocuSign
	- ITSM, IT service management
+ OpenID Connect, OIDC, OIDC authentication, OIDC identity providers, OAuth
+ experience with database analysis:
	- AWS
	- SQL
	- Elastic Search
	- Kibana, a data visualization dashboard software
+ skill set:
	- PostgreSQL
	- ElasticSearch
	- MongoDB
+ SQL working experience (Redshift/PostgreSQL/MySQL)
+ Experience wrangling very large datasets by writing and maintaining data processing pipelines with Hadoop, Spark, BigQuery, Redshift, or similar
+ Experience with MPP databases, such as Snowflake, Redshift, BigQuery, Vertica, etc.
+ all types of data stores - NoSQL, wide column, Graph, triplestores
+ Hands-on experience with NoSql databases and Big Data processing system
+ Familiarity with a range of database solutions, e.g. Redis, NoSQL, PostgreSQL and general storage solutions like S3 and R2
+ Strong programming capabilities in SQL and other DB technologies– Stored Procedures, Functions, Triggers, Views, Transactions, Data Flow, etc
+ skill set:
	- Tenstorrent is growing and we are looking for an IT Systems & Support Administrator for our Austin office. You will be our Austin IT Lead to support daily activities and work on cross site projects.
	- Deployment, maintenance, support and monitoring of our infrastructure in the Austin office
	- Be the onsite IT presence, therefore support and troubleshoot local technical issues from laptops to AV equipment, Administration of local and in part of entire network wide CentOS and Ubuntu Linux systems; systems security tasks including security scanning, patching and systems hardening
	- Configure, add and maintain laptop, server, and network inventory
	- Partnership across sites on various IT projects
	- Other task and duties as determined from time to time
	- This role will have a physical as well as remote administration and support aspect.
	- Degree/diploma in Computer Science or demonstrated combination of education and experience
	- 2 plus years of Systems Administration experience in a small to mid-size company using Linux (CentOS/Redhat) with a Linux+ certification
	- 2 plus years of installation and maintenance of enterprise level hardware, small deployments count provided its standard equipment
	- Network management, both physical infrastructure cabling and switches/firewall - 1 year experience is preferable
	- Experience with Veeam Back-ups or similar tools, managing backup & restoration implementation and testing
	- Proficient with SNMP/Nagios/Managed Engine based Monitoring, ElasticSearch, Jamf and Grafana
	- Worked with firewalls (Fortinet) and Linux based firewall (IPTABLES/IPCHAINS)
	- Strong documentation abilities necessary to ensure timely problem resolution
	- Experience with container orchestration systems (such as Kubernetes, Docker, Amazon ECS, EKS, Rancher)
	- Skilled at managing cloud-based environments in AWS and Azure
	- Proven ability to work autonomously with limited direction and oversight
	- Network Administration with CCNA certification
	- Strong technical problem-solving skills and experience in IP networking and static routing FTP, SSH, SMTP, DNS, HTTP/S, DHCP
	- Python and other scripting languages such as bash, Ruby etc (Nice to have)
	- Intermediate knowledge of TCP/IP including the usage of stateful and stateless firewalls, access control lists, packet capture and logging, network design, Subnetting, IP routing, IPSEC virtual private networks, and intrusion detection and prevention
	- Worked with automation/configuration management tools using Ansible, Chef or an equivalent an asset
	- Mac support and experience with Jamf is preferable
+ skill set:
	- We are looking for a Data Analyst Intern to \#JoinTheBand and support our Tech Learning team. As a part of our Collaborative Learning Product Area, The Tech Learning team creates peer-to-peer learning experiences that upskill and reskill Spotify employees in technical domains. We are looking for a Data Analyst Intern to consume, understand, and interpret data across our programs and create tools to better understand topics like customer demand and customer learner journeys.
	- Perform analyses on large sets of data to extract practical insights that will help drive decisions across the business
	- Build dashboards and recurring reporting results, empowering creative growth and business decisions
	- Communicate data-driven insights and recommendations to key collaborators
	- Work closely with the Tech Learning team, Data Scientists, and other key stakeholders across disciplines
	- You are pursuing a Bachelor’s, Master’s degree, or bootcamp certification in Data Science, Computer Science, Statistics, Economics, Mathematics, or a similar quantitative subject area
	- You have a graduation date of 2023 or later
	- You currently have valid work authorization to work in the country in which this role is based that will extend from June to August 2022
	- You harbor a passion for numbers and the use of data to make decisions
	- You have the technical competence to perform more analytics in one or more of the following areas:Coding skills (such as Python, Java, or Scala)Analytics tools experience (such as Bigquery, SQL, or Tableau)
	- Experience and passion for performing analysis with large datasets
+ skill set:
	- At ApertureData, we are on a mission to solve data infrastructure challenges for machine learning on big-visual-data through our unique visual database, ApertureDB. We are an angel and NSF grant backed, fast growing startup looking for a Database Architect with experience designing and building database internals. If you think locking and logging have a lot more to them than MVCC or redo, connected graph-like queries excite you, and being among the first five hires fires up your imagination on what all hats you get to wear, we are looking forward to hearing from you!
	- Minimum qualifications
		* Databases serve a vital purpose particularly as we get to define them for unstructured data. That requires a certain set of skills to even start off.
		* 5+ years of experience in Computer Science, or a related technical field
		* 2+ years of experience in C++
		* Understand concurrency and ACID implementations well
		* Systems level data structure and algorithm effects (kernel and driver level included)
		* Data structure and query optimization techniques
		* Different consistency models
		* Valid work status in the US
	- Additional qualifications
		* It would be great if you already came to us with a few more tricks up your sleeve.
		* Be comfortable with JSON, Python, Git, and Linux
		* Object mapper interfaces
		* Indexing vector data types
		* Understand the effects of cache/memory/disk as they interplay with each other and processing
		* Experience architecting a system to run as a service independent of the cloud vendor
+ skill set:
	- The Eon Data Analyst is responsible for the planning, execution and closure of all assigned deliverables. The Data Analyst will work to accomplish both software implementation as well as support efforts. The Data Analyst will be responsible for a wide range of both technical and customer facing tasks, however the end state of every transaction should be overall customer satisfaction.
	- Primary Responsibilities:
		* Manage all assigned tasks to ensure on time delivery and complete execution.
		* Foster quality relationships with customer contacts (end user, technical, support etc) as well as internal resources to ensure efficient and complete delivery of assigned tasks.
		* Must adopt a customer centric approach to all assigned tasks, ensuring the end state of every effort is a satisfied customer.
		* Participate in and contribute to all necessary implementation, support, process improvement and team meetings to ensure every opportunity for coordination is leveraged and all necessary communication is made.
		* Ensure all customer sourced issues are routed via the proper process in a timely manner with complete information ensuring a positive outcome for the customer.
		* Ensure all assigned implementation tasks are executed completely within the assigned time frame.
		* Own all HL7 data mapping efforts and processes ensuring that all implementations are delivered on time.
		* Own coordination and communication with customer resources in regard to all implementation deliverables as well as customer sourced issues.
		* Ensure responsive communications via any necessary means ie Intercom, JIRA and email.
		* Responsible for advocating for the customer in the overall implementation and issue resolution processes.
		* Develop and monitor/control a detailed plan for assigned efforts ensuring a critical path to delivery whenever possible.
		* Report and escalate to management as needed.
		* Perform all other duties as assigned.
	- Qualifications and Experience
		* Bachelor’s Degree Required
		* Healthcare experience preferred
		* 1 year of experience working in data mapping preferred
		* Outstanding interpersonal skills are preferred
	- Primary Performance Measurables
		* Customer satisfaction scores averaging above 90%
		* Issue resolution averaging less than 5 days
		* Time to value of all implementation is averaging 70 days or less
+ skill set:
	- In this data explosion age, massive scale transaction processing, analysis, and management of data are the core activities of almost all businesses. Envisioning the importance of Database as a Service (DaaS) in cloud, the Seattle database team is rapidly expanding, and now inviting talented candidates to join the team to re-innovate, re-shape and re-define the best Database for the future.
	- Our ideal Senior Staff Database engineer will participate in next generation database plan development, aiming to secure database technologies in the leadership position. We are luckily in the eve of new era of databases: you will identify core and important features for future database and realize them with state-of-art technologies and contribute to the long-term engineering teams’ growth. You are the idea booster and code keeper: you can demonstrate your finest ideas with POC code, or you develop solid code for new features in production quality. With Futurewei’s full stack research capacity, there is no limit of your innovation: you can work on any components.
	- We have opening in three major database components:
		* Query Optimizer: framework, inline/un-inline CTEs, extend relational algebra, ML based CE/costing, etc.
		* Query Executor: codeGen/vectorization, approximation, provenance, HW-acceleration, etc.
		* Storage Engine: scalable ACID, online DDL, schema relaxation, storage organization, etc.
	- You are highly interested in hands on experience of database engines, distributed systems
	- Good at the following languages: C/C++/C#
	- Solid knowledge in Database system, including OS, storage and network
	- Experience in online product or service development.
	- Experience in relational databases engines, NoSQL datastores
	- Experience in core database components, including optimizer, executor or storage engine
+ skill set for Database Kernel Technology Researcher:
	- Research optimization technologies, new algorithms, and new hardware of database kernels, and apply them to the design and delivery of cloud databases to lead the industry.
	- Computer science or related major. Proficient in C/C++ programming on Linux. Profound theoretical understanding of databases and good knowledge of the latest database architecture and research
	- Proven research background in database kernel architecture and algorithms, and related experience
	- Proficient in kernel code implementation of one of an open-source database, such as MySQL, PostgreSQL, or NoSQL.
+ skill set:
	- The Center for Policing Equity (CPE) is looking for a skilled Data Analyst for our Compstat for Justice Program (C4J) with an unwavering passion for social justice issues. C4J is a new initiative aimed at providing recommendations and strategic services to law enforcement agencies (LEAs) in support of their efforts to reduce racial disparities in policing outcomes within the communities they serve.
	- The Data Analyst will be an integral part of CPE's C4J service delivery model. They will analyze data collected from partner law enforcement agencies (LEAs) to conduct analyses to identify department needs and perform ongoing analyses to aid the department in implementing recommendations. Additionally, the Data Analyst will be responsible for continuously iterating on the 'Needs Assessment' and C4J Implementation frameworks based on data and insights gathered from each C4J development site. This position will report to the Senior Director of C4J.
	- Key Responsibilities
		* Perform data pre-processing/wrangling tasks to prepare the data for analysis
		* Use quantitative and qualitative data to answer C4J research questions
		* Execute on the data analyses required for the Needs Assessment phase of C4J
		* Documenting all code developed to conduct the analyses
		* Produce compelling data visualizations
		* Design and run regression models and provide interpretation of the results
		* Conduct quality assurance checks to ensure there are no errors in the analyses or the interpretation of the output
		* Conduct meticulous quality control and record-keeping procedures to ensure the highest levels of data integrity
		* Translate findings into recommendations, strategy, and change management practices for partner agencies to employ
		* Conduct project post-mortem and lead continual process improvement for data analysis following each C4J implementation
	- Qualifications
		* Bachelor's degree required; specialization in Mathematics, Statistics, Physics, Computer Science, Economics, Finance, or other quantitative fields is preferred
		* 5+ years of experience working in a Data Analyst, Business Analyst or similar role with a focus on performing advanced regression techniques, to include multivariate regression, multilevel (hierarchical) modeling, and Bayesian inference
		* Possess end-to-end data expertise including data sourcing, merging, cleaning, restructuring/wrangling, and visualization
		* Highly proficient in using R and/or Python for statistical programming
		* Ability to produce markdown notebooks with Jupyter and/or RMD/Knitr
		* Expertise in interpreting syntax across statistical software platforms (including, but not limited to, R, Python, SAS, Stata, and/or SPSS)
		* Ability to explain analysis findings in clear terms that can be understood by non-technical audiences
		* Experience producing interactive data visualizations to communicate findings (i.e. Tableau, R Shiny, Qlikview, D3.js, etc)
		* Comfort with collaboration across a geographically distributed team
		* Experience working with clients and managing internal and external relationships
		* Experience working with geographic data (GIS shapefiles; desired, but not required)
		* Familiarity with constructing SQL queries is a plus
+ skill set:
	- GitHub helps companies, organizations, and groups of individuals succeed by allowing them to build better software, together. The engineering organization is looking for a Product Data Analyst who will report into our Data Science group and closely partner with the Product organization to provide best in class analysis and insight generation to help our product managers make better decisions, faster. You’ll also interface with our Analytics Engineering team to put your findings “into production” to ensure that our customers receive ongoing benefits from your work at scale.
	- GitHub is an exciting place to work joining a tight-knit environment of technical and business-minded individuals. This role is an individual contributor role with significant growth potential.
	- Responsibilities
		* Work with Product owners to assess metric needs and turn them into relevant, synthesized consumable outputs
		* Embed in and build business understanding across product domains leading to effective cross collaborative partnership
		* Partner with our Analytics engineering team to develop solutions for ongoing product performance reporting that scale and reduce effort to outcomes
		* Develop and automate reports and iteratively build dashboards to provide insights at scale, solving for on-going analytical needs
		* Leverage relevant product data to track and inform objectives and goals with product partners
	- Minimum Qualifications
		* A Bachelor’s Degree in statistics, math, engineering, computer science, economics, business, or a related technical field.
		* 2+ years prior relevant experience in an Analytics role covering product or engineering
		* Strong intellectual curiosity and desire to understand different parts of GitHub’s product
		* Experience translating requirements into analysis questions that can be directly actioned
		* Expertise in writing SQL queries and working with multiple data sets to develop analytical solutions for defined business problems
		* Experience with engineering practices around analytical rigor and the ability to make reproducible your analyses
		* Demonstrated ability to synthesize and communicate analysis into insights that inform business decision making
		* Demonstrated willingness to both teach others and learn new analytical problem solving techniques
		* Demonstrated effective written and verbal communication skills
		* Ability to lead without authority and a strong sense of responsibility
	- Preferred Qualifications
		* Experience conducting statistical analysis using Python / R (or equivalent)
		* Experience developing and maintaining ETLs (Airflow experience is a big plus)
+ skill set:
	- Familiarity with NOSQL storage (MongoDB, Redis, Elastic, etc.)
	- A strong background in relational database theory and excellent knowledge of Relational Databases (Postgres, MySQL, SQL server, Oracle)
+ skill set:
	- Experience in defining and implementing MDM data models, matching rules, Duplicate Match Review processing, Match-Merge Rules, Workflow Configuration, complex hierarchy relationship management and data governance.
		* MDM, master data management
		* MDM, metadata management
		* MDM, mobile device management
	- System endurance improvement through root cause analysis (RCA), fixes and deployment support
+ skill set:
	- ClickHouse columnar databases
	- Spring Boot
+ skill set:
	- Big Data Query Optimization Engineer
	- Mountain View, California
	- Evaluate and enhance query optimization capabilities of our lakehouse platform.
	- Research, design and implementation of Logical and Physical Query Plan Optimization techniques.
	- Demonstrate expertise in Query Optimization topics like cost models, logical plan rewrite techniques, materialized view rewrites etc.
	- Address both simple and complex product defects as a part of normal development
	- Translate complex functional and technical requirements into detailed design
	- Facilitate knowledge sharing by creating and maintaining detailed, comprehensive documentation.
	- Mentor less knowledgeable team members as needed.
	-A Bachelor’s degree is required in the field of Computer Science, or Engineering.
	- 5+ years experience working on SQL Query Optimization, contributing to one or more optimization techniques at a DB vendor.
	- Experienced in the query optimization implementation of one or more open source engines such as Apache Spark, Presto, Trino, and Calcite.
	- Familiarity with development and deployment of enhancements on open source platforms such as Spark, Presto, Trino.
	- Experience working in Open Source. Apache contributor and/or Committer a strong plus
	- At least 10 years of relevant work experience
	- Engineering experience with the following technologies preferred : Linux, Java, Scala, Python.
+ skill set:
	- Data Analyst
	- Vancouver, British Columbia, Canada
	- Later is the enterprise leader in social media and influencer marketing software, services, and data, trusted by leading brands and agencies worldwide. Following our acquisition of Mavely, the Everyday Influencer Platform®, Later enables brands to scale creator partnerships from nano to premium influencers while managing social media content and campaigns across all major social and affiliate networks. Through proprietary performance data, marketing leaders can drive attributable sales and optimize social commerce with our software platform or award-winning services. 
	- Later is founded on two success stories that began in 2014: Mavrck, the industry-leading influencer marketing solution (now Later Influence™), and Later, the best social media management platform (now Later Social™) and first-to-market link in bio tool, Later Link in Bio. In 2024, Mavrck and Later officially joined together as one unified business, with a shared vision: to enable the world to make a living with their creativity.
	- We’re trusted by the top social platforms, with partnerships and integrations with Meta, TikTok, LinkedIn, YouTube, and Pinterest.
	- We enable marketers to create high-performing content and engage in authentic collaborations with creators to reach new audiences, drive engagement, and generate predictable ROI. 
	- We’re looking for a Data Analyst In this role, you’ll work closely with senior analysts and business stakeholders to turn data into actionable insights. You’ll help design reports and dashboards, conduct analysis on business performance, and support data-driven decision-making across the company.
	- This is a great opportunity for someone early in their data career who wants to grow their technical skills and business impact in a fast-paced, high-growth environment.
	- What you'll be doing:
		* Strategy
			+ Analyze performance data to identify trends, opportunities, and areas for improvement.
			+ Translate findings into clear recommendations that support business objectives.
			+ Support KPI development and reporting for marketing, product, and finance teams.
			+ Conduct exploratory data analysis to uncover new insights that influence strategy.
		* Technical/ Execution
			+ Build and maintain dashboards in tools like Tableau, Looker, or Power BI.
			+ Develop automated reports that make business-critical data accessible to stakeholders.
			+ Ensure reporting is accurate, consistent, and aligned with company metrics.
			+ Write SQL queries to extract and manipulate data from our data warehouse.
			+ Apply basic data cleaning and transformation techniques to prepare data for analysis.
		* Team / Collaboration
			+ Partner with senior analysts, product managers, and marketers to define reporting needs.
			+ Support ad-hoc analysis requests from cross-functional teams.
			+ Document data definitions and reporting processes to enable self-service analytics.
			+ Collaborate with senior analysts on improving data quality and reliability.
		* Research/Best Practices
			+ Stay curious about industry trends in analytics, BI tools, and data-driven decision-making.
			+ Develop your technical skills (Python, R, dbt a plus) and apply new methods to improve outcomes.
			+ Identify opportunities to streamline reporting processes and share learnings with the team.
	- What success looks like:
		* Deliver clear, accurate reports and dashboards that are regularly used by business teams.
		* Improve decision-making speed and accuracy by providing timely insights.
		* Consistently meet deadlines while balancing multiple analysis requests.
		* Demonstrate growth in technical skills and take on increasingly complex analyses over time.
	- What you bring:
		* Bachelor’s degree in Economics, Statistics, Business, Data Science, or related field (or equivalent experience).
		* 1–3 years of experience in data analysis, reporting, or business intelligence.
		* Strong SQL skills and comfort working with relational databases.
		* Experience with at least one BI tool (Tableau, Looker, Power BI, or similar).
		* Strong problem-solving skills and ability to explain data insights in plain language.
		* Curiosity and eagerness to learn more advanced analytics skills.Nice to have: exposure to SaaS, subscription, or digital marketing businesses.
	- How you work: 
		* Driven by Impact: You deliver results that matter—prioritizing high-value work, meeting deadlines, and adapting quickly while keeping outcomes clear.
		* Strategic & Customer-Centric: You anticipate risks and opportunities, connect decisions to long-term growth, and build trust through proactive insights.
		* Curious & Growth-Oriented: You seek knowledge, ask sharp questions, and apply learnings fast—challenging the status quo with a mindset of improvement.
		* Collaborative & Resilient: You thrive in change by staying resourceful, solution-focused, and positive—removing roadblocks, sharing insights, and keeping morale high.
		* Accountable & Honest: You own your work, hold yourself and others to a high bar, and use transparent feedback to drive growth.
		* Emotionally Intelligent: You build trust through empathy and collaboration, foster inclusion, and inspire others with grit, optimism, and integrity.
	- We take a market-based & data-driven approach to compensation. We leverage data from trusted third-party compensation sources to help us understand the market value of a role based on function, level, geographic location, and scope. We evaluate compensation bi-annually, including performance and market-related factors.
	- Our salaries are benchmarked against market Total Cash Compensation for the geographic location of our job posting. Compensation for some roles is structured as On Target Earnings (OTE = base + commission/variable) while for others it is structured as Salary only.
	- To comply with local legislation and ensure transparency, we share salary ranges on all job postings. Skills, experience and other factors help determine the final salary we offer which may vary from the original range posted. 
	- Additionally, all permanent team members are eligible to participate in various benefits plans as part of their overall compensation package.
	- Salary Range: $ 76,000 - 87,000 CAD
	- LI-Hybrid  
	- We have offices in Boston, MA; Vancouver, BC; Chicago, IL; and Vancouver, WA. For select positions, we are open to hiring fully remote candidates. We post our positions in the location(s) where we are open to having the successful candidate be located. 
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.
+ skill set:
	- BLAH.



















##	Skill Sets for GIS-Based Software Applications, Computer Systems, and Cyber-Physical Systems


Skill set for GIS-based software applications, computer systems, and cyber-physical systems:
+ skill set:
	- experience with multi-imaging modalities
		* RGBN
		* multi-/hyper- spectral
		* thermal
		* LiDAR
		* radar
	- experience delveoping algorithms and methods for object detection, classification, and characterization
	- compiled production-class OO or FP programming language
		* Java
		* Scala
		* C++
		* C#
		* Haskell
		* ML
		* Erlang
		* Clojure
		* Rust
	- geospatial/GIS tools:
		* ArcGIS/QGIS
		* ENVI
		* ERDAS Imagine
	- ability to think broadly and creatively about real-world problems, and technical and scientific solutions that produce value for customers
	- experience with deep learning or machine learning methodologies
	- cloud-based implementation of software via:
		* AWS
		* Azure
		* GCP
	- Bayesian image analysis, Bayesian statistics, optimization methods
	- sensor and image fusion, multi-scale and multi-temporal data processing, video processing, 3-D scene reconstruction methods
+ Location technologies (maps, GIS).
* OpenLayers
	* OpenLayers is a free, open source JavaScript library that lets you display dynamic maps from any source. 
	* OpenLayers is an open-source JavaScript library for displaying map data in web browsers as slippy maps. It provides an API for building rich web-based geographic applications similar to Google Maps and Bing Maps.
	* OpenLayers makes it easy to put a dynamic map in any web page. It can display map tiles, vector data and markers loaded from any source. OpenLayers has been developed to further the use of geographic information of all kinds.
+ A skills matrix of your expertise in using Google OR Tools, Python, C++, geospatial analysis, and related tools (geopandas, shapely, QGIS, postgis, etc.)
+ skill set:
	- Administrative experience with one or more IaaS cloud platforms
	- Knowledge of user authentication protocols (basic, forms, Kerberos, NTLM, PKI), authentication systems (IWA, proprietary/custom token, SAML), and authorization concepts and systems (RBAC, OAuth)
	- Experience with “emerging technologies” such as IoT, big data analytics, containerization, AI/ML
	- Extensive experience with Esri software including ArcGIS Enterprise components, ArcGIS Enterprise geodatabases, and ArcGIS client applications (mobile and desktop applications)
	- Experience with modern software implementation patterns including service-oriented architectures and cloud environments and concepts such as single sign-on authentication or mobile application development patterns
	- Working knowledge of modern web technology; understanding of web servers, the HTTP protocol and methods, modern web browsers, developer tools, web proxies such as Fiddler, and the use of these technologies to troubleshoot web service and website functionality and performance.
	- Experience with programming languages; working knowledge of Python, JavaScript, PowerShell, and SQL and when and where they should be used and what they can accomplish
	- 4+ years of experience designing, implementing, and/or administering enterprise GIS solutions/systems that leverage the ArcGIS platform, specifically ArcGIS Enterprise components
	- Experience with relational database management systems, including SQL Server, PostgreSQL or Oracle, including knowledge of standard SQL usage and database design concepts such as views, triggers, and schemas
	- Understanding of enterprise geodatabase concepts in the ArcGIS platform, and where and why they are used for geospatial data workflows
	- Experience with operating system concepts: Windows environments, domain management, sharing and security of filesystems in Windows environments, Linux system administration concepts, and general conceptual understanding
	- Knowledge of networking concepts and topics such as firewalls, troubleshooting DNS entries, assessing network configuration and performance
	- Bachelor’s or master’s in computer science, engineering, mathematics, GIS, or a related field, depending on position level
	- Design Effective Enterprise Systems
		* Asses and provide recommendations to customers related to the appropriate and effective use of ArcGIS software components
		* Provide actionable advice related to system design, performance implications, functional limitations, and any other topics that affect system design
		* Interface with solution architects and enterprise IT staff from a customer organization to explain and guide them through ArcGIS design decisions
		* Communicate technical concepts and ArcGIS platform capabilities to technical and non-technical audiences
	- Implement Advanced ArcGIS Enterprise Deployments
		* Deploy and configure ArcGIS Enterprise in a wide variety of environments across operating systems, cloud providers, security architectures, customer types, industries, and organizations
		* Configure ArcGIS Enterprise system to meet the customer’s service level agreement including high availability, disaster recovery, and security needs
		* Plan and execute the migration of content from one ArcGIS Enterprise deployment to another, encompassing different architectures, deployment strategies, and software components
	- Assist with Technical Troubleshooting and Issue Resolution
		* Assist Esri customers with troubleshooting advanced ArcGIS Enterprise deployments
		* Assist with communicating troubleshooting guidance to related platform components such as storage, networking, security
	- Assess and Optimize Performance
		* Identify performance requirements and understanding from a customer perspective
		* Use Esri tools and industry standard tools to identify performance issues with maps, web services, databases, web applications, and other contexts
		* Provide recommendations to customers related to performance monitoring, management, and improvement across the entire ArcGIS system
	- Participate in a Technical Community
		* Participate in a technical community of advanced enterprise staff, sharing experiences; writing best practices documentation, technical guides, and helpful scripts; and participating in product direction discussions and recommendations
		* Work collaboratively with teammates to solve difficult technical challenges
+ skill set:
	- Demonstrated experience with Esri ArcGIS 10.x, cartographic design, graphic editing, and standard office applications.
	- Familiarity with ArcGIS Online, Tableau, and a statistical package (R, STATA, etc.)
	- Ability to work in a fast-paced environment under tight deadlines while producing quality content.
	- Strong organizational, interpersonal, oral and written communication skills.
	- Strong analytical skills, data lover, ability to think big picture strategy and navigate details to ensure accuracy.
	- Ability to participate in a 1-2-week field experience, which may involve domestic and/or international travel and overnight stays for up to 2 weeks.









































###	Data Modeling


Skill set for data modeling:
+ skill set:
	- Looker is seeking a Senior Software Engineer to join our Data Model team (database semantics, programming languages, and integrated development environment (IDE)). This team's core responsibilities include Looker's SQL normalization and code generation engine (the heart and lungs of our application), the LookML language itself, Looker's in-browser IDE for composing and versioning LookML, and the data pipeline within the Looker application.
	- The ideal candidate will take an active role in contributing to our long-term technical roadmap and have a deep background in programming language fundamentals (e.g. compiler design) or databases (e.g. building SQL optimizers) or building IDEs, in addition to tried and true experience with software engineering best practices.
+ 





