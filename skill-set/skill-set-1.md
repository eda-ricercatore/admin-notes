#	Sets of skills













+ skill set:
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
+ skill set:
	- Excellent understanding of video compression. Extensive experience with compression standards such as H.264/AVC, HEVC and VP9.
	- Strong background in image and signal processing, both algorithm design and implementation.
	- Implemented a video codec from scratch
	- Background in video quality metrics, video understanding, computer vision or machine learning
	- Experience with large-scale distributed systems and cloud-computing
	- Involvement in open-source multimedia projects (such as ffmpeg, x264, webm, VLC)
	- Design and prototype algorithms for improving the quality and performance in our cloud-based video ingest and encoding pipeline
	- Study current codec implementations and find areas for improvement in quality and speed
	- Conduct research on next-generation image and video coding and propose technology for industry standards
	- Participate in research conferences and standardization meetings
+ skill set:
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Cloud computing platforms and large web-scale distributed systems
+ skill set:
	- Recommender Systems and Personalization. Almost every aspect of the Netflix experience is personalized, and much of that personalization is driven by our various flavors of recommendation algorithms. You'll apply a number of techniques, from the latest in deep learning, reinforcement learning, to causal inference.
	- Search Ranking and Query Understanding. You'll work on the algorithms that allow our members to interactively query and explore our catalog. Using the latest in NLP techniques, you'll solve problems including: query understanding, knowledge graph discovery, and learning to rank across our global catalog of titles.
	- Large Scale Machine Learning. Netflix is available in over 190 countries, with over 148+ million members. This gives us a unique dataset to work with, but also unique challenges in how we scale our models. You'll work on cutting edge techniques to scale your models for use in our production systems.
	- Strong background in machine learning with a broad understanding of unsupervised and supervised learning methods
	- Strong software development experience
	- Successful track record of delivering results in complex cross-functional projects
	- Strong mathematical skills with knowledge of statistical methods
+ skill set:
	- Experience building or maintaining databases (MySQL, Hive, etc.)
	- Experience building or maintaining Big data & streaming systems (Hadoop, HDFS, Kafka, etc.)
	- Cross-platform coding
	- Large-scale, large-user base website development experience
	- Data mining, machine learning, AI, statistics, information retrieval, linguistic analysis
+ skill set:
	- Application code development in C++
	- Embedded peripherals and drivers (UART/CAN/SPI/I2C)
	- Wireless communication systems (Cellular, Wifi, BLE)
	- Networking protocols (TCP/UDP)
	- Embedded Linux
	- FreeRTOS or similar low level RTOS
	- Masters degree or similar job experience
	- Experience with algorithm packages (Eigen, OpenCV, etc.)
	- Worked with TI M4 class processors
	- Worked with Linux on ARM processors
	- Worked with Embedded GCC
	- Experience with automotive systems or UAV systems
+ skill set:
	- Design and implementation of state of the art monocular computer vision algorithms
	- Solve problems involving odometry, landmark detection, structure from motion and segmentation in large scale outdoor environments
	- Integrate vision based algorithms into our probabilistic fusion framework
	- Help in identifying core requirements for camera sensors
	- Code development in C++/Python
	- Work with real data on our self driving car
	- Masters or PhD Computer Science, Electrical Engineering or both.
	- Deep Experience in SfM, VO, and classical computer vision algorithms
	- Expert knowledge in computational geometry
	- Experience in machine learning, feature detection and classification
	- Experience with open source computer vision and linear algebra frameworks
	- A solid background in statistics, probability and linear algebra
	- Experience with real world datasets
	- Experience with real time algorithm implementation
	- Ability to work independently without direct supervision
	- Experience with CV algorithm packages (Eigen, OpenCV, etc.)
	- Knowledge of Deep learning techniques applied to CV
	- Experience in Linux based environments
	- Experience in SLAM and/or motion planning
	- Experience with CUDA, OpenCL or other GPU frameworks
	- Experience with automotive systems or UAV systems
	- Ability to lead a small technical team that balances research and application
+ skill set:
	- This role is focused on delivering the best in localization performance from our sensors.
	- Candidates should have extensive experience working with raw navigation data. You should be very comfortable with estimation theory and have implemented complex filters in practice. Real world experience with RTK, Integer Ambiguity Estimation and other high precision techniques are a huge plus.
	- Inertial Measurement Unit Algorithms, Extended Kalman Filter, Visual Odometry, Gnss, RTK-GPS
+ [Sphinx](https://en.wikipedia.org/wiki/Sphinx_(search_engine))
+ skill set:
	- Expertise in image and video processing, computational photography, single and multiview geometry, keypoint extraction, description, association, etc.
	- Experience in efficient large-scale numerical optimization
	- Experience in the area of camera calibration, SLAM, point cloud processing are highly desired
	- Publication records in leading conferences such as CVPR, ICCV, ECCV, NIPS, ICML or PAMI is a plus
+ skill set:
	- Strong knowledge of the state-of-the-art in computer vision and machine learning algorithms with a solid understanding of OpenCV
	- Experience working with point cloud processing and Point Cloud Library (PCL)
+ skill set:
	- As a Deep Learning Engineer at Simbe Robotics, you will be part of a talented team designing and training state of the art deep learning algorithms to identify placement, presentation, pricing, and availability of products in retail stores across the globe.  
	- In this role you will lead various initiatives designing, developing, and training in-house character recognition and image caption algorithms powered by deep learning.
	- Participate in planning and prioritizing, write functional specifications and lead design reviews for our character recognition and image caption algorithms.
	- Generate, clean, and curate real world training datasets
	- Create photorealistic synthetic training data for augmentation
	- Develop, test, tune, and deploy character recognition and image caption systems across a wide variety of customers
	- Evaluate existing character recognition and image caption methods for speed and accuracy performance improvements
	- Collaborate with other developers, quality engineers, product managers, and documentation writers
	- Ph.D. or M.S. preferred
	- Strong machine learning background, with 2+ years of hands-on experience in building real systems
	- Deep understanding of state of the art machine learning and deep learning algorithms, techniques and best practices
	- Solid understanding of linear, non-linear, and dynamic programming
	- Experience using or building synthetic image generation systems, data augmentation pipelines, and OCR/image caption systems
	- Proficient in at least one of the following: Tensorflow, Keras, PyTorch. Tensorboard knowledge is a plus
	- Must be fluent in Python, other languages are a plus
	- Should be familiar with training and running deep learning models on GPUs (both commodity and otherwise)
	- A good understanding of recurrent neural networks (including LSTMs and GRUs)
	- Experience in debugging and diagnosing performance problems with ML algorithms
	- Must have excellent written and verbal communication skills
	- Experience with attention models, text localization, Google Cloud Platform, AWS, and serverless is a plus
	- Strong Linux & Command Line background
	- Ability to work hands-on in cross-functional teams with a strong sense of self-direction
+ [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
	- [TensorBoard, TensorFlow's visualization toolkit](https://www.tensorflow.org/tensorboard)
	- https://databricks.com/tensorflow/visualisation
+ skill set:
	- We are seeking a strategic technical leader who will be responsible for delivering the core infrastructure for machine learning on Databricks. This includes the ML runtime (a packaged environment containing Spark, Tensorflow, and other frameworks), our own machine learning algorithms, storage and IO optimizations, as well as higher level abstractions such as hyper parameter tuning and feature registries.
	- Grow a team of application developers responsible for the Databricks ML Runtime.
	- Grow Databricks' machine learning capabilities - increase YoY product revenue and adoption at > 100%
	- Manage technical debt, including long term technical architecture decisions and balance product roadmap
+ skill set:
	- Use Databricks to build internal data warehouse and integrate it with BI and CRM services used internally
	- Use Databricks to analyze usage data, and create dashboards and reports
	- Build self-serving internal data products to make data simple within the company
	- Work closely with Product Management and other stakeholders to understand product usage patterns and trends and to make data-driven decisions and forecasts
	- Provide product feedback to PM and Engineering teams
	- Strong desire to work at a rapidly growing startup
	- Knowledge of data processing and applied statistics
	- Proficient in data analysis and visualization using R or PyData
	- Familiar with SQL and databases like MySQL or PostgreSQL
	- Experience with distributed data processing systems like Spark and Hadoop
	- General-purpose languages such as Python and Scala
	- Desire to explore lots of data to find unexpected insights
	- Strong communication and presentation skills
	- [Plus] Advanced degrees in statistics, computer science, math, or similar fields
	- [Plus] Familiarity with interactive data visualization using tools like D3.js
+ skill set:
	- Architect and operate high quality, large scale, multi-geo data pipelines that drive business decisions.
	- Redesigned data pipelines using the applicable DBR features, and incorporating external tools where necessary to have better reliability and tighter SLAs.
	- Established conventions or new APIs for logging feature usage for PM use-cases.
	- Understandable SLAs for each of the production data pipelines.
	- Improved test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests.
	- CI and deployment processes and best practices for the production data pipelines.
	- Reduction in overall alert noise and increase responsiveness by rethinking the current alert categories and priorities.
	- Design schemas for financial, sales and support data in the data warehouse.
	- Experience building, shipping and operating multi-geo data pipelines at scale.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Excellent communication (writing, conversation, presentation) skills, consensus builder
	- Strong analytical and problem solving skills
	- Passion for data engineering and for enabling others by making their data easier to access.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- If you are looking for an unparalleled opportunity to build the next generation big data processing platform, and learn how to launch hundreds of thousands of VMs a day at scale while running thousands of Kubernetes clusters, you have come to the right place. The platform team builds and manages the core systems powering Databricks, allowing it to seamlessly scale and run across various geographic regions/clouds, and making Databricks the go-to product for big data processing in the cloud.
	- You will be a senior software engineer responsible for architecting scalable systems to power Databricks, making it the de-facto platform for running Big Data and AI workloads. You will build and extend the Databricks cloud platform, which is based on a micro service architecture and includes systems for managing thousands of Kubernetes clusters at scale, systems for streaming and consuming gigabytes of log data per minute, onboarding and managing thousands of data scientists on Databricks, scalable API gateway, rate limiting framework, network security and encryption, build infrastructure (we use Bazel), and scalable CI/CD framework among many others.
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python or Javascript and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with internal APIs.
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- To achieve this, we build data reporting pipelines that support the underlying pricing infrastructure supporting tens to hundreds of millions of DBUs (Databricks Units) across multiple clouds and regions, UIs that allow Databricks administrators to view and manage their bill, and APIs and integrations to downstream processors to handle payments for all customers.
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
	- Unified Analytics Platform
+ skill set:
	- Our team drives state-of-the-art, open source Delta Lake project bringing reliable, scalable, ACID transactions to Apache Spark and other Big Data engines. Our mission is to deliver a robust and performant engine that enables users to build reliable data pipelines that ingest massive data volumes, optimize data layout, generate metadata and evolve data schemas all while guaranteeing transactional correctness and high query performance.
	- Build the core features that make Delta Lake the world's best Big Data storage abstraction in terms of performance, stability, security and scalability.
+ [Delta Lake Community; Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.](https://delta.io/)
+ Experience with our web stack (React, Redux, TypeScript, protobuf, Apollo, GraphQL) and Spark
+ skill set:
	- You will build tools and features to make Databricks the best place for large-scale enterprise R workloads.
	- Improve state of distributed R computing through Apache Spark and R integration on Databricks
	- Implement new features on Databricks platform for R users (e.g., ACL)
	- Improve and extend Databricks R notebooks to satisfy R users' use cases and requirements
	- Implement new R-based APIs on Databricks platform (e.g., secret management API)
	- Expand Databricks workspace through integration with third-party tools such as RStudio and Shiny.
	- Integrate critical packages from the R ecosystem into Databricks Runtime
	- Provide engineering support and thought leadership to Databricks field engineering teams on R
	- Give talks and write blog posts about R on Databricks
+ [MLflow, An open source platform for the machine learning lifecycle](https://mlflow.org/)
+ Production quality coding standards and patterns.
+ skill set:
	- The Machine Learning Platform team is hiring strong engineers to help us design MLflow, an open source tool for managing the Machine Learning lifecycle. In this role you will help define the APIs creating the standard that organizations use to manage their Machine Learning, from tracking offline experimentation through deployment to production systems. You will also build the services supporting the APIs in the open source and their integration into the Databricks product, a unified analytics platform that helps manage data processing and machine learning workloads in a collaborative, enterprise grade product.
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Be responsible for full software development lifecycle - design, development, testing, operating in production
	- Architect solutions to achieve a high level of reliability, scalability and security
	- Communicate effectively with other engineers in the same team, with other teams and with various other stakeholders such as product managers
	- Mentor junior engineers or other engineers on the team to help level up their skillset
	- 7+ years of production experience developing services in: Java, Scala, C++, Go, or Python
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- Preferred Experience working on a SaaS platform or with Service Oriented Architectures
	- Preferred Experience with software security and systems that handle sensitive data
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
+ Build system experience like Maven, Bazel, or Gradle
+ skill set:
	- We've built features such as autoscaling compute and storage, credential passthrough and notebook-scoped libraries, that simplify resource administration in the cloud, secure data in the enterprise and empower data scientists an data engineers in their organizations. You have the opportunity to join us and solve the infrastructure and data management problems of enterprises as they transition from on-prem data centers to the future of the cloud.
	- JVM or lower-level programming languages for systems programming.
	- Experience with services infrastructure.
	- Experience with distributed systems, databases, and big data systems.
	- Experience with cloud APIs preferred (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
	- Exposure to container technologies, such as Docker preferred
+ skill set:
	- As a Software Engineer on the Spark Benchmarking team at Databricks, you are responsible for ensuring that the Databricks Runtime is the world's best Spark execution environment in terms of performance and scalability.
	- You will be part of the team that is continuously improving the methodology and benchmarking infrastructure, helping to increase the frequency of the releases while maintaining high quality and performance standards. Continuously improving performance is an increasingly challenging job given the high volume of commits that go into a release. In order to meet this challenge, your team will continuously increase the level of automation and provide powerful benchmarking tools to evaluate the performance impact of each change. Engineers on the Spark Benchmarking team also drive the Databricks runtime performance sign-off process, they are the gatekeepers making sure that all performance regressions are addressed before a new version is released.
	- Experience with: Large scale distributed computing, Big Data engines e.g. Spark, Hadoop.
	- Passion for software automation and Continuous Integration experience.
	- Excellent communication and teamwork.
	- Strong foundation in algorithms and data structures and their real-world use cases.
	- Solid understanding of computer systems and networks.
	- Production quality coding standards and patterns.
	- 4+ years of general software programming experience.
	- 4+ years of modern, production level experience in one of: Java, Scala, JavaScript, or C++.
	- BS in Computer Science, Math, related technical field or equivalent practical experience.
	- Experience with benchmarking big data systems
	- Experience with developing infrastructure for testing distributed systems
+ skill set:
	- Experience with: Large scale distributed computing, Big Data engines e.g. Spark, Hadoop.
	- Passion for software automation and Continuous Integration experience.
	- Excellent communication and teamwork.
	- Strong foundation in algorithms and data structures and their real-world use cases.
	- Solid understanding of computer systems and networks.
	- Production quality coding standards and patterns.
	- 4+ years of modern, production level experience in one of: Java, Scala, JavaScript, or C++.
	- Experience with benchmarking big data systems
	- Experience with developing infrastructure for testing distributed systems
+ skill set:
	- Full ownership including: Designing, Implementing, Testing and Metric Analysis.
	- Production quality coding standards and patterns.
+ skill set:
	- The workflow team operates at the core of the Databricks infrastructure: it orchestrates all the workloads scheduled by the customers of Databricks, from the one-off experiment to the massive multi-day query running on hundreds of machines. As part of this team, you will be responsible for maintaining mission-critical operations, and at the same time pushing the boundary in terms of integrating with innovative AI solutions built on top of the Databricks platform. The responsibility covers mainly the backend service itself and all its adjacent functions, from low-level systems in Scala to dashboards and health monitoring, and public APIs for remote management.
	- We are looking for talented engineers who are passionate about large-scale, high availability systems, and who want to make a strong impact on the growth of the company.
	- Maintain the existing backend of Databricks' core scheduling service
	- Own (as a team) the alerting and deployment systems around the backend
	- Scale the scheduling service by 10x
	- Own the testing infrastructure of the backend.
	- Architect the workflow management component of Databricks
	- 3+ years of experience with backend systems written in java, scala, go, or c++
	- Deep understanding of high-concurrency, reliable services
	- Production quality coding standards and patterns
	- Strong foundation in algorithms and data structures and their real world use cases
	- Experience with SAAS/PAAS services (experience with developing cloud-based services strongly desirable)
	- Experience working on complex, data-heavy applications
	- Experience instrumenting services
+ skill set:
	- As a DevOps Engineer at Simbe Robotics you will be part of a talented team ensuring quality in our software as well deploying & managing our cloud services and world-wide fleet of autonomous robots.
	- Has experience with automated build and continuous integration systems (e.g. Jenkins, TravisCI)
	- Has knowledge of application/system level monitoring (Nagios, CloudWatch, Munin, Splunk)
	- Experience with configuration management (Chef, Puppet, Ansible) tools
	- Has experience with various application packaging and deployment technologies (Debian packages, Docker/Linux containers)
	- Experience configuring web servers (e.g. Apache/Tomcat, nginix)
+ Working on the robot's navigational systems for mapping, localization, path planning, obstacle detection and avoidance. Our robots are designed to work safely and reliably alongside shoppers and employees during normal store hours.
+ programming languages, frameworks and tools
	- .Net
	- .Net Framework
	- ABAP
	- Ada
	- Akka
	- Alice
	- AngularJS
	- Ansible
	- Apex
	- ASP.net
	- assembly language
	- Awk
	- Backbone.js
	- Bash
	- C
	- C\#
	- C++
	- CakePHP
	- CFEngine
	- Chef
	- Clojure
	- COBOL
	- Codeigniter
	- CSS
	- D
	- Dart
	- Delphi/Object Pascal
	- Django
	- Docker
	- Ember.js
	- Erlang
	- Express.js
	- F\#
	- Flask
	- Fortran
	- Go
	- Groovy
	- Haskell
	- HTML
	- Java
	- JavaScript
	- jQuery
	- Kubernetes
	- Ladder Logic
	- Linux
	- Lisp
	- Logo
	- Lua
	- Matlab
	- Meteor
	- MQL4
	- Nagios
	- NodeJS
	- Objective-C
	- Perl
	- Phalcon
	- PHP
	- Play!
	- Prolog
	- Puppet
	- Python
	- Q
	- R
	- React
	- Redux
	- Revel
	- Rkt
	- RPG (OS/400)
	- Ruby
	- Ruby on Rails
	- Rust
	- RxJS
	- SAS
	- Scala
	- Scheme
	- Scratch
	- Spring Framework
	- SQL
	- Swift
	- Symxfony
	- VHDL
	- Visual Basic
	- Visual Basic .Net
	- Windows
	- Zend
+ skill set:
	- You revel in building features quickly and iterating in a data-driven fashion
	- You lay awake thinking about improving the design, implementation and maintenance of large software systems with millions of users
	- Passion to hack social commerce
	- Data & Relevancy engineers work on our massive semi-structured datasets. They have domain experience in data mining, information retrieval, or machine learning, and a strong system orientation. Key product initiatives include product feed relevance, ad targeting, information extraction, and recommendations.
	- Infrastructure engineers scale a massive, highly-available platform end-to-end. They design distributed systems, validate performance, factor in security, and proactively monitor every corner of our stack. When things do go wrong, they are on-hand to fight the fires.
+ Hibernate ORM is an object-relational mapping tool for the Java programming language
+ skill set:
	- Data/Model Validation Engineer
	- We are looking for someone passionate about learning how machine learning systems are developed to assist with validating and processing training data to evolve our state of the art systems.
	- Engage with software engineers on the Perception team to identify and collect training data to evolve our machine learning systems.
	- Working with engineers on the Perception team, train new machine learning models and perform analysis to quantify how they perform based on changes to the training data sets.
	- Provide feedback on tools and processes for efficient workflow of training data creation and validation.
+ skill set:
	- Knowledge of robotics concepts and tools (ROS)
	- Understanding of and ability to implement machine learning methods, particularly for applications in autonomous vehicle decision making and prediction
	- Experience in production C++ development
+ skill set:
	- Experience with Robotics perception
	- At Starsky, the Perception team is responsible for processing sensor information and making it available to the other teams in a clean and consistent format. The models and algorithms developed aim to achieve robust real time detection and tracking of our truck and other objects in the local environment, including lane lines, vehicles, and pedestrians.
	- As a Robotics Perception Engineer, you will be responsible for filtering, fusing and post-processing the outputs of different deep learning models and sensors. You will apply state of the art tracking and fusion algorithms which are robust to sensor noise and environmental variability. This requires working with teams across the driving stack to scope requirements and understand the strengths and limitations of different modules.
	- Background in linear algebra, probability, 3D geometry
	- Experience with multi-camera sensor fusion and camera-radar sensor fusion
	- Experience with real time tracking of objects and lanes
	- Experience with real time mapping and localization
	- Experience with camera and radar sensor calibration
	- Ability to write efficient real time algorithms in C++
	- Experience in developing on linux environment
	- 2+ years experience in C++/Python development in a fast paced production environment
	- 2+ years experience in perception system of mobile robots
	- Experience with ROS
	- Experience in sensor noise analysis
	- Experience in machine learning/deep learning
+ skill set:
	- Starsky Robotics is looking for a full-time Senior Data Scientist. Your job will tackle a wide variety of problems in autonomous vehicles. From finding every time a car cut in front of our truck, to figuring out how to report on the quality of autonomous driving, to creating new tools and statistical methods for robotics engineers to characterize the behavior of their systems, we're looking for someone motivated to attack self-driving problems with mounds of data. Tackling these problems will require learning about the whole suite of robotics fields applied to make autonomous vehicles: motion planning, controls, perception, and behavior planning.
	- You'll own high-level decisions such as “How do we determine if a route fits our current driving capabilities”. Day-to-day projects may have mission statements as technical as “Help us solve this spike in cross-track error on curves”, or as business-focused as “Can we get a heatmap of all the places our trucks have driven over the last year”.
	- Additionally, you can bring best-practices for data-science to the company, including helping build up the base platform and infrastructure necessary to speed up data-centric work. Starsky has a solid base of tooling around our data, but it is ripe for improvement.
	- Demonstrated expertise in the data scientists modern toolkit: Pandas, R, SQL, etc, and don't mind sharing your experience with the team
	- Deep quantitative thinker: Masters or PhD in a quantitative field, or multiple years of experience in a quantitative-focused position
	- Relish delivering answers and metrics and seeing change affected by your work
	- Can take high level directives and take them through from research project, proof of concept, to applied & implemented feature.
	- Are constantly looking for problems that could be solved with liberal application of data
+ skill set:
	- Develop pipeline for data tagging, labelling and munging to be consumed for training of ML models for vision based tasks.
	- Architect and train machine learning models for object detection and tracking
	- Build testing environment to test the model and simulate edge-case performance scenarios
	- Experience with at-least one of Tensorflow/Caffe/Theano/Torch
+ skill set:
	- Qt (or PyQt)
	- Microcontroller firmware
+ Infrastructure as code experience (we use terraform)
+ Knowledge of TypeScript, React, Jest, Cypress is a plus
+ Background in linear algebra, probability, 3D geometry and abstract problem solving skills
+ Experience in Computer Vision / Computational Geometry / Structure from Motion / SLAM
+ advanced optimization algorithms:
	- Evolutionary Algorithms
	- surrogate model optimization
	- particle swarm optimization
	- Bayesian optimization
+ [Numerical Library](https://en.wikipedia.org/wiki/List_of_numerical_libraries)
+ skill set:
	- Experience in developing and debugging multi-threaded/parallel applications.
	- Experience in image processing, computational geometry, large data application, high performance computing and scientific simulation is a good plus.
+ Research, evaluate, and present statistical or Machine Learning methods to provide actionable insights.
+ Direct or indirect experience in OPC (Optical Proximity Correction), including rogorious lithography simulation (Hyperlith, Prolith), RET, and advanced mask technology.
+ AWS DynamoD
+ Enforce SOX & GDPR compliance across the analytics database and reporting tools
+ Solid understanding of imaging theories (Abbe, Hopkins).
	- Abbe-PCA (Abbe-Hopkins): microlithography aerial image analytical compact kernel generation based on principle component analysis
	- Hybrid Hopkins-Abbe method for modeling oblique angle mask effects in OPC
	- Application of the hybrid Hopkins–Abbe method in full-chip OPC
	- transmission cross coefficients (TCCs)
+ Good grasp of statistical concepts (e.g. hypothesis testing, regression)
+ Love Github, Slack, Asana, AWS, Meteor, Node
+ skill set:
	- Experience with data processing frameworks and data warehouses such as Hadoop, Spark, Redshift
	- Experience with designing, implementing, and optimizing ETL in Pentaho
+ skill set:
	- Technical fluency in one language and tool such as Python, Java or Scala, AWS (S3/EMR/Athena/Glue) and SQL.
	- Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow.
	- Experience working with either a MapReduce system of any size/scale.
+ skill set:
	- GatsbyJS
	- ElasticSearch
+ skill set:
	- Experience in development using MEAN stack (Node.js, Angular.js, Express.js, MongoDB)
	- Data exchange technologies like JSON
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
	- Highly preferred Web UI or dashboarding experience (ie CSS, HTML, Tableau, Qlik, etc.
	- Knowledge and hands on experience on implementation of Chatbot using Microsoft Bot Framework, API.AI or Watson
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
+ skill set:
	- Minimum 3 years of designing, building and operationalizing large scale enterprise data solutions and applications using one or more of Azure / AWS / GCP data and analytics services in combination with custom solutions -  Spark, Azure Data Lake, HDInsights, SQL DW, DocumentDB, Search, Elastic Pool etc.  
	- Minimum 3 years experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on AZURE.
+ skill set:
	- Big Data platforms e.g. Cloudera, Hortonworks MapR
	- Big Data Analytic frameworks and query tools such as: HDINsight, Spark, Storm, Hive, Impala
	- IoT protocols, gateways, queues, messaging hubs such as IoT Hub, MQTT, XMPP, CoAP, etc.
	- IoT development experience on at least one of the industry leading platforms (Azure IoT, AWS IoT, GE Predix, Siemens Mindsphere, PTC Thingworx, SAP Leonardo, GCP)
	- Streaming data tools and techniques such as Apache Kafka, Azure Streaming Analytics, AWS Kinesis
+ skill set:
	- Experienced and interested in Ruby, Elixir, Java, Python, Node.JS, Phoenix Elixir, or other backend programming language or framework
+ skill set:
	- Minimum 1 year of building and coding applications using at least two Hadoop components – MapReduce, HDFS, Hbase, Pig, Hive, Spark, Scoop, Flume, etc
	- Minimum 1 year coding one of the following: Python, Pig programming, Hadoop Streaming, HiveQL
	- Minimum 1 year understanding of data modelling & data pipeline design: iterative data pipeline development from raw, curated, integrated to published data, with fit for use data modelling on Hadoop and NoSQL platforms
	- Minimum 1 year of experience implementing large scale cloud data solutions using Cloud Service Providers:  AWS data services (e.g. EMR, Redshift, GLUE) or Azure (Data Lake Store/Analytics, SQL Data Warehouse) or Google Cloud Platform Google Cloud (Big Data:  Big Query, Big Insights)
	- Minimum 1 year of experience delivering an operational Big Data solution using one or more of the following technologies: Hadoop, HortonWorks, Cloudera, Cassandra
	- Minimum 1 year of experience throughout the SDLC of a Hadoop implementation technologies including HortonWorks, Cloudera, Hive, Pig, MapReduce 
	- Minimum 1 year of experience throughout the SDLC of a HortonWorks, Cloudera, Cassandra / Hbase implementation 
	- Minimum of a Bachelor's Degree or 3 years IT/Programming experience
	- Minimum 1 year of experience developing REST web services
	- Industry experience (financial services, resources, healthcare, government, products, communications, high tech)  
	- Experience leading teams
	- Machine Learning tools, interfaces & Libraries: R, R-Studio, Spark R, sparklyr, MLlib, H2O etc.  
	- Experience with other tools, databases and Apache projects: Google BigQuery, Presto, Drill, Kylin, OpenTSDB, Spark Streaming
	- Enterprise data integration, BI and analytics platforms: Informatica, Talend, InfoSphere, SAS, RevoR, QlikView, Qlik Sense, Tableau, Spotfire, D3.js
	- Processing frameworks & programming tools: Spark (Scala/Python/Java), Kafka, Flink
	- Client facing skills: ability to build trusted relationships with client stakeholders and act as a trusted adviser
+ skill set:
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
+ Experience with higher education SIS and LMS systems (Banner, Colleague, Jenzabar, Canvas, Blackboard, etc.) strongly preferred
+ Experience with monitoring and tracking tools such as  Splunk, NewRelic, Adobe/Google Analytics
+ Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (Redis, Cassandra or DynamoDB)
+ skill set:
	- Experience with Java, Spring Boot
	- Experience with React, Backbone, Marionette or equivalent framework
	- Experience with Protractor, RSpec or equivalent integration test framework
	- Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (DynamoDB) is a plus
+ skill set:
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- A fascination with the PoseNet research since its release in 2015.
	- Fundamental understanding of Bundle Adjustment or Non-Linear Optimization.
	- PhD or exceptional MSc involving 3D-Reconstruction, SLAM, Camera Calibration or Computational Geometry from a top ranking university or lab.
	- Geometry from Vision is at the heart of Scape Technologies. As a research engineer on the Scape Technologies team, you will take a key role in designing and building the pipeline for cloud-based 3D-reconstruction and real-time global localization. This will require implementing and building upon existing research in Structure-from-Motion, Dense 3D-reconstruction, Camera Calibration and SLAM.
	- This can range from non-linear optimization to efficient graph traversal, considering optimized computational parallelization.
	- Making sure that Scape's large scale reconstruction and localization pipeline is the most efficient in the world.
	- We are looking for curious and enthusiastic computer vision scientists who are keen on working on moonshot projects.
+ skill set:
	- 7+ years Experience with large-scale distributed systems and client-server architectures.  Examples include Java/Spring Boot, CQRS, event streaming, Kafka, Spark
	- Deep experience with Cloud Computing platforms (e.g. Amazon AWS, Microsoft Azure, Google App Engine)
	- Knowledge and understanding of relevant legal and regulatory requirements, such as SOX, PCI, HIPAA, Data Protection, etc.
	- Demonstrated experience implementing and managing high capacity, redundant, and mission critical environments
	- Knowledge in databases and comfortable with various databases technologies.  Examples include relational database (Oracle) and/or NoSQL data technologies (Mongo, Cansandra, Couchbase) and related toolsets.
	- Proficiency in TCP/IP networking, architecture and core technologies ( DNS, routing, iptables, tc, etc.)
	- Experience running and maintaining a 24x7 production environment
+ skill set:
	- Demonstrates knowledge of the data engineering domain with experience in building and supporting non-interactive (batch, distributed) or real-time, highly available data, data pipelines.
	- Able to build fault tolerant, self-healing, adaptive computational pipelines
	- Contribute to the decision-making process related to the selection of software solutions that make up the architecture
+ skill set:
	- At least 5 years demonstrated results in areas of Operations Research and/or Supply Chain Projects (inventory optimization, network design, and S&OP) in sophisticated and complex environments including the use of simulation and modeling tools (Llamasoft, CPLEX, Gurobi, or other similar)
	- At least 5 years performing data analytics and modeling with advanced languages (e.g. Python or R)
+ skill set:
	- Minimum of 3 years' delivery experience in advanced modeling environment: strong understanding of statistical concepts and predictive modeling. (e.g., AI neural networks, multi-scalar dimensional models, logistic regression techniques, machine-based learning, big data platforms, SQL, etc.).
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
	- Possess a blend of marketing acumen, consulting expertise and analytical capabilities that can create value and insights for our clients.
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Knowledge of Deep Learning (CNN, RNN, ANN) using TensorFlow.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 1 year of building Business Data Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies (e.g  Alation, Informatica or custom portals).
+ You're familiar with business intelligence reporting platforms like OBIEE, Tableau, MicroStrategy, and Business Objects
+ skill set:
	- You know how to work with data engineering technologies like Spark, no SQL DB or Lambda
	- You know everything there is to know about Robotic Process Automation
	- A minimum of 7 years experience in deep learning, machine learning or artificial intelligence applications like virtual agent, RPA, or video/image/text analytics
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of experience designing and building data models to support large scale BI, Analytics and AI solutions for Big Data.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
+ skill set:
	- Minimum 3+ years of architecting, implementing and successfully operationalizing large scale data solutions in production environments using Hadoop and NoSQL ecosystem on premise or on Cloud (AWS, Google or Azure) using  many of the relevant technologies such as  Nifi, Spark, Kafka, HBase, Hive, Cassandra, EMR, Kinesis, BigQuery, DataProc, Azure Data Lake etc.  
	- Minimum 2+ years of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale and others
	- Minimum 3+ years of architecting data and building performant data models at scale for Hadoop/NoSQL ecosystem of data stores to support different business consumption patterns off a centralized data platform  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
	- Minimum 3++ years of architecting and industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scale
	- Minimum 2+ years of experience implementing large scale BI/Visualization solutions on Big Data platforms
	- Minimum 3+ years of experience implementing large scale secure cloud data solutions using AWS data and analytics services e.g. S3, EMR, Redshift
	- Minimum 2+ years of experience implementing large scale secure cloud data solutions using Google data and analytics services e.g. BigQuery, DataProc
	- Minimum 2+ years of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for modern data platforms that use Hadoop and NoSQL on premise or on AWS, Google and Azure cloud
	- Minimum 2+ years of experience securing Hadoop/NoSQL based modern data platforms on-premise or on AWS, Google, Azure cloud
	- Minimum 2+ years of Re-architecting and rationalizing traditional data warehouses with Hadoop or NoSQL technologies on premise or transition to AWS, Google clouds
	- Experience implementing data wrangling and data blending solutions for enabling self-service solutions using tools such as Trifacta, Paxata
	- 4 years industry systems development and implementation experience OR Minimum of 3 years of data loading, acquisition, storage, transformation, and analysis
	- Minimum 2+ years of using Talend, Informatica like ETL tools within a Big Data environment to perform large scale metadata integrated data transformation
	- Minimum 1+ years of building Business Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies
	- Architect modern data solutions in a hybrid environment of traditional and modern data technologies such as Hadoop, NoSQL
	- Create technical and operational architectures for these solutions incorporating Hadoop, NoSQL and other modern data technologies
	- Implement and deploy custom solutions/applications using Hadoop/NoSQL
	- Lead and guide implementation teams and provide technical subject matter expertise in support of the following:
	- Designing, implementing and deploying ETL to load data into Hadoop/NoSQL
	- Security implementation of a Hadoop/NoSQL solutions
	- Managing data in Hadoop/NoSQL co-existing with traditional data technologies in a hybrid environment
	- Troubleshooting production issues with Hadoop/NoSQL  
	- Performance tuning of a Hadoop/NoSQL environment
	- Architecting and implementing metadata management solutions around Hadoop and NoSQL in a hybrid environment
+ skill set:
	- Work in an agile, CI/CD based, test-driven development environment
	- Semantic Web (RDF/SPARQL)
+ skill set:
	- At least 2 years designing and building healthcare data analysis solutions for the business payer or provider industry
	- At least 2 years using new developments in AI, machine learning, cognitive systems, and robotics to build amazing analytical tools
	- At least of 2 years working with tools like SAS, Python, SPSS, R, or SQL
	- At least 2 years working with data integration tools to streamline processes in platforms like Cerner EMR, Apache Spark, MapReduce, MongoDB and Couchbase
	- You can use data mining techniques to solve real world business problems
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years' experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ skill set:
	- You've got a Master's degree in statistics, econometrics, mathematics, or deep learning architectures including convolutional, recurrent, autoencoders, GAN's, and ResNets
	- You're a coding wizard with Python, C# (.NET), Scala, MxNet, CNTK, R, H2O, TensorFlow, PyTorch, cuDNN, NumPy, and SciPy
+ skill set:
	- At least 4 years' experience in deep learning, machine learning or artificial intelligence applications like virtual agent, robotic process automation and video/image/text analytics
	- Minimum of 2 years' experience in AI/ML/RPA functional expertise with developing use cases and building/leading Proofs of Concept
	- At least 2 years architecting AI Pipelines orchestrating multiple analytics engines
+ skill set:
	- Minimum 5 years of developing machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other.
	- Strong quantitative and analytical skills with minimum 3 years of experience with data science tools, including Python, R, Scala, Julia, or SAS
	- Ability to technically lead data science projects
	- Deadline-driven, organized and able to multi-task
	- Familiarity with using cloud services (AWS, Google, Azure) or Big Data tools (Hadoop, Hive, Spark) in data science solutions
+ skill set:
	- Proven experience with caching, queuing, RPC frameworks and other building blocks of a large scale distributed systems.
	- Experience with NoSQL AWS data stores like DynamoDB, CloudSearch or their open source equivalents like Cassandra, HBase, Solr or ElasticSearch
	- Experience with React or other modern javascript frameworks.
	- Experience with MySQL, Redis, Memcache and related web-backend technologies.
	- Experience with data pipelines (Kafka, AWS Kinesis, AWS Data Pipeline)
	- Experience building web applications, widgets, or interactive experiences.
+ skill set:
	- Project management skills - JIRA, roadmapping, etc.
	- Experience with any of Ruby, Java, or modern web frameworks like React
+ skill set:
	- Investigate the feasibility of applying scientific principles and concepts to business problems.
	- Understand the Goodreads/Amazon data structures (MySQL/Data Lake/Redshift).
	- Acquire data by building the necessary SQL ETL queries.
	- Import processes through various company specific interfaces for RedShift and Data Lake storage systems.
	- Analyze data for trends and input validity by inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies.
	- Build models using statistical modeling, mathematical modeling, econometric modeling, network modeling, social network modeling, natural language processing, machine learning algorithms, genetic algorithms, and neural networks.
	- Validate models against alternative approaches, expected and observed outcome, and other business defined key performance indicators.
	- Develop metrics to quantify the benefits of a solution and influence project resources. Partner with Engineering/Data Engineering to improve the quality of existing data and bring additional data sources in line. Audit metric data and measure project progress and success. Build/automate reports/dashboards (in Tableau) that allow the business leaders to get a clear snapshot of their operations. Design and analyze A/B tests to quantify impact of customer-facing changes. Develop innovative experimental design and measurement methodologies to understand customer growth and business efficacy. Participate in discussions, team planning, office hours, and metric reviews. Design and implement scalable and reliable approaches to support or automate decision-making throughout the business. Communicate insights to the business partners, Goodreads leadership, and Amazon stakeholders, with an emphasis on clarity, completeness, and actionability.
+ skill set:
	- Experience designing and operating very large Data Warehouses
	- Deep understanding and knowledge of AWS stack - Redshift, EMR, S3
	- Ability to work with search technologies such as Elasticsearch
	- Knowledge of graph databases such as AWS Neptune
+ Lead evaluate of next generation technologies by conducting RFI, RFP, and POCs.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Participate in cutting edge research in machine intelligence and machine learning applications.
	- Develop solutions for real world, large scale problems.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- Solid Machine Learning background and familiarity with standard speech processing and machine learning techniques
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Industry or academic experience in deep learning research.
	- Strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, KDD, PAMI, JMLR, TACL, IJCV).
+ skill set:
	- You have industry experience with writing code (e.g., Python, Scala, PySpark, Java) and taking ML models/ algorithms to production. Preference for 5+ years of industry experience (without PhD); at least 2-3+ years of industry experience with PhD. This is not an entry level / new college graduate role.
	- Experience with Apache Spark platform (including Datasets, SparkML) and/or experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe or PyTorch).
+ skill set:
	- **Salesforce Research and Einstein.AI (formerly MetaMind) are looking for extraordinary deep learning or research engineers.**
	- As a deep learning or research engineer, you will work with research scientists and engineers to develop and productize new cutting edge models and associated artifacts such as data preparation pipeline and model characterization logic. You will ensure these models are developed to support accuracy, performance or other specific customer requirements.
	- You will work with platform team to support deployment of these models. In other words, you are problem solver, a deep learning model designer, and an engineer who makes sure the model is deployed at scale to serve our customers with state-of-the-art speech, vision, and language technologies.
	- You have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, voice, or reinforcement learning. Additionally, applicants should have in-depth experience with problems such as text categorization, information extraction, question answering, text summarization, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Partner with product managers to understand customer requirements
	- Conduct research (including reviewing relevant literature) and collaborate with our research team to identify appropriate solution candidates
	- Develop prototypes, then design and carry out experiments to validate and improve the prototypes
	- Bring the ideas to production
	- Monitor model behaviors in production and iteratively improve quality of services over time
	- Work on cutting-edge research in machine learning
	- MA/MS or PhD degree in computer science, artificial intelligence, machine learning, speech recognition, natural language processing, or related technical field such as operations research, computational mathematics, etc.
	- Research experience or contributions in deep learning, machine learning, NLP, computer vision, reinforcement learning, or optimization.
	- Solid Machine Learning background and familiarity with machine learning techniques
	- Problem solving and ability to reuse, customize, and implement latest research
	- Experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, or PyTorch)
	- Industry experience in deep learning research
	- Can thrive in team environments; using agile methodology and interacting with Product Leaders, Scientists and Engineers to solve technology's greatest challenges
	- In particular, we are looking for experienced engineers with Deep Learning experience and domain expertise around Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Vision to provide the best possible experience for our customers.
	- Experience designing and implementing machine learning pipelines in production environments.
	- Experience in building speech recognition and natural language processing systems (e.g. commercial or government-funded speech products) is a huge plus.
	- We value professional industry experience; advanced degrees alone do not replace real world experience.
	- Excellent communication, leadership, and collaboration skills.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.
	- Experience with one or more general purpose programming languages including but not limited to C/C++ or Python.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- As a research engineer at Salesforce Research, your role will be at the intersection of software engineering and research, and may range from implementing novel research models to rapid-prototyping demos that show off applications of deep learning on production data. You will work closely with research scientists to develop models, prototypes, and experiments that push the state of the art in AI research, paving the way for innovative products for the Einstein AI Platform. You will have the opportunity to take on real-world problems from Salesforce's enterprise customers with the latest deep learning models.
	- You have strong programming skills and a background in one or more of the following domains: deep learning, machine learning, natural language processing, or computer vision, with applications such as: text categorization, text summarization, sentiment analysis, information extraction, question answering, dialogue learning, language and vision, image classification, image segmentation, and object detection.
	- Knowledge of linear algebra, calculus, statistics, and machine learning.
	- Practical experience in natural language processing, computer vision, crowdsourcing, or information retrieval.
	- Exposure to industry or academic research, particularly in deep learning, neural networks, or related fields.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Experience with Amazon Web Services and Mechanical Turk.
	- Strong computer systems experience in topics such as filesystems, server architectures, and distributed systems.
	- Experience in GPU programming, data visualization, or web development.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, object detection or reinforcement . Our postdoctoral researchers have the ability to give talks, attend conferences and build relationships with academic institutions if desired.
	- Collaborate on research to advance the science and technology of artificial intelligence.
	- Contribute to cutting edge research projects in machine intelligence and machine learning applications that can be infused into our world-class CRM.
	- Develop solutions for real world, large scale problems.
	- Influence progress of relevant research communities by producing publications.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Create a year long project proposal with research managers.
	- First-author publications at AI conferences and journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
+ skill set:
	- Salesforce Research Asia is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision,
		* speech recognition, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue systems, language and speech, machine translation, language and vision, image classification, object detection, or image semantic segmentation, etc.
	- Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, ECCV, SIGKDD, PAMI, JMLR, TACL, IJCV) are preferred.
	- Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	- Experience with one or more deep learning libraries and platforms, e.g. PyTorch, TensorFlow, Caffe, or Chainer
	- Strong background in machine learning, natural language processing, speech, computer vision, or reinforcement learning
	- Strong algorithmic problem solving skills
	- Programming experience in Python, Java, C/C++, Lua, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV) are preferred.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area
	-  Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	-  Experience with one or more deep learning libraries and platforms, e.g. Torch, TensorFlow, Caffe, or Chainer
	-  Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	-  Strong algorithmic problem solving skills
	-  Programming experience in Python, Lua, Java, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for an outstanding entry level research scientists focused on ethics in AI. It is our belief in the words of our CEO Marc Benioff, “The business of business is improving the state of the world." The way we behave — with integrity, transparency, alignment, and accountability — builds trusted relationships. We believe that companies can do well and do good in the world. We know technology is not inherently good or bad. It's what we do with it that matters. With AI, we believe that we can go even further to advance and support its effectiveness by ensuring equality, transparency, and accountability in the models we create and how we implement them in our products.
	- As a research scientist, you discover new research problems, develop novel models, design careful experiments and generally advance the state of the art in AI. At Salesforce, the research team is committed to collaboration with the wider research community. In this unique role, you will have the opportunity to work directly on advancing technologies that nonprofits use to solve problems in the real world that create positive impact for the world while accomplishing publications at major conferences. We believe that making substantive progress on hard problems can drive and sharpen the research questions we study, and, in turn, scientific breakthroughs can spawn entirely new applications. With this in mind, the team maintains a portfolio of projects, some with an immediate path to production, others that may not find an application for several years. Research scientists have the freedom to set their own research agenda and move between pure and applied research.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area (e.g., Machine Learning, AI, AI ethics, law and policy)
	- Excellent understanding of deep learning models and techniques (i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods)
	- Experience with one or more deep learning libraries and platforms (e.g. PyTorch, TensorFlow)
	- Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	- Programming experience in Python or a similar language
	- Strong algorithmic problem-solving skills
	- Demonstrable experience implementing machine learning models and algorithms, e.g., through open-source implementations, or shareable code
	- Strong presentation and communication skills
	- Experience applying deep learning models to ethical issues in AI or social causes (e.g., racial disparity in facial recognition, explainability of AI for redress and remediation)
	- Experience researching artificial intelligence ethics, including areas such as fairness, safety, privacy and transparency in artificial intelligence
	- Published in top-tier conferences or journals (e.g., FAT*, NIPS, AIES, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV)
	- Open-source implementations of machine learning research projects.
	- The ideal candidate will have a keen interest in producing new science to understand intelligence and technology and how to apply it safely and fairly in real-world settings.
+ skill set:
	- Experience testing web services with tools such as SoapUI.
	- Excellent knowledge of web technologies such as React, NodeJS, AngularJS, D3JS, JavaScript, GWT, EXTGWT, CSS3, HTML5
	- Excellent knowledge of at least one server-side programming language
	- Excellent knowledge of web services development (SOAP, REST, Web Socket, RPC)
	- Proficiency with major development tools and processes such as revision control, requirement specs, unit & system tests, etc.
	- Proven ability to deliver on time working in a fast-paced agile environment
	- Ability to work with team to clarify and prune requirements, strong verbal and written communication skills
	- OO design and Java/Scala development experience.
	- Experience with Java Spring Framework
	- Good understanding of virtualization, Linux Container, Docker
	- Working knowledge of Big Data frameworks such as Hadoop, Storm, Spark, Flume, Kafka
+ skill set:
	- Proficient programmer that can code efficient algorithms (like map-reduce, preferably in Java) that traverse data partitioned in a distributed architecture
	- Design, build, and deploy distributed querying strategy to achieve highly scalable and resilient transactional processing and reporting for different size and shape workloads
	- Perform analysis on data access patterns to uncover opportunities to improve query throughput and drive decision making on new architectures. Recommend best practices.
	- Design efficiently distributed query service for low latency access and traversal for transactional and reporting use cases
	- Influence and collaborate cross functional teams in coming together towards a common, data architecture
	- Learn and fundamentally understand the Workday technology stack including a home-grown meta-data driven application development environment
	- Be responsible for system stability by proactively identifying and diagnosing issues and rapidly deploying code to address production issues
	- Strong coding experience in any language
	- Good working experience of distributed systems gossip protocols and consensus algorithms
	- Experience implementing distributed computing frameworks and architectures
	- Good knowledge of network protocols, routing and handshaking
	- Good experience performance tuning/ garbage collection / JVM internals
	- Proficient knowledge of maintaining and debugging live, business critical software systems
	- Good understanding and hands on experience with SQL, especially in the area of data aggregation and query performance tuning
	- Communicates clearly to engineering peers including ability to identify and communicate data-driven insights
+ skill set:
	- Strive for high code standards (continuously improving testability and code quality).
	- Disciplined, methodical, minimalist approach to design and construct layered software components that can be embedded within larger frameworks or applications.
+ skill set:
	- Projects around data ingestion, detection and the MITRE ATT&CK framework
	- Handle alarms and alerts, across multiple clouds and all infra (prod/corp/dev)
	- Tune alarms; we're allergic to false positives
	- Create the glue between systems to make your life easier
	- Automate responses
	- Build runbooks
	- Perform triage and incident response
	- Build reporting and present to leadership
	- Technical depth, a desire to get things done, and be recognized for your achievements
	- Experience at a SOC and you want to do more
	- Knowledge of Linux and/or Mac
	- Exposure to at least one cloud environment (AWS, GCP, Azure)
	- Some prior Security experience
	- Ability to script in a language like Python, Ruby or Perl is a plus
+ Proven capability to create maintainable, adaptable software that is non-brittle and capable of change
+ Take pride in the quality of the code you write. Your code is readable, testable, and understandable six months later. You adhere to the Zen of Python.
+ **Experience and knowledge of programming languages, data analysis packages (e.g., Python, R, SAS, MatLab, Stata, GAMS, SPSS, Hadoop, BigML, Pandas). Experience and knowledge of visualization tools (e.g., Tableau, Sigma JS) is preferred.**
+ Experience working with distributed computing tools like Spark or Hadoop
+ Working knowledge of data analysis packages (e.g., SAS, MatLab, Stata, GAMS) is strongly preferred.
+ Proficiency with prototyping and design tools (e.g., Sketch, InVision, Adobe Creative Suite, Axure).
+ Create product backlog/requirement documents (epics, stories, scope docs, etc) to guide engineering in developing the platform features, and work closely with the engineering and QA teams to execute the product feature per specification
+ Experience with DFIR techniques and tools (FTK, EnCase, SIFT, including Volatility) and eDiscovery
+ Familiar with orchestration components (Chef-Puppet-Ansible-Kubernetes-VSTS)
+ Supports the full lifecycle of a capability from Early Adopter Programs (EAP) to General Availability (GA) and across a full range of commerce events including new sales, upgrades, renewals, downgrades, and cancellations with robust monitoring and instrumentation.
+ skill set:
	- Achieve security architecture compliance on requirements, including: Sarbanes-Oxley, payment card industry standards, HIPAA/HITECH, global data privacy requirements, as well as state and federal regulations.
	- Exceptional experience in designing cloud security architecture for Azure and/or AWS.
	- Establish a strategic security architecture vision, including standards and frameworks that are aligned with overall business strategy.
	- Provides architectural oversight and direction for enterprise-wide security technology.
	- Review existing architecture, identify design gaps, and recommends security enhancements.
+ skill set:
	- Application systems, network architecture, multiple platforms and new technologies from a security perspective to include, but not limited to, Firewalls; Intrusion Detection/Protection Systems; Operating Systems (UNIX, Windows); Networking (switches, routers, protocols, etc.); Network Services and Security Vulnerabilities; Network Architecture; Remote Access; Multiâ€factor Authentication; Platform Security (Application, Database, OS); Antivirus; Federated Identity Management; Cryptography; Active Directory; and high-level programming languages.
	- System and network exploitation, attack pathologies and intrusion techniques (such as denial of service, sync attacks, malicious code, password cracking, etc).
+ skill set:
	- Experience managing enterprise monitoring solution; System Center Operations Manager (SCOM), Solarwinds, and/or CA UIM preferred
	- Experience managing server automation tool and server patching tool; System Center Configuration Manager (SCCM) preferred
	- Experience creating or modifying scripts or automation, such as Perl, PowerShell, Python, TCL/TK, Ruby or similar for cloud orchestration required
+ Experience working with SOAP, REST APIs and micro services
+ The ideal candidate will have at least 5 years of experience as a UX designer including user research, prototyping and using visual communication tools (e.g. Sketch, InVision, Axure).
+ Ideal candidate will have at least 5 years of experience as a UX designer, including; user research, prototyping and using visual communication tools (eg. Axure, Balsamiq, Sketch).
+ skill set:
	- 2+ years SQL working experience (Redshift/PostgreSQL/MySQL)
	- Experience with BI & reporting dashboards (Periscope, Tableau, etc)
+ skill set:
	- Experience developing, troubleshooting, tuning and managing hardware systems including:
		* LIDARs
		* Standard or Depth Cameras
		* Light systems
		* IMUs, encoders or other odometry methods
	- Experience designing/developing one or more of the following software systems/algorithms:
		* Mapping, Localization, and/or SLAM
		* Robot perception, Sensor calibration and/or fusion
		* Motion planning and/or Autonomous navigation
		* Production level configuration, log and system management
		* Remote monitoring and control
		* Performance verification and monitoring
		* Test automation of combined software and hardware components
		* Automated release pipelines
+ Modern networking: HTTP, TCP, MQTT and can write software that tolerates network outages
+ skill set:
	- Develop and maintain scalable codebase used to calibrate sensors and actuators used on a robot system including:
		* Camera: Intrinsic, Extrinsic, White-balance calibration of both standard and fisheye cameras
		* Time-of-Flight cameras
		* Lidar
		* IMU
		* Odometry
	- Improvement of existing calibration processes and procedures
	- Develop calibration methods for new sensors added to robot
	- Hands-on deployment of calibration fixtures at contract manufacturing line
	- Hands-on calibration of robot system to validate new calibration processes
	- Assist mechanical, electrical and other teams to design fixtures needed for calibration
	- Work with manufacturing and field engineers to debug field issues related to calibration
	- Work with hardware and software teams to provide well calibrated robot system
	- Good C++, C and python coding skills
	- 2 years of industrial experience
	- Previous experience with extrinsic calibration of at least two of the following sensors: RGB Camera, ToF Camera, Lidar, Odometry, IMU
	- Understanding of standard camera calibration methods
	- System level understanding of how calibration affects robot performance
+ skill set:
	- Work with product management to write visualizations and searches in ELK.
	- Design queries and system to monitor data quality from our robot fleet
	- Extend the ELK platform to support ongoing use cases through adding infrastructure
	- Support ongoing management of the ELK cluster
	- Build tooling and infrastructure to increase observability of services.
	- Support cloud operations by performing triage and responding to incidents.
	- Production experience with the ELK platform
	- Experience managing infrastructure and services on a public cloud provider; AWS, GCP, or Azure
	- Experience with configuration as code; Puppet, SaltStack, Ansible, or Chef
	- Proficiency with Java, Python, or Go
+ skill set:
	- Exposure to containers or orchestration services:  Kubernetes, Mesos, or Docker Swarm
	- Experience with configuration as code; Puppet, SaltStack, Ansible, or Chef
+ skill set:
	- Experience using native APIs from higher ed core systems (SIS, ERP, LMS) a plus
	- 3+ years' experience with enterprise level data integration working with multiple systems simultaneously; Including extracting data utilizing API integration from a variety of platforms, performing data mapping, data transformation, and loading data to the target system
+ skill set:
	- Talend ETL, SQL, Postgres, AWS
	- Design, develop, and implement advanced ETL pipelines that bring together data from disparate sources, making it available to users using a variety of ETL tools.
	- Facilitate cross-functional data-integration efforts upstream and downstream
	- Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues
	- Extract data from multiple sources, and integrate them into a target database, application, or file using efficient programming processes.
	- Implement and deploy solutions in a CI/CD pipeline
	- Write and refine code to ensure performance and reliability of data extraction and processing.
	- Communicate with all levels of stakeholders as appropriate, including product managers, application developers, business users.
	- Participate in requirements gathering sessions with product managers and technical staff to distill technical requirements from business requests.
	- Recommend process improvements to increase efficiency and reliability in ETL development.
	- Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
	- Some of our technologies might include: Talend as well as various data stores such as Postgres SQL, S3, Aurora and AWS services.
	- 2+ years of experience on Data Warehousing and building data pipelines.
+ Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Knowledge of Big Data, SAP ERP, Docker, Kubernetes, CXF or another ETL product is a plus;
	- AWS, Azure, Google cloud, Apache Beam, NoSQL
	- Experience of working with JDBC, XML, Junits, Maven, Avro and JSON;
	- Good understanding of Web Services (SOAP/REST), knowledge of CXF is a plus.
+ skill set:
	- Experienced in data sanitization, data import and export (ETL).
	- Familiar with SQL Server products, i.e. SQL Integration Services and Reporting Services.
	- Work with the application team to create and maintain effective database-coupled application logic stored procedures, triggers and user-defined functions (UDFs); these are programs that are under the control of the DBMS (SQL, MySQL, Postgres, MongoDB)
+ skill set:
	- Using different protocols as needed for different data services (NoSQL/JSON/REST/JMS/JMX) and providing related tests (Test-driven development/Unit Testing and automation)
	- Designing, implementing, testing, and deploying a data processing infrastructure that is fault tolerant and scalable to support multiple Talend Products
	- Building High-Availability (HA) architectures and deployments
	- Between 3 and 5 years in professional Java programming with RESTful, Message-/Event-Driven technologies, Multi-threaded applications
	- An understanding of distributed and cloud computing, incl. deployment related experience (ideally including Docker, AWS or Azure)
	- Experience with OAuth and other IAM related technologies would be a plus
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
	- Document structure, process and design of all implemented solutions
	- Between 3 and 5 years in professional Java programming with RESTful, Message-/Event-Driven technologies, Multi-threaded applications
	- Experience with development using Scala/Akka
	- An understanding of distributed and cloud computing, incl. deployment related experience (ideally including Docker, AWS)
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Experience with Apache Big Data technologies such as Hadoop, Spark, Hive, Flink, Kafka, Beam etc
	- Experience with messaging systems such as ActiveMQ
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- TMC runs as load-balanced application server instances on Amazon Web Services (AWS). After building a data pipeline in Talend Studio, the job is executed on one or several so-called “Cloud Engines” or “Remote Engines”.
	- Cloud Engines are Java-based runtimes deployed via an Amazon Machine Image (AMI) based on CentOS.
	- Remote Engines are optional Java-based runtimes deployed by the customer to process data behind the firewall or on a Virtual Private Cloud (VPC), e.g. this can be on-premises or in third party clouds like Google, Azure or AWS.
	- Within this position the development will mainly be focused on server-side Java development, leveraging technologies from the Hadoop Ecosystem (like HDFS, Spark, Kafka, Zookeeper), Log Data Mgmt (like ElasticSearch), Identity & Access-Management, Metric Collection Solutions, and others. This provides the applicant with the ability to work with up to date technologies, in a modern cloud-based development and deployment environment.
	- Built High-Availability (HA) architectures and deployments
	- Worked and communicated in a cross-functional geographically dispersed team environment comprised of software engineers, product managers, software test engineers, and product support engineers.
	- Good knowledge in Java ecosystem (Java 8, Spring, junit, logging)
	- Skills in Restful Service and the microservice architecture (SpringBoot)
	- Some basic knowledge in databases like NoSQL DB (MongoDB)
	- Knowledge in CI tools (Maven, Jenkins)
	- Some knowledge in AWS and Docker would be a plus
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Experience building or maintaining databases (MySQL, Hive, etc.)
	- Experience building or maintaining Big data & streaming systems (Hadoop, HDFS, Kafka, etc.)
	- Cross-platform coding
	- Security
	- Large-scale, large-user base website development experience
	- Data mining, machine learning, AI, statistics, information retrieval, linguistic analysis
	- Strong mathematical background
+ skill set:
	- Help us iterate quickly and deliver high-quality software releases, on-time
	- Understanding of service based cloud architectures
	- Work closely with the software engineering, product management and customer support teams to design, deliver, and manage our services with high uptime
	- Implement monitoring, develop automated provisioning, and develop self-healing automation
	- Perform incident resolution and root cause analysis of critical outages
	- Experience with design and maintenance of a cloud based highly-available (HA) architecture
	- Experience with configuration management and monitoring tools
+ skill set:
	- Partner with engineering leadership to buildout data driven roadmap items to address performance in critical areas
	- Established performance test environments and frameworks
	- Experience evangelizing performance engineering techniques within a data driven engineering culture
	- Deep hands on experience with JVM tuning techniques
	- Supported efforts in performance testing and improvement in common JavaScript frameworks (Angular, React, JQuery)
	- Experience with Ruby (JRuby) and JavaScript
	- Extremely well versed in solving data access performance challenges across SQL data stores
	- Experience in AWS and other cloud providers when exploring different approaches to performance engineering
	- Experience with distributed architectures
	- Passionate about driving a performance engineering culture
+ skill set:
	- Looker is seeking a Senior Software Engineer to join our Data Model team (database semantics, programming languages, and integrated development environment (IDE)). This team's core responsibilities include Looker's SQL normalization and code generation engine (the heart and lungs of our application), the LookML language itself, Looker's in-browser IDE for composing and versioning LookML, and the data pipeline within the Looker application.
	- The ideal candidate will take an active role in contributing to our long-term technical roadmap and have a deep background in programming language fundamentals (e.g. compiler design) or databases (e.g. building SQL optimizers) or building IDEs, in addition to tried and true experience with software engineering best practices.
+ skill set:
	- Machine learning. You should be able to understand and apply major machine learning methods, such as logistic regression, SVM, Decision Trees, Principal Component Analysis and K-means. Completion of Andrew Ng's Machine Learning course on Coursera is sufficient to meet this criterion.
	- Deep learning. You should be able to understand and apply major deep learning methods, including neural network training, regularization, optimization methods (gradient descent, Adam), and be familiar with major neural network architecture types such as Convolutional Networks, RNN/LSTM. Completion of the deeplearning.ai specialization is sufficient to meet this criterion.
	- Implementation. You should have prior experience taking a dataset, cleaning it if necessary, and applying a learning algorithm to it to get a result. You should be able to implement a learning algorithm “from scratch” using a framework such as NumPy, Tensorflow, Pytorch, Caffe, etc.
	- General coding. You should be able to code non-trivial functions in object-oriented programming, such as popular sorting or search algorithms.
	- Mathematics (including probabilities and statistics.) You should be able to use mathematical notations and linear algebra (matrix/vector operations, dot products, etc.), and understand basic probability theory (distributions, independence, density functions, etc.) as well as statistics (mean, variance, median, quantiles, covariance, etc.)
	- Software Engineering. You should know how to use your terminal, work with version control systems (Git), relational databases, APIs, and build the back-end of web or mobile applications.
	- Mean Stack
		* MEAN is a free and open-source JavaScript software stack for building dynamic web sites and web applications.
		* The MEAN stack is MongoDB, Express.js, AngularJS (or Angular), and Node.js.
+ LAMP is an archetypal model of web service stacks, named as an acronym of the names of its original four open-source components: the Linux operating system, the Apache HTTP Server, the MySQL relational database management system (RDBMS), and the PHP programming language.
+ LYME and LYCE are software stacks composed entirely of free and open-source software to build high-availability heavy duty dynamic web pages. The stacks are composed of:
	- Linux, the operating system;
	- Yaws, the web server;
	- Mnesia or CouchDB, the database;
	- Erlang, the functional programming language.
+ skill set:
	- The steps of an end-to-end machine learning project. This includes, but is not limited to:
		* Conducting a structured and deep literature review of a specific field.
		* Strategizing your machine learning project end-to-end.
		* Collecting, cleaning, labeling, and augmenting your own dataset.
		* Training a model for a real-world application.
		* Setting-up an efficient and organized experimentation process.
		* Defining task-specific metrics to optimize in your experiments.
		* Performing error analysis to improve your models.
		* Deploying an AI product.
		* Exposure to real-world problems that multiple AI teams in our community work on.
	- Hands-on experience in designing, building, and deploying end-to-end AI solutions through curated content and instructor-led workshops.
	- Career mentorship and connections with teams aligned with your career aspirations.
	- Meet and share experiences with other machine learning engineers and data scientists.
	- Everyone who successfully completes the Bootcamp will be awarded a certificate of completion and join the AI Bootcamp Alumni community.
	- Machine learning Engineers and Data Scientists who have already worked on Machine Learning projects and want to get exposed to different Machine Learning problems.
	- Demonstrated AI, data science and/or data analysis experience from previous work experience or publications.
	- Demonstrated strong coding from previous work experience or publications. This means you're able to write a non-trivial program in Python, Java, or C++.
	- Solid CS foundation (including but not limited to Operating Systems, Computer Networks, Database, etc.)
+ skill set:
	- This person will assist in developing data migrations, writing SQL and reports and mentoring other engineers in optimizing and writing efficient queries.
	- Develop and proactively review the monitoring of production PostgreSQL databases
	- Participate in system capacity planning
	- Participate in database design, data modeling and provide recommendations for improvement or optimizations
	- Provide query / index optimizations
	- Participate in an agile software development life cycle including providing testing guidelines for database related changes
	- Provide SQL development support and query tuning
	- Mentor other engineers in developing efficient SQL queries
	- Follow the Quality Management System for developing and deploying software
	- Write reports
	- 3+ years experience managing a production RDBMS including experience with PostgreSQL
	- Experience in SQL development and database design
	- Expertise in SQL DML and DDL
	- Have a solid understanding of query planning
	- Understand PostgreSQL tuning and optimization parameters
	- Excellent interpersonal and communication skills in both oral and written English
	- Able to collaborate with cross functional team members
	- Familiarity with PostgreSQL replication techniques, data warehouse design, and Amazon RDS support beneficial
+ skill set:
	- The Project Portfolio Analyst oversees the activities that support the company's most complex strategic projects by ensuring reporting and governance alignment, providing portfolio performance measurements and recommendations, and trending analytics. This position is responsible for the Portfolio's intake, supply and demand (resource management) methods, analytics and recommendations. Reporting responsibilities includes detailed analysis, dashboards and decision support updates (KPI's) and recommendations on a regular cadence. The Portfolio Analyst will at times provide project support to Project and Change managers.  This position reports to the Associate Director, Organization Engineering.
	- Owns and administrates the project collaboration toolset including project portfolio management software and various collaboration tools
	- Develops and maintains a project risk tracking mechanism to centralize risk tracking across the company's project portfolio
	- Drives agile governance process that maintains control and compliance but improves project team effectiveness and improves decision making
	- Assists in developing project management processes, tools and training as well as project status presentations and reports
	- Supports the strategic planning process and cross-functional business reviews to include development of in-depth analysis, executive presentations, and resource modeling
	- Owns the project idea intake process to ensure project goal and scope are clearly defined, reasonable budget and timelines are established and the project is resourced effectively to deliver the desired solution
	- Owns the resource planning process to include maintaining resource management information and resource utilization modeling
	- Generates, validates, distributes, archives and supports project management documentation and reporting collateral
	- Monitors quality assurance of project implementation across a large portfolio of business and process improvement projects by establishing, monitoring and reporting on metrics to determine whether projects meet quality, cost and schedule targets
+ skill set:
	- Identify, recommend and implement improvements on the project management process and tools – examples:
		* Project idea intake process
		* Resource planning process
		* Project prioritization process
		* Project close-out process
		* Project portfolio management software and collaboration tools
		- Project management process, templates and tools documentation and training
	- Demonstrated experience working with project and portfolio practice, including project ideation
- 	Demonstrates basic understanding of project management methodology
	- Strong influencing skills; demonstrated ability to challenge and persuade, directing a group of stakeholders to the best decision
	- Solid analytical and problem-solving skills; ability to think strategically
	- Demonstrated ability to communicate complex ideas clearly and concisely as well as proven ability to effectively interface and influence at all levels of an organization
	- Moderate proficiency with MS Project as well as MS Visio, Excel and PowerPoint
	- Minimum of 2 years of experience with transactional-based continuous improvement projects
+ skill set:
	- Software Engineer (Media Streaming) - Periscope
	- We are a small team that develops media streaming services and client libraries for Periscope and Twitter's professional live streams. The service ingests thousands of concurrent live video feeds and serves them to viewers in a way that scales to large audiences while maintaining low latency. The service also provides low-latency transcoding and stores live streams for on-demand viewing. On the client side, our cross-platform libraries power low-latency broadcasting and playback in Periscope and Twitter mobile apps. You can learn more about our techniques for low-latency streaming at scale here (https://medium.com/@periscopecode/introducing-lhls-media-streaming-eb6212948bef) and here (https://youtu.be/RbH_2l77Pm8).
	- The role of our service and client libraries is expanding beyond the Periscope use case to handle content from an ever-increasing set of professional broadcast sources and for increasingly high-profile events. We are growing the capabilities, reliability, and quality of our service libraries to power more and more of Twitter's live video.
	- We are looking for a generalist software developer. The ideal candidate has distributed systems experience but would also be comfortable contributing on the Android and iOS client side. While experience in developing media streaming systems is a plus, the position doesn't require domain knowledge in media codecs and streaming.
	- The majority of our server-side codebase is in Go. If you have never worked in Go but you are comfortable in C, C++, or Java, you'll pick it up quickly. Experience with AWS is a plus. Our service uses EC2, DynamoDb, S3, SQS, and Redis, for example. On the client side, our codebase is primarily in C++. Experience in Objective-C and Java would also be a plus. In your day-to-day work, you will encounter media technologies including RTMP, RTP, HLS, H.264, AAC, Opus, the WebRTC native stack, and CDN infrastructure.
+ skill set:
	- Backend development experience with a strong interest in work involving data pipelines, distributed systems, performance analysis, and/or large-scale data processing Experience with software engineering practices (e.g. unit testing, code reviews, design documentation)
	- Able to take on complex problems, learn quickly, and persist towards a good solution
	- Experience designing fault-tolerant distributed systems
	- Experience with data pipelines
	- Experience with Hadoop or other MapReduce-based architectures
	- Experience with Kafka, Druid or other Streaming Compute based technologies is a plus
	- Experience with ad tech is a plus
+ skill set:
	- Experience with asynchronous I/O and coroutines
	- Experience with event driven service architecture
+ skill set:
	- The Consumer Data Science organization works closely with our cross-functional partners, and Twitter's leadership to understand user behavior, inform product decisions, safeguard the health and integrity of our services, and to influence company strategy. We are currently hiring for the following subteams on Consumer Data Science. These high-impact teams value creativity, critical thinking, and teamwork.
	- The Consumer Data Science organization is hiring Senior Data Scientists in the following areas:
		* Health (SF, Boulder) - The goal of this team is to improve the health of the public conversation, ensuring that users feel safe while using our platform. As part of our team, you'll help the Health Organization make strategic decisions that ensure that Twitter is a safe and informative experience for our customers. You will do this by performing and mentoring others through analyses, metrics, experimentation, research, and more.
		* Growth (SF) - Their mission is to increase Twitter's daily utility for new and returning users through impactful and creative applications of experimentation and data analysis. As a key member of Growth Data Science, your work will directly influence exciting new product areas and help grow Twitter usage around the globe.
		* Metrics (SF) - This team works to support company strategy by helping to define, maintain, and understand key success metrics to ensure that we continue to meet the demands of our customers.
		* Video (NY) - This team works with the Live Video, Video on Demand, Publishers, and Camera products. The team is involved in opportunity sizing, experiment setup and analysis, and metric design in order to influence video and media strategy at Twitter.
	- Support the entire product development lifecycle from product ideation to opportunity sizing to measurement design to experimentation and causal analysis to post-launch learning and iteration into the next development cycle.
	- "Design and implement experiments or other econometric methods to understand how changes to the platform affect user behavior."
	- Build novel metrics, identify the impact of product and policies, and study causal impact of our Product launches and Health initiatives.
	- Work in tandem with team members, applying advanced statistical methods; writing complex data flows using multiple languages/frameworks such as SQL, Scala (Scalding, Spark), Python; and using data visualization tools.
	- Communicate findings to executives and cross-functional stakeholders.
	- You are a self-starter who is capable of learning on the job, taking initiative, and thriving within a large team.
	- You are excited to learn and apply new data analysis techniques and tools. You are passionate about insights, not just data and methods. You are a strategic thinker and are able to synthesize technical concepts into actionable recommendations for a diverse audience.
	- You communicate your findings clearly and effectively to a wide audience of relevant partners and are capable of building meaningful presentations and analyses that tell a story.
	- You are rigorous, care about data quality, and strive to understand surprising results and underlying mechanisms in your analyses. You combine business insight with detailed data knowledge and statistical expertise to ensure an accurate interpretation of results.
	- You are a capable mentor. You enjoy knowledge sharing and working with junior teammates to up-level their skills and take the time to learn from them.
	- You value teamwork and teammates. You contribute positively and meaningfully to cultivate an inclusive team culture. You are personable, empathetic, and able to connect with each and every person on the team and throughout the company.
	- Experience using SQL, R, or Python for analysis, modeling, and data visualization.
	- 5+ years experience working with and analyzing large datasets to understand behavior, solve problems, and answer business questions.
+ skill set:
	- Cortex is a team of software engineers and machine learning scientist to developing state-of-the-art machine learning capabilities to refine and transform our products.
+ tech stack:
	- Netflix culture resonates with you.
	- You can communicate effectively with experts of all backgrounds.
	- You are an expert analyst and can pick up any tool (e.g. Tableau, D3) to get the job done.
	- You dream in SQL and Python (or other similar languages).
	- You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Expertise in SQL, programming (e.g. Python, Scala), ETL and data warehousing concepts at scale (TBs of data)
+ Expertise in broad technical skills spanning data access, data storage, data processing, and data visualization.  Skills include: SQL, logical / semantic data modeling, ETL and data warehousing concepts, programming languages (Python)
+ Expertise in statistical inference including experimentation and observational methods to causal inference
+ Strong coding experience. Experience with open-source ML packages (specifically sklearn, TensorFlow/Keras/PyTorch).
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
	- Experience in optimization algorithms and numerical computation
	- Experience with Spark, TensorFlow, or Keras
	- Experience with cloud computing platforms and large web-scale distributed systems
	- Open source contributions
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Optimization algorithms and numerical computation
	- Spark, TensorFlow, or Keras
	- Cloud computing platforms and large web-scale distributed systems
+ tech stack:
	- Write C++ code to tackle scientific algorithmic problems such as: transforming 3D coordinates, Metropolis Monte Carlo simulation, Gradient Descent minimization, and others.
	- Implement highly optimized multi-threaded C++ or CUDA code that scales well on cloud infrastructure.
	- Work closely with other software engineers via code reviews and testing to foster high-quality software and systems.
	- Solid computer science fundamentals, with strong competencies in data structures, algorithms, and compilers.   
	- Experience profiling C/C++ code to find and fix performance bottlenecks.
	- Comfort with the Linux command-line environment.
	- Background in Biology, Chemistry or Physics.
	- Familiarity working with Docker and Kubernetes.
	- Knowledge of parallel computing paradigms and demonstrated proficiency in some of the following: openMP, CUDA, or openCL.
	- https://www.atomwise.com/jobs/senior-software-engineer-scientific-computing/
+ tech stack:
	- You should think about joining us if you care about making a difference in treating disease and saving human lives. But also, if you are up to tackling some of the hardest open challenges in deep learning today:
		* Non-stationary, unbalanced, and noisy data: Our training data is seldom i.i.d.; new medicines are unlocked by pushing out into newly-discovered biology. Classes are extremely unbalanced, ratios of 1 positive to 70,000 negatives are typical. Help us reason about how to learn appropriately without dismissing nor overfitting to the data; identify when we can trust a label or have confidence in a prediction; and develop techniques to find and correct for systemic biases.
		* Extreme scaling: Medicinal chemists can synthesize about a trillion trillion molecules today. Help us scale predictive algorithms to orders of magnitude beyond those contemplated in any other problem domain today.
		* Multi-parameter optimization: Medicine has to be both safe and effective, so we have to concurrently optimize a number of criteria such as potency, selectivity, solubility, toxicity, synthesizability, etc. Help us efficiently explore the Pareto frontier and avoid mode collapse.
		* Adversarial generation of synthetic data: Data augmentation has shown utility in improving the robustness of predictions. Help us find ways to best integrate molecular physics simulation and machine learning to impute new data.
		* Explainability and visualization: Subtle patterns govern molecular recognition. Help us to understand how they lead to the discovery of fundamental chemistry by AI.
	- Ph.D. or M.Sc. in computer science, statistics, data science, or related field
	- 5+ years of extensive practical experience and proven track record of developing, implementing, debugging, and extending machine learning algorithm
	- Knowledge of modern neural network frameworks such as Tensorflow, Torch, or Theano
	- Strong analytical and statistical skills
	- Scientific rigor, healthy skepticism, and detail-orientation in running and analyzing experiments
	- Familiarity with processing large data sets in a Linux environment
	- Software engineering skills and coding experience in at least one high-level programming language (Python, R, Java, C/C++, etc.)
	- Biomedical knowledge or experience in processing chemical or biological data is preferred but not required
	- Experience with cloud computing environments (AWS/Azure/GCE)
	- https://www.atomwise.com/jobs/senior-machine-learning-research-scientist/
+ tech stack:
	- Interact with customers to understand their project requirements.
	- Generate and analyze predictive results, and deliver these to our clients.
	- Communicate our results and capabilities through scientific publications.
	- Analyze, curate, and automate the processing of our biochemical databases.
	- Help to develop our modeling software.
	- Ph.D. in chemistry, biology or a related field.
	- Extensive knowledge of medicinal chemistry.
	- Minimum 3 years of experience in lead optimization at a pharmaceutical company.
	- Experience in Computer-Aided Drug Design: structure/ligand-based drug design, molecular docking, virtual screening, QSAR, pharmacophore modeling, PK/PD data analysis and modeling.
	- Comfortable on the Linux command-line.
	- Undergraduate knowledge of statistics.
	- Good knowledge of Python, Perl, Bash, or related scripting languages.
	- Statistical modeling.
	- Experience with cloud computing environments (AWS/Azure/GCE).
	- https://www.atomwise.com/jobs/senior-computational-chemist/
+ tech stack:
	- M.Sc/Ph.D. in Computer Science, Statistics, Cheminformatics, Bioinformatics, Computational Biology or B.S. in Computer Science with 7+ years experience.
	- Strong computer science fundamentals
	- 5+ years of experience in database engineering, data processing pipelines, and HPC
	- Strong database design and software-engineering best practices
	- Strong knowledge of statistics, data analytics, and data visualization
	- Strong coding skills in at least one high-level programming language (Python, Java, C++, etc)
	- Good familiarity with Linux command-line environment
	- Experience in bioinformatics or cheminformatics, working with ingestion of third-party and internal data sources
	- Experience working with scalable algorithms utilizing large amounts of data
	- Experience with cloud computing environments (AWS/Azure/GCE)
	- Experience with non-relational databases
	- Familiarity with organic chemistry
	- https://www.atomwise.com/jobs/senior-data-engineer-cheminformatics-bioinformatics/
+ tech stack:
	- Hadoop ecosystem and its components.
	- Hadoop, Hive, HBase, and Pig
	- Working experience in HQL
	- Pig Latin Scripts and MapReduce jobs
	- Hands-on experience in backend programming, particularly Java, and Node.js
	- Analytical and problem-solving skills
+ Expertise in additional statistical methods (e.g., Bayesian approaches, dyadic analysis, causal inference approaches, factor analysis, SEM)
+ Use of Jira and A-Ha planning tools.
+ Build data tooling to enable data lake, data warehouse, and analytics workflows within the AWS cloud (S3, Redshift, DynamoDB, Spark, Kinesis, Kubernetes, etc.)
+ Experience with SAML, OAuth
+ Working with Jira and Confluence a plus.
+ Experience working with modern deep learning software architecture and frameworks including: Tensorflow, MxNet, Caffe, Caffe2, Torch, and/or PyTorch.
+ Experience with configuration management systems (Ansible and/or Puppet, Saltstack)
+ tech stack:
	- Remote hardware administration with IPMI
	- Configuration and management of
	- SGE/Univa, Slurm, LSF or other DRMS
	- Jenkins CI
	- Phabricator
	- FlexLM licensing
	- Puppet, Ansible, Nagios
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
	- Web programming – HTML/DOM, JavaScript, SQL
	- A solid knowledge about how orchestration tools (Kubernetes, Swarm, OpenStack, etc) can be used to deploy, scale, and operate virtualized entities
	- Understand CPU virtualization and container technology from the inside out (hypervisors, Xen, LXC, Docker)
	- The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
	- Good knowledge of management and security frameworks (SNMP/MIB agents, CLI, RESTful API, OpenBMC) is very useful
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
	- Test automation experience (Some exposure to CTest is desirable)
+ Bonus: Prior experience with Zendesk, Jira or Asana, SQL, in Customer Service or Hospitality
+ Producing effective and interactive data visualizations (Tableau, Shiny, D3)
+ Experience with Databricks or Spark, EMR
+ Bonus points for experience building interactive data visualizations using libraries like D3, Highcharts, and Leaflet, and for experience working with big data systems like Hive, Hadoop, Scalding and Spark.
+ Experience with Scala, Scalding, Luigi,Hive, machine learning pipelines and model training is a plus
+ Automate DB (Oracle, Postgres, MongoDB, ...) configuration, deployment, backups, ...
+ Experience with at least one of: Oracle, Postgres, MongoDB, Solr
+ Chef/Puppet/Ansible/Terraform experience is nice to have
+ Experience with full-text search engines (Solr, Elasticsearch).
+ Familiarity with Docker (and Kubernetes/Mesos Marathon)
+ Familiarity with Angular framework
+ You will work with technologies like: AWS, Docker (Mesos/Kubernetes), HashiCorp tools (Terraform, Consul, Vault,...), Chef, Ansible, SQL and NoSQL databases, Nginx, ...
+ Elasticsearch, Hadoop, Big Data experience is a plus
+ workflow management tools like Airflow
+ search backends like ElasticSearch
+ Spark and/or other big data architectures (Hadoop, MapReduce) in high-volume environments
+ VM embeddings in other systems (e.g., DBMSs, Big Data frameworks, Microservices, etc.)
+ Virtual machines: Managed runtime systems (e.g., JVM, Dalvik VM, Android Runtime (ART), LLVM, .NET CLR, RPython, etc.)
+ We preferred students experienced in the use of ROS (Robot Operating System) and simulation engines such as Unity3D and Unreal Engine 4.
+ ***Knowledge of parallelism in shared (Intel TBB, OpenMP) and distributed (Intel MPI, Apache Spark, Dask) memory***
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Knowledge of OpenCL/SYCL languages
+ ***Experience with large-scale, distributed data processing frameworks (e.g., Spark, Kafka, YARN, Tachyon, Mesos, etc.) is a plus***
+ Domain knowledge and project experience in below area will be a plus: x86 architecture; Linux kernel; Virtualization; Cloud SW stacks; Big data; Machine Learning, compiler and run time optimization, etc.
+ Knowledge of Linux and/or Windows programming and computer graphics including OpenCL\*, OpenGL\*, DirectX\*
+ ***Open-source projects that demonstrate relevant skills  and/or publications in relevant conferences and journals (e.g. NIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICASSP)***
+ ***Experience working with analytics tools such as Google Analytics, Heap Analytics, Chartmogul, Baremetrics, Periscope, Tableau, Mode Analytics, Looker, or similar***
+ tech stack: Golang, AWS (DynamoDB, Lambda, EC2, Kinesis, SQS, S3), ReactJS, Snowflake, Terraform, Redis, SolrCloud, Kafka, Riak, Docker/Kubernetes, and Linux
+ tech stack:
	-  Solid knowledge in control theory, especially model predictive control
	- Experience in one or more of the following areas:
		* Robust control
		* Adaptive control
		* Nonlinear control
		* State estimation
		* Parameter estimation
		* Model identification
		* Optimization
	- Experience in one or more of the following areas:
		* Control theory
		* Motion planning
		* Optimization
		* Formal logic
		* Game AI development
	- Experience with sensors: cameras, lidars, ultrasonic, etc.
	- Computer vision experience, image processing experience
+ You know what the CAP theorem is and you feel confident that you can speak to what it does and does not cover in systems design.
+ Expert in prototyping traditional ML (GBMs, scikit, etc.) and AI frameworks (keras, tensorflow, mxnet, pytorch, etc.) for a variety of applications
+ tech stack:
	- Knowledge of dsx, IGC, and IA
	- Web service development with NodeJS (Backend server Javascript), front-end Javascript and Flask (Open source python library)
	- Both relational database and NoSQL database technologies
	- The main responsibility will be to enable different personas like Data Scientist, Data Curator, and Data Engineer to collaborate with each other on their Data Journey and work with various products like dsx, IGC, IA, and DFD.
+ tech stack:
	- PHP5, PHP7, HTML5, CSS3, JavaScript, Jquery, MySQL, NoSql
	- Should have in depth understanding in LAMP stack
	- Must have Good OOP (Object Oriented Programming) concepts
	- Knowledge in MVC Framework e.g (Zend Framework, Laravel)
	- Clear understanding of JSON, AJAX, XML, CURL, Web Service
	- Knowledge of the Linux command line tools optional
	- Good to have experience in Angular JS and Node JS, concept of UI/UX
	- Experience in handling large database, including designing & advanced querying in MySql
	- Good knowledge of version control tools like GIT, SVN
+ Versatile with languages and technologies. You pick the right technology for the job. You might have dabbled with iOS, Android, React Native or Flutter; you tinkered with TensorFlow, Caffe or Torch; you know when to use Mongo vs Redshift; and you have an opinion on web frameworks.
+ OpenStack:
	- Dashboard (Horizon)
	- Compute Service (Nova)
	- Networking (Neutron)
	- Object store (Swift)
	- Identity service (Keystone)
	- Metering & Data Collection Service (Ceilometer)
	- Orchestration (Heat)
	- Bare Metal Provisioning Service (Ironic)
	- Container Orchestration Engine Provisioning (Magnum)
	- Computable object storage (Storlets)
	- Deploys OpenStack using OpenStack itself (Tripleo)
	- Billing and chargebacks (Cloudkitty)
	- Optimization Service (Watcher)
	- Distributed SDN controller (Dragonflow)
	- OpenStack Networking integration for containers (Kuryr)
	- NFV Orchestration (Tacker)
	- Networking Automation for Multi-Region Deployments (Tricircle)
	- Command-line interface for all OpenStack services (Openstackclient)
	- Instances High Availability Service (Masakari)
	- Lightweight OCI containers (LOCI)
	- EC2 API proxy (EC2API)
	- Official Python SDK for OpenStack APIs (Openstacksdk)
	- Block Storage (Cinder)
	- Image service (Glance)
	- Big Data Processing Framework Provisioning (Sahara)
	- Application Catalog (Murano)
	- Containers Service (Zun)
	- Puppet modules to deploy OpenStack (Puppet-openstack)
	- Clustering service (Senlin)
	- Event, Metadata Indexing Service (Panko)
	- Root Cause Analysis service (Vitrage)
	- Load balancer (Octavia)
	- Accelerators resource management (Cyborg)
	- Deploys OpenStack in containers using Helm (Openstack-helm)
	- OpenStack Storage integration for containers (Fuxi)
	- Client library for interacting with OpenStack clouds (Shade)
	- Database as a Service (Trove)
	- Shared filesystems (Manila)
	- DNS service (Designate)
	- Key management (Barbican)
	- Governance (Congress)
	- Software Development Lifecycle Automation (Solum)
	- Deploys OpenStack in containers using Ansible (Kolla-ansible)
	- Monitoring (Monasca)
	- Workflow service (Mistral)
	- Functions Service (Qinling)
	- RPM package specs to deploy OpenStack (RPM-packaging)
	- Messaging Service (Zaqar)
	- Ansible playbooks to deploy OpenStack (Openstack-ansible)
	- Benchmark service (Rally)
	- Application Data Protection as a Service (Karbor)
	- Backup, Restore, and Disaster Recovery (Freezer)
	- Packaging-rpm (Packaging-rpm)
	- Indexing and Search (Searchlight)
	- Deploys OpenStack in containers using Charms and Juju (Openstack-charms)
	- Resource reservation service (Blazar)
	- Alarming Service (Aodh)
	- Ansible playbooks using ironic (Bifrost)
	- Chef cookbooks to deploy OpenStack (Chef-openstack)
	- EC2 API compatibility layer for OpenStack (Ec2-api)
	- Python Software Development Kit (Python SDK)
	- Ansible playbooks and roles for deployment (OpenStackAnsible)
+ Silicon bring-up
+ Silicon characterisation
+ Massively parallel computing systems
+ Laravel
+ SQL working experience (Redshift/PostgreSQL/MySQL)
+ Experience working with a CI system is preferred (ex. TeamCity, Concourse, Jenkins, etc.)
+ Functional test automation tool experience is preferred (ex. Junit, TestNG, Serenity, etc.)
+ Experience with profiling tools like PerfView (CPU, Memory, Garbage collection)
+ Expert knowledge of debugging and crash dump analysis in Windbg
+ Experience wrangling very large datasets by writing and maintaining data processing pipelines with Hadoop, Spark, BigQuery, Redshift, or similar
+ Google Data Studio
+ The successful candidate would be strong in SQL, AWS, Snowflake, Databricks, and Python.
+ Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
+ Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
+ Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
+ [Plus] Familiarity with interactive data visualization using tools like D3.js
+ Experience with MPP databases, such as Snowflake, Redshift, BigQuery, Vertica, etc.
+ A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
+ Experience with at least one prototyping tool (eg. Axure, Framer, Principle)
+ Proficiency in developing pixel perfect mockups using Sketch and/or Adobe Design tools.
+ Analyze all aspects of the Snowflake Query Engine and drive initiatives to understand what bottlenecks may exist and to improve them.  
+ Build integration code with many cutting-edge technologies and processes, including Python 3, Go, Presto, AWS, ML, NLP.
+ Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ Prior experience with infrastructure automation frameworks (Ansible, Terraform, Chef or Puppet, etc.)
+ ***Experience with one of the ML platforms: Python / scikit-learn, Spark, vowpal wabbit, etc***
+ Virtualization and containerization (Xen, LXC, cgroups, Docker, Kubernetes)
+ tech stack:
	- Well-versed in one or more of the following languages and functional programming in general: Scala, Java, Python, JavaScript
	- Expert in SQL and comfortable designing, writing and maintaining complex SQL based ETL.
	- Experience with building large-scale batch and real-time data pipelines; ETL design, implementation, and maintenance.
	- Experience with schema design and data modeling, and the analytical skills to QA data and identify gaps and inconsistencies.
+ tech stack:
	- Write complex data flows using SQL, frameworks (e.g., Scalding, Spark), and scripting languages (e.g., Python, R)
	- Use data visualization tools (e.g., Tableau, Zeppelin) to share ongoing insights.
	- Skilled with Figma (or Sketch) and prototyping tools such as Framer, Principle
	- You have a deep and nuanced understanding of statistics, especially involving class imbalance problems.
	- Scalding
	- Full Stack Development
	- Presto or Hive
	- Spark
+ Knowledge of source control tools (Git, CodeCommit, SVN, and TFS), build/release tools (Jenkins, CodeBuild, CodeDeploy, CodePipeline), and infrastructure as code tools (Terraform, CloudFormation)
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift)
+ skill set:
	- B.S. or M.S. in Economics, Statistics, or a similar field and 1+ year work experience in data science or analytics, or Ph.D. in a quantitative social/behavioral science (e.g. Economics, Sociology, Psychology, Statistics, or a similar field)
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Familiarity with Python or similar scripting language
	- Experience communicating technical statistical concepts clearly, for example, teaching or consulting
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework
+ skill set:
	- B.S., M.S., or Ph.D. in a quantitative field
	- 4+ years work experience in an analytical or quantitative role as a Data Scientist
	- 2+ years experience working on product analytics in a two-sided marketplace
	- Extensive experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
	- Active Quora user with curiosity about the product
	- Deep experience with MySQL, NoSQL data stores like HBase or similar
	- Strong grasp of Configuration Management (Chef, Puppet, Ansible, Salt Stack)
- skills to develop:
	- Deep understanding of at least one popular server side MVC Framework (e.g Django, Rails, AngularJS etc).
	- Knowledge of backend storage systems like MySQL, HBase, Memcached, Redis, Kafka etc.
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
+ skill set:
	- Deep knowledge of web technologies, e.g. HTML, CSS. Experience with LESS or SASS is a plus.
	- Deep knowledge of JavaScript frameworks, e.g. jQuery. Experience with pure Javascript is a plus.
	- Some knowledge of server-side languages and web frameworks. Experience with Python is a plus.
	- Experience debugging across multiple browsers. Experience with UI testing tools like Selenium or phantomJS is a plus.
	- Experience optimizing the speed and performance of websites.
	- Experience maintaining large and growing code bases in a fast-moving environment.
	- Interest in staying current with new and evolving web technologies.
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
	- BS, MS, or PhD in Computer Science, Engineering, Statistics or a related technical field
	- Love of the Quora product
+ skill set:
	- BS, MS or PhD in Computer Science, Machine Learning, NLP or a related technical field
	- 5+ years of industry experience preferred
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
	- Love of the Quora product
+ ***Experience building shallow or deep learning models (GBDT, CNN, RNN, LSTM), toolkits e.g. OpenCV, Matlab, RStudio, Weka, MLLib and frameworks PyTorch, TensorFlow, CNTK***
+ ***Expertise in multivariate analysis, graphical models, Bayesian hierarchical modelling, Markov chain Monte Carlo (MCMC), mixture models, stochastic processes, generalized linear models (GLMs), dimensionality reduction (PCA/CCA/MDS/tSNE) and other machine learning techniques***
+ Experience working with Atlassian products (JIRA, Confluence)
+ Knowledge of Internet protocols (e.g., TCP/IP, BGP, OSPF, TACACS, IPSEC, SNMP, SYSLOG)
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ ***Programming skills sufficient to extract, transform, and clean large (multi-TB) data sets in a Unix/Linux environment.***
+ Experience with NLP libraries such as SpaCy, Stanford CoreNLP, OpenNLP, or NLTK
+ ***Experience with big data techniques (such as Hadoop, MapReduce, Hive, Pig, Spark)***
+ ***Familiar with one or more machine learning, statistical modeling tools such as R, Matlab, scikit learn and deep learning frameworks, such as tensorflow, keras, caffe, torch.***
+ skill set for data science:
	- ***Technical mastery in one or more of the following languages/tools to wrangle and understand data: Python (NumPy, SciPy, scikit-learn), Spotfire, Tableau.***
	- ***Experience with Spark (MapReduce, PIG, HIVE)***
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
+ stress testing (locust.io)
+ designing high availability systems
+ application security hardening
+ distributed tracing (OpenTracing/Zipkin)
+ collecting and analyzing performance metrics (InfluxDB, Prometheus, statsd, Grafana)
+ Docker orchestration systems and cluster managers (Kubernetes, Mesos/Marathon, ECS)
+ Experience in the technologies we use is helpful but not required. They are: Go for core infrastructure; ObjC, Java and C# for native UI development on iOS, OSX, Android and Windows; Node.js and IcedCoffeeScript for Web development; FUSE for client file systems; MySQL/InnoDB, DynamoDB, S3 and EC2 for hosting.
+ Load testing frameworks/tools like JMeter, Gatling, Locust
+ Java, Selenium, JUnit, Cucumber-JVM
+ API Testing experience
+ BDD (Cucumber, Gherkin)
+ Experience implementing search solutions with technologies such as SOLR, Elasticsearch, Lucene is preferred.
+ Python, Gherkin, Cucumber, Espresso, XUI Test
+ Experience with testing technologies (JUnit, Espresso, Mockito, Robolectric)
+ Unit Testing Tools  –  Google Test or CPPUnit ; Code quality tools
+ Familiarity with Linux, Maven, Git/Stash, Jira, Bamboo/Jenkins
+ Experience with distributed messages systems ( Apache Kafka)
+ Experience in CFD combustion or other reacting-flow simulations.
+ tech stack
	- Jira, Confluence, DevOps, Continuous Integration and Continuous Delivery, Microsoft Development Tools
	- Git, MS Build, Team Foundation Server, Jenkins, Unit Testing, Powershell, Perl, C#, .NET, Visual Studio, Python
+ tech stack:
	- Excellent skills in creating high-fidelity prototypes using Invision, Principle, Code or similar
	- Relevant experience in agile methodologies (Scrum, Agile, etc) and PM tools (e.g. Jira, Pivotal Tracker, Confluence etc.)
	- Experience with relational (e.g. MySQL, PostgreSQL) and NoSQL (e.g. MongoDB, ElasticSearch) databases
+ tech stack:
	- Expertise in Go preferred, but not required. If you're new to Go, then proficiency in a mainstream language such as Java, Python, C++, Scala, etc.., and a willingness to learn Go required.
	- You've got experience writing, deploying and monitoring microservices.
	- Working knowledge of SQL and relational databases(we use Postgres)
	- You've used an RPC framework like gRPC or Thrift.
	- You have high level experience working in a containerized infrastructure deployed in the cloud(AWS, GCP, Azure)
+ tech stack:
	- Experience with NoSQL databases. MongoDB is a plus
	- Experience with real-time and streaming data processing
	- Experience with queuing platforms like Kafka
	- Knowledge of BigQuery
	- Familiarity with GCP/AWS cloud services
	- Familiarity with TensorFlow
	- Comfortable with CI/CD Pipelines
	- Experience with Git version control
+ tech stack:
	- Ability to configure and maintain webservers (e.g. apache & nginx), DNS servers, Firewalls, LDAP servers, Tomcat servers
	- Ability to back up the Data infrastructure
	- Ability to manage/configure  Git, Maven and Jenkins
	- Managing QA/production release and deployment
	- Ability to Install/Configure/Manage VM servers using OpenStack
	- Ability to install configure or manage Monitoring servers using Opensource softwares
	- Experience with Amazon Web Services:
	- autoscaling, & use of Netflix Asgard
	- ELB management,
	- EBS storage management
	- S3
	- RDS
	- Manage configuration using Puppet
	- Familiar with Cloud Computing in genera
+ tech stack:
	- ReactJS
	- GraphQL
	- Apollo Client & Server
	- Some Redux
	- Using ES6/7 features throughout the app so knowledge on those is a plus.
	- We use Cypress for testing
	- CircleCI for continuous integration.
	- Functional programming principles in React with Recompose
+ Celery
+ Elasticsearch and ELK pipeline
+ LibreOffice, Apache OpenOffice, and NeoOffice.
+ Tech stack is described as:
	- Front­end: JavaScript (ES5/ES6), AJAX, jQuery, React/Angular/Vue, Bootstrap, templating, markdown/markup, built tools, task runners, PWAs, etc...
	- Middle­tier: REST and RESTful interfaces, AJAX, RPC, WebSockets/Socket.io, Web Workers, Node.js/Express, etc…
	- Back­end: SQL/No­SQL databases, Message Queue Systems, Big Data systems, Node.js, MongoDB, Redis, etc...
+ data Science:
	- Knowledge of ElasticSearch/Solr/Lucene is a big plus.
	- Understanding in Java server platform and system tuning is a plus.
	- Knowledge with vector space models, text classification and categorization.
	- Implement high-quality code in an agile software development environment.
+ data science skill set:
	- Implement scalable algorithms and services using technologies such as Scala, Akka, elasticsearch, Kafka, Cassandra and Hadoop technologies such as Hive, Spark or MapReduce
	- Hands-on experience in analyzing large datasets (e.g. with SQL, Python, R, Hive, etc.)
	- Some knowledge and experience in working with technologies such as Kafka, Cassandra, Elasticsearch, Akka, Kubernetes, etc.
	- Experience in Scala or Java is a plus
	- You are fluent in English; German skills are a plus
+ AWS cloud services: EC2, EMR, RDS, Lambda, Redshift
+ NoSQL databases, such as HBase, Cassandra, MongoDB, or DynamoDB
+ messaging systems, such as AWS SQS, AWS Kinesis, Kafka, or RabbitMQ, ZeroMQ
+ big data tools and stream-processing systems: Hadoop, Spark, Storm, Spark-Streaming
+ **Expertise and experience in Revit, Dynamo and/or other Revit scripting languages**... Strong background in computational design and design analysis... Fluency in a technical programming language (python, javascript, C#) is highly desired.
+ Understanding of standard networking protocols and components such as: TCP/IP, HTTP, DNS, ICMP, the OSI Model, Subnetting, and Load Balancing
+ Knowledge of routing protocols such as BGP and OSPF
+ data pipeline and workflow management tools: Azkaban, Luigi, Airflow
+ Very well versed with ADT, ORU, ORM and document exchange messages specification
+ Develop public APIs on either APIGEE
+ DBT experience
+ Object oriented programming experience (e.g. using Java, J2EE, EJB, .NET, WebSphere, etc.).
+ data interchange formats like JSON and XML
+ Knowledge in machine learning framework - Tensorflow, Caffe, Torch or Theano
+ Django, Ruby on Rails, Flask
+ Must have experience with working on few technologies such as spring framework, SpringBoot, SpringMVC, JPA, MyBatis, Tomcat, Nginx
+ Experience with performance optimization of queries in Redshift & Postgres
+ Knowledge of authentication protocols such as basic and digest authentication, SAML, LDAP, and OAuth.
+ In-Memory caching technologies, such as memcached or Redis
+ Cutting edge C++ knowledge (C++17, C++20)
+ stream pipelines and all sorts of data stores (SQL, NoSQL, triplestores, wide column, graph)
+ Knowledge of data standards, file formats, and biomedical ontologies and vocabularies such as SNOMED-CT, UMLS, etc. DICOM
+ all types of data stores - NoSQL, wide column, Graph, triplestores
+ Spark, Kafka
+ Experience with stream pipelines and data store technologies (nosql, wide column and graph). We are Currently using Cassandra, Kafka, Amazon dynamoDB, Redis, Neo4j and Mysql.
+ NLP library: spaCy, NLTK, GATE, CoreNLP, gensim
+ Deep Learning applied to NLP, for example through distributed representations (e.g. Word2Vec, fastText, etc)
+ large databases (e.g. THIN)
+ Monitoring solutions experience (ELK, NewRelic)
+ Infrastructure-as-code and automation tools (e.g. Terraform, Ansible/Chef, Cloud Formation)
+ Configure and Monitor SIEMS and DLP systems
+ RxJava, Kotlin, Dagger
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ Experience with common security tools such as nmap, Burp Proxy, Brakeman, etc.
+ Experience with bug bounty programs and reporting issues to them (send examples, please!)
+ Familiarity with search domain (Information retrieval, NLP, Solr/ Lucene or related tech)
+ data management tools in on a big data plate form such as Atlas, Ranger , Knox
+ implementing BI solutions in a heavily regulated environment e.g. PII, GDPR, HIPPA & SOX
+ big data platform tools such as Hadoop, Hive Druid, Kafka, Ambari, Spark, Zeppelin
+ PowerBI, Tableau, Qlikview
+ Production experience with AWS tools including at least some of the following: EC2, S3, Kinesis, CloudFormation, Redshift
+ Experience with at least one data warehousing platform (Redshift, Athena, Hive, Snowflake, etc.)
+ Knowledge of a majority of the following: Elixir, Erlang, Ruby, JavaScript, PHP, Postgresql, MySQL, Apache Solr, Elasticsearch.
+ Knowledge of web frameworks (like Sinatra/Rails), testing frameworks (like Rspec/Minitest) and Javascript. Experience with Ruby, MySQL and Apache Solr is a plus.
+ Experience with Java, Boost, QML, Jira, JavaScript, React, or DDP
+ Demonstrated proficiency with Docker and container orchestration technologies (Kubernetes, ECS, etc.)
+ Expertise with AWS services such as EC2, IAM, S3, etc.
+ Expertise with several continuous integration technologies (Jenkins, Ansible, CloudFormation, Terraform, etc.)
+ Experience with load balancing technologies such as ELB, NGINX, etc.
+ Experience with network technologies like DNS, AWS security groups, VPCs, etc.
+ Extensive experience manipulating and analyzing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
+ Tools: Slurm, Docker, Grafana.
