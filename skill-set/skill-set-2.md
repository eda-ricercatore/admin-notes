+ skill set:
	- Bachelor's degree in Computer Science, Math, Statistics, Economics, or other quantitative field; Masters or PhD strongly preferred
	- Significant experience in custom ETL design, implementation and maintenance, including serving machine learning models in production for multiple high-growth companies, preferably those with technical products
	- Track record of developing and evolving complex data environments and intelligence platforms for business users
	- Demonstrable ability to relate high-level business requirements to technical ETL and data infrastructure needs, including underlying data models and scripts
	- History of proactively identifying forward-looking data engineering strategies, utilizing cutting-edge technologies, and implementing at scale
	- Hands-on experience with schema design and dimensional data modeling
	- Understanding of statistical modeling, machine learning and data mining concepts
	- Demonstrable critical thinking and analytical skills, including the ability and confidence to make conclusions and recommendations from data
	- Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable engineering strategies
	- Experience with Big Data/distributed frameworks such as Spark, Kubernetes, Hadoop, Hive, Presto,
	- Experience with job schedulers; Airflow, Luigi, Azkaban, etc.
	- Experience with continuous integration and automation tools and processes
	- Advanced SQL and relational database knowledge (MySQL, PostgreSQL) in addition to warehousing and dimension modeling
	- Scripting in Python required, experience with Scala/Go a plus
	- Programming against APIs required
	- Experience with Snowflake and/or Looker a plus
	- Effective communication and interpersonal skills
+ skill set:
	- Bachelor's degree in Computer Science, Math, Statistics, Economics, or other quantitative field; Masters or PhD strongly preferred
	- Previous experience in data science or quantitative analytics role, preferably in a high-growth company
	- Comprehensive understanding of statistical modeling, machine learning and data mining concepts, and experience applying these methods within a business environment
	- Strong knowledge of Python. Familiarity with at least one statistical modeling / machine learning tool such as R or Matlab is a plus, as well as experience with languages such as Scala or Go
	- Expert knowledge of, and hands-on experience with, SQL
	- Demonstrable critical thinking and analytical skills, including the ability and confidence to make conclusions and recommendations from data
	- Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable data-driven analysis and implementing science-driven data products
+ skill set:
	- Agile development approaches
		* Lean-startup and design-thinking inspired methods incorporating short product development, business-hypothesis-driven experimentation, iterative product releases, and validated learning
	- Programming languages and operating systems
		* Python
		* Javascript
		* Unix/Linux/MacOSX
		* RDF, JSON-LD, SPARQL
	- Tools
		* Git
		* Slack
		* JIRA
	- Web app development
		* Django
		* Django REST Framework (for API development)
		* Node
	- Backend
		* SOLR/Lucene or ElasticSearch
		* Amazon Web Services (AWS) or other cloud service providers
		* “Serverless” computing (e.g., AWS Lambda)
		* Application containerization (such as Docker or Kubernetes)
	- Javascript frameworks
		* Vue and/or Nuxt
		* Webpack
	- Mobile development
		* Using responsive design and development techniques, possibly including the use of Progressive Web Application (PWA) techniques and technologies
	- Other
		* Topic modeling (ideally using Mallet)
		* Video formats and metadata (for both archiving and streaming)
		* RDF, JSON-LD, Sparql and GraphQL for knowledge graph development and use
		* Content markup including:  HTML, XML, ePUB, PDF
		* Named Entity Recognition (NER)
		* Experience applying statistics, modeling, and machine learning
+ skill set:
	- SQL, javascript, and google appscript
	- Familiarity with HR systems and tools (e.g. Workday, Reflektive, Zugata, Culture Amp)
	- You are a skilled engineer. You've got a strong background working in our stack (Python, Django or React, ES6 or Node.js, etc.) -- even if you aren't a daily coder, you regularly exercise your coding muscles and try to be an asset on any technical context your team may need.
	- Help the team build tools for partners and developers that come to our platform to create Apps that live on Zapier. Imagine Postman meets OpenAPI on steroids.
+ tech stack:
	- Research publications at relevant conferences such as SIGGRAPH, ACM Trans on Graphics, CVPR, ICCV, ICCP, SPIE, JOSA a major plus.
	- Expertise in Deep Learning, Machine learning and familiarity with tools like Scipy, Boost, Caffe, TensorFlow, OpenCV, DLIB etc. and related areas.
+ Because compilers, interpreters, JIT, pre-processors, grammars, register allocation, term rewriting, LLVM and more are what brought us to computer science in the first place, Raincode Labs forms the largest independent compilation technology company in the world.
+ tech stack:
	- Publication record in top conferences (ICML, ICLR, NIPS, KDD, IJCAI, AAAI etc )
	- Good knowledge and handson experience in distributed technologies such as Hadoop, Hive, Spark Experience in Scala programming language.
	- Publications in relevant top venues (e.g., KDD, NIPS, ICML, AAAI, IJCAI, ICDM, ACL etc.)
	- You have publications in communities such as WWW, SIGIR, FAT*, NeurIPS, WSDM, SIGDIAL, RecSys, CHI, KDD, AAAI, ACL, ICML, or related.
	- You have hands-on experience implementing production machine learning systems at scale in Java, Scala, Python, or similar languages. Experience with XGBoost, TensorFlow is also a plus.
	- You preferably have experience with data pipeline tools like Apache Beam or even our open source API for it, Scio and cloud platforms like GCP or AWS.
	- Extensive experience manipulating and analysing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
	- Familiarity with marketing tracking platforms (e.g. DoubleClick, Google Tag Manager, Google Analytics) preferred
	- Become an expert on leveraging existing state-of-the-art tooling into the Spotify eco-system (TensorFlow, TFX, Kubeflow Pipelines, Cloud Bigtable)
	- Contribute to new and existing Spotify open source machine learning and data processing products (scio, zoltar)
	- You preferably have experience with data processing and storage frameworks like Google Cloud Dataflow, Hadoop, Scalding, Spark, Storm, Cassandra, Kafka, etc.
	- Extensive publication record at peer-reviewed ML conferences (e.g. NIPS, ICML, AISTATS, UAI, COLT, ICLR, AAAI, etc) as well conferences with applied ML (e.g. KDD, WSDM, WWW, CIKM, RecSys, etc).
+ tech stack:
	- Security and privacy
	- Virtualization and container technologies (e.g., Xen and Docker)
	- Cloud services (e.g., AWS and Azure)
	- Distributed programming tools (e.g., Hadoop, Cassandra, and ZooKeeper)
	- In-home wireless network protocols (WiFi, Bluetooth, Zigbee, and Z-wave)
	- Systems for machine learning training and inference (Tensorflow, MXNet, Caffe etc)
	- Storage systems
+ Experience with open source platforms like Hadoop, Spark, Hive, Pig; and/or ML life-cycle/collaboration/automation platforms like AirFlow, FB Learner, MLFlow; and/or assistants like Alexa, a plus.
+ "Knowledge of Bayesian Global Optimization tools and technique"
+ Working with Big Data, ML, AI. Keras, TensorFlow, Python, Redshift, S3, Spark, Random Forests and Vowpal Wabbit
+ Experience implementing production-ready machine learning solutions is a plus
+ You'll lead, analyze, implement, and socialize a robust A/B/multivariate testing program, collaborating closely with product, engineering, marketing, and content.
+ You're familiar with all aspects of SEO: on-page, external, and technical, and you have used tools such as ahrefs, DeepCrawl, Screaming Frog, SEMRush, and Google Search Console to optimize for search.
+ Set of skills:
	- Experience with modern programming languages (Java, Scala, Go, TypeScript)
	- Database / Data Storage experience (SQL / MySQL, MongoDB, DynamoDB)
	- Interest in Infrastructure Tooling (Docker, Nomad, Consul, Vault, Prometheus)
+ RStudio packages: The tidyverse, R Markdown, and Shiny
+ A sample of the technologies you'll be exposed to: Python, Javascript/Angular, Impala (Big data data database), AWS, Docker, Kubernetes, Git.
+ Experience with Python ORMs like SQLAlchemy and Python libraries like Pandas, Scikit-Learn, Numpy and Scipy
+ Should have experience in dealing with XML and JSON data formats.
+ Knowledge in Hadoop (HDFS, YARN), its programming models (MapReduce, Spark), and its services such as Hive, HBase etc.
+ Technical Fluency.  Languages and tools such as Python/Java/Scala, AWS (S3/EMR/Athena/Glue) and SQL. Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow. Experience working with either a MapReduce system of any size/scale.
+ Experience writing production datasets in SQL/Hive OR building internal/production data tools for ETL, experimentation, or exploration in a scripting language (Python, R, etc.)
+ Very strong experience in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in the OLTP, OLAP and Data Warehouse environments
+ Passionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.
+ Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark)
+ Ansible: it's not that bad, and helps us move quickly, but any configuration management tool is applicable.
+ Elasticsearch / Kibana: You can readily access information & love metrics
+ Familiarity with common web application testing tools for DAST, SAST, and IAST analysis such as Burp Suite, Checkmarx, Veracode
+ Completed graduate-level coursework in survey statistics—bonus points if you've completed coursework in adjacent fields/methods (e.g., econometrics, NLP, experimental design, political science, or quantitative social psychology)
+ Exposure to container technologies - container orchestrators (Kubernetes, Mesos, Docker Swarm Mode) is a plus
+ Experience with Cloud based services, Microservices a Cloud Computing class or similar experience
+ Technical Skills needed: vSphere, vSAN, NSX, vROps, Storage, Database, Middleware, and Scripting
+ Experience of Unity, C# and 3D application development.
+ Working knowledge of HMD (ie Oculus, HTC Vive, Hololens)
+ Experience with Hololens, HTC Vive, Oculus, Google Cardboard and other leading AR/VR platforms
+ Knowledge of NoSQL technologies (e.g. Cassandra, MongoDB, Redis, etc.) and/or search-based data stores and libraries (Lucene, Solr, etc.)
+ Experience within the domain of Advanced Analytics and Data Science is highly desirable, e.g. hands-on experience with solutions such as Spark, MapReduce, Python, Redshift, Hive, Pig and visualization tools.
+ Hadoop data platform is capable of supporting a growing list of downstream platforms like Tableau, Zeppelin etc.
+ Expertise with Hive, YARN, Spark, Presto, Kafka, SOLR, Oozie, Sentry, Encryption, Hbase, etc.
+ API development
+ You highly experienced with JavaScript/Node.js, SQL/NoSQL databases
+ We are fans of the Lean Startup methodology, we love Trello, Jira, Slack
+ We are cloud agnostic and run our infrastructure and systems on Azure, AWS, as well as dedicated servers.
+ Experience utilising Portfolio & Project Management (PPM) tools such as CA PPM (Clarity), ServiceNow, JIRA, Microsoft Project Server, etc.
+ project management tools (JIRA, Confluence),
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ web analytics platforms such as Google Analytics, Appsflyer or Mixpanel
+ NoSQL databases, such as MongoDB, Cassandra, HBase
+ Proficiency in using query languages such as SQL on a big data platform e.g. Hadoop, Hive
+ data visualisation tools, such as D3.js, GGplot, Tableau etc.
+ Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.
+ Apache Kafka
+ vw / xgboost
+ Knowledges of Web test frameworks like Selenium, React.js, Headless Chromium is a plus
+ set of skills:
	- Statistical analysis and modeling
	- Database architectures
	- Hadoop-based technologies (e.g. MapReduce, Hive and Pig)
	- SQL-based technologies (e.g. PostgreSQL and MySQL)
	- NoSQL technologies (e.g. Cassandra and MongoDB)
	- Data modeling tools (e.g. ERWin, Enterprise Architect and Visio)
	- Python, C/C++ Java, Perl
	- MatLab, SAS, R
	- Data warehousing solutions
	- Predictive modeling, NLP and text analysis
	- Machine learning
	- Data mining
	- UNIX, Linux, Solaris and MS Windows
	- Python (3.5>=), packages: argparse, shapely, Munkres, numpy, cv2, logging, Pillow
* ES6
* Plotly.js
* OpenLayers
+ UI/UX:
	- Experience using design tools such as Photoshop, Illustrator, Sketch, InDesign, etc. for creating highly-detailed mockups
	- Some awareness of the technology which will serve your designs and implementations, such as apache/nginx, Flask/django, PostgreSQL/MySQL, git, websockets, etc.
	- Bootstrap, bulma, etc.
+ Areas of interest:
	- Distributed and parallel systems
	- Information retrieval
	- Large software systems
	- Web application development
	- Database management
	- Cloud computing
	- Cloud security
	- DevOps
+ technologies/frameworks:
	- ReactJS
	- Java/Scala
	- Spark
	- Ruby/JRuby
	- ElasticSearch
	- MySQL
	- Kubernetes
	- Amazon Web Services
	- MS Azure
	- Google Cloud Platform
+ Knowledge of NoSQL technologies (e.g. Cassandra, MongoDB, Redis, etc.) and/or search-based data stores and libraries (Lucene, Solr, etc.
+ Experience with Cloud based services, Microservices a Cloud Computing class or similar experience
+ Produce high quality and well-documented code in an automated CI/CD environment
+ Collaborate with engineering and product teams to design, develop, and publish software supporting a highly available, fault-tolerant SaaS platform
+ skill set:
	- Expertise in Golang and proficiency in other languages (Preferably C/C++,NodeJs, Python).
	- Commercial experience with REST, RPC and message exchange protocols.
	- Experience with frameworks such as: Gin, Gorilla, Dep, Ginkgo
	- Knowledge around message queuing and distributed tasking (SMS,ZeroQ, RabbitMQ etc)
	- Working knowledge in Kubernetes, Rancher or Docker swarm.
	- Ability to write clean and effective Godoc comments
+ Preferred Skills: Tensorflow, Slurm, Kubernetes
+ data science skills:
	- Excellent understanding of ML, NLP, and statistical methodologies
	- Excellent programming skills (Java/Python/R/Sas)
	- Ability to test ideas and adapt methods quickly end to end from data extraction to implementation and validation
	- Experience with search engines, classification algorithms, recommendation systems, and relevance evaluation methodologies a plus
	- Specific Big Data experience on cloud computing platforms with technologies such as Hadoop, Mahout, Pig, Hive and Spark a plus
	- 7+ years of experience with Data Science and Statistics, preferably in Life Sciences, and more specifically, in pharmaceuticals.
	- The ability to tell a story about data, in particular with visualization.
	- Solid understanding of statistics and the design and analysis of experiments. Solid skills in statistical language, SAS.
	- Provides automated and ad-hoc analysis of experiments.
	- Assesses and validates reliability of source data and business systems used to develop performance metrics.
	- Prepares recommendations and conclusions based on data summaries and communicates this information in a credible, convincing and timely manner.
	- Explores existing data for insights and recommends additional sources of data for improvements.
	- Guide the architecture of “big-data” business processes with an eye towards robustness, parsimony and reproducibility (at senior levels)
	- Define and develop software for the analysis and manipulation of large and very large data-sets
	- Narrate stories (sometimes to a non-technical audience) about our content and processes by data analysis and visualization
	- Collaborate with scientists, product groups and content groups to perform “big data” aggregations, fusion and manipulations of important data-sets
	- This is a thought leader.
	- Define, manipulate, aggregate and use both structured and unstructured "big data" in order to support descriptive and predictive analytics across the businesses.
	- Adept at all aspects of technical communications, including using presentations technologies (e.g. WebEx, PowerPoint) and software demonstrations.
	- Data Collections: Expertise in large data collection and processing including ETL, workflow and delivery of data
	- Business Intelligence (BI) tool like Qlik or Tableau
+ data science skill set:
	- Advanced knowledge of ElasticSearch/Solr/Lucene.
	- Advanced knowledge of backend paradigms
	- Knowledge with vector space models, text classification and categorization
	- Implement high quality code in an agile software development environment.
	- Able to respond and present work to peers, answer in-depth questions, accept constructive feedback, and modify product accordingly.
+ Testing and directly mitigating against common application security issues such as the [OWASP Top 10](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project).
+ Experience in a UGC (user generated content) environment
+ Design services for performant application of machine-learned models
+ Polyglot developer (e.g. Java, NodeJS, Python)
+ Experience with any one of segmentation, object detection, image classification, GANs, monocular depth estimation or a related field
+ Successful record of publication in top-tier international research venues (e.g. ICLR, AAAI, NeurIPS, CVPR, ECCV, ICCV, SIGGRAPH)
+ Very strong programming skill in C++14. You will be expected to know and use C++ templates, lambdas, and high-performance data structures.
+ Research and develop CNN/RNN neural network compression algorithms, focusing on quantization and pruning
+ embedded deep learning:
	- Analyze, test and improve neural network compression algorithms
	- Experience with at least one deep learning algorithm, such as CNN/LSTM/GRU
	- Experience with at least one deep learning framework, such as Tensorflow, Pytorch, Caffe, Kaldi
+ Demonstrated expertise with C++ with at least one of std::thread / OpenCL / CUDA
+ You know your way around SQL-like databases (e.g. PostGres, Impala, Hive) and even better if have experience with Spark and other big data platforms.
+ ***Strong publication record in top-tier research publications and conferences such as IJCV, CVPR, ICCV, ECCV, ICRA.***
+ https://www.openrobotics.org/interns
+ machine learning:
	- Develop backend services and infrastructure to expand our answer engine to support 10M+ documents and 100K+ QPS
	- Ship web applications and APIs using Python, Flask, MongoDB, MySQL, Lucene, Spark, React, Go, and/or TensorFlow
	- Optimize the performance of our indexing, processing, and query pipelines
	- Take product ideas from ideation to implementation
	- Implement state-of-the-art algorithms in Question Answering, Machine Reading Comprehension, Text Summarization, in a scalable, production-ready fashion using Tensorflow and Spark
	- Build systems to evaluate and tune performance of a real world deep learning system, from data collection to processing to model implementation to post-processing and visualization
	- modern Big Data stack (Spark or Hadoop, Kafka or RabbitMQ, ZooKeeper, Redis, Memcache, Lucene, MongoDB, MySQL)
	- Familiarity with containerization and dev-ops (Docker, Kubernetes, Docker Swarms, Jenkins, Phabricator, Continuous Integration, Continuous Delivery) is a plus
	- Familiarity with modern Deep Learning and Natural Language Processing / Natural Language Understanding (NLP, NLU), including Neural Networks, RNNs, seq2seq models, and real world machine learning in TensorFlow (incl. regularization, cross-validation, dropout) are a huge plus
	- Adaptable, humble, and interested in pushing the boundaries of what's possible
	- Work with world class talent (our team consists of former Facebook, Palantir, Dropbox, and LinkedIn Engineers; we have 2 ACM ICPC World Finalists)
+ data science:
	- Experience in modern Deep Learning and Natural Language Processing / Natural Language Understanding (NLP, NLU), including Neural Networks, RNNs, seq2seq+attention models, and real world machine learning in TensorFlow (incl. regularization, cross-validation, dropout)
	- Experience building production-ready NLP systems
	- Familiarity with non-standard machine intelligence models (Reinforcement Learning, Hierarchical Temporal Memory, Capsule Networks) is a plus
	- Familiarity with Distributed systems (Docker, Kubernetes, Kafka, Spark, Redis, AWS S3/EC2/RDS/KMS, MongoDB, or Lucene) is a plus
	- Adaptable, humble, and interested in pushing the boundaries of what's possible
	- Proficiency in Python, R, or Java
+ Installation, setup and calibration of MOCAP system, binocular cameras
+ Unigraphics or Pro-engineer fluency.
+ robotics engineering:
	- Background in multiple of the following: SLAM, mapping, localization, calibration, sensor fusion, tracking, scene understanding, robotic systems.
	- Experience in multiple of the following: non-linear optimization, 3D geometry, state estimation.
	- Experience with advanced algorithms, data structures and working with real sensor data.
	- Experience with Python and developing in the Linux and/or Mac OS environment.
	- Familiarity with real-time, multi-process and multi-threaded coding.
	- Strong C++ development skills.
	- Be an essential part of a team of engineers and scientists developing state-of-the-art estimation algorithms used for a variety of tasks, including calibration, localization and tracking.
+ Perform analysis of security incidents & threat actors for further enhancement of Detection Catalog and Hunt missions by leveraging the MITRE ATT&CK framework.
+ Experience with information security tools such as an enterprise SIEM solution (Splunk preferred), IDS/IPS, endpoint security, EDR and security monitoring solutions (NSM,DLP,Insider, etc).
+ Scikit-learn, Keras/Tensorflow, Spark MLlib, Spacy or other ML libraries
+ tech stack:
	- Docker (Kubernetes)
	- Spark (on Hadoop)
	- Kafka
	- Cassandra (or other NoSQL DBs)
	- AWS and some of its services
	- Azure and some of its services
+ Interesting projects with technologies like Scala, Java, Groovy, Akka, SpringBoot, Cassandra, MongoDB, Apache Kafka, JavaScript, TypeScript, React, Angular.
+ tech stack:
	- Python (numpy, pandas, sklearn, xgboost, TensorFlow)
	- MySQL, Hive
	- Java
	- Google Cloud Platform
	- Tableau, Looker
+ tech stack:
	- Python (numpy, pandas, scikit-learn, tensorflow, keras)
	- Google Cloud Platform
	- Machine Learning (e.g. regression, ensemble methods, deep learning, etc.)
	- Statistics (Bayesian methods, experimental design, causal inference)
	- Tableau, Looker
	- Snowflake (SQL)
+ skill set:
	- Familiarity with containerization technologies (docker, lxc, rkt, etc.).
	- Familiarity with container orchestration technologies (Kubernetes, Marathon, etc.).
	- Experience working with cloud computing services providers (AWS, Google cloud platform, Azure, etc.).
	- Experience with Elasticsearch /Apache Solr and Logstash
	- Experience working with Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ, Celery, Kafka, etc.
	- Scalable data processing techniques: Kafka, Spark, ElasticSearch, Celery
	- Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ
	- Have proven experience with ORMs (e.g. Django) and RDBMS (e.g. MySQL) including development of complex SQL queries.
+ skill set:
	- A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
	- Experience with MPP databases, such as Snowflake, Redshift, BigQuery, Vertica, etc.
	- Familiarity with data visualization tools/frameworks as well as notebooks.
	- Experience with time-series forecasting.
	- Experience building and deploying production-grade models in a real-time setting.
	- Expert-level abilities building and deploying unsupervised, semi-supervised, and supervised models on large-scale data (in that order of importance).
	- A degree of comfort at the command line. That means a thorough understanding of basic file-system commands, as well as the ability to ssh into remote machines and troubleshoot without a GUI, grep through logs, and deploy scripts and applications.
+ skill set:
	- Exceptional SQL skills and experience working with granular web clickstream data and behavior tracking tools like SiteCatalyst
	- Fluency in data analysis, including defining KPIs, statistical and predictive modeling concepts, descriptive statistics, experimental design and multivariate A/B testing
+ skill set:
	- Hadoop, HDFS, Hive, HBase, MapReduce, and Mahout.
	- Large-scale graph algorithms, clustering, page-rank, and community detection.
	- Apache Spark, SparkSQL, MLlib, and Scala Actors.
	- Ensemble Methods, Deep Learning, and other trendy topics in the Machine Learning community.
+ skill set:
	- Extensive hands on experience with administering some of the following: HBase, Impala, Spark, EMR, Hive on Tez or Presto
	- Experience operating large scale Hadoop clusters running Cloudera distribution
	- Operational mindset with ability to do Problem, SLA and Incident Management
	- Experience installing and managing Kafka is good to have
	- Experience managing infrastructure in AWS using EMR
	- Experience with SQL and Python(Boto3 Library)
	- Experience with AWS products including EC2, S3, RDS, ElastiCache, ElasticSearch, Kinesis, Lambda, etc
	- Exposure to Big Data on AWS (DataPipeline, Batch, AWS Glue, S3, EMR/EC2)
	- Hands on expertise in AWS (S3, EMR, EC2, Hadoop, Hive, Spark, Kafka, Storm, Druid, Cassandra, Columnar Databases and Graph Databases like DSE Graph is huge plus.
+ skill set:
	- Efficient in SQL, Hive, SparkSQL, etc.
	- Serve as technical “go to” person for our core technologies – Hadoop, Spark, AWS, Vertica, Tableau, Cassandra, Graph Databases and others
	- Advanced SQL skills to perform data segmentation and aggregation from scratch; experience working with granular web clickstream data a plus!
	- Knowledge of programming languages and stats packages (e.g. python, R); comfortable running multiple regression analyses
+ Strong hands-on experience with at least one of the main stream deep learning frameworks such TensorFlow, PyTorch, BLVC Caffe, Theano
+ skill set:
	- Expert knowledge of either Python or R, strong experience with database management systems like SQL, preferably also Spark ML, Scala, Hive and Impala
	- Confidence and comfort working on projects and goals that are happening in a hypothesis driven environment (build – measure – learn mindset)
	- An excellent understanding of SQL.
	- Experience with big data technologies (Hive, Impala, Spark, etc.) would be a plus.
+ Automated testing: Karma, Protractor
+ skill set:
	- Proxmox, KVM virtualization, LXC and Docker containers
	- large scale object storage (Ceph, cloud-based object storages)
	- Puppet
	- PostgreSQL
	- Distributed architecture (RabbitMQ, Kafka)
	- Icinga/Prometheus/ELK monitoring
+ Knowledge of container (docker or others) and orchestration (K8S or others)
technology is a plus+ At least 2 years of experience using deep learning techniques (CNN, RNN, LSTM) on computer vision tasks (object detection and tracking, classification, action recognition)
+ skill set:
	- 2+ years of cloud experience using AWS (e.g., EC2, ECS, Batch, Lambda)
	- Familiarity with Continuous Integration tools (e.g., Jenkins, Travis)
+ Knowledge of distributed machine learning framework (distributedTensorFlow/MXNET) or cloud/edge federation is a plus
+ At least 2 years of experience using deep learning techniques (CNN, RNN, LSTM) on computer vision tasks (object detection and tracking, classification, action recognition)
+ skill set:
	- 2+ years of cloud experience using AWS (e.g., EC2, ECS, Batch, Lambda)
	- Familiarity with Continuous Integration tools (e.g., Jenkins, Travis)
+ skill set:
	- Technologies : AWS Batch,  Spark, Hive, EMR, Presto, Docker, Jenkins, Bitbucket
	- Databases: RDS MySQL, Redshift
	- Machine Learning: Distributed TensorFlow, Keras, PyTorch, Caffe2, scikit-learn, Apache MxNet, SageMaker
	- Developing DAOs (data access objects) and APIs
	- Extensive practical experience using a wide range of AWS technologies, including: S3, EC2s, Lambda, Step Functions, Glue, EMR, API Gateway
+ Experience with CoreAudio, Audio Unit, ASIO and VST APIs.
+ Familiarity with automated build systems such as Jenkins or buildbot.
+ Experience with distributed analytic processing technologies (Hive, Pig, Presto, Spark)
+ skill set:
	- Linux
	- Puppet
	- nginx
	- AWS
	- Datadog
	- Cloudformation
+ skill set:
	- API rest
	- Ruby on Rails
	- CDNs
	- Microservices
	- Python
	- MySQL
	- Bento4
	- FFmpeg
	- Docker
	- We are looking for students or graduates with strong knowledge of objected-oriented programming, SQL and NoSQL.
+ skill set:
	- Experience with advanced ML models and concepts: HMMs, CRFs, MRFs, deep learning, regularization etc.
	- Experience with distributed databases such as HBase, Redis, CouchBase etc.
	- Experience in search platform such as Solr, Elastic Search
+ Cross platform C++17, OpenCL, Qt/QML, git, Python 3, clang-tidy, clang-format, Jenkins, CMake, Catch2, Docker, Vagrant, KVM, C++/CLI.
+ Work with cutting edge technologies such as C++17 (anything that passes VS2017, latest Clang, GCC, clang-tidy, ...). C++/CLI, GPGPU, Python, Jenkins, docker, KVM, libclang, boost, Qt and CMake
+ skill set:
	- Java, mahout, Hadoop
	- Scala, Spark, SparkML
	- Spark with Python, numpy and pandas
+ skill set:
	- As part of the Advanced Product Development team, immediate responsibilities include:
		* Exploration and development of machine learning algorithms for spatiotemporal analysis, including multiclass classification, clustering, temporal segmentation, sequence labeling, and spatial segmentation.
		* Development of new technologies and digital products to improve surgeon and team performance on robotic surgery platforms.
		* Support clinical and academic collaborations in related fields.
	- Additional responsibilities include:
		* Contributing to multiple areas of research, including but not limited to the following:
		* Designing and applying machine learning algorithms to novel, surgical applications
		* Characterizing surgeon and team behavior and workflow to optimize new technologies
		* Fully integrating machine learning into core digital products and intelligent systems
		* Conducting user studies to evaluate digital product concepts
	- Establishing strong academic collaborations across research disciplines
	- Presenting research at international conferences and publishing research in top academic journals
	- Qualifications... Skill/Job Requirements:
		* Competency Requirements: (Competency is based on: education, training, skills and experience).
		* In order to adequately perform the responsibilities of this position the individual must have:
			+ Doctoral degree in Computer Science, Statistics, Applied Mathematics, or Neuroscience, or Master's degree with minimum (5) years industry experience developing machine learning applications
			+ Demonstrate excellent communication skills both written and verbal
			+ Interested in early research and development through to product roll-out
			+ Solid understanding of statistics, machine learning, and deep learning algorithms and techniques is required
			+ Experience with sequence modeling, image analysis, activity recognition, and/or temporal segmentation on real-world data is required
			+ Experience with Python is required
			+ Hands-on experience with deep learning frameworks such as Tensorflow, Theano, Caffe, and/or Torch is required
			+ Hands-on experience with CNNs, RNNs, and LSTMs is ideal
			+ Experience with R, SQL is ideal
			+ Experience with C/C++ is ideal
			+ Experience with clinical studies is a plus
			+ Ability to travel domestically and internationally (10%)
+ skill set:
	- Intuitive Surgical designs and manufactures state-of-the-art robot-assisted systems for use in minimally-invasive surgery. These systems are revolutionizing the way in which surgery is being done and offer a unique platform that is being used routinely at hospitals worldwide for exploring the potential of digital surgery. Joining Intuitive Surgical means joining a team dedicated to using technology to benefit patients by improving surgical efficacy and decreasing surgical invasiveness, with patient safety as our highest priority.
	- The Applied Research group within Intuitive Surgical has an immediate opening in Sunnyvale, CA for a research scientist with focus on Computer Vision, Deep Learning and Image Analytics, contributing to new technology development in the area of 3D scene understanding/reconstruction and spatial AI systems for next-generation robot-assisted surgery platforms. This role is an exciting opportunity to join a newly formed team and contribute to its growth and it will give you an opportunity to test your knowledge in a challenging problem solving environment.
	- Research, design and implement algorithms in deep learning for computer vision and image analytics
	- Contribute to research projects that develop a variety of algorithms and systems in computer vision, image and video analysis.
	- Advance the state-of-the-art in the field, including generating patents and publications
	- Develop prototypes of 3D recognition models that scale to large clinical datasets
	- Develop prototypes of dense 3D reconstruction systems based on multi-view image sensors
	- Contribute to building new clinical datasets and data pipelines
	- Participate in integration of new ML/CV algorithms into existing and future robotic platforms
	- Experiment with several users and clinical advisors to iterate prototype designs based on feedback and performance.
	- Develop new technologies and digital products to improve surgeon and team performance on robotic surgery platforms.
	- Support academic collaborations in related fields.
	- Contribute to multiple areas of research, including but not limited to the following:
		* Design and apply CV/ML algorithms to novel, surgical applications

		* Design/bring-up of novel sensing technologies

		* Characterize surgeon and team behavior and workflow to optimize new technologies
	- Establish strong academic collaborations across research disciplines
	- Doctoral degree in computer science, electrical and computer engineering, or Master's degree with minimum (5) years industry experience developing computer vision and machine learning applications
	- Strong understanding of machine learning: you should be familiar with the process (data collection, training, evaluation, and making iterative improvements) of building effective learning systems.
	- Strong hands-on experience with at least one of the main stream deep learning frameworks such TensorFlow, PyTorch, BLVC Caffe, Theano
	- Strong hands-on experience with Python (proficiency), C/C++ (proficiency), Shell Script, Matlab
	- Strong engineering practices, debugging/profiling skills, familiarity with multi- threaded programming.
	- Train machine learning and deep learning models on a computing cluster to perform visual recognition tasks, such as segmentation and detection
	- Hands-on experience with GPU accelerated algorithms and implementations
	- Hands-on experience with state-of-the-art models based on CNNs, RNNs, and LSTMs
	- Excellent communication skills both written and verbal
	- Interested in early phases of product exploration and iteration based on incomplete requirements.
	- Solid understanding of computer vision, machine learning, and deep learning algorithms and techniques is required
	- Experience with visualization tools is a plus
	- Self-starter and able to work in a collaborative and results-oriented environment
	- Ability to travel domestically and internationally (5-15%)
	- Able to view live and recorded surgical procedures
+ Experience with CUDA, VTK/ITK, and/or physics based simulation
+ skill set:
	- Experience developing virtual world environment for games, such as Lumberyard, Unity3d, Unreal, CryEngine and/or other world simulation environments
	- Experience with graphics APIs and frameworks such as OpenGL, DirectX, or Vulkan
	- Experience with physics engines (e.g. Bullet, Havok, PhysX)
	- Experience developing agent behaviors, physics, gameplay, tools, or GUIs
+ skill set:
	- Google Dialogflow
	- The Visual Fusion Engine, VFE
	- algolia - Search Made Powerful
	- Cruzr - Humanoid service robot
	- Descartes Labs
	- Blue River Technology - Smart Agricultural Machines
	- NLPBOTS - Intelligent Chat-bots
	- BrainShop - AI for developers
	- Flyr
	- Sisense
	- Mookkie
	- NanoNets
	- Workfusion - Rpa Express
	- MSG.AI
	- Keepers - Advanced Child Monitoring
	- Neura - User Awareness with AI
+ Distributed Deep Learning
+ Automate and integrate security into CI/CD pipelines, such as static code analysis and dynamic code analysis.
+ Enjoys working and deploying technologies such as Chef, AWS, Ruby, Rails, MySql, and Redis
+ Docker, Heroku, Kubernetes
+ Experience with monitoring (NewRelic, Prometheus, PagerDuty)
+ Authentication: CAS (Java)
+ skill set:
	- Experience in at least one of the following sequencing methodologies: cell-free DNA sequencing, methylation sequencing, single cell sequencing
	- Deep understanding of next generation sequencing methods, platform-specific bias and errors, and data interpretation
	- Hands-on experience with large-scale genomic data (e.g. whole genome sequencing, exome-seq, target enrichment, amplicon sequencing, RNA-seq, etc.)Hands-on experience with genome alignment, mapping, variant calling, and annotation tools
	- Familiarity with human and mammalian genomics, standard genomic databases (UCSC, Ensembl, NCBI), formats, and tools.
	- Proficiency in python, ruby, Perl, or another high-level scripting language (Python highly preferred)
	- Familiarity with UNIX-based operating systems and shell scripting
	- Excellent data analysis and visualization skills (Tableau, R, matplotlib, or similar)
	- Excellent understanding of molecular biology and cell biology
	- Experience with Seven Bridges, DNA Nexus or similar platform
+ Experience with applied ontology development (RDF(S)/OWL, SPARQL, the Semantic Web or Frame based KR systems).
+ Good understanding of high-throughput single cell assays such as single cell transcriptomics, single-cell epigenomics, FACS, and CyTOF.
+ Experience with relational (e.g. postgres, SQL) and/or distributed (e.g. MongoDB) databases
+ Experience with RNA-seq, ATAC-seq and/or ChIP-seq experiments and data at the single cell level.
+ skill set:
	- Developing scalable bioinformatics algorithms, pipelines and tools to solve complex, distributed computing challenges
	- Providing statistics, scientific computing and data analysis support to research, product development and manufacturing operations
	- BS in bioinformatics or related field with 6-8 years industry experience or MS/PhD in bioinformatics or related field with 4-6 years industry experience
	- Advanced proficiency in Python, R or Rust in a Linux environment
	- Broad experience with NGS data analysis, manipulation and presentation (in Python or R)
	- Excellent data analysis and visualization skills (in Python, R and using tools like Tableau)
	- Excellent understanding of molecular biology, with an emphasis on DNA chemistry preferred
	- Proven ability to synthesize and present conclusions from complex analyses in a clear and visually compelling way
	- Familiarity with common tools and file formats for genome-scale interval data manipulation and analysis (e.g. bedtools, bedops, etc.)
	- Formal software development experience, the software development lifecycle, unit and functional testing (via py.test) and version control via Git
	- Familiarity with database (PostgreSQL, NoSQL and Redshift) design, implementation and efficient query construction
	- Experience building and using Docker containers
	- Familiarity with cloud-based computing environments, especially Amazon Web Services
	- Experience using workflow management tools (e.g. Airflow, Galaxy, Luigi)
	- Familiarity with web application development including building REST-ful interfaces
	- Familiarity with DNA analysis software (MacVector, VectorNTI, SnapGene)
+ skill set:
	- M.S. or Ph.D. degree in Genetics, Biology, Bioinformatics, Biostatistics, Computational Biology, Computer Science, or related field
	- Knowledge and experience in the field of cancer or rare disease genomics.
	- Experience in at least one of the following sequencing methodologies: cell-free DNA sequencing, methylation sequencing, single cell sequencing
	- Deep understanding of next generation sequencing methods, platform-specific bias and errors, and data interpretation
	- Hands-on experience with large-scale genomic data (e.g. whole genome sequencing, exome-seq, target enrichment, amplicon sequencing, RNA-seq, etc.)
	- Hands-on experience with genome alignment, mapping, variant calling, and annotation tools
	- Familiarity with human and mammalian genomics, standard genomic databases (UCSC, Ensembl, NCBI), formats, and tools.
	- Proficiency in python, ruby, Perl, or another high-level scripting language (Python highly preferred)
	- Familiarity with UNIX-based operating systems and shell scripting
	- Excellent data analysis and visualization skills (Tableau, R, matplotlib, or similar)
	- Excellent understanding of molecular biology and cell biology
	- Strong interpersonal communication skills
	- Excellent publication track record
	- Additional desired qualities include:
		* Familiarity with version control (git, subversion)
		* Experience with target enrichment
		* Experience in clinical laboratory testing
		* Experience with Seven Bridges, DNA Nexus or similar platform
+ skill set:
	- Design and architect the backend for our computer aided genetic circuit pipeline, including the genetic compiler and simulation engine
	- Develop our cloud infrastructure, data warehouse, build and deploy system, and machine learning platform. You will drive discussion and decisions on microservices vs monoliths, managed services vs deployed, etc.
	- Work on tooling to help improve the efficiencies of our synthetic biologists including data analysis platforms and robotic automation
	- solid at communicating architectures internally through clear design documents
	- embracing ambiguity and driving for impact
	- excited to partner closely with and learn from our synthetic biology team
	- interested in helping define the company product, direction and culture
	- You are a highly skilled technical lead in software engineer with years of experience in industry. You are passionate about joining an early stage startup where autonomy, passion to learn and excitement to engineer biology take precedence over process and ego. If you have a background in genetics or cellular biology, great! If not, you have strong experience with ETL systems, data storage and access, cloud infrastructures, build and deploy systems with a passion to learn the science.
+ skill set:
	- Architect the overall automation software, hardware, wetware ecosystem including hardware configurations, experimental protocols, and software control and scheduling systems.
	- Mentor other automation engineers.
	- Design, create, and deploy software to schedule, control, and manage the automation system.
	- Design, configure, and assemble a state-of-the-art automation system for biological engineering.
	- Work with synthetic biologists to translate manual protocols into automated processes and validate them at high throughput.
	- Work with hardware engineers to develop custom automation hardware to physically perform synthetic biology's experimental procedures.
	- Work with software engineers to develop custom software tools to schedule, control, and manage the automation system.
	- Develop metrics to measure and track the success of the automation platform.
	- Partner with automation vendors to vet software platforms and work to integrate them into our larger pipeline though the development of new APIs and methods.
	- Partner with automation vendors to obtain new hardware, develop new prototype hardware elements, build interfaces, and integrate hardware into the automation platform.
	- Embrace ambiguity and drive for impact.
	- Contribute to the company's product, direction, and culture.
	- B.S. or higher degree in Bioengineering, Computer Science or a related field. PhD preferred.
	- 5+ years direct experience in biotechnology industry. 2+ years directly with automation.
	- Strong coding skills in modern software development environments. GitHub (or similar) portfolio of work ideal.
	- Publication or patent record related to biotechnology.
	- Strong written and oral communication skills.
	- Ability to physically relocate to Cambridge, Massachusetts.
	- You are a highly skilled programmer with years of experience in industry or research. You want to work on software development projects with strong algorithmic and research foundations. You are highly skilled technical lead in laboratory automation with years of experience in industry or research. You are passionate about joining an early stage startup where autonomy, passion to learn, and excitement to engineer biology take precedence over process and ego. You ideally have some laboratory automation and biology wetlab experience. You are interested in learning more about how to integrate hardware, software, and wetware.
+ skill set:
	- Design, configure, and assemble a state-of-the-art automation system for biological engineering.
	- Work with synthetic biologists to translate manual protocols into automated processes and validate them at high throughput.
	- Work with software engineers to develop custom software tools to schedule, control, and manage the automation system.
	- Develop metrics to measure and track the success of the automation platform.
	- Partner with automation vendors to obtain new hardware, develop new prototype hardware elements, build interfaces, and integrate hardware into the automation platform.
	- Embrace ambiguity and drive for impact.
	- Contribute to the company's product, direction, and culture.
	- You are a highly skilled technical lead in laboratory automation with years of experience in industry or research. You are passionate about joining an early stage startup where autonomy, passion to learn, and excitement to engineer biology take precedence over process and ego. You ideally have some software programming and biology wetlab experience. You are interested in learning more about how to integrate hardware, software, and wetware.
+ Working knowledge of industry leading configuration management tools and methods (Salt, Ansible, Chef, Puppet, etc)
+ Ideal candidate would have experience with metabolomic datasets and familiarity with analytical chemistry/assay methods, automation systems, and LIMs
+ Good understanding of protocols like AXI, ACE, ACP, CHI etc.
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ skill set:
	- Knowledge or hands-on experience in Container technologies such as Docker is a plus.
	- Knowledge of Virtual machines, Hypervisors is a plus
+ At least one graphics API (OpenGL, Direct 3D, Vulkan)
+ Either in AI tools; or autonomous embedded area; or FPGA
+ Seeking motivated graduate interns in the intersection of computer vision, 3D graphics and machine learning with applications to interactive visual effects processing. In this position, you will collaborate with VFX artists and fellow researchers in NEXT team to invent algorithmic solutions to VFX challenges. This is a full-time position located in Hillsboro, Oregon.
+ Programming experience in Python and MATLAB. Experience with LabView, VPItransmissionMaker, Tensorflow is a plus.
+ You find the 10% work that gets us 90% of the value
+ skill set:
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- Experience with distributed data processing systems like Spark and Hadoop
	- Familiarity with interactive data visualization using tools like D3.js
+ skill set:
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- [Preferred] Experience working on a SaaS platform or with Service Oriented Architectures
	- [Preferred] Experience with software security and systems that handle sensitive data
+ Experience with: Large scale distributed computing, Database internals, Big Data engines e.g. Spark, Hadoop
+ skill set:
	- Passion for process automation
	- Build system experience like Maven, Bazel, or Gradle
	- Continuous integration and testing experience like Jenkins
	- [Preferred] Kubernetes and Docker
	- [Preferred] Experience on working with services provided by AWS, Azure, or GCP
+ Strong familiarity with server-side web technologies (eg: Nodejs, Java, Python, Scala)
+ Experience with our web stack (React, Redux, TypeScript, protobuf, Apollo, GraphQL) and Spark
+ skill set:
	- Passionate about developer experience: you want tools to be fast, CLIs intuitive, giving your colleagues (and yourself) the best experience possible doing development at Databricks.
	- Passionate about automation: automated workflows, automated testing, automated deployments, automated monitoring, our job is to automate away any tedious work out of our own lives and that of our fellow engineers
	- Comfortable in a heterogeneous environment and able to quickly learn new technologies. The team deals with a variety of environments from Kubernetes services to Jenkins automation, Scala libraries to Python scripting, writing graph algorithms to debugging 3rd-party Javascript, Dev-Tools does it all.
	- Able to quickly learn their way around unfamiliar codebases: Dev-Tools routinely dives into unfamiliar third-party projects (in unfamiliar languages!), pushing them far beyond what they say they can do “on the box”
	- Experience with Continuous Integration/Continuous Deployment frameworks
	- Experience with monitoring frameworks for large systems
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience architecting, developing, deploying and operating large scale distributed systems at scale
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
	- Experience working on a SaaS platform or with Service Oriented Architectures
	- Experience with Continuous Integration/Continuous Deployment frameworks
	- Experience with Docker and Kubernetes
+ skill set:
	- 2+ years experience with R or Python
	- 2+ years experience with predictive modeling
	- Familiarity with data visualization in R or Javascript
	- Understanding of relational data or SQL
	- French is not required and all European languages are appreciated
	- Experience with PySpark, SparkR or Scala
	- Experience developing WebApps
	- Experience building APIs
	- Experience with HDFS and NoSQL databases (MongoDB, Cassandra, etc)
	- Construct end-to-end data flows from raw data to predictions
	- Crunch, analyze and investigate any kind of data
	- Explore new machine learning algorithms
	- Build attractive visualizations
	- Communicate results to non-technical colleagues and clients
	- Provide data science expertise to sales, marketing, and R&D teams
+ skill set:
	- DSS is an on-premises application that connects together all big data technologies. We work with SQL databases, MongoDB, Cassandra, ElasticSearch, Hadoop, Spark, MLLib, scikit-learn, Shiny, … and many more. Basically, our technological stack is made of all technologies of the big data and machine learning landscape.
	- Our backend is mainly written in Java but also includes large chunks in Scala, Python and R. Our frontend is based on AngularJS and also makes vast usage of d3.js
	- One of the most unique characteristics of DSS is the breadth of its scope and the fact that it caters both to data analysts (with visual analytics) and data scientists (with deep integration in code and libraries, and a web-based IDE).
	- You are the ideal recruit if:
		* You are a full stack engineer. You know that low-level Java code and slick web applications in Javascript are two sides of the same coin and are eager to use both.
		* You know that ACID is not a chemistry term.
		* A first experience (either professional or personal) building a real product would be a big plus. Experience with some technologies of the Big Data and analytics stack (distributed databases, large-scale data processing, statistics, machine learning, JS visualizations, ...) is also desirable.
+ You are curious and pragmatic: you want to explore extensions to our product but are motivated by delivering production code for business use cases
+ skill set:
	- 2-4 years of experience in C/C++ development
	- Object oriented programming experience
	- Experience with applications design and implementation
	- Experience in multi-threaded programming
	- Proven track record of finding bottlenecks and delivering optimized, high-quality code
	- Knowledge in algorithms development and implementation
	- Fast learner, team player, reliable, motivated, hard worker
	- Experience in Android NDK development
	- Experience in image processing algorithms
	- Experience in runtime optimizations on embedded accelerators (e.g. Neon, DSP, GPU).
	- Experience in writing OpenCL kernels
	- Experience in Matlab
+ Open64 is a free, open-source, optimizing compiler for the Itanium and x86-64 microprocessor architectures.
+ Skill set:
	- Psyco, Nukita, Shed skin.
+ skill set:
	- Elastic Search, Lucene, SQL Server, Kibana, or similar experience
	- Prior experience aligning platform architecture with security, data
	- Demonstrated ability to document architectural standards and decisions
	- SaaS or high scale cloud experience
+ skill set:
	- Databases / Data warehousing
	- Writing complex SQL
	- Strong Microsoft Excel skills
	- Experience with Big Data platforms such as Hadoop
	- Experience with other reporting/visualization tools such as Tableau
	- Experience with monitoring and tracking tools such as  Splunk, NewRelic, Adobe/Google Analytics
+ skill set:
	- Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (Redis, Cassandra or DynamoDB)
	- Experience writing crisp, clean REST APIs
	- Experience with RSpec or equivalent integration test framework
	- Self-motivated individual who proactively identifies team bottlenecks and works with the team to resolve them
+ skill set:
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Working experience and detailed knowledge in Java, JavaScript, or Python
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Agile development (Scrum) experience
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- Experience with Backbone, Marionette or equivalent framework
	- Experience with Protractor, RSpec or equivalent integration test framework
+ skill set:
	- Experience with system level monitoring tools such as Nagios or Zabbix and application performance monitoring tools such as NewRelic, AppDynamics or Dynatrace.
	- Understand configuration management tools such as Puppet, Chef or Ansible.
	- Strong understanding of the DevOps landscape from orchestration to instrumentation , from VCS to CI tools, and from APM to monitoring tools
+ skill set:
	- Working experience with AWS services (Aurora DB, Dynamo DB, Athena, EMR, Redshift, Data Catalog etc)
	- Experience with log analysis tools like Splunk
	- Experience with Issue trackers tools like Jira etc
+ You approach problems with a scrappy, creative, and entrepreneurial mindset.
+ You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Familiar with the operation of instrument -- VSG, VSA, Signal analyzer, channel emulator.
+ skill set:
	- LLVM compiler internals.
	- Polyhedral models.
	- Familiarity with HPC kernels and their optimization.
+ skill set:
	- Experience working with modern deep learning software architecture and frameworks including: Tensorflow, Pytorch, ONNX, MxNet, Caffe/Caffe2, and/or Torch;
	- Experience working with modern deep learning models including: Resnet, Mask-RCNN, RNN/LSMT, Bert, Transformer etc
+ Help the current effort of the AI research community, and contribute to cutting edge research in machine intelligence, starting from areas including Deep Learning, Generative Models, Reinforcement Learning, Evolutionary Computing, Sequence Modelling, Large-Scale Distributed Optimization and Low-Precision Numerical Formats.
+ Working on the IPU architecture compiler. Understanding code generation & optimization of C / C++ code to the instruction set of the machine. The architecture compiler and its ability to target the IPU for maximum performance and flexibility, is a fundamental component of the Poplar framework.
+ skill set:
	- Candidates should have a solid background with standard networking protocols (TCP, RPC, UDP, IPSec), low-latency protocols (RDMA, RMA) and Clustering.
	- Preferably, you should also have a background or interest in host device and network virtualisation (SR-IOV, Xen, Containers)
+ The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
+ skill set:
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
	- Good knowledge of common development tools such as yocto/git/gtest
+ Experience working with PCIe form-factor accelerators such as GPUs, DSPs or FPGAs
+ skill set:
	- SGE or other DRMS
	- XML and XPath/XSLT
+ skill set for DevOps:
	- Remote hardware administration with IPMI
	- Configuration and management of
		* SGE/Univa, Slurm, LSF or other DRMS
		* Jenkins CI
		* Phabricator
		* FlexLM licensing
	- Puppet, Ansible, Nagios
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
	- Web programming – HTML/DOM, JavaScript, SQL
+ skill set:
	- Hardware-In-the-Loop (HIL) Systems
	- Development of test automation projects
	- Modeling of physical systems for HIL simulation
	- Specialized support of HIL systems and models
	- Development, testing, or familiarity of embedded control systems (mechatronics, automotive control systems)
	- HIL simulation systems
	- In-vehicle communication networks such as CAN, LIN, FlexRay
	- MIL/model-in-the-loop, SIL/software-in-the-loop, PIL/plant-in-the-loop, and HIL/hardware-in-the-loop
	- The implementation of software is done in project-specific technologies and thus extends over a wide area, including C/C++, C # (including WPF, WCF), Python, MATLAB ®, VHDL.
+ skill set for HPC Infrastructure Engineer:
	- Management of an on-premise computing cluster in a High Performance Computing (HPC) setting
	- Develop usage policies for deep learning training
	- Develop tools and infrastructure to scale deep learning
	- Maintain network infrastructure for local and cloud compute
	- Data management and backups
	- Familiarity with environments including LDAP, NFS, bare metal GPU servers, deployments and automation / conﬁguration management, modular user shell environments, networking
	- Hands on server hardware configuration experience
	- Experience with cluster management software
	- Comfortable with GPU servers
	- Strong knowledge of Linux systems and internals (Debian preferred) with a good understanding of networking and related protocols, OS customization, and package management (APT)
	- Hands on Infiniband experience
	- Have used or developed metrics/analytics tools for usage
	- Experience with Slurm or similar job systems
+ skill set for Deep Learning Research Scientist at DeepScale:
	- DeepScale was founded by the deep learning researchers from UC Berkeley who created SqueezeNet. DeepScale is developing perception systems that enable automated vehicles to interpret their environment in real-time using low-cost hardware.
	- A PhD in electrical engineering, computer engineering, or computer science.
	- A track record of advancing the state-of-the-art in an application of deep learning (ideally a computer vision or imaging application … but if you did speech-recognition or text-analysis, that's pretty good too)
	- Published papers that either (a) are in top peer-reviewed conferences such as CVPR, NIPS, ECCV, ICCV, or ICML … or … (b) a significant (>100) number of citations on one of your deep learning research publications
	- The ability to design, implement, train and test models in one or more of the leading deep learning frameworks like PyTorch or TensorFlow.
+ skill set:
	- Machine learning areas of special interests include: CNNs, RNNs, distributed GPU computing, detection, prediction, motion planning, mapping and localization
	- Work with lidar sensor firmware and low level signal processing
	- Perform Multi-Object Tracking & Multi-Sensor Fusion
	- Sensor calibration & Perception algorithms (all types and flavors)
+ skill set:
	- Familiar with the basics of lidars, radars, cameras, and ultrasonic systems.
	- Are familiar with Physics-based modeling and simulation of sensor systems (eg, link budget analysis, wave propagation, sensor noise sources, etc.).
	- At least 1 year of experience in radar, lidar, or camera modeling and/or evaluation.
	- Familiarity with test and calibration processes for autonomous vehicle sensors.
	- Familiarity with EVT, DVT, and PVT processes for sensor bring-up and validation as well as FMEA principles.
+ Working experience with large-scale data platform and pipelines such as Hadoop, Hive, Pig, MapReduce, Spark, Kafka, Flumes, etc.
+ Familiarity with GPU computing (CUDA, OpenCL) is preferred
+ Familiarity with modern planning approaches including randomized search methods and trajectory optimization
+ Solid work experiences with state-of-the-art mapping and localization algorithms
+ Experience with scheduling engines such as Airflow
+ Published papers (e.g. in Supercomputing, IPDPS, or PPOPP) and/or open-source code that demonstrate your skills in writing fast code.
+ Design and implement a set of compiler tools to translate from ML-oriented domain-specific languages (TensorFlow, Caffe, Theano) to proprietary binary format
+ Buzzwords: Gitlab, Docker, MongoDB, AWS, Node.js.
+ You have experience in material characterisation: AFM SEM, TEM, XPS and others.
+ Experience with Analytics tools is appreciated  (Segment, Amplitude)
+ skill set:
	- are as passionate about teaching AI/ML as you are about AI/ML itself
	- are experienced in software engineering, machine learning engineering in Python using scikit-learn: regression, trees, ensembles (nice-to-have: catboost, XGBoost)
	- are comfortable in collecting and manipulating data in Python: APIs, web scraping, Pandas, numpy/scipy
	- have tackled Deep Learning (DL), including LSTMs, RNNs, CNNs (nice-to-have: YOLO) with real-world application experience such as computer vision or NLP
	- have implemented ML and DL at scale using SparkML, Keras, TensorFlow and/or Pytorch
	- have a strong understanding of software engineering best practices, including version control, testing, monitoring and debugging
	- are highly proficient in the curriculum topics in our program
	- are available for weekly, 30-minute video check-ins with students to help them set and achieve learning goals, answer subject matter questions, provide feedback on projects, and career advice
	- have at least 3 years of experience solving real-life machine learning problems and building models
	- are empathetic and have excellent communication skills
+ skill set:
	- Experience in one or more of the following areas:
	- Control theory
	- Motion planning
	- Optimization
	- Formal logic
	- Game AI development
	- Experience in developing safety-critical, embedded or real-time systems
	- Published research in any of the above mentioned areas
	- Experience in machine learning and data analysis
	- Programming in Python, working with Linux
+ Familiarity with neural network framework such as Caffe, Torch, Theano, TensorFlow, CNTK
+ skill set:
	- 4+ years of embedded software applications development, debug, deployment (including drivers development)
	- Superior knowledge of RTOS-based software systems and/or Embedded Linux
	- Comfortable with parallel paradigms (notions of pthread and/or OpenCL/Cuda)
	- Agile methodology ; Git, continuous integration, test driven development
	- Experience of AUTOSAR architecture is valuable
	- Knowledge of Computer Vision libs (OpenCV, OpenVX) and Machine learning technology (TensorFlow, Caffe) would be a plus
	- Knowledge of application middleware (e.g. ROS)and communication layers (TCP/UDP, DDS) would be a plus
+ The Computational Neuroscientist's primary job function is to work with the CTO to identify innovative technologies, and to research and develop practical Spiking Neural Network based products by using BrainChip's spiking neural model and previous research.  This involves the development of an architecture that is flexible in its application. The resulting SNN architecture will have a wide application range in areas such as Computer Vision, Olfactory, Auditory and Tactile feature learning and extraction.  Will also work on a development kit and API which will form the foundation for products for the in-house product development team and will also be made available to external research and development facilities. The computational Neuroscientist will work with associated educational facilities such as the Cerco, UCI and UCSD and evaluate technologies that are relevant to BrainChip's SNN technology.
+ Experience in one application field (Image processing, ADAS, FinTech, CyberSecurity)
+ Familiar with big data processing tools such as Hadoop, Spark, HBase
+ skill set:
	- Ability to performance tune and scale models
	- Experience with Docker/Containerization
	- Experience with Django
	- Experience in a micro-service architecture
+ skill set:
	- 8+ years of meaningful industry experience and a background in high-speed processor design (i.e. Graphics, Microprocessors, Network Processors, or Mobile / Multimedia SOCs)
	- Knowledge of GDDR5, LPDDR4 or DDR3 or related protocols, or knowledge of PCIE and high-speed Serdes
	- Experience with all stages in the ASIC design flow including emulation, prototyping, DFT, timing analysis, floorplanning, ECO, bringup & lab debug, and ATE test development
	- Experience with high speed clocking, cache interfaces and protocols
+ skill set:
	- Monitor and Metrics gathering (Prometheus, StackDriver ...)
	- Experience with HashiCorp tools - Vault, Consul, Nomad
	- Experience with NixOS
+ skill set:
	- Assisting developers to apply best practices to ensure fully working test, training and production environments using Gitlab-CI, Docker, Ansible.
	- Designing, building and maintaining CI/CD, testing, and operations infrastructure for our systems.
	- 5 years of DevOps experience, both building end-to-end automated CI/CD pipelines, as well as application and operations support. This experience should include hard-core hands-on Linux admin, networking, security, and AWS experience in a dev-through-production environment.
	- A proven track record of excellent customer service delivery, including working with developers, ops, and users to troubleshoot and resolve challenging problems in a timely manner, and being an embedded DevOps member of an application scrum team, as well as requirements gathering, design, project planning, and implementation of DevOps process and tooling.
	- Strong architectural level of understanding of software, networks, security, and operations, with the knowledge and know-how to influence software and operations design and process.
	- Strong hands-on familiarity with infrastructure automation tools such as Jenkins, GoCD, Terraform, Artifactory/Nexus, Ansible, Puppet, Chef, InSpec, etc.
+ skill set:
	- Building, maintaining and improving the modeling infrastructure using cutting edge tools including: Hadoop, PIG, Kafka, Flume, Ssamza, Zookeeper, PySpark, Elasticsearch, Python, Django
	- Learning how to build and maintain an ETL pipeline for scalability and stability.
	- Supporting data analysis, mathematical modeling, machine learning and data mining for price testing and its optimization
	- Explore and employ external data sets, finding them, adding them to the system, adjusting and reformatting/cleaning up so that they can be consumed by our infrastructure. Developing models and machine-based understanding of interactions between external datasets and client data.
+ skill set:
	- Experience turning ideas into actionable designs. Able to persuade stakeholders and champion effective techniques through product development
	- 5+ years of industrial data-mining / analytics experience including applied techniques in data mining, machine learning, or graph mining using Python, R, and/or Spark
	- Comfortable working in a dynamic, research-oriented group with several ongoing concurrent projects
+ skill set:
	- Develop ETL operations using Python, Spark, SqlServer, Redshift and Kafka.
	- Develop the core tooling library to support Airflow data pipelines.
	- Design and implement the testing framework for Airflow dags and write test cases.
	- Document our systems for internal and external stakeholders
	- Support business stakeholders, analysts and data scientists on diverse projects.
	- Monitor and debug data pipelines running on Airflow.
	- Participate in code reviews.
	- Deliver quality work on tight deadlines.
	- Experience building systems with a framework, ideally Airflow (web frameworks are helpful, too)
	- Experience with data manipulation tools like Pandas, or ideally Spark
	- Experience with test automation
	- Experience accessing data via an API
	- Total control of Git
	- Working knowledge of Linux
	- Deployment tools (i.e. Ansible, Puppet, Chef)
	- Working knowledge of set-based querying (joins, etc...) ideally indexing, too.
	- Hadoop/Spark experience
	- Flume/Gobblin/Kinesis
	- AWS experience
	- Web analytics experience
	- Docker experience
	- Jupyter (notebook) experience
	- Log processing experience
+ skill set:
	- Knowledge of JIRA, Test Rail, Jenkins, Splunk, New Relic, Allure Reports
	- Knowledge of Selenium or TestCafe or Cypress is a bonus
+ skill set:
	- Experience with modern web frameworks, like Django, Flask, or Rails (Ruby)
	- Experience with modern Javascript technologies like Node.js, React, Webpack and ESLint plugins
	- Culture of code reviews and collaborating closely with other people (not a solo hacker)
	- You are familiar with translation management/memory tools.
	- Continuous integration & continuous deployment of localized product experience.
+ skill set:
	- Design fault tolerant systems that can scale, and allow us to move quickly without impacting customer access
	- Build observable systems that track important metrics and automatically notifies when something is off
	- Experience with multi-region data center architecture
	- Experience working with a distributed service-oriented architecture
	- Experience building admin features in SaaS applications
	- Experience with modern security and access control practices
	- Experience with continuous integration (frequent deployments)
	- Experience with GraphQL
	- Material contributions to open source projects
	- Linux (Ubuntu) configuration and administration
+ skill set:
	- Back End: We write lots of micro-services, primarily with Java 8. Our APIs are RESTful and use the minimal Dropwizard framework. We take advantage of Kafka, Spark, Hadoop for processing volumes of data.
	- Front End: Our web applications are complex, single-page apps written in JavaScript (React, ECMAScript 6, Sass).
	- Core Data, Infrastructure, & Reliability: Building the systems that power thousands of services with Singularity on Apache Mesos, and empowering access to massive datasets with HBase, Elastic Search, ZooKeeper, Redis, MySQL, and Memcached.
+ Are language agnostic. We aren't overly concerned with tech stack - if you are interested in learning new things, we're interested in teaching you.
+ Key Performance Indicator (KPI) definition and propagation. Uses statistical inference to determine what KPIs are truly impactful to our customer engagement and adoption efforts. Work cross-functionally to make sure these KPIs are available for all relevant data sources and business units.
+ Refactoring existing C++ libraries for modularity and extensibility.
+ To bring visualization and easy-to-use data analysis tools to our users, our research and development teams work on full data processing cycle: from database's query processing, to data science (AI, machine learning, etc.), UI design, and cloud computing.
+ Integrate our data pipeline with available Augmented Analytics models, tools, and applications.
+ Leverage and advance our query processing engine to build a data pipeline that enables the development of features and integration with available Augmented Analytics models, tools, and applications.
+ TeamCity: the Hassle-Free CI and CD Server by JetBrains
+ skill set:
	- Strong understanding of core Hadoop concepts including Yarn, MapReduce, Hive, Pig, Sqoop, HDFS
	- Experience implementing large scale data loading, manipulation, processing and exploration using a range of AWS based technologies such as Spark, Kinesis, Athena, RedShift, Postgres etc
	- Extensive experience in data profiling, source-target mappings, ETL development, SQL optimisation, testing and implementation
+ Identify scaling bottlenecks and propose solutions.
+ Preferred (but not essential) experience any Big Data technologies such as languages like R, Hadoop, Machine Learning and Data Lakes
+ semi-supervised learning
+ Experience with or exposure to Big Data (Hadoop/YARN, Spark, Nifi, Storm, Cassandra, Solr) is a definite plus.
+ skill set:
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Proficient in Scala, Java and SQL
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ Familiarity with Agile methodologies, such as Scrum or Kanban, as well as software development practices such as Continuous Integration, Test-Driven Development and DevOps.
+ Advanced level programming skills in Python, ideally developed from experience working on long-term commercial projects, including significant experience using SciPy and machine-learning packages (Numpy, Pandas, scikit-learn, etc) for the development of maintained components. Additional experience using PySpark a big plus.
+ skill set:
	- Working experience of Databricks or similar
	- Knowledge of concepts such as data warehouse, star schemas, KPIs
	- Hands on experience with Linux, HDFS, HIVE, HADOOP
	- Experience with ETL / Informatica, JSON structures and REST Based web services a plus
+ Second, help build a Data Science team to support the delivery of the Predictive Analytics core AI offerings for US-based pharmaceutical companies. These offerings include developing algorithms to find undiagnosed patients (often patients with rare diseases) or algorithms to predict patients likely to experience rapid disease progression and / or switch therapies. These core offerings involve very large datasets (typically many millions of patients) extracted from claims and prescription data and use Python and PySpark tools and libraries as part of IQVIA's Hadoop cloud environment.
+ In depth experience with Spark/Hadoop and either Theano/Tensorflow/Caffe/Torch.
+ Here are some of the key technologies used to build our document management platform and its applications.
	- Server: REST, Scala, SQL, Java
	- Client: JavaScript, React, HTML5, Less/CSS, Kendo UI, webpack
	- Database & Search: PostgreSQL, Solr
	- Build Automation: sbt, webpack, Bamboo
	- Tools & Infrastructure: JIRA, Crucible+Fisheye, git, nexus
	- Technical/functional expertise .
+ skill set:
	- ***Competence with industry-standard tools like: Git, npm, JIRA, Docker, etc.***
	- Comfortable with Cloud services, specifically AWS services (in particular: ECS, EC2, S3, ECR) and their respective APIs, is a major plus.
	- Experience building for SaaS/PaaS and distributed applications.
	- Intimate knowledge of web services and building and interacting with REST APIs is essential.
+ skill set:
	- Experience of working with Flask (or similar web framework) to build APIs.
	- Excellent understanding of SQLAlchemy, data modelling, Alembic, Postgres, Celery, Redis.
	- 3 to 5 years prior related experience in developing web applications, graduate experience, or demonstrated success in development with DrugDev Spark or equivalent system
+ skill set:
	- Experience with the [Common Workflow Language (CWL)](https://www.commonwl.org/), [Arvados](https://arvados.org/), PERL, C++, AWS, Docker
	- [Dockstore, developed by the Cancer Genome Collaboratory, is an open platform used by the GA4GH for sharing Docker-based tools described with the Common Workflow Language (CWL), the Workflow Description Language (WDL), or Nextflow (NFL)](https://dockstore.org/)
	- [Nextflow](https://www.nextflow.io/)
	- [Workflow Description Language (WDL)](https://software.broadinstitute.org/wdl/)
	- https://fairsharing.org/bsg-s000606/
	- https://dnastack.com/#/
	- https://curoverse.com/about
+ skill set:
	- Java (Spring Boot, Java EE)
	- Google Cloud Platform
	- Relational Databases (MySQL, PostgreSQL, BigQuery)
	- REST and OpenAPI
	- JavaScript (AngularJS)
	- TypeScript (Angular)
	- 12-factor application model
	- Bash
	- Git
	- WDL (Cromwell)
	- Microservices, Docker, and Kubernetes
	- Continuous deployment
+ skill set:
	- Advanced level programming skills in Python, ideally developed from experience working on long-term commercial projects, including significant experience using SciPy and machine-learning packages (Numpy, Pandas, scikit-learn, etc) for the development of maintained components. Additional experience using PySpark a big plus.
	- Ability to integrate and scale solutions that involve large data sources in SQL databases and/or distributed systems such as Hadoop, as well as considerable experience deploying at scale on cloud technologies such as AWS, GCP, Azure.
	- A set of software-development values that ensures high-quality, readable and maintainable code is produced within an open and collaborative environment.
	- A pragmatic approach in scope and design, seeking simple iterative solutions wherever possible to shorten the time-to-value of work.
	- Knowledge of supervised machine learning methods, such as regularised regressions, ensemble tree classifiers (e.g. xgboost), Support Vector Machines, deep learning, etc.
	- Additional experience developing in C++, R, Java, Scala, Java, JavaScript, or advanced ability user of shell scripting commands (grep, sed, awk, etc).
	- A demonstrable interest (e.g. public GitHub repo, or online course completion) in one of the following machine learning libraries (or equivalents): TensorFlow, Spark MLLib or CRAN packages for machine learning.
	- Knowledge of healthcare / life science issues involving Real-World Evidence.
+ skill set:
	- Good understanding of SQLAlchemy, data modelling, Alembic, Postgres.
	- Understanding of RESTful APIs and web services.
	- Experience of working with Flask (or similar web framework)
	- Previous experience with scrum and Jira.
+ skill set:
	- Experience working in an Agile environment using Test Driven Development (TDD) and Continuous Integration (CI)
	- Experience refactoring code with scale and production in mind.
	- Proficient in Scala, Java and SQL
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Experience with integration of data from multiple data sources
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ skill set:
	- You will join a team of highly talented Engineers and Data Scientists, with your main focus being to write highly performant and scalable code that will run on top of our Big Data platform (Spark/Hive/Impala/Hadoop). As such, you will work closely with the Data Science team to support them in the ETL process (including the cohorts building efforts).
	- Working in a cross-functional team
	- Building scalable and high-performant code
	- Mentoring less experienced colleagues within the team
	- Implementing ETL and Feature Extractions pipelines
	- Monitoring cluster (Spark/Hadoop) performance
	- Working in an Agile Environment
	- Refactoring and moving our current libraries and scripts to Scala/Java
	- Enforcing coding standards and best practices
	- Working in a geographically dispersed team
	- Working in an environment with a significant number of unknowns – both technically and functionally.
	- BSc or MSc in Computer Science or related field
	- Strong analytical and problem-solving skills with personal interest in subjects such as math/statistics, machine learning and AI
	- Solid knowledge of data structures and algorithms
	- Experience working in an Agile environment using Test Driven Development (TDD) and Continuous Integration (CI)
	- Experience refactoring code with scale and production in mind.
	Proficient in Scala, Java and SQL
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Experience with integration of data from multiple data sources
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ skill set:
	- Java, Scala & Python
	- Kubernetes & Docker
	- Data analysis tools such as R
	- We rely on Big Data / Hadoop technologies such as Apache Spark, Impala and Amazon Redshift
	- Participate in Agile practices such as daily stands-ups, sprint planning and retrospectives
+ skill set:
	- Serving in the role of Senior Security Researcher, you will have a direct impact on the direction of the company by researching threats, understanding how they appear on the network, reversing malware and helping technically shape the product direction.
	- Perform leading edge security research – malware analysis, fuzzing, web-based threats, network/protocol analysis, etc. – and generate intelligence which will be incorporated into the product
	- Create and enhance the company's security content framework, including malware intelligence and the process workflow
	- Research new threat detection technologies and investigate approaches
	- Apply your expert insights and experience in classifying new threats and mitigation techniques
	- Collaborate across Vectra to identify new detection models – working hand-in-hand with members of the data science team
	- Pursue security research topics that contribute to the knowledge and enumeration of new threats
	- Provide an attackers-eye-view to the evidence presented by Vectra products and educate customers to the technical nature of the threat
	- 5+ years direct experience in areas of security research, malware analysis, networking/system administration or software development
	- 5+ years of attack and penetration testing experience
	- Advanced technical degree
	- Knowledgeable in exploitation technology such as shellcode, heap spray, ROP, etc.
	- Knowledgeable in network and application protocols, and traffic analysis (network forensics)
	- Proficiency with reverse engineering tools like standard debuggers, IDA pro, etc.
	- Proficiency with network traffic analysis and network forensics tools such as Wireshark and tcpdump
	- Proficiency with host forensics and memory analysis tools related to studying active exploitation
	- Knowledge of corporate security investigation and incident response processes, along with malware detection and mitigation technologies
	- Solid programming skills with scripting languages such as Python
	- Deep working knowledge of networking and network application concepts: TCP/IP, HTTP, TLS, FTP, IRC, RPC, DNS, SMB, Kerberos, etc.
	- Strong problem solving, troubleshooting and analysis skills
	- Excellent written and verbal communication skills
	- Excellent inter-personal and teamwork skills
	- Proactive, hard-working team player with a good sense of humor
	- Self-driven, able to efficiently work remotely without close supervision
	- Professional or academic research in advanced security threats
	- Operational experience in infosec as an incident handler, administrator, or internal consultant
	- Experience with big data technologies such as Hadoop and Spark
	- Participation in the broader infosec community with requisite contacts and access to external intelligence sources
	- Understanding the lifecycle and economics of modern malware and advanced threats
+ skill set:
	- Knowledge of web language abstractions (Babel, Sass, etc.)
	- Experience developing bespoke data visualizations with D3.js
	- Understanding of resource-driven API structure
	- Experience developing complex single-page applications with a UI framework (e.g. Ember, Angular, React, Vue)
+ skill set:
	- Work with product management to help define requirements for new high value features for customers
	- Help break down and plan work for your team
	- Help continue to grow the team through interviewing and hiring
	- Mentor and grow team members both technically and non-technically
	- Participate in architecture discussions and guide the technical direction of the team
	- Ensure smooth delivery of software with automated tests in a modern CI toolchain
+ skill set:
	- Design and implement high-performance libraries/APIs for machine learning and statistical techniques.
	- Experience with streaming and event-based programming
	- Experience with continuous integration and deployment workflows
+ skill set:
	- Node.js or Python, JavaScript, HTML, CSS. Experience with at least one major web framework (django / symfony / express or others)
	- Decent knowledge of modern frontend framework or libraries, preferably React
	- Voracious appetite for learning
	- Experience with Elastic Search or alternative search data technology, managing significant amount of data
	- Advanced experience with AWS (Lambda, DynamoDB, EC2, etc.) with particular focus on scalability.
	- Previous experience with CI/CD, preferably with Jenkins, deploying multiple times a day to production.
	- Experience with CapnProto or BRO logging format is a plus.
+ skill set:
	- Database hands-on experience (MySQL, couchdb, ElasticSearch, etc.)
	- Understand modern web app architecture and be able to develop features in JavaScript as well as implement the Python-based web server APIs that field requests and responses.
	- Excellent collaboration skills are required due to the cross functional communication necessary to help build the product.
	- Experience with Python, JavaScript, and a modern web framework (e.g. Ember, React, etc.)
	- Database hands-on experience (MySQL, couchdb, ElasticSearch, etc.)
	- Experience with Python web framework (e.g. Django)
	- Familiarity with web-based data visualization libraries (e.g. D3.js)
	- Understanding of semantic markup (HTML) and web styling
	- Knowledge of web language abstractions, such as ES6/ES2018, CSS preprocessors, etc.
+ Comfortable with AWS/Azure/On Premiss  deployments, networking, and security best practices.
+ skill set:
	- Database hands-on experience (Elasticsearch and Mongo preferred)
	- Administration of systems including Jenkins, Elasticsearch, Kibana, Mongo, Grafana, etc.
+ skill set:
	- Tensorflow
	- Object oriented programming, C++ or Java
	- Spark or Map Reduce
	- SQL and noSQL database experience
	- Linux
	- Source control experience, preferably GIT
+ skill set:
	- Familiarity with Hadoop, Map/Reduce, Spark, and distributed computing
	- Understanding of data pipeline architectures (e.g. Lambda, Kappa)
	- Database hands-on experience (MySQL, MongoDB, couchdb, ElasticSearch, etc.)
	- Knowledge of real-time data pipelines (e.g. Kafka and Spark Streaming)
	- Experience with continuous integration and deployment workflows
	- Experience with Docker, AWS/Azure/On-Prem deployments, and networking
+ skill set:
	- The candidate must have a sufficient understanding of and practical experience with classic statistical modeling techniques (e.g. logistic regression, CART, K-means clustering) and machine learning algorithms (e.g. gradient boosting, neural networks, random forests)
	- Comfort with ambiguous and large streams of data across different formats and entry points; Hands-on experience working with large data processing (processing large datasets); hands on experience with cloud environments (e.g. AWS, Azure) and Big Data technologies (e.g. Hadoop, Spark)
	- Expert-level Python, R and SQL coding; Experience with TensorFlow or Microsoft Cognitive Tool Kit required
	- Experience developing high value features; Hands-on experience deploying models in real-time environments
+ skill set:
	- NBA's Team Marketing and Business Operations ("TMBO") group is a unique in-house consulting arm within the NBA league office that drives best practice sharing and innovation across all NBA, WNBA, NBA G-League and NBA2K teams.
	- The Data Scientist role will be a technical expert within TMBO in all matters surrounding statistical analysis, data manipulation and interpretation, and process automation. You will be a thought leader, tasked with the responsibility to leverage the NBA's various internal data sources to create new and innovative analytical products and outputs to inform league executives about the state of team businesses. You will uncover insights through predictive analyses and data visualization, drive better decision making by using various statistical modeling techniques, and promote efficiencies in reporting to various stakeholder groups within the league office. The demand for advanced analytical solutions to business operations issues in sports continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.
	- Help lead TMBO's efforts to develop and maintain analytical products for communicating the state of team businesses to key stakeholders at the league office
	- Work with TMBO executives to further proprietary analytical research for presentation to teams at various league workshops
	- Enhance the current report generation processes within TMBO through the lens of potential automation and conversion to different, more useful development environments (e.g., R, Python, Tableau, etc.)
	- Develop an understanding of current needs in data collection at the league office and create solutions to gather the required information
	- Perform statistical analysis and create and maintain descriptive and predictive models as needed on a project basis
	- Demonstrated skills, knowledge and experience in converting data into insights
	- Passion for developing methods to streamline processes and data flow through automation
	- Experience in understanding existing data structures, including collection and standardization processes, and ability to help shape future processes going forward with an eye toward business needs
	- Ability to consult with business analysts and operators to understand their data needs, develop systems to access the data and convert the outputs into various formats for analysis
	- Comfort with ambiguity in data and experience in working with partners to optimize third-party data streams
	- Detail-oriented, extremely organized with ability to manage projects from inception through execution
	- Strong communication skills, both verbal and written, particularly for presentations
	- Expertise in leveraging R and Python to perform statistical analysis, build models and automate processes
	- Expertise in using SQL to tap expansive databases
	- Expertise in creating informative data visualizations through Tableau
	- Familiarity with other coding languages, statistical analysis tools and business intelligence platforms preferred
	- Familiarity with Microsoft Office software required, strong skills in Excel and VBA preferred
	- Expertise in developing, executing and implementing: various classification and regression models and familiarity with more advanced machine learning techniques
	- Expertise in parsing existing code and developing ways to increase its efficiency
	- Familiarity with accessing and manipulating data via APIs
	- Experience in an advanced analytical role, preferably in an industry focused on leveraging data to develop loyal fans or customers
+ skill set:
	- Throughout the year, instructors will work on lecturing the course, curriculum development for new and existing courses, hosting office hours, managing a Teaching Assistant, assist with marketing, and others on an hourly basis.
	- Leading course discussions
	- Providing personalized support to students
	- Managing a Teaching Assistant
	- Hosting office hours
	- Improving the quality of our curriculum
	- Participating in course marketing activities
	- Is a data scientist with outstanding knowledge of and experience in Python, Calculus, Linear Algebra, Probability, Statistics and various Data Science concepts and algorithms which include supervised and unsupervised learning, dimensionality reduction, exploratory data analysis, and programming best practices
	- Is equally passionate about data science and teaching
	- Strong communication skills
	- Preferably has teaching experience alongside data science
	- Approaches problems with a design perspective
+ skill set:
	- Implement and maintain the League's data analytics platform(s) to manage the ongoing monitoring of internal and external information related to Sports Betting/gaming;
	- Coordinate with other departments and third-parties to identify internal and external sources of meaningful information and assist in the process of data collection;
	- Work with and coordinate the integration of multiple data sets into the selected data analytics platform(s) and/or visualization tool(s);
	- Work with technical partners to automate data flows and reduce reliance on manual data development;
	- Develop queries and manage the technical aspects of data interrogation;
	- Maintain a reporting mechanism and protocols to ensure appropriate communication of alerts and/or findings, including through visualization and dashboards;
	- Oversee information requests and project activities to ensure accurate, timely, and efficient reporting deliverables; and
	- Support the investigation and review of any alerts
	- Five+ years of experience in data analytics or a comparable area of expertise
	- Integrity monitoring experience and/or understanding of the Sports Betting/Gaming industry preferred
	- Ability to exercise discretion and use independent judgment in making decisions and work with minimal functional guidance; demonstrated project management skills
	- Excellent oral and written communication skills, ability to share findings with non-technical audience, and deal effectively with the senior management, staff members, and vendors
	- Exceptional problem solving and issue-spotting skills
	- Excellent interpersonal and time management capabilities
	- Experience with data analytics platforms, such as Symantic Pro, Symantic Cortex, IBM i2
	- Understanding of statistical modeling techniques
	- Proficiency in at least one of the following languages: R, Python, SQL query writing, working with JSON/XML data
	- Data transfer and encryption knowledge, such as sftp, pgp, and others; and
	- Knowledge of Cloud platforms, such as AWS, Azure, and others
+ skill set:
	- Openstack
	- Linux kernel namespaces internals
	- Linux kernel KVM internals
	- K8s networking internals
	- OpenVSwitch
	- Workload optimization in HPC / HTC (High Throughput Computing)
	- In depth knowledge of Linux operating systems
	- Networking protocol development experience
	- Storage experience and/ or virtual machine experience
	- Understanding of the operational, maintenance, monitoring and support aspects of a business - critical system.
	- Understanding of underlying hardware in large - scale systems and how to best use it.
	- Kernel virtualization mechanisms
	- Leading software defined storage projects (Gluster, Ceph, Hadoop, etc...)
	- Experienced in working in large bare-metal data centers
+ skill set:
	- Design, deploy and operate cyber technologies in order to solve the security challenges in a technically diverse and complex environment
	- Implement and develop in-house security tools and integrate open-source solutions
	- Perform POCs and remain up-to-date and knowledgeable in regard to the latest security trends and emerging technologies
	- In-depth knowledge of security concepts, architecture and methodologies from end to end perspective
	- Strong background in most of the following topics: SIEM, Firewalls, IPS, NAC, EDR, DLP, OS Hardening, Vulnerability Management
	- Knowledge with security aspects of networking, operating systems (Windows, Linux) and virtualization
	- Coding/scripting capabilities – shell scripts (Python preferred)
	- Self-motivated and an autodidact
	- Team player
	- Familiarity with working in DevOps oriented environment (Jenkins, OpenStack, Salt and Dockers) - Advantage
	- Hold a professional certification in good standing (CISSP, GSEC) - Advantage
+ skill set:
	- Final Israel Ltd. is one of the world's leading high-frequency trading (HFT) companies.
	- We use proprietary prediction and trading algorithms as well as highly innovative schemes for handling large amounts of data. As a major participant in the HFT industry, the challenge Final faces is two-fold: to analyze large and complex data sets off-line, as well as to process massive flows of real-time data.
	- Final's most important asset, its employees, comprises a hand-picked group of extremely talented, highly motivated, and diverse individuals.
	- The position entails research, development and implementation of algorithms in the fields of machine learning, signal processing, data mining and statistics.
+ Strong understanding of the Hadoop ecosystems (especially Kafka, Flume, Avro, Parquet, HBase, Hive, and Hue) and related technologies with +2 years of experience
+ skill set:
	- Mongodb
	- Elasticsearch
+ skill set:
	- Good working knowledge in a wide range of machine learning methods and algorithms for classification, regression, clustering, and others – a must.
	- Good Statistical analysis, e.g. hypothesis testing, estimation theory and mathematical skills.
	- Experience dealing with end to end machine learning projects: data exploration, feature engineering/definition, model building, performance evaluation and help in implementation.
	- The ideal candidate should be able to use his/her experience in implementing advanced analytical methods (machine learning, statistical/mathematical modeling) on large amounts of raw data – dealing with all parts of modeling workflow (from data extraction, feature engineering to model building and implementation) and should be able to clearly present and communicate the findings.
	- As a Data Scientist at Playtika you will take part in projects in which analytical solutions are used for solving and/or optimizing business/product problems like user experience modeling, churn prediction, advanced segmentation and others.
	- Experience working with big data tools e.g. pySpark.
	- DL toolbox (e.g. pyTorch, Tensorflow etc. it a plus).
	- Knowledge in Deep learning models (CNN, RNN, etc …)
	- Reinforcement Learning
+ skill set:
	- Reviewing the ETL Design and Data Roadmap, and continuously improving the processes.
	- Ensuring the ETL Codes are running and constantly improving/following the game features and Business Requirements.
	- Analyzing, developing and defining data integration solutions related to big data storage;
	- Applying data modeling, data design and implementation to support business requirements;
	- Participating in all agile development lifecycle activities: estimating, planning, designing, developing, documenting and testing;
	- A minimum of 3 years of professional experience in Data Transformation (ETL) and systems integration processes.
	- A deep understanding of Data flow, Database and data transformation principles (Big data and object storage concepts knowledge considered a major asset).
	- Knowledge of big data environment is an asset (Kafka, Databricks, Vertica, Others)
+ skill set:
	- Undertake analysis to monitor and report key performance indicators (KPI's)
	- Building out our KPI Dashboards for monitoring game performance
	- Design, run, analyse and report on A/B tests
	- Experience with business intelligence software like Tableau, Looker etc
+ Experience with Hadoop, Vertica, HBase, Spark.
+ Skilled in Spark , JavaScript, Hadoop, Vertica, HBase, Angular, React, Docker, Kubernetes – advantage
+ Experience with relational (MySQL), NoSQL databases (Couchbase/Aerospike), search engines (ElasticSearch), caching solutions;
+ skill set:
	- Experience with ETL process, real time or batch pipelines
	- Experience with Kafka
	- Experience with Vertica
	- Knowledge in containerized environments (Docker, Kubernetes)
+ 3+ years of experience working with agile at scale vs scaling agile solutions such as SAFe, LeSS, Nexus (or others).
+ Experience with Docker, AWS, and CircleCI is a bonus.
+ skill set:
	- Research and design scalable Machine Learning infrastructures with real time applications utilizing TB of data.
	- You will be working with talented and passionate teams and drive the evolution of Big Data and Data Science. You'll work with many exciting technologies such as Spark, Airflow, Hadoop, HP-Vertica, Aerospike, Redis, TensorFlow and Kafka.
	- Direct management of architects team. Cross-functional interaction with a wide range of people and teams, work closely with data engineers and data scientists to ensure high level of professionalism and standards, as well as satisfying project requirements and timelines.
	- Design scalable and reliable data pipelines to move huge amounts of data.
	- Design complex marketing platforms enabling effective player acquisition and retention at scale of millions of daily users.
	- Experienced with current SW development practices – SCRUM, test automation, TDD, CI, CD.
+ skill set:
	- Deep understanding of micro-services architecture, Kubernetes & Docker
	- Deep understanding of designing No-SQL databases (such as: MongoDb, Redis, etc.)
+ skill set:
	- Experience with Docker, Kubernetes, ceph, AWS a plus
	- Comfortable around data stores, MySQL, PostgreSQL and Redis a plus
	- You appreciate a well-designed REST API
	- You calmly triage production issues across microservices and approach the creation of software with a DevOps mindset
+ skill set:
	- You are humble, and play well with other people. You thrive in team settings, and exhibit excellent communication and collaboration skills. For example, offering constructive feedback in code reviews, writing user-centered documentation, and chatting with masterful use of text and emojis in Slack.
	- You thrive on solving problems for your customers through product-mindedness. You are comfortable with a little bit of ambiguity, and enjoy the opportunity to shape the future of the product.
	- You are an experienced "T-shaped" software engineer exhibiting broad knowledge of the entire discipline with deeper specializations in a few areas such as architecture, design, test automation, front-end technologies, etc.
	- You are internally driven by curiosity and continuous learning. You have proven that you can be entrusted with big decisions, and strive to bring understanding and empathy to the entire team.
	- You can function effectively in a distributed team. This means you are reliable, you know when to ask for help, you invest in strong relationships with your colleagues, and you know yourself well enough to be accountable for your own self management.
+ skill set:
	- Many of us feel that we've found our home for the next decade. This motivates us to take a long-term approach to our work: ***writing clean, maintainable code, investing in automated testing, and building in budgets for technical debt***. This also motivates us to value mentorship; we want our most junior engineers to be in a position to mentor others and to independently take on more responsibilities in the coming years. We love it when our engineers take full ownership of a feature and drive it from design through validation.
	- Familiarity with some enterprise user authentication technology (such as LDAP, SAML, Kerberos)
	- Experience building, shipping, and maintaining the back-end of a web-accessible product
	- Experience working with compiled programing languages (e.g. Go, Rust, C++, C# or Java) and SQL databases (any)
	- Experience building and shipping on-premise enterprise software
	- Experience with information security -- either via certifications (CISSP, CEH, etc.) or professional experience (security audit, pentesting, etc.)
+ skill set:
	- Have previous experience applying machine learning, and (big) data analytics frameworks such as TensorFlow, Scikit-learn, and the Apache Hadoop ecosystem, to real world problems.
	- Are well versed in the Linux operating system. Experience with cloud technology, and containers like Docker or Kubernetes, are highly desirable.
+ skill set:
	- software engineering
	- big data analytics framework
	- product innovation
	- services and applications
	- communicating with industrial and academic experts
	- machine learning
	- statistics
	- interested in personalized medicine
	- strong interest in industry projects
	- data mining
+ Knowledge of Apache Spark
+ skill set:
	- We are looking for multidisciplinary machine learners with sharp skills in one or more of the following fields:
		* Deep Learning,
		* Bayesian Modeling,
		* Natural Language Processing.
+ skill set:
	- Ursa Labs's work is focused on the Apache Arrow open source project, a cross-language development platform for in-memory analytics. Within the Arrow project, most of our time is spent on the C++, Python, and R libraries. It is our goal to make everyday tools for data access, cleaning, wrangling, analytics, and visualization a great deal more powerful and interoperable than they are now.
	- We are looking for an experienced engineer to take a lead role in the build and testing systems of the Apache Arrow project, including automated building (continuous integration), testing, and benchmarking on a range of architectures and operating systems. The ideal candidate should have experience playing a significant role in shipping software with high technical complexity.
	- Help maintain the Apache Arrow open source project: code review, documentation, and mentor junior contributors.
+ skill set:
	- Experience in developing and maintaining SQL databases and writing SQL queries; NoSQL (MongoDB) experience preferred but not required
	- Experience establishing and maintaining AWS, Google Cloud, Heroku, Tableau or similar cloud-based environments
	- Background in extracting and storing data from APIs preferred but not required
	- Knowledge of HTML, CSS and JS and lightweight web frameworks such as Django, Flask
+ System-level functional and performance debugging (perf, gdb, valgrind)
+ skill set:
	- Modern databases like Cassandra, ElasticSearch, Redis, and Postgres.
	- Frameworks like Django, Tornado and the PyData stack (e.g. Pandas).
	- Running Kafka, Storm, Spark in production atop massive data sets.
	- Easy system management with Fabric and Chef.
	- Parse.ly's data engineering team already makes use of modern technologies like Python, Storm, Spark, Kafka, and Elasticsearch to analyze large datasets.
+ Experience in the technologies we use is helpful but not required. They are: Go for core infrastructure; ObjC, Java and C# for native UI development on iOS, OSX, Android and Windows; Node.js and IcedCoffeeScript for Web development; FUSE for client file systems; MySQL/InnoDB, DynamoDB, S3 and EC2 for hosting.
+ skill set:
	- Experience with build infrastructure (e.g. Jenkins), and build tools (e.g. Maven, Ant, Make, CMake, etc)
	- In-depth knowledge of version control systems
	- Infrastructure as code mindset
	- Experience with infrastructure design and operations (monitoring, alerting, BDR)
	- Experience with provisioning clusters on Amazon EC2, and/or Google Compute
	- Experience with private/public clouds, virtualization technologies (VMWare, KVM, or similar) and container technologies (Docker, Kubernetes)
	- Familiarity with concepts of InfoSec/AppSec
+ skill set:
	- Advanced modeling and data visualization skills in Tableau Desktop
	- Advanced Python coding talent including familiarity with common and emerging data science and scientific computing toolsets/libraries (matplotlib, pandas, scipy, numpy, et al)
	- Savvy with collaborative data workbench tools and environments like Cloudera Data Science Workbench, Jupyter, Shiny, RStudio
	- Data manipulation and ETL skill in Talend or a similar tool
	- Strong interpersonal skills, including the knack for translating customer business needs and user stories into requirements and real-world outcomes
	- Relevant education or experience in Computational Statistics
	- An autodidact who keeps on learning
	- Expert-level Excel | Google Sheets skills with scripting
	- Understanding of ITIL concepts
	- Experience with an RDBMS (Oracle, MySQL, Teradata, etc)
	- Experience with containerization tools (Kubernetes, Docker)
+ 3+ years of writing experience or experience in the technical field of security. Knowledge of Hadoop security concepts, especially that of Apache Ranger.
+ skill set:
	- Impala is a scalable and fast SQL query engine that runs directly on Hadoop and supports a wide range of analytic workloads, from subsecond interactive dashboards with hundreds of concurrent users to petabyte-scale rich data exploration.  Impala is currently supported by a large paying customer base, and is a key component of the Cloudera data platform. It is open sourced under the Apache 2.0 license and is developed publicly, including code reviews and bug tracking (see http://impala.apache.org/). It is distributed by Cloudera, MapR, AWS and Oracle among others.
	- Feel free to read our CIDR Paper on Impala: http://bit.ly/cidr15impala or http://pandis.net/resources/cidr15impala.pdf.
	- Knowledge of database concepts, RDBMS internals, and SQL is a strong plus
	- Knowledge of the Hadoop space, containers, or Kubernetes is a strong plus.
+ skill set:
	- Cloudera is looking for a highly experienced software engineer with strong expertise in Java development and specialty in platform architecture to join the Data In Motion - Flow Management team focusing on [NiFi](https://nifi.apache.org/)
		* ***Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic.***
	- Lead architecture, design, and implementation of a platform for managing usage of components
	- Collaborate with other Data In Motion component teams to develop highly integrated solutions
	- Work closely with UX and front end teams
	- Interact with product teams to help define roadmap and shape technology
	- Work closely with open source projects in Apache NiFi and other ecosystem projects
+ skill set:
	- Use your understanding of the Hadoop eco-system to build reliable and scalable automated systems for validating Cloudera products.
	- Work with our world-class development team to define new tools and tooling features that assist in triaging and debugging test failures.
	- Work with cross component teams and be an effective team player.
	- Experience developing in a containerized, Kubernetes environment
	- Experience with Kibana, Elastic Search, Ansible, and Helm a plus
	- Prefer familiarity with large-scale distributed systems and/or data management systems
	- Experience working with open source automation tools and familiarity with Git, Maven, Gerrit a plus
	- Experience with Apache Hadoop and its related technologies a big plus
+ [CUDA LLVM Compiler](https://developer.nvidia.com/cuda-llvm-compiler)
+ After CUDA, learn OpenACC before OpenCL.
+ Good experiences with technologies used by GitBook: Go, Google Cloud Platform, Algolia, Firebase
+ If you work on any of these topics, publish in relevant conferences (CVPR, ICCV, ECCV, ACCV, NIPS, ICML, BMVC, WACV, ICIP, ICASSP, ICPR, EUSIPCO, MLSP, ICME, ACM Multimedia) and/or journals (IEEE PAMI, TIP, SMC, CSVT, TNNLS, TMM, Pattern Recognition, Information Sciences, Neurocomputing, Neural Networks, Image and Vision Computing, IJCV, ACM Transactions on Intelligent Systems and Technology, ACM Computing Surveys) and you want to get relevant regular news or post your own news, you can subscribe as shown above.
	- Ioannis Pitas
		* Professor, PhD, Director of the Artificial Intelligence and Information Analysis Lab, AUTH, Greece
		* IEEE fellow and Distinguished Lecturer
+ Accessibility Analyst
	- You will have working knowledge of HTML, CSS and the concepts of digital accessibility (i.e., WCAG2.1 to AA); an up-to-date knowledge on the range of assistive technologies used by individuals with disabilities; a flexible and team oriented approach.
	- Work includes user acceptance testing, training, design and development consultancy, and development of materials alongside the Accessibility Team and overseen by the Accessibility Product Manager.
	- Hands on accessibility tester with experience of quality assurance, testing, access needs and inclusion.
	- Strong experience with assessing compliance to WCAG2.1 (or 2.0) to level AA.
	- Understanding of digital service design and product development lifecycles.
	- Use of common web-technologies including CSS, HTML and Javascript.
	- Use of accessibility testing tools to resolve issues and potential fixes.
	- Use of accessibility tools like NVDA, JAWS, Switch Access, and/or ZoomText to enable assessments.
	- General awareness of technical, legal and compliance issues related to accessibility Day-to-Day responsibilities.
	- Undertaking a brief initial assessment of a product to resolve an appropriate accessibility test approach.
	- Briefly describing the plan, including what will be covered and any technical dependencies
	- Assessing products against at least WCAG2.1 level AA using a range of methods, tools and approaches.
	- Producing simple reports highlighting issues around accessibility and the impacts.
	- Working with various QA and vendor/partner teams to help with fixes and re-testing.
	- Working knowledge of HTML, CSS and the concepts of web/mobile accessibility.
	- Current knowledge on the range of assistive technologies individuals with disabilities use day-to-day.
	- Experience with training small and/or large groups of people (in person and/or remotely).
	- Ability to work independently and as part of a team.
	- Self-motivated and dedicated with use of good judgment.
	- Confident individual who can work across different media with accessibility.
	- Ability to lead diverse workload.
	- Strong problem-solving skills.
	- Excellent time management and social skills.
	- Demonstrate effective communication skills
+ skill set:
	- IBM Research Lab in Mulhuddart, Dublin, Ireland.
	- AI for Health and Social Care team
	- state-of-the-art in AI and Healthcare Informatics.
+ skill set:
	- Experience with NoSQL databases (eg MongoDB, Redis) is an advantage
	- An advantage: experience with Docker / Rancher or Kubernetes and with Native Mobile Development or NativeScript / ReactNative knowledge
+ skill set:
	- Scale is a rapidly growing post-Series B startup. Our mission is to accelerate the development of AI applications. Our first product is a suite of APIs that allow AI teams to generate high-quality ground truth data. Our customers include Alphabet (Google), Zoox, Lyft, Pinterest, Airbnb, nuTonomy, and many more, and we've become an industry standard for the self-driving car market.
	- https://scale.ai/careers/444bb0b1-932f-4065-a3bc-5a45004f5440
	- In this role, you will apply statistical models, design and interpret experiments, build mission-critical dashboards, and help structure and order our data in the pursuit of transparency over how we operate and how we can improve.
	- Build machine-learning models that power core operations, such as quality assessment and fraud detection
	- Expert knowledge of a scientific computing language (*e.g. *R, Python) and SQL
	- Strong knowledge of statistics (clustering, regression, etc.) and experimental design
	- Comfort setting up and using BI tools
	- Experience with ETL tools and building / maintaining a data warehouse
+ skill set:
	- https://scale.ai/careers/5d709886-b586-44c7-b112-4e04501a4ca0
	- Create optimized and efficient tooling, like [Guided Automatic Segmentation](https://scale.ai/blog/automatic-segmentation), for taskers to complete complex tasks with speed and accuracy.
	- Reliably evaluate data quality at scale.
	- Intelligently route tasks from customers to specialized taskers for low turnaround and high accuracy.
	- Automatically hire, train and onboard taskers.
	- Deep Learning: building CNNs.
	- Classical Machine Learning: non-deep learning methods (random forests, collaborative filtering, HMMs, etc.)
	- Applied ML Engineering: building large-scale data and machine-learning pipelines.
	- Experience with TensorFlow and/or Pytorch.
+ skill set:
	- Expert-level familiarity with reporting and visualization platforms (e.g. Tableau, Mode)
	- SQL expertise beyond querying: query optimization, schema design, and ETL maintenance
+ Proficiency in UI design tools (e.g., Figma, Sketch) and prototyping tools (e.g., Principle, Origami Studio)
+ Interface with global power semiconductor suppliers to develop solutions (IGBTs, FETs, Diodes, SiC devices, GaN devices)
+ Development software by ensure Alige/ASPICE development process
+ skill set:
	- Spring framework (Spring Boot, Spring Cloud)
	- NoSQL and relational databases (MongoDB, Postgres)
	- Be a team player and open for new technologies and challenges
	- Gradle
	- Unix/Linux
	- Docker
	- Terraform
	- AWS
	- Microservices
	- Frontend development (JS, React, CSS)
+ skill set:
	- Backend technologies such as Java (min 8)
	- Frontend technologies such as React JS
	- Building a CI/CD pipeline
	- Technologies we also use: Git, Jenkins, AWS, Terraform
+ [OpenLabNotes – An Electronic Laboratory Notebook Extension for OpenLabFramework](https://doi.org/10.1515/jib-2015-274)
	- Electronic laboratory notebooks (ELNs)
+ Kubernetes, the open source system for managing containerized applications across multiple hosts
+ skill set:
	- Stitch is a cloud data ingestion platform... We power data infrastructure for thousands of businesses.
	- Stitch is made up of these main components:
		* Source integrations - lots of them! - that stream data out of SaaS applications, databases, and filesystems
		* A high-volume data pipeline, moving hundreds of billions of records per month
		* Destination loaders that efficiently import data into cloud systems like Redshift, Snowflake, and BigQuery
		* A mid-tier and UI that tie it all together
	- Our backend services are written in Clojure and Python, and our front-end is written in React. We use MySQL and PostgreSQL to manage the state of our system. The entire system runs on Amazon Web Services (EC2, RDS, ELB, VPC, and more).
	- [Simple, Composable, Open Source ETL; Singer powers data extraction and consolidation for all of your organization's tools.](https://www.singer.io/)
	- https://github.com/singer-io
	- Destination loaders that efficiently import data into cloud systems like Redshift, Snowflake, and BigQuery
	- Our backend services are written in Clojure and Python, and our front-end is written in React. We use MySQL and PostgreSQL to manage the state of our system. The entire system runs on Amazon Web Services (EC2, RDS, ELB, VPC, and more).
+ Experience working with Golang, Ruby, Docker, Sinatra, Rails, Postgres, MongoDB or Redshift.
+ skill set:
	- Experience with a Data Warehouse (Redshift, Snowflake, etc) and data analysis using SQL
	- Experience with ETL and Data Integration Platforms like Mulesoft, SnapLogic and Airflow
	- Experience in at least one programming language (e.g. R, Python, Java, Ruby, Scala/Spark, or Go)
	- Understanding of data modeling, performance analysis and production DB migrations
	- Familiar with SFDC, Netsuite and Workday data models
	- Develop pub/sub, streaming and batch data loads into our Data warehouse (Snowflake)
	- Design scalable integrations with our product management teams that move key business data between our big rocks (SFDC, Workday, Netsuite, Internal Systems)
	- Experience with Looker, Tableau or other business intelligence platform
	- Experience with platform data like SFDC, NetSuite, Workday
+ Own and optimize P&L and adoption KPIs for the business.
+ skill set:
	- Familiarity with ETL/ELT and related techniques
	- Exposure to CI / CD (with either Docker, Kubernetes, SaltStack or Jenkins)
	- Prior experience in adtech or martech
+ skill set:
	- You've worked with: Kafka, Cassandra, Hadoop, Hive, Spark, or similar technologies
	- Knowledge of Machine Learning, Distributed Systems or Big Data.
	- Exposure to CI / CD (with either Docker, Kubernetes, SaltStack or Jenkins)
+ skill set:
	- Take research from an initial idea all the way to a merged PR in our open source libraries.
	- Speak to the Rasa community (developers who use our libraries) to help prioritize our research roadmap, and assess the impact of the research we've shipped.
	- Design and conduct experiments that bring us closer to our vision.
	- Collaborate on research papers.
+ skill set:
	- You are an expert user of tools like Sketch, Figma, etc.
	- You have demonstrable experience building new products (please show us examples!). You can iterate quickly by putting an MVP in people's hands, and test ideas and hypotheses before any code is written.
+ Comfortable with most of the following: linux, python, docker, kubernetes
+ skill set:
	- Software Development at Rasa is not only about writing Code. You'll have to come up with good architectural designs, quality code, and break an ambitious long-term vision down into milestones and issues.
	- We don't draw a hard line between our engineering and research teams, we all work on the same stack and share work, knowledge, and tools. A lot of the code we create is open source and used by a large community of developers. The driver for our development efforts is this: what would help developers build great conversational software? What can we enable them to build that they couldn't do currently?
	- We do fundamental machine learning research, and we ship commercial quality software that puts it to use. We mostly work in Python, but dip into other languages when it makes sense to. Because engineering is so close to research, you'll quickly learn a lot about Machine Learning, Model Management, Data Analysis workflows, and what it takes to ship machine learning applications into production.
+ GitBook
+ Mixed-Signal Circuit Techniques for Near-Sensor Machine Learning and Data Analysis
	- Mixed-signal interfaces are the essential bridges between the physical world and the digital information processing backbone. In recent years, innovation in such interfaces has been increasingly fueled by application-level insight and the data-driven nature of modern systems. As a result, the traditional building block boundaries are blurring, and the extraction of information occurs through symbiotic interplay between analog and digital signal processing. In this talk, I will illustrate this trend using examples of small-scale machine learning and data analysis functions that operate at the physical interface. Specific examples include mixed-signal feature extraction circuits and compute fabric for machine learning inference, as well as data-compressive interfaces for high-dimensional sensor inputs.
	- Boris Murmann is a Professor of Electrical Engineering at Stanford University. He joined Stanford in 2004 after completing his Ph.D. degree in electrical engineering at the University of California, Berkeley in 2003. From 1994 to 1997, he was with Neutron Microelectronics, Germany, where he developed low-power and smart-power ASICs in automotive CMOS technology. Since 2004, he has worked as a consultant with numerous Silicon Valley companies. Dr. Murmann's research interests are in mixed-signal integrated circuit design, with special emphasis on sensor interfaces, data converters and custom circuits for embedded machine learning. In 2008, he was a co-recipient of the Best Student Paper Award at the VLSI Circuits Symposium and a recipient of the Best Invited Paper Award at the IEEE Custom Integrated Circuits Conference (CICC). He received the Agilent Early Career Professor Award in 2009 and the Friedrich Wilhelm Bessel Research Award in 2012. He has served as an Associate Editor of the IEEE Journal of Solid-State Circuits, an AdCom member and Distinguished Lecturer of the IEEE Solid-State Circuits Society, as well as the Data Converter Subcommittee Chair and the Technical Program Chair of the IEEE International Solid-State Circuits Conference (ISSCC). He is the founding faculty co-director of the Stanford SystemX Alliance and the faculty director of Stanford's System Prototyping Facility (SPF). He is a Fellow of the IEEE.
+ Expert level knowledge of hyperscaler SDN environments. Operational experience strongly preferred.
+ skill set:
	- Interface with design team to ensure DFT design rules and guidelines are met.
	- Understanding of different fault models such as stuck-at, transition and path-delay fault models.
	- Understanding of JTAG, Boundary Scan, IJTAG, MBIST architecture and Mentor MBIST&Repair methodology
	- Understanding of Scan Compresssion & ATPG process
	- Understanding of timing and related tools.
	- Ability to analyze MBIST, ATPG DRC/Simulation failures.
	- Ability to analyze DFT timing violation
	- Scripting skills in perl/tcl
	- Experience in Silicon MBIST/ATPG pattern Bring up and Silicon Failure Analysis
	- Prefer to have experience in system failure analysis
	- Design Implementation CAD tools:
	- Synopsys DC & DFT compiler, DFT MAX, Tetramax, Mentor (LogicVision) JTAG/MBIST/Memory Repair.
	- Simulation tool: synopsys VCS & Cadence NC-Verilog.
	- Waveform debug: both novas and simvision.
	- Timing:  primeTime & cadence tempus
	- Silicon bring up:
	- Understand ATE ATPG test pattern format: STIL (WGL)
	- Understand JTAG SVF format.
	- Experience in Mentor LogicVison CPUBIST bring up.
+ Knowledge of multi-domain clock synchronization and high-speed serial interfaces
+ Strong understanding of noise, EM/IR, process variation, and low voltage design techniques
+ skill set:
	- Juniper's Technical leader at the site for end to end manufacturing for sustaining, NPI and repair responsible for all ODM (Own Design Manufacturers) factories at Taiwan.
	- ODM's escalation/contact point for all technical issues. Provide direction and advice next course of action.
	- Juniper team's escalation point for any technical related issues with the factory.  Drive closure for team.
	- Approve all deviation request from factory to Juniper and approve all deviation implementation from Juniper into factory.
	- Lead and co-ordinate Line Stop and Ship Stop at the factory by pulling in the right Juniper team to support while directing the ODMs actions at the factory.
	- Drive root cause analysis for DOA(Dead on arrival)/Field failure related to factory/manufacturing.
	- Quarterly KPI planning and goal setting with ODM to drive continuous improvement.
	- Monthly rolling audit and reporting.
	- Quarter budget planning for Capital expense and Operating expense.
	- ODM NPI(New product introduction) readiness assessment to support new awards and drive action plan.
	- CEM/ODM technology roadmap planning and capability gap assessment based on Juniper product/technology roadmap.
	- Drive DFM(Design for manufacturing)/DFA(Design for Assembly) closure with the Juniper NPI/product team.
	- Scorecard coordination with respective cross functional team and finalizing the score for the site.
	- Define and check/buyoff all critical reports from CEM/ODM to Juniper.
	- Drive process improvements required by Juniper related to manufacturing process and test, packaging, labelling, data collection
	- Initiate ECO(Engineering Change Notice) and deviations for product changes impacting the site products involving shippable images, pack-out and labelling.
	- Monitor capacity and proactively drive changes as required.
+ skill set:
	- Perform channel margin analysis to provide design tradeoffs amongst package, board, connector. Develop SerDes channel simulation models and correlate to test structures. Correlate TX and RX SerDes simulation models with measurements and work with SerDes vendors to improve model accuracy.
	- Proficient with lab equipment such as oscilloscopes, Vector Network Analyzers, Time Domain Reflectometer, Spectrum Analyzers, phase noise analyzers.  Good lab debug skills a plus.
	- Perform SI DVT measurements on boards and correlate simulations with DVT measurements.  Document SI DVT measurements and correlation to simulations.
+ skill set:
	- knowledge of software for creating interactive prototypes (Framer, Invision);
	- experience in using visual mapping tools like Plectica;
	- experience in type design and editorial design;
	- talent or predisposition for vector and / or raster illustration.
	- experience in the design and implementation of user interfaces and experience;
	- excellent knowledge of vector graphics software such as Adobe Illustrator and Sketch.
+ Nice to Know: React, React native, Ruby/Rails, GraphQL, apollo
+ skill set:
	- Write APIs that handle millions of calls every day. Work with modern backend technologies like Node.js, GraphQL, Redis, and Elasticsearch.
	- Build completely serverless systems on top of AWS services like Lambda, Kinesis, and Api Gateway. At Bustle, we are always trying out the latest and greatest that AWS and competitors have to offer.
	- Write clean, functional, reusable, and testable "ES Next" code through tooling with Babel and TypeScript.
	- Eager to improve the areas in which you live and work. (Code, Processes, Communication)
	- Comfortable with cloud infrastructure (Google Cloud or AWS)
+ skills set:
	- ***security of machine learning***
	- ***robustness and reliability of machine learning technologies***
+ skill set:
	- Redesigning our data systems from a warehouse-centric to a lake-centric architecture to be more cost-effective as our data volume grows.
	- Rolling out a structured events pipeline for analytics and productization of high-volume event streams.
	- Building a centralized, discoverable data catalog to enable users across the company to efficiently find and use data.
	- We currently support a 100TB data warehouse used by hundreds of people to make mission-critical decisions for our products and business.
	- An experienced data engineer. You have several years of experience operating large-scale distributed data processing systems—advanced ETL pipelines and data lakes or warehouses. Experience with Spark and AWS data services (EMR, Glue, Redshift) is preferred.
+ Use of online tools such as Userzoom, usertesting.com and Optimal Workshop a plus
+ Experience working with financial planning systems (Board, Hyperion, SAP, etc.)
+ Experience automating reporting with data in JIRA and other similar tools
+ Familiar with working in a SOC/SOX-compliant environment and how to design/ follow procedures as well as write documentation to maintain compliance
+ https://www.glassdoor.co.uk/Jobs/Google-vlsi-design-engineer-San-Jose-Jobs-EI_IE9079.0,6_KO7,27_IL.28,36_IC1147436.htm?countryRedirect=true
+ You enjoy building UIs with modern tools like ES6, React, Mobx, Webpack, etc..
+ skill set:
	- Porting libc and libc++
	- Porting an open-source BIOS
	- Writing a micro-kernel similar to L4
	- Modifying and/or writing logic generators in C++ for hardware functional units
	- Writing compiler toolchain code, including working on our own code generator back end and modifying LLVM to support some unusual capabilities of the Mill such as quad precision, overflow detection and decimal floating point
	- Writing test sequence generators in C++ for individual operations
+ skill set:
	- Use machine learning methods (e.g., cluster analysis, decision trees, random forest, neural networks, logistics regression) to model and predict research outcomes
	- Use advanced mathematical techniques (correlation, regression, time series analysis, analysis of variance, etc.) to forecast outcomes
	- Identify appropriate methods to conduct analyses including simulation methods (e.g., bootstrap, monte carlo, bagging methods), machine learning, and statistical analyses
	- Develop data visualizations to capture trends and summarize data
	- Create reports of data metrics, trends, outliers, etc.
	- Hands on experience with R and/or Python to manipulate and transform data
	- Ability to communicate through graphical representation/visualizations, reports, algorithms, models, and dashboards
	- Process and prepare both structured and unstructured data sets to ensure successful modeling further downstream of the insight generation and modeling process
	- Hands on experience with R, SAS and/or Python to manipulate and transform data
	- Experience working with databases (Teradata, Oracle, SQL and NoSQL dbs) and interpreting data; experience with feature engineering and data wrangling of both unstructured and structured data sets;
	- Exposure to data visualization tools and techniques (matplotlib, ggplot or Tableau);
	- Experience with business case analysis (problem identification, quantitative modeling and problem solving)
+ Complex routing and switching solutions ( using Juniper, Cisco, Arista) utilizing BGP, OSPF, ISIS VxLan , EVPN
+ skill set:
	- cPanel, Apache, Nginx, CEPH, Virtualization (KVM)
	- PHP Configuration, MySQL Configuration (Postgres a plus), WordPress Configuration,
	- Exim, Zabbix, OpenStack, Content Delivery Networks, DNS Management (PowerDNS),
	- Hardware Configuration, Network Configuration, Linux Kernel Configuration, Malware
	- Detection/Mitigation, FTP, Hardware & Software RAID (LSI, 3Ware, MDADM), & backup technologies at scale
+ skill set:
	- Ability to leverage structured formats like JSON, YAML, XML to build scalable, testable RESTful applications
	- Experience with NoSQL technologies like CouchDB, Redis
+ Experience with testing frameworks: Selenium Webdriver, Rspec, Cucumber, Jasmine, etc.
+ Experience with Rspec, Jenkins, Maven
+ skill set:
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
	- Extensive experience developing big data, business intelligence, marketing automation and/or other analytics infrastructure or pipelines - data lake experience preferred
	- 10 years experience in developing and architecting solutions using big data, data warehousing
	- 3 ++ years hands-on experience developing data lake solutions using Amazon's AWS (certification preferred)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Working experience and detailed knowledge in Java, JavaScript, or Python
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
+ Stata for statistical analysis
+ skill set for Deep Learning Research Engineer at Crossing Minds:
	- San Francisco, CA.
	- At Crossing Minds we are building a future where AI-powered software helps maximize human happiness. Currently, we are creating the continuous recommendation experience that provides personalized recommendations for all the things you love but haven't discovered yet.
	- Crossing Minds incubates artificial intelligence products and services, enriching the human experience using deep learning. Our first product, Hai, is a universal recommendation engine that maps taste across categories (music, movies, games, etc.) to help find more of what you love. Through our website, app, and messenger interaction, Hai syncs with other platforms to serve as a cultural home and launchpad for personal exploration.
	- Sponsored by Nvidia, GCP and Stanford's StartX program, we're looking for a deep learning expert to help our product understand tastes and help our growing team develop the heart of AI.
	- Collaborate with our agile team of researchers and engineers.
	- Design and implement efficient systems to understand users textual queries in real time.
	- Elaborate algorithms to extract relevant informations from text reviews or descriptions.
	- Elaborate algorithms to extract relevant informations from images data.
	- You want to join a small team of hard-workers, to build an awesome product that'll help everyone you know.
	- You have a PhD in Deep Learning, with focus on Computer Vision and/or Natural Language Processing.
	- You have strong knowledge on Computer Science, Computational Learning and HPC fundamentals: algorithms, systems, linear algebra, numerical methods, statistics.
	- You have at least two years of experience in Theano, TensorFlow or PyTorch.
	- You used CUDA on clusters of GPUs professionally.
	- Python, Machine Learning, Natural Language Processing, Deep Learning, theano, Convolutional Neural Networks, Machine Learning Data Science Python
	- Compensation: $140k – $170k, 0.0% – 1.0% of stock options.
+ skill set:
	- Familiarity with NOSQL storage (MongoDB, Redis, Elastic, etc.)
	- A strong background in relational database theory and excellent knowledge of Relational Databases (Postgres, MySQL, SQL server, Oracle)
+ Working knowledge of ROS + Gazebo. Experience with OMPL / PCL / OpenCV is a plus.
+ skill set:
	- We will prefer candidates who have experience working hands-on with physical systems like robots. Example: manipulation of a robotic arm, ground robots, self-driving cars, quad rotor or any other mobile robot.
	- High proficiency in C. Experience with C++/Python is a bonus.
	- Experience working hands-on with micro-controllers like STM32 / NXP is a big bonus.
	- Experience with RTOS.
	- Rock solid software engineering foundation and a commitment to writing clean, documented and well architected code.
	- B.S in Computer Science, Electrical, Robotics or a related field. > 2 years prior professional experience developing firmware for embedded systems.
+ skill set for: Perception and Localization - Autonomy Software Engineer
	- M.S & 4+ years of experience or Ph.D. in Computer Science, Electrical, Mechanical, Aerospace, Robotics or a related field. Ph.D.candidates will be given preference.
	- Experience working hands-on with physical robots like quadcopters, AGVs, UGVs etc.
	- High proficiency in C++.
	- Rock solid software engineering foundation and a commitment to writing clean, documented and well-architected code
	- Strong fundamental understanding of Kalman Filtering, SLAM, particle filtering, and other estimation and sensor fusion techniques.
	- Strong math fundamentals in large scale optimization, dynamic programming, non-linear optimization, linear algebra etc.
	- Competency in ROS/ OMPL / PCL / OpenCV is a plus.
+ skill set:
	- Experience with Jira or any ticketing management systems
	- Understanding of AWS-VPC and basics of TCP and Routing.
	- Experience with large-scale distributed systems
+ skill set:
	- Experience in defining and implementing MDM data models, matching rules, Duplicate Match Review processing, Match-Merge Rules, Workflow Configuration, complex hierarchy relationship management and data governance.
	- System endurance improvement through root cause analysis (RCA), fixes and deployment support
+ skill set:
	- Experience in Monitoring tools like Nagios, New Relic, and Splunk
	- Strong understanding of DNS, DHCP, NTP, SMTP, TCP/IP, SSH, HTTPS, TLS, IPSec, concepts of VPN and other internet protocols
	- Experience in Application and Database level Monitoring and Troubleshooting (like Apache, Tomcat, and MySQL)
	- Experience in Application & Infrastructure Monitoring (Nagios, New Relic, Datadog, Splunk, Sumologic, etc...)
+ skill set:
	- Advanced understanding of OOPS
	- Knowledge of SDL (Software development Lifecycle)
	- Experience of working in Agile environment
	- Good at Code review and design review
	- Ability to design for scale and Performance
	- Solid problem solving skills
	- Experience with hands-on programming
	- Expertise in data structures and algorithms
	- A background in Engineering with sound oral and written communication skills
	- Degree in Computer Science or equivalent practical experience
	- Experience with large-scale systems
	- Intermediate knowledge of Ruby on Rails
	- Prior experience with AWS
	- Experience with open-source projects
	- Experience troubleshooting in a SaaS environment with an assertive deployment schedule
	- Experience in leading engineering team
+ skill set:
	-  5+ years of experience engineering and managing MySQL or PostgreSQL database server for high traffic applications
	- Deep expertise in designing and maintaining MySQL or PostgreSQL
	- Experience in Distributed Systems engineering, Linux performance, memory management, I/O tuning, cluster management, sharding, Data durability, security, networking, and system crash analysis
	- Experience in setting up and operating Highly Available MySQ
	- Experience in debugging time critical DB problems like deadlock analysis and transaction locking
	- Experience in engineering solutions for data backup and encryption
	- Expertise in Database internals and ability to optimize and tune DB engines
	- Experience in working with Percona and innoDB engine
	- Experience in debugging time critical DB problems like deadlock analysis, transaction locking etc
	- Familiar with Agile SDLC
	- Good communication skills and ability to work with remote teams.
	- Ability to program in Bash shell scripts, Python, C/C++ and SQL.
	- Self motivated engineer who can solve problems independently
	- Ruby programming is an added advantage
	- Experience with AWS is an added advantage.
	- Exposure to different MySQL HA technologies like XtraDB cluster is an added advantage
	- Exposure to MySQL native plugin development is an added advantage
	- Build innovative monitoring systems that monitors the Computing infrastructure and the DB engine
+ skill set:
	- Experience troubleshooting in a SaaS environment with an assertive deployment schedule
	- Experience building large-scale performant services
+ skill set:
	- Experience with complex data flow/analytics infrastructure, e.g. Kakfa, Kinesis, Redshift
	- Experience stopping bad actors in a system, e.g. ad fraud, e-commerce fraud
	- A working understanding of fraud vectors, both small scale (client side hacks) and large scale (botnets)
	- Experience with software development via distributed development teams
	- Have built/trained statistical / machine learning models at scale to solve real world problems
	- Experience with ad-tech / marketing tech ecosystem
	- Familiarity with the world of cryptocurrency, especially Ethereum
+ skill set:
	- Experience with Go, JavaScript/TypeScript
	- Experience with SQL, Postgres, MongoDB, Redis and other related data storage technology
	- Experience building robust API endpoints
	- Experience with AWS and deploying production systems at scale
	- Experiencing deploying monitoring solutions like Prometheus, Zabbix, Pingdom, etc
	- A 24x7 uptime mindset and the willingness to manage and be part of pager duty rotation
	- Experience with software development via distributed development teams
	- Comfortable working in an open source setting
	- A passion for helping protect users' privacy and security
	- Written and verbal communication skills in English
	- Proven record of getting things done
	- Experience with Rust
	- Experience with Ruby on Rails
	- Experience with payment systems
	- Familiarity with the world of cryptocurrency, especially Ethereum
+ skill set:
	- Excellent skills in AWS, MapReduce
	- Experience in machine learning, quantitative analysis
	- Experience in knowledge graphs, statistical relational learning, trends-driven analysis
	- Experience in modelling both structured and unstructured data
	- Excellent skills in C++, Python
	- Excellent skills in Postgres
	- Ability to work/report remotely
+ skill set:
	- close familiarity with ad tech
	- familiarity with Thompson sampling, contextual bandits, and related techniques
	- familiarity with privacy-preserving ML techniques
	- familiarity with security and fraud prevention (optional)
	- ability to read and understand scientific papers and to reduce research findings into practice
	- experience with software development via distributed development teams
	- comfortable working in an open source setting
	- excellent written and verbal communication skills in English
	- proven record of getting things done
	- good knowledge of python, or other language for model training, working knowledge of  C++/Rust for browser components
	- MS or PhD in Computer Science; we may consider a related quantitative discipline in exceptional cases
+ skill set for Machine Learning Researcher:
	- Specific Skills That Will Set You Apart from the Competition:
		* Privacy-preserving machine learning
		* Decentralized machine learning
		* Deep knowledge of adtech
		* Ph.D. in computer science from a top-tier school
	- Demonstrates a high level of initiative and consistently delivers high-quality answers
	- Has an enquiring mind and a disciplined scientific approach to extracting facts and understanding observed behavior
	- Is excited by the potential of realising high-value commercial outcomes and change the way that the advertising business operates
	- Want to be part of a high-growth startup company with global ambitions
	- Comfortable with JavaScript and C++ so that they can effectively interact with the rest of the team. Knowledge of Python, Java or C# is also strongly encouraged.
	- Has a proven track record implementing data driven products and a broad understanding of the state of the art in machine learning
	- Comfortable working in an open source setting
	- Able to create and deploy machine learning pipelines
	- Has a passion for helping protect users' privacy and security
	- A Ph.D. in computer science is highly preferred, but we may consider some exceptions
	- Logistic Regression, Decision Trees, Random Forest, Naive Bayes, Clustering, etc. and a good grasp of the strengths and weaknesses of specific approaches
	- Basic data cleansing and preparation, variable preprocessing/transformation
	- Univariate analysis, performing statistical tests, covariance analysis, multivariate analysis and linear/non-linear regression
	- Preparation of data sets for predictive modelling, robust predictive model building, validation and application
	- Automation of statistical processes and integration into a bigger product
	- Experience in Hadoop technologies such as Hive / Impala and distributed data pipelines such as Airflow / Luigi
+ skill set for Ph.D.-Level Internships at Brave:
	- https://brave.com/careers/?gh_jid=896018
	- San Francisco, CA, or London, U.K..
	- Brave is proud to offer summer internships for Ph.D.-level students that involve working on ground-breaking technologies that make the web faster, safer, and more private for millions of people worldwide. Brave's mission is also to change the way advertising is done on the web. If you are interested in technologies ranging from JavaScript to cutting-edge blockchain, web security, privacy, anonymity, and smart contracts, come and work with us to improve the web for millions of users!
	- You should be ready to tackle hard problems that involve building software that will be shipped to millions of people worldwide. Brave is delving into challenges that are deeply connected to cutting-edge academic research. To address many of these complex issues, we are in touch with several academic groups worldwide and we are excited to offer research internships for students who are currently enrolled in a Ph.D. program.
	- You will be part of Brave, will be paired with a mentor, and will be working alongside a larger Brave team. We offer internships in both the US and Europe. Most internships start in June 2019 and last three months, although there is some amount of flexibility for exceptional candidates.
	- Please email us at internships@brave.com if you have any questions.
+ skill set for Visiting Professor at Brave:
	- San Francisco, London, Remote
	- Brave is looking for visiting professors to join Brave for both short and long-term projects, especially in the summer. We are interested in having productive and well-published researchers working with us in the following areas:
		* software and hardware security
		* software reliability
		* privacy
		* blockchain
		* cryptography
		* performance
		* privacy-preserving machine learning
	- We are open to both local (San Francisco and London) arrangements. Candidates should expect to engage closely with researchers and engineers, but also to publish.
+ skill set:
	- Experience with Devops tools such as Jenkins, Nagios/icinga, Ansible, hashicorp vault.
	- Experience with Docker, Kubernetes is required.
+ skill set:
	- You have worked with Relational databases like MySql, Postgres and understand partitioning, sharding, as well as NoSQL databases such as mongoDB/Couchbase etc.
	- You should be at ease with maintaining cloud instances on AWS, Rackspace, Digital Ocean and the like, and you should be at home with provisioning tools like Puppet or Ansible. Experience with Docker in production would be prized.
+ skill set:
	- You have setup end to end data pipeline
	- You have worked on real world data sets that range into millions of data points that contain missing values, and unclean data
	- You have owned and delivered data science projects that are live on production
	- You have practical knowledge of a wide variety of data science methodologies and an intuitive understanding of which methodology is applicable to which problem
+ Detailed theoretical and practical knowledge of operating system and network basics (i.e. a successful candidate should know how technical elements such as DNS, TCP/IP, or BGP work, and will probably have managed their own Linux machines)
+ skill sets:
	- Conception and development of product prototypes and commercial products according to customer-specific requirements (real-time system development (μC))
	- Connection of sensors, definition of interfaces, integration of BUS systems etc.
	- Point of contact for technical questions about your project (internal and external)
	- Research on relevant hardware topics, e.g. new μC, hardware components etc.
	- Bachelor‘s or Master‘s degree in electrical engineering, information technology, computer science or similar
	- Knowledge in C, C++
	- Experience in the programming of embedded systems (µC)
	- Practical experience in building and testing prototypes, e.g. through private projects
	- Communication skills, ability to work independently and in a team
	- German and english language skills
+ Intermediate to advanced proficiency with R (tidyverse, ggplot2)
+ skill set:
	- Responsible to handle multiple clients at the same time
	- Work alongside Project Management team to gather requirements, design and implement custom solutions for customers
	- Perform discovery work sessions to determine the solution requirements
	- Perform design work sessions and produce deliverables, including mock-ups, flow diagrams, etc.
	- Provide customer specific configuration for Splash products
	- Provide consulting services to Splash customers on how to configure Splash products to achieve a customer's desired use cases
	- Review and validate solution designs produced by other Services team members
	- Serve as the Splash subject matter expert (SME) on Splash product capabilities
	- Develop tools, processes and best practices to ensure customers are realizing the greatest possible value from Splash
	- Expand upon the Splash solution portfolio, demonstrating best practices in design for use cases across variety of industries and business functions
	- Bachelor's degree plus 1-2+ years of work experience in a client-facing professional services/consulting role; prior experience in event marketing and SaaS service delivery preferred
	- Demonstrated ability to present solutions to clients, manage client expectations, and implement and deliver solutions
	- Strong communication skills (written, verbal, presentation), with the ability to explain technical subjects to non-technical end user personnel in large enterprises
	- Hands-on working experience in the configuration, customization, and implementation of SaaS applications
	- Experience with BI tools (e.g., Tableau) is a plus, but more important to be able to interpret and explain data in a meaningful way
	- A working knowledge and proficiency in the Google Suite, MS Word, Excel, PowerPoint, and Mail, JIRA and Confluence, and Microsoft Project (or equivalent Project Management software)
	- A working knowledge of the following is preferred:
		* Working HTML, CSS, JavaScript (JQuery / Node.js)
		* JSON, HTTP Requests, REST
		* Types of Authentication including Oauth and Basic Auth
		* PHP, SQL, XML
		* Understanding of UTM / URL parameters
	- Excellent problem solving ability
	- Deadline-driven
	- Willing to travel periodically (up to 25%) based on customer and business need
+ ***topological data analysis and graph network***
+ skill set:
	- This is an excellent opportunity for an experienced Senior Embedded Software Engineer to develop key elements of an open-source platform for GPS/INS navigation systems. The role spans development of platform code, drivers, and tools on both 32-bit ARM Coretex platforms as well as in the future small Linux platforms.  Core elements of the firmware are contributed and maintained as open-source projects on GitHub for use by the Aceinna community of customers.  Projects include enhancement of existing code for the immediate release to open source, as well as development of new platform level code for future versions of hardware including at some point a Linux version.
	- Write significant amount of well-tested and documented firmware for small embedded hardware platforms
	- Apply logic and software engineering practice to organize code in a sustainable and comprehendible manner for an open-source community
	- Maintain related GitHub repositories for branches, pull requests, and merge requests.  Help manage GitHub community and identify good contributors on our projects as well as other related open-source projects
	- Help hardware team evaluate new CPU architectures with benchmarks and requirements
	- Develop driver code for both communication protocols, sensors, and other peripherals
	- Perform other job related duties as assigned
	- Expert C/C++ embedded firmware architecture, development, and debugging skills
	- Experience with small RTOS systems with hard real-time constraints, and intensive numerical processing.
	- Experience with writing real-time code on Linux or Linux-like platforms such as Raspberry Pi or similar
	- Experience with hardware serial/bus protocols such as I2C, SPI, UART, CAN, Bluetooth and Ethernet buses
	- Experience with Python and/or Node.JS is desired but not required
	- Must be a self-starter and self-motivating individual
	- Must be capable of working in a team
	- Must be capable of training and generating documentation that will be posted on our GitHub account and other online documentation/blog posts
	- Postion is located in Santa Clara, CA
	- Education: BS or MS in Computer Science or related Technical Field
	- Years of Experience:  7+
+ Good knowledge of Apache Big Data stack (any of Kafka, Hadoop, Hive, Storm etc) and open source machine learning and NLP tools (Stanford CoreNLP, NLTK, TensorFlow, scikit-learn etc)
+ skill set:
	- Desire to work in a fast-paced, fluid environment
	- Analytical mind with problem-solving aptitude
	- Ability to work independently
	- Knowledge of at least one of Ruby/Rails, Golang, JavaScript, Elixir, or Java, with an appetite for learning
	- Experience using system monitoring tools (e.g. New Relic) and automated testing frameworks
	- In-depth knowledge of relational databases (e.g. PostgreSQL, MySQL) and NoSQL databases
	- Experience with large-scale systems
	- Working knowledge of Docker and Kubernetes, as well as Google Cloud
	- Grasp of Terraform
+ skill set:
	- Experience with Continuous Integration systems (e.g., Jenkins, Travis, GitLab)
	- Experience working with Docker containers
	- Experience with AWS or Kubernetes
	- Expand our existing test automation framework and test coverage.
	- Develop new tests and tools for our GitLab.com frontend, backend APIs and services, and low-level systems like geo replication, CI/CD, and load balancing.
	- Identify and drive the adoption of best practices in code health, testing, testability, and maintainability. You should know about clean code and the test pyramid, and champion these concepts.
	- Analyze complex software systems and collaborate with others to improve the overall design, testability and quality.
	- Strive for the fastest feedback possible. Test parallelization should be a top priority. You see distributed systems as a core challenge of good test automation infrastructure.
	- Configure automated tests to execute reliably and efficiently in CI/CD environments.
	- Track and communicate test results in a timely, effective, and automated manner.
	- Experience using test automation tools like Capybara, Watir, Selenium
+ Experience with a variety of data sources. Our data includes Salesforce, Zuora, Zendesk, Marketo, NetSuite, Snowplow and many others (see the [data team page](https://about.gitlab.com/handbook/business-ops/data-team/#-extract-and-load))
+ Experience with caching mechanisms
+ skill set:
	- Good knowledge of machine learning algorithms like Neural network, CNN, Logistic regression, KNN, Random forest, decision tree, clustering etc.
	- Decent depth in understanding ML algorithm concepts like supervised/unsupervised, regression/classification, time series algorithms
+ skill set:
	- System Verilog, Verilog, UVM/VMM
	- Experience in at system level and block level verification.
	- Expert in System Verilog and OVM/UVM based verification.
	- Strong experience in ASIC design/validation experience in front end processes including RTL development, functional and performance verification.
	- Expert in coding SV Testbench, drivers, monitors, scoreboards, checkers
	- Experience in C/C++,Shell/Perl scripting.
	- Understanding of AHB, AXI and other bus protocols and system architecture is a plus.
	- Preferred Expertise in MIPI UniPro/UFS Protocol and UVM.
	- To help the team to verify the existing design (UFS/UniPro)
	- Help on improving the Functional & code coverage, by identifying critical issues & helping the team with new ideas.
	- Expertise in verification of design blocks (IP) for system-on-chip (SoC) components.
	- Preferable: Experience in one/more of the following areas PCI_Express, USB, SATA, SDIO, MIPI and /or AMBA standards (OCP, AXI, AHB etc.)
+ skill set:
	- As a senior software engineer at Breker, you will work closely with our core product team to develop cutting-edge technologies to address the SoC verification crisis. You will communicate with our product definition team and translate the product specification into executable R&D tasks. You will independently design and implement the software components and take the ownership of one or more software modules. Your day-to-day work will also include participating in technical meetings and design reviews. As a product developer, you will also have the opportunity to interact directly with our customers and receive feedback about the work you have done. We are a small software team and believe in agile development philosophies. We insist on clean design and readable code. When Breker customers produce successful, reliable SoC products, you will know that you helped make them possible.
	- requirements:
		* MSEE/MSCS with 3 years of software industry experience
		* Excellent understanding of software design concepts, including object-oriented design and patterns
		* Extremely strong C/C++ coding skills and practices, including ability to debug complex software systems
		* Excellent written and oral communication skills
		* Self-starter, high energy, and able to develop effective plans for software component implementation
		* Eagerness to be part of a startup and a desire to experience the excitement of offering powerful new technology
	- preferred experience:
		* PhD with relevant experience in circuit design and verification
		* Knowledge about constraint solving techniques, such as Boolean Satisfiability (SAT), Binary Decision Diagrams (BDD), and Satisfiability Modular Theory (SMT)
		* Adoption of agile software development methodologies
		* Familiarity with hardware design and verification languages, such as Verilog and SystemVerilog
		* Hands-on use of software construction tools, such as GNU Make and SVN
+ skill set:
	- You will drive the effort to develop a complete software stack for an FPGA-based machine learning inference accelerator card reference platform. You will adapt existing open-source and university software when possible, and develop new software from scratch as needed, to assemble a complete full-stack, end-to-end software solution. You will work closely with sales, marketing, systems engineering, EDA tool developers, and FPGA architects to support diverse use models from FPGA micro-architecture exploration, memory subsystem design optimization, place-and-route software verification, system prototyping, pre-sales demonstration development, and customer deployment and scaling.
	- Prior experience is required working with a machine learning accelerator micro-architecture and ISA, as well as current knowledge of state-of-the-art research. You must have a background in open-source compiler hacking. Experience desired with compiler intermediate representations (IRs) and back-ends, JIT compilers, as well as kernel-mode and user-mode runtime environments and device drivers. Familiarity is desired with industry-standard machine learning frameworks, acceleration libraries, domain-specific languages, and with common DNN models.
	- Two years of work or educational experience in machine leaning accelerator micro-architectures and compilers
	- Skilled practitioner in C++ or Java.
	- Experience in Python, Verilog, and System-C.
	- Experience required in one of more of the following:
		- Machine learning accelerators such as OpenTPU, NVDLA, VTA, EIC
		- Compilers such as Glow, TVM, CLANG, LLVM, or GCC
		- Machine learning frameworks such as TensorFlow, PyTorch, Caffe2, and Keras
		- Acceleration libraries such as MXNet
		- Domain-specific languages such as Halide and Spatial
		- Common DNN models such as AlexNet, ResNet50, Inception, YOLO, RNN, and LSTM
		- Embedded system runtime environments and device drivers
+ skill set:
	- 5+ years experience developing data frameworks using Python
	- 5+ years of using SQL for data manipulation in a fast-paced work environment
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark, or similar
	- Experience building and optimizing ‘big data' data pipelines, architectures and data sets
	- Mastery of data warehousing development and fundamentals
	- Passion for business-oriented data development
+ skill set:
	- Strong foundation in Infrastructure as Code and configuration management using tools like Terraform, SaltStack, Ansible, Chef, Puppet
	- Knowledge of Internet engineering fundamentals (load balancing, DNS, CDNs)
	- Experience with monitoring, metrics, and logging tools (Graphite, Grafana, Nagios/Icinga, SumoLogic, etc.)
	- Experience with continuous integration and deployment (Jenkins, GitLab, CircleCI, etc.)
	- Experience performing root cause analysis and troubleshooting and resolving production issues
	- Obsession with code quality, task tracking, and writing documentation and runbooks that allow globally-distributed engineers to understand and support the infrastructure.
+ skill set:
	- Demonstrated experience in algorithm development and application of image processing, registration, segmentation, etc.
	- Working knowledge of geometry and application experience
	- OpenGL or DirectX, Cg knowledge and development experience.
	- Proficiency in C++ development, parallelization, unit testing, and performance measurement
	- C++ GUI development experience
	- Experience in developing SQLite database application
	- Microsoft Visual Studio development experience
	- Solid software engineering foundation and a commitment to writing high quality code, including clear and understandable design and implementation, well-defined interfaces, ease of build and use & ease of extensibility.
	- Computer Vision and Image Processing algorithm development experience
	- OpenCV, IPP, CUDA experience
	- Familiarity with data formats, such as STL, OBJ, FBX, etc.
	- Familiarity with serial communication protocol, TCP/IP server/client communication protocol
	- Familiarity with software development tools and methodology
+ skill set:
	- 2+ years of experience with C/C++/C#/Objective C or Java
	- Intermediate-level expertise and experience working in Maya, Blender, or 3ds Max
	- Intermediate-level expertise and experience working in Adobe Creative Cloud applications.
	- Programming and/or scripting experience
	- Experience with the latest mixed reality hardware (HoloLens, Leap Motion, Vive/Rift/PSVR/Daydream) a plus
	- Amazing attention to detail, self-motivated and collaborative
	- Passion for creating new, innovative, and ground-breaking user experiences
	- Excellent communication skills (written and verbal)
	- User interface and rapid prototyping experience
	- Understanding of computer vision algorithms, spatial mapping, shaders
	- Possess new technology curiosity and a history of self-technical education
+ skill set:
	- Flume/Gobblin/Kinesis
	- Develop ETL operations using Python, Spark, SqlServer, Redshift and Kafka.
	- Develop the core tooling library to support Airflow data pipelines.
	- Design and implement the testing framework for Airflow dags and write test cases.
	- Monitor and debug data pipelines running on Airflow.
+ skill set:
	- Experience with JVM tuning is a plus
	- Experience with Apache Solr is a plus
+ skill set:
	- Familiarity with Ansible, TeamCity
	- Experience with BrowserStack; GraphQL, React, NodeJS
	- Experience writing Splunk queries and tools like New Relic
+ GraphQL experience is a plus
+ skill set:
	- Python 2.7, 3.7
	- SQL Server, MySQL, Memcached, Solr, Ansible, Linux
	- Kafka, RabbitMQ
	- React, GraphQL
+ skill set:
	- Architect, build, and operate AWS environments using infra-as-code best practices
	- Define and implement standards around the utilization of AWS services
	- Migrate on-premise services to AWS
	- Help define AWS account structure, IAM, and AWS account security standards
	- Collaborate with engineers to deploy, support, and monitor the application stacks
	- Lead in the improvement of  the availability and scalability of our infrastructure
	- Requirements:
		* 3+ years of experience with AWS, including but not limited to heavy experience with IAM, VPC, EC2, ALB, S3, CloudWatch, Cloudfront, and Cloudformation
		* Experience building well architected environments in AWS
		* Demonstrable experience leveraging Configuration Management tooling such as Anisble, Cloudformation, Terraform, and proven strategies for maintaining large infrastructure-as-code deployments
		* Systems Administration experience with Linux and/or Windows and the automation/scripting of operations within those OS environments
		* Experience with container runtime environments, and container orchestration frameworks such as Docker or Kubernetes
	- Experience with Python in either a software engineering or devops environment
	- A solid understanding of network protocols and common services
	- Experience and knowledge of Git and JIRA
	- Experience with monitoring tools such as New Relic, Cloudwatch, or Datadog
+ skill set:
	- In-depth, hands-on knowledge of JavaScript and experience working with the relevant tools/libraries (React, jQuery, Webpack, etc)
	- Able to write efficient SQL queries and design schemas for relational databases
	- Experience with Python and web frameworks (Pyramid, Django, Rails, etc)
+ skill set:
	- Docker container orchestration platforms like Kubernetes or Mesos
	- Databases and key value stores like MySQL, MongoDB, or Redis
	- Distributed streaming platforms like Kafka or AWS Kinesis
	- Monitoring and alerting tools like Grafana and Sensu
	- React, Angular 1.5, Webpack
	- Relational databases (like PostgreSQL, MySQL)
	- Key Value stores (like Redis)
	- AWS services: AWS Lambda, Aurora, Elasticache, RDS, and other AWS services
+ skill set:
	- Data technology: Relational databases (like PostgreSQL, MySQL), Key Value stores (like Redis)
	- Container technology like Docker, and familiarity with Swarm/Kubernetes/Mesos
	- Distributed systems, including fault tolerant design, event sourcing and other distributed system architectural pattern
+ skill set:
	- Strong computer science fundamentals: data structures, algorithms, programming languages, distributed systems
	- Developing, deploying, and monitoring services on cloud infrastructure like Amazon
	- Understanding of modern application development and architecture
	- Understanding of DevOps principles such as fail early and often, The Three Ways, small batches, CI/CD
	- Software development methodology, like TDD and BDD
	- Distributed systems, including fault tolerant design, event sourcing and other distributed system architectural pattern
	- Container technology like Docker, and familiarity with Swarm/Kubernetes/Mesos
	- DevOps skills: Kubernetes, Docker, Terraform, Helm
	- Software design: Domain Driven Design, Design patterns
	- Data technology: Relational databases (like PostgreSQL, MySQL), Key Value stores (like Redis)
	- AWS services: AWS Lambda, Aurora, Elasticache, CloudFront, SNS, SQS, and other AWS services.
+ Orchestration platforms for containers (i.e. Docker, Kubernetes, Swarm or ECS)
+ skill set:
	- Experience working with Single Sign On (SAML 2.0).
	- Strong service oriented background including ability to multitask.
	- Experience with Microsoft Windows domain environment including Active Directory.
	- Familiarity with server virtualization and hosted infrastructure: AWS, Hyper-V, VMWare, clustering, high availability.
+ practical knowledge of machine learning tools (e.g. scikit-learn / LightGBM / PyTorch)
+ skill set:
	- intermediate proficiency in Java and Python,
	- proficiency in Spark framework,
	- basic understanding of REST,
	- practical knowledge of Docker Kubernetes,
	- good english communication skills,
	- ability to share your knowledge,
	- willingness to learn continuously,
	- basic machine learning knowledge (regression, classification, clustering, validation methods, time series forecasting).
	- knowledge of Kafka platform,
	- knowledge of Hadoop framework,
	- basic programming skills in Scala/R,
	- familiarity with ELK stack (Elasticsearch, Logstash, Kibana),
	- knowledge of NoSQL databases (e.g. Cassandra, ELK),
	- Storm, Nginx technologies.
+ knowledge of object-oriented and functional programming paradigms,
+ skill set:
	- As an intern on our Compiler team, you will work with leaders from industry and academia to develop entirely new solutions for the toughest problems in AI compute.
	- As deep neural network architectures evolve, they are becoming enormously parallel, and distributed.  Compilers are needed to optimize the mappings of computation graphs to compute nodes. In this position, you will build the tools that generate distributed memory code from evolving intermediate representations.
	- Design and devise graph semantics, intermediate representations, and abstraction layers between high-level definitions (like TensorFlow's XLA) and low-level distributed code.
	- Use state-of-the-art parallelization and partitioning techniques to automate generation, exploiting hand-written distributed kernels.
	- Identify and implement novel program analysis and optimization techniques.
	- Employ and extend state of the art program analysis methods such as the Integer Set Library.
	- Graduate and undergraduate students in Computer Science with a background in compilers and parallel programming.
	- Two or more years of related work experience on compilers and distributed systems.
	- Compiler experience; experience generating and optimizing code.
	- Familiarity with high-level parallel program analysis and optimization
	- LLVM compiler internals.
	- Polyhedral models.
	- Familiarity with HPC kernels and their optimization.
+ skill set:
	- Design and verification of the Image/Video processing algorithm
	- AI based pattern recognition, video scaling, video frame interpolation, video enhancement algorithm design.
	- Knowledge in deep learning theory
	- Experience building system based on deep-learning framework
	- Familiar with at least one of AI platforms, such as TensorFlow, PyTorch, Caffee, Keras, etc.
	- Solid mathematics fundamentals
	- Knowledge in image/video processing algorithm (preferred)
	- Knowledge in c/c++ and python; Master degree or Ph.D.
+ skill set:
	- Responsible for SoC architecture development to achieve good area/power/performance
	- Collaborate with Marketing and System team to define chip specifications and product roadmaps
	- Co-work with algorithm team, design team, verification team to ensure the chip implementation follows architectural intend
	- Help with chip level RTL design, integration and verification, including FPGA emulation
	- Maintain chip level c-model to support full chip simulation
	- Support product validation, ATE test and other chip level issues during high volume ramp-up
	- Master/Ph.D Degree in Electrical/Communication Engineering or related fields\nMinimum 3 years industry SoC development experience. Video processing SoC design experience is preferred
	- Fluent in Verilog, C/C++
	- Familiar with clock, reset, low power design
	- Experience in bus fabric development, integration & performance optimization
	- Familiar with OCP protocol is a plus
	- Experience in Perl/python is a plus
	- Fluent speaking/writing in English and Good communication skill
	- Self-motivated, Organized, Team Player, Result Oriented, and Fast learning on challenging task
+ skill set:
	- Independently handle key IC design tasks: Comprehend algorithm or transaction protocol specifications
	- Develop block-level micro-architecture and implementation with RTL
	- Preparation of technical documentation
	- Co-development of verification plan and test case definitions
	- Chip- and block-level verification debug
	- Subsystem- and SoC-level integration, including timing constraint definition, logic synthesis, power analysis and timing closure
	- Support FPGA validation and silicon bring-up
	- Interface with 3rd party vendors for successful IP integration into SOC
	- BSEE Degree or above
	- 3 to 5 years of hands-on experience in digital IC design
	- Familiar with ASIC design methodology and SoC implementation flow
	- Experience with common digital IC design CAD tools for simulation, logic synthesis, formal verification and static timing analysis
	- Attention to detail, self-motivated and the ability to be a team player while working independently
	- Strong analytical and problem-solving skills
	- Good communication skills and proficient in written and verbal English
	- Working experience in one of the following areas is a plus:
	- Image or video processing IP design
	- Computer architecture
	- Interface controllers such as DDR, MIPI, PCI-Express, SATA, USB and HDMI
	- Mobile or low-power SoC design
	- Scripting languages such as bash/csh, Perl or Python
+ skill set:
	- Independently handle key IC design tasks: Understanding the algorithm/protocol requirement and implementation with RTL
	- Block level Micro-architecture, RTL coding
	- Chip/block level verification debug
	- Module and top level integration, including constraints definition, synthesis, power analysis and timing closure
	- Support FPGA validation and Silicon bring-up
	- Interface with 3rd party vendor for successful integration into SOC
	- BSEE Degree or above
	- About 3~5 years of experience in hands-on digital IC design
	- Familiarity with ASIC design methodology and SoC implementation flow
	- Familiarity with standard CAD tools including simulation, synthesis, formal verification tools
	- Self-motivated in solving problems
	- Good communication skills and fluent in English
	- Good team player. A plus to have: Experience of image or video IP design
	- Good scripting skills
	- Knowledge of power and low power design
+ skill set:
	- Develop software to control Pixelworks chip on Android system
	- Light up MIPI DSI panel with Pixelworks' chip
	- Do video and image post processing
	- Validate new Pixelworks' chip
	- Tune PQ performance and system stability
	- Light up MIPI CSI camera
	- Integrate Pixelworks algorithm to camera
	- Validate functionality of ISP pipeline
	- Support customer
	- Good education background or knowledge base of image processing and color management is required
	- In-depth knowledge of Android SurfaceFlinger and HWC is required
	- Good knowledge background of MIPI DSI is preferred
	- Good knowledge background of MIPI CSI is preferred
	- Familiar with camera data processing from hardware and software perspectives
	- Experience with ISP driver or image quality tuning is preferred
	- Experience with camera HAL implementation is preferred
	- Familiar with Android multimedia architecture
	- Familiar with MDP/SDE of Qualcomm Application Processor from hardware and software perspectives
	- Excellent C/C++ programming skills required. Java is a plus
	- English communication capability to work with colleagues around the world
	- Basic GPU programming is a plus
+ skill set:
	- Co-work with Architect team and Algorithm team to launch Image/Video processing product
	- Design and verification of the Image/Video processing engine
	- Image/Video Algorithm integrating, optimization, analysis
	- Familiar with ffmpeg, x264,x265,vp8/vp9
	- Experience of developing product with ffmpeg
	- linux c/c++ environment programming
	- parallel programming
	- Basic understanding about video codec and post-processing
+ skill set:
	- Definition of block level and product level SoC architecture
	- Work closely with IC design, verification, software, physical design, and marketing teams to ensure SoC meets feature and performance requirements
	- Modeling, Architecture, Micro architecture, Digital design, & RTL coding
	- Interconnect exploration including AHB, AXI, ACE, AXI-stream, and NoC  interconnect architectures
	- Bachelors Science or better in Electrical Engineering or related field.
	- Masters or better in Electrical Engineering or related field.
	- Willingness to travel domestically and internationally
	- Strong communication skills
	- Experience working with 3rd party teams
	- Expertise in scripting languages such as Python, TCL, or Perl
	- Experience developing design models in high level languages like System Verilog and System C
	- Solid experience in design convergence cycle, including synthesis, timing closure, and verification
	- Strong background in memory interfaces including DDR3 / DDR4 / LPDDR4 systems
	- Familiarity with SoC security capabilities and protocols
	- Experience in profiling SoC system performance
	- 12 + years of experience in definition of SoCs
+ skill set:
	- Responsible for development of EDA database and FPGA design tools.
	- Responsible for optimizing EDA database and enhancing current software architecture.
	- Responsible for the design of state-of-art infrastructure for next generation FPGA products.
	- Working with other software development teams to strongly support EDA tools capacities efficiently.
	- Improve development methodologies and processes
	- BS/MS/PHD in Electric Engineering or Computer Science or Mathmatics.
	- 1+ years of experience in EDA, CAD, ASIC or FPGA.
	- Experience with C/C++, Python, TCL on LINUX and/or WINDOWS platforms.
	- Strong background in EDA algorithms and data structures is preferred.
	- Strong and effective inter-personal and communication skills.
	- Self-motivated, self-disciplined with the ability to set the team goals and work consistently towards achieving them.
	- Experience of development of large existing software systems is highly desirable.
	- Individuals with strong desire and ability to explore new technologies and who are able to demonstrate excellent analysis and problem-solving skills are preferred.
+ Drive the generation, audit and validation of secondary views (IBIS models, Timing files, Power files) in support of faster integration and promote IP re-use
+ skill set:
	- We are looking for a senior staff engineer with good understanding of neural network implementation on resource constrained embedded devices. In this position you will have an opportunity to influence how customers can efficiently utilize Lattice solution for neural network inference. In-depth understanding of neural network topologies, training with TensorFlow and/or Caffe framework is essential. Candidate need to possess good programming skills with Python, C/C++ with basic understanding of data structures. Candidates need to have the expertise to use embedded architectures such as FPGAs, GPUs, or other embedded processors to realize complex applications using deep learning.
	- The ideal candidate would possess the following skills:
		* Ability to create and use deep learning training frameworks for creating new applications.
		* Expertise in one of Tensorflow, Caffe, Keras, or any other deep learning framework.
		* Python programming expertise.
		* Understand different kinds of numerical precisions in implementing deep learning solutions. Ability to translate networks trained with floating point to different fixed point formats. Understand the accuracy loss, and come up with ways to address them.
		* Be up to date with the current research in neural networks, implementations, competitive solutions, etc.
	- Able to present solutions to customers, understand their requirements and come up with solutions to address them.
	- Create neural network models, add new features to neural network compiler for efficient inference, mentor junior engineers, deliver functionally correct software, debug customer issues, document features, develop new testcases, publish papers, file patents
	- Behaviors
		* Enthusiastic: Shows intense and eager enjoyment and interest
		* Detail Oriented: Capable of carrying out a given task with all details necessary to get the task done well
		* Innovative: Consistently introduces new ideas and demonstrates original thinking
	- Motivations
		* Self-Starter: Inspired to perform without outside help
		* Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
	- Education
		* Required: Masters or better in Electrical Engineering or related field.
		* Preferred: Doctorate or better in Computer Engineering or related field.
	- Experience
		* PHD on relevant topic, MS with 8+ years experience, BS with 10+ years experience
+ skill set:
	- Machine learning engineer who develops neural network for a given application and map to FPGA solution. Need to develop technology to map the neural network into FPGA including neural network compiler and HW acceleration engine development.
	- Need to know the details of machine learning including network design, network training, and training dataset build up. Also need to understand the general accelerator structure of machine learning inferencing engine.
	- Behaviors
		* Team Player: Works well as a member of a group
		* Functional Expert: Considered a thought leader on a subject
		* Enthusiastic: Shows intense and eager enjoyment and interest
	- Motivations
		* Self-Starter: Inspired to perform without outside help
		* Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
	- Education
		* Doctorate or better in Electrical Engineering or related field.
	- Experience
		* Hands on experience on network design and training. RTL design experience, FPGA design experience
+ skill set:
	- A successful candidate will join a team designing and developing Lattice FPGA software tools at San Jose. The candidate will contribute to delivering software solution for Lattice FPGA development with emphasis on placement and routing tools. The candidate is expected to research and develop novel algorithms to improve Lattice FPGA placement and routing engines to achieve better Fmax, runtime as well as memory footprint. The candidate is also expected to support new FPGA architecture evaluation and assess its potential impact on existing software tools. The candidate will be responsible for maintaining existing software product too and interacting with other teams to facilitate a value added solution.
	- Responsible for new architecture evaluation, placement and routing tool's QoR improvement and support for Radiant/Diamond software release.
	- Behaviors
		* Team Player: Works well as a member of a group
		* Dedicated: Devoted to a task or purpose with loyalty or integrity
		* Functional Expert: Considered a thought leader on a subject
	- Motivations
		* Self-Starter: Inspired to perform without outside help
		* Growth Opportunities: Inspired to perform well by the chance to take on more responsibility
		* Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
	- Education
		* Required: Masters or better in Electrical Engineering or related field.
		* Preferred: Doctorate or better in Computer Science or related field.
	- Experience
		* 8 years: 8+ years experience in EDA place and route development. Expertise in C++, knowledge of data structure and graph algorithms, and strong communication skills are required. Knowledge or experience in FPGA development is a plus.
+ skill set:
	- Principal Security Architect will develop a strategic direction for the security-related architecture of Lattice products, working closely with FPGA system and architecture experts to advance the company's presence within security-sensitive systems
		* Reduce risks, threats, and vulnerabilities in systems built with Lattice products
		* Drive Lattice's roadmap for data and design security within our programmable logic product line
		* Meet with customers and ecosystem partners to ensure requirements are understood, implementations are sound, and the broader ecosystem is moving to meet the future challenges
		* Coordinate across functional business units to ensure robust, secure posture from design to implementation
		* Represent Lattice at industry forums related to data and design security
		* Coordinate development of tools, methods, and training to support staff in achieving security objectives and ensure effectiveness of security standards
		* Will partner closely with engineering, sales, marketing, applications, and end to end customers
		* Strong customer facing skills
	- Behaviors
		* Dedicated: Devoted to a task or purpose with loyalty or integrity
		* Team Player: Works well as a member of a group
		* Leader: Inspires teammates to follow them
	- Motivations
		* Goal Completion: Inspired to perform well by the completion of tasks
		* Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
		* Entrepreneurial Spirit: Inspired to perform well by an ability to drive new ventures within the business
	- Education
		* Required: Bachelors or better in Computer Science or related field.
		* Preferred: Masters or better in Computer Science or related field.
	- Experience
		* Expert knowledge/background in secure code authentication/attestation and chains of trust
		* Experience in industry attack, vulnerabilities and solutions around design and data security
		* Working knowledge of online service architectures for connected/IOT systems
		* Well versed in Secure Architecture and Design fundamentals
		* 10 years: 10+ years of experience in the hardware data security industry
+ skill set:
	- A successful candidate will join a team designing and developing Lattice FPGA software tools at San Jose. The candidate will contribute to delivering software solution for Lattice FPGA development with emphasis on device support including device modeling, simulation model and bitstream generation. The candidate is expected to work closely with FPGA silicon design teams and FPGA SW implementation team (MAP, Place & Route and Timing Analysis) and programming teams to provide an entire FPGA solution from Synthesis to bitstream download. The candidate is also expected to support new FPGA architecture evaluation and assess its potential impact on existing software tools. The candidate will be responsible for maintaining existing software product too and interacting with other teams to facilitate a value added solution.
	- Behaviors
		* Team Player: Works well as a member of a group
		* Innovative: Consistently introduces new ideas and demonstrates original thinking
		* Detail Oriented: Capable of carrying out a given task with all details necessary to get the task done well
	- Motivations
		* Growth Opportunities: Inspired to perform well by the chance to take on more responsibility
		* Goal Completion: Inspired to perform well by the completion of tasks
		* Self-Starter: Inspired to perform without outside help
	- Education
		* Required: Bachelors or better in Electrical and Electronics Engineering or related field.
		* Preferred: Masters or better in Electrical Engineering or related field.
	- Experience
		* Required
			+ C++ programming and data structure
			+ 5 years: Expertise in C++, Verilog, and scripts, knowledge of logic design, simulation and data structure
		* Preferred
			+ knowledge of FPGA, logic design & simulation
			+ EDA tool development
			+ Logic design & familiar with Verilog & VHDL
			+ Strong communication skills are required
			+ Knowledge or experience in FPGA development and System Verilog is a plus
			+ 3 years: Experience in EDA development
+ skill set:
	- Solid background in multiple programming languages, e.g. Python or C/C++ and willingness to pick up any new programming languages or frameworks.
	- Fearless about jumping around the stack (from improving driver to writing CUDA kernel to putting together React webapp) and working on aspects that have significant impact on product
	- Having built enough systems to recognize what are the pragmatic designs (not the most fancy ones)
	- Shape the vision and architecture of the system infrastructure that powers the next-generation of intelligent machines.
	- Implement mission-critical software in a reliable and sustainable manner.
	- Craft tools, processes and frameworks to redefine the software development in the rise of autonomous systems powered by artificial intelligence.
	- Collaborate with, learn from, and mentor a team of diverse roles, which include software engineers, roboticists and AI researchers.
	- Have worked at a startup before. Enjoy the fast-paced environment.
	- Know how to scale yourself as the company grows, via mentorship, delegation, increased role and responsibilities.
	- Are self-directed and enjoy figuring out what is the most important problem to work on.
	- Own problems end-to-end, and are willing to pick up whatever knowledge you're missing to get the job done.
	- Are detail oriented, and like to get the system working but know when to cut corners versus get it exactly right.
	- Have a strong interest in AI & robotics.
	- At covariant.ai we don't just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Covariant.ai is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
+ skill set:
	- Drawing on recent advances in Deep Imitation Learning and Deep Reinforcement Learning, covariant.ai is developing AI software that makes it easy for robots to learn new, complex skills.
	- As a part of a rapidly growing startup, you will have the rare opportunity to build and develop software that mimics human behavior without the help of engineers, while also growing and developing your own skills and passions as the company expands. Join us on an exciting journey as we bring the latest breakthroughs in artificial intelligence to the future of robotics.
	- Work side-by-side with the top talent in industry and academia in the field of AI + robotics.
	- Implement mission-critical software in a reliable and sustainable manner.
	- Evolve best practices for traditional software development to address the needs of cyber-physical systems and deep-learning-based software -- including scalability, maintainability, and security.
	- Collaborate with and support a diverse team, which includes software engineers, mechanical and electrical engineers, roboticists, and ML researchers.
+ skill set:
	- You have strong coding skills in several of the following languages/libraries: Python, NumPy, TensorFlow, PyTorch.
+ skill set:
	- We are looking for Research Scientist interns who have applied experience in the fields of in the fields of NLP, human-computer interaction, user behavioral analysis and/or the intersection of HCI, NLP and Machine Learning including algorithmic bias. We are open to a variety of approaches and methods to answer research questions related to how people interact and engage with Spotify. You will be part of an interdisciplinary team focused on making every user interaction with Spotify amazing through personalization and discovery, and in the process pushing state of the art and contributing to the wider research community by publishing papers. We work on a broad range of Spotify features – personalized playlists such as Discover Weekly and Daily Mix, the Homepage, Search and other ML systems powering recommendations of music and podcasts to 180 million users with billions of interactions.
	- You are currently in a PhD program in human-computer interaction, psychology, NLP, statistics, IR/search/agents, data science or related area.
	- You have publications in top tier venues such as CHI, ACL, UbiComp, SIGIR, Interspeech, HRI, CSCW, RecSys, or related.
	- You are intrigued by how interaction design, data collection strategies, and people's perceptions affect Machine Learning outcomes.
	- You have a demonstrated interest in speech/natural language, personalized recommendations, crowdsourcing, diversity in AI – and music or podcasts.
	- You are a creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results.
	- You have experience with the complexities of real-world data, and understand the value of both in-depth, qualitative and web-scale, quantitative data working together to create a deep understanding of people's interaction with technology.
+ skill set:
	- We are looking for Research Scientist interns who have applied experience in the field of machine learning, machine Intelligence, user behavioural analysis, IR, NLP, and more broadly, AI. You will be part of an interdisciplinary team focused on making every user interaction with Spotify amazing through personalisation and discovery, and in the process pushing state of the art and contributing to the wider research community by publishing papers. We work on a broad range of Spotify features – personalised playlists such as Discover Weekly and Daily Mix, the Homepage, Search and other ML systems powering recommendations.
	- Courses: PhD programme in Computer Science, Data Science, or related areas with a strong computational focus.
	- Experience: Publications in top tier venues such as WWW, SIGIR, WSDM, RecSys, CHI, KDD, AAAI, ACL, NeurIPS, ICML, or related, in the following topics: search, marketplace research, recommendations, user understanding, large scale experimentation, linguistics or (more broadly) machine learning.
+ skill set:
	- Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
	- Leverage best practices in continuous integration and delivery.
	- Help drive optimization, testing and tooling to improve data quality.
	- Collaborate with other engineers, ML experts and stakeholders, taking learning opportunities that will arise every single day.
	- Work in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.
	- You are pursuing a Bachelor's or Master's degree or a bootcamp certification in Computer Science or Computer Engineering or a related field of study.
	- You've dabbled in high volume data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
	- You've had exposure to data modeling, data access, and data storage techniques.
	- You have an interest in agile software processes, data-driven development, reliability, and responsible experimentation.
	- You understand the value of collaboration within teams.
+ skill set:
	- We are looking for Data Scientist Interns to #JoinTheBand and help drive a data-first culture across our Finance teams at Spotify. Data scientists within our Finance team has the mission of utilizing data insights to drive decision-making for the organization. You will study user behavior, strategic initiatives, markets, content, and new features and bring data and insights into every decision we make. Above all, your work will impact the way the world experiences music.
+ skill set:
	- experience in the use of data visualization libraries (d3.js, Processing);
	- knowledge of tools and languages ​​for data analysis and visualization (eg Tableau, R, Python, Gephi, NodeBox);
	- interest and propensity to learn work dynamics in an international company, dealing with highly innovative and experimental projects;
	- academic background in information design and data visualization;
	- experience in the design and implementation of static and / or interactive data visualizations and infographics;
	- imagine, design and collaborate with UX / UI and development teams to create static and interactive data visualizations, infographics, reports and dashboards;
	- interact and design solutions with customers that can satisfy both explicit and unidentified needs.
+ skill set:
	- Accurat is looking for UX / UI design intern to be included in the design team of the Milan office.
	- Our ideal candidate knows:
		* imagine, design and collaborate with data viz and development teams to create effective and innovative user experiences;
		* interact and design solutions with customers that can satisfy both explicit and unidentified needs.
	- Our ideal candidate has:
		* interest and propensity to learn work dynamics in an international company, dealing with highly innovative and experimental projects;
		* an academic background in visual design, UX / UI design;
		* experience in the design and implementation of interfaces and user experience;
		* excellent knowledge of vector graphics software such as Adobe Illustrator and Sketch.
	- Also welcome:
		* knowledge of software for creating interactive prototypes (Framer, Invision ...);
		* experience in the use of visual mapping tools such as Plectica;
		* experience in type design and editorial design;
		* talent or predisposition for vector and / or raster illustration.
+ skill set:
	- Advanced understanding of SQL, Python, PySpark, Scala, Hive, h2o, as well as machine learning techniques and algorithms, such as linear / logistic regression, tree-based learners, Gradient boosting, SVM, NLP, time series modeling, clustering, etc.
+ skill set:
	- Personality Traits
		* Ability to work independently under tight deadlines with accountability.
		* Strong results driven personality with a high level of enthusiasm, energy and confidence.
		* Strong problem-solving skills.
	- Required Skills
		* In-depth knowledge of various Natural Language Processing/Understanding (NLP/NLU) domains such as entity extraction, speech recognition, topic modeling, parsing, question answering, etc.
		* Expertise in text mining (probabilistic topic model, word association mining, ontology learning, semantic similarity, etc.)
		* Expertise in NLP/NLU (word representation, relation extraction, natural language inference, semantic parsing, etc.)
		* Excellent background in Machine Learning (generative model, discriminative model, neural network, regression, classification, clustering, etc.)
		* Experience in deep learning on NLP/NLU is a big plus
		* Extensive experiences in using NLP related techniques/algorithms such as HMM, CRF, deep learning & recurrent ANN, word2vec/doc2vec, Bayesian modeling, etc.
		* Experience in applied statistics including sampling approaches, modeling, and data mining techniques
		* Experience in building analytical models and working with structured and unstructured data
		* Experience with data structures and algorithms; ability to work in a Unix environment and building robust data processing and analytics pipelines
		* Contributions to research communities, e.g. ACL, NIPS, ICML, EMNLP, etc. is a Plus
	- Responsibilities:
		* You will be part of a high impact team that's building the next generation of intelligence and language understanding for Cuddle
		* Build text/voice-based search engine and question answering system for Cuddle
		* You'll utilize the latest techniques in AI, ML (including Deep Learning approaches) and NLU
		* Build topic analysis, text classification, named entity recognition methods for unstructured and semi-structured data
		* Develop and perform text classification using methods such as logistic regression, decision trees, SVM and maximum entropy classifiers
		* Perform text mining, generate and test working hypotheses, prepare and analyze historical data and identify patterns
		* Generate creative solutions (patents) and publish research results in top conferences (papers)
	- Qualifications
		* Master's degree in Mathematics, Statistics, Computer Science, or related fields
		* 2+ years of ML + NLP experience
		* Software programming in Java or Python
		* Knowledge of SQL and NoSQL databases
		* Knowledge of open source machine learning libraries like scikit-learn, tensorflow, NLP tool as NLTK
+ skill set:
	- We use regression, Bayesian methods, tree-based learners, SVM, RF, XGBOOST, time series modelling, dimensionality reduction, SEM, GLM, GLMM, clustering etc on a regular basis.
	- Ability to deliver AIML based solutions around a host of domains and problems, with some of them being: Customer Segmentation & Targeting, Propensity Modelling, Churn Modelling, Lifetime Value Estimation, Forecasting, Recommender Systems, Modelling Response to Incentives, Marketing Mix Optimization, Price Optimization
	- Experience of working on a project end-to-end: problem scoping, data gathering, EDA, modelling, insights, and visualizations
	- Detailed knowledge of XGboost, classification models, RF, understanding of Error metrics (RMSE, MSE, MAE), model fine tuning, feature selection, model selection  
+ skill set:
	- Building descriptive or predictive models using Python, PySpark, Spark ML, Scala
	- Building and deploying models in a Big Data environment; comfort with using Hive, MapReduce, Kafka, Spark Streaming, Spark SQL, JavaScript, Sqoop to run data processing tasks
	- Building AI/ML models using techniques such as Regressions, Random Forest, Gradient boosting, neural networks, such as ANN/CNN, Hidden Markov, NLP, SVM, Bayesian techniques, etc.
	- Experience in a Linux computing environment and use of command-line tools including knowledge of shells. Python scripting for automating common tasks is a plus
	- Working knowledge of AIML packages such as Keras, Theano, TensorFlow. Software such as H2O is a plus
	- Working knowledge of cloud infrastructures such as AWS, Azure or GCP is a plus
+ skill set:
	- Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage
	- Operating knowledge of cloud computing platforms (AWS/Azure ML)
	- Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and Hadoop ecosystem components -- Hive, Spark, HBase, Storm, Kafka etc.
+ skill set:
	- Advanced understanding of SQL, Python, PySpark, Scala, Hive, h2o, as well as machine learning techniques and algorithms, such as linear / logistic regression, tree-based learners, Gradient boosting, SVM, NLP, time series modeling, clustering, etc.
+ skill set:
	- 6-7 years of overall experience working with data science methodologies & techniques like dimensionality reduction (PCA, Factor Analysis, etc.), clustering (supervised, unsupervised), time series forecasting, optimization, parametric modelling (SEM, GLM, GLMM, etc.), ML (tree based learners like RF, GBM, XGBOOST, CATBOOST, etc.), BBN, Neural Networks (ANN, CNN, RNN, etc.), DL, RL, Transfer learning, text mining & NLP, etc.
	- Graduate or Under-graduate degree holder in Statistics/Mathematics/Economics/Science/Engineering/Business Management
	- Any industry certification(s) related to Data Science techniques (DL, RL, NLP, etc.) \& platforms (SAS, R, Python, Spark, etc.), and Big Data technologies (AWS/MS Azure/GCP)
	- P\&C Insurance domain knowledge preferredAt least 2-3 years working experience in P\&C Insurance domain
+ skill set:
	- Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop (YARN, MR, HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc..
	- Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.
	- Operating knowledge of cloud computing platforms (AWS/Azure ML)
	- Grasp at databases including RDBMS, NoSQL, MongoDB etc.
+ skill set:
	- Knowledge of variety and advanced architecture, tools and concepts across all layers of the modern distributed technology stack (Hadoop, Spark, Kafka, Cassandra, MongoDB and similar)
	- Knowledge and experience in cloud architectures and cloud tools (Azure/GCP/AWS)
+ skill set:
	- General knowledge in machine learning, natural language processing, knowledge graph, image processing and computer vision, with deep insight in recent progress and practical experience in one of the fields
	- Understanding principles of deep learning, probabilistic inference, graphical models, reinforcement learning, transfer learning and adversarial learning, with extensive knowledge in one of the fields mentioned
+ Good knowledge of popular deep learning platforms (Tensorflow, Pytorch, Paddlepaddle)
	- https://github.com/PaddlePaddle/Paddle
	- https://www.paddlepaddle.org.cn/
+ simultaneous machine translation (SMT) system
+ skill set:
	- You have experience with C++/C, CUDA, DX, or OpenGL.
	- RISCV vector processor
	- Familiarity with GPU computing (CUDA, OpenCL) and HPC (MPI, OpenMP)
	- Experience with Linux and/or QNX operating systems.
	- You bring experience with CUDA and deep learning frameworks (i.e TensorFlow or Torch)
	- Strong background in research with publications from top robotics and AI conferences (i.e. RSS, ICRA, CVPR, NIPS)
	- Familiarity with parallel programming (OpenMP, OpenACC, MPI)
	- Direct experience with LLVM IR
	- Knowledge of Modern C++ semantics
	- You are familiar with how to use open-source tools like Postman, cURL, Swagger UI/Editor/Codegen to design, build, test, document and consume RESTful APIs
	- You have a passion for building and deploying microservice applications and experience using open source tools like Docker, Kubernetes and Open Shift
	- Knowledge of industry-standard web technologies like HTTPS, REST, JSON, OData, OAuth, JWT, NoSQL
	- Knowledge of industry-standard web technologies like HTTPS, REST, JSON, OData, OAuth, JWT, NoSQL
	- Define, implement and validate backward- and forward-compatible, secure, robust RESTful APIs. Integrate these APIs with industry-standard tools and open source frameworks
	- Experience with any of the deep learning frameworks: Tensorflow, Caffé, Theano, Torch.
	- Experience in applying Deep Learning to 3D Computer Vision problems.
	- Design, run, and analyze A/B tests to evaluate the effectiveness of your solution on real-world data.
	- Research, implement and evaluate deep-learning-based image analysis and data-driven algorithms such as active learning or semi-supervised learning for mining and leveraging frames in massive amounts of unlabeled data.
	- Strong background in computer architecture, preferably in the areas of CPUs, GPUs, and networks
	- Investigate new hardware and software mechanisms to extend state-of-the art networking in multi-GPU systems
	- Model, and analyze the performance of applications on proposed architectures
+ skill set:
	- Requirements:
		* The ability to quickly pick up new languages and technologies
		* Experience with relational databases (MySQL, Postgres, etc.)
		* Strong modern Perl and Python experience
		* Software architecture and design
		* Database normalization, profiling, and tuning
		* Experience with at least one major platform (Linux, MacOS, Windows) and exposure to the others
		* The ability to write clean, well-tested code with clear documentation
		* Excellent written and spoken skills, both technical and non-technical
		* A willingness to engage in the process of defining our work through conversations with product management, other engineering teams, and the rest of the company
		* The ability to help others on the team become better at their jobs through mentoring, thoughtful code reviews, and generally being a team player
	- Assets
		* Agile processes, including breaking large projects up into smaller stories, estimation, working in branches (GitHub Flow), code review, and CI/CD
		* Microservices and message queues, especially Kafka
		* Experience with one or more upstream language repositories (PyPI, RubyGems, CPAN)
		* ETL experience
		* Software estimation and planning
		* Statistics and data analysis
		* Testing large data sets via samples
		* Good working knowledge of Docker
		* Experience with Mesos, DCOS, Kubernetes
		* Experience with Go and/or Bazel
		* Experience with creating parsers, compilers, or code intelligence systems
+ Strong understanding of Model optimization, Pruning ,Tuning ,ONNX, Distiller , Quantization
+ skill set:
	- Proficient in one or more of the Conversation/UX design tools (IBM Watson Content hub, IBM digital experience manager, Botsociety, MS Visio, Sayspring Flinto, UxPin, Omnigraffle, proto.IO, MockFlow etc.)
	-  Strong understanding of the NLP space and Voice based processing (Natural language Processing, understanding, sentiment analysis, Intent classification, dialogue flows, personality insight, Text to Speech, Speech to Text, )
	-  Experience in conceiving and delivering fantastic end-to-end cross-channel experiences with appreciation of flow, context, micro-interactions, multi -modal possibilities, performance and tone for multiple customers
+ skill set:
	- several years industry experience designing and developing a core compiler component and strongly prefer at least one of the following:
		* 1. a research record
		* 2. expert knowledge in some compiler or compiler related domain
		* 3. contributions to an open source infrastructure (tensorflow, ...)
		* 4. experience retargeting LLVM, GCC, or some other retargetable compiler infrastructure
		* 5. linear algebra compilers
		* 6. experience with a compiler related technology (profiling, static analysis, ...)
		* 7. compiling to hardware
		* For example, you might have a master's degree with compiler and algorithms course, implemented and you have spent several years at a job developing the IR of custom compiler.
+ skill set:
	- Open-source Platforms:
		* Investigate and determine how to integrate open-source compiler frameworks into Mythic's propriety compiler technology, e.g., LLVM.
		* Investigate how to translate the output of deep learning frameworks such as Keras, Caffe, and Pytorch into programs that execute on Mythic's AI processor.
	- Optimization:
		* Develop Deep Neural Network (DNN) optimization algorithms that target a novel dataflow architecture composed of heterogeneous compute tiles.
		* Investigate analog-aware algorithms that consider reduced precision, new datatypes, linearity, and noise requirements.
		* Consider code generation schemes that tradeoff system latency, throughput, utilization, and power.
	- Programming models:
		* Innovate in a new compiler domain: mixed-signal computing.  Generate code for a heterogeneous processor with a mix of analog and digital compute accelerators.
		* Develop a rapidly retargetable compiler infrastructure that models new custom accelerators for next-generation computer architectures.
		* Investigate AI Domain Specific Languages (DSLs).
	- Product Impact:
		* Coordinate with engineering teams on the successful execution of the release of compiler products.
		* Innovate across the entire product tool chain including parser, optimizer, code generator, linker, assembler, debugger, and related tools.
		* Stay abreast of current industry and university compiler research and communicate key ideas to others at Mythic.
	- ***Specific responsibilities will depend on background and skills.  If working at the intersection of compilers, AI, analog computing, and processor architecture sounds exciting, this is the role for you.***
	- Required:
		* Bachelor's in computer science or related field with 3+ years compiler development.
		* Experience with system software and development tools.
		* Experience with C and C++ programming Languages.
		* Experience implementing, testing and upstreaming optimization and code generation solutions.
		* Previous experience collaborating with others as part of a team.
		* Strong software engineering skills.
	- Nice to have but not required:
		* Master's or PhD in Computer Science or related field with 3+ years compiler research experience.
		* Experience with python.
		* Familiarity with Agile software development processes
		* Experience with embedded systems and instruction set architecture
		* Experience with deep learning graph-compilers and related tools such as Glow, XLA, or TVM.
		* Experience with DNN frameworks such as Pytorch, Tensorflow, or Caffe.
+ skill set:
	- Understanding On TensorRT is an Added Advantage
	- Knowledge of Frameworks like DNN,BLAS,RAND,SPARSE.
+ skill set:
	- Networking and asynchronous IO applications;
	- IoT frameworks and messaging protocols;
	- Database design;
	- Real time operating systems;
	- Low level serial protocols such as SPI; and
	- Low level networking standards such as Ethernet and Wi-Fi.
	- Strong C/C++ programming skills;
	- Excellent problem solving skills;
	- Experience of developing unit tests, and preferably of Test Driven Development;
	- Familiarity with the use of SCM systems, and preferably a working knowledge of Git; and
	- An understanding of Continuous Integration systems.
	- Collaboration with agile planning processes
	- Software design, implantation and documentation
	- Automated test implementation
	- Code reviews and support for other development on going within the team
	- Collaboration with engineers from a range of disciplines to deliver complete, production ready systems
+ skill set:
	- Strong C/C++ and Python programming skills;
	- Experience with embedded systems and hardware;
	- Working knowledge of Git.
	- Familiarity with deploying machine learning on embedded devices using TensorFlow Lite or similar framework;
	- Experience training models using machine learning frameworks like TensorFlow, PyTorch or MXNet;
	- Experience optimizing code to run on GPU, DSP or neural processors;
	- Experience with real-time operating systems like FreeRTOS or ThreadX;
	- Experience with unit tests and Test Driven Development;
	- An understanding of Continuous Integration systems.
	- Software design and implementation;
	- Collaboration in the team's agile planning processes;
	- Code reviews and support for other development on going within the team;
	- Collaboration with engineers from a range of disciplines to deliver production ready systems.
+ skill set:
	- Principal Security Architect will develop a strategic direction for the security-related architecture of Lattice products, working closely with FPGA system and architecture experts to advance the company's presence within security-sensitive systems
	- Reduce risks, threats, and vulnerabilities in systems built with Lattice products
	- Drive Lattice's roadmap for data and design security within our programmable logic product line
	- Meet with customers and ecosystem partners to ensure requirements are understood, implementations are sound, and the broader ecosystem is moving to meet the future challenges
	- Coordinate across functional business units to ensure robust, secure posture from design to implementation
	- Represent Lattice at industry forums related to data and design security
	- Coordinate development of tools, methods, and training to support staff in achieving security objectives and ensure effectiveness of security standards
	- Will partner closely with engineering, sales, marketing, applications, and end to end customers
	- Strong customer facing skills
	- Expert knowledge/background in secure code authentication/attestation and chains of trust
	- Experience in industry attack, vulnerabilities and solutions around design and data security
	- Working knowledge of online service architectures for connected/IOT systems
	- Well versed in Secure Architecture and Design fundamentals
	- 10 years: 10+ years of experience in the hardware data security industry
+ skill set:
	- Definition of block level and product level SoC architecture
	- Work closely with IC design, verification, software, physical design, and marketing teams to ensure SoC meets feature and performance requirements
	- Modeling, Architecture, Micro architecture, Digital design, & RTL coding
	- Interconnect exploration including AHB, AXI, ACE, AXI-stream, and NoC  interconnect architectures
	- Willingness to travel domestically and internationally
	- Strong communication skills
	- Experience working with 3rd party teams
	- Expertise in scripting languages such as Python, TCL, or Perl
	- Experience developing design models in high level languages like System Verilog and System C
	- Solid experience in design convergence cycle, including synthesis, timing closure, and verification
	- Strong background in memory interfaces including DDR3 / DDR4 / LPDDR4 systems
	- Familiarity with SoC security capabilities and protocols
	- Experience in profiling SoC system performance
	- 12 + years of experience in definition of SoCs
+ skill set:
	- Contribute to designing, building, evaluating, shipping, and refining Neural Magic's machine learning product including libraries, demos, and notebooks
	- Prototype and iterate on state of the art research against proprietary, in-house software
	- Work closely with customers to understand specific needs, implementation details, and successful deployment using Neural Magic's engine
	- Collaborate with a cross functional team about market requirements, best practices and how machine learning is deployed in the wild
	- Be a trusted advisor and partner, providing deep analysis of deep learning approaches, helping to define and conduct pilot tests
	- Master's or PhD degree in computer science or math, or equivalent experience. Prefer a focus on machine learning.
	- Solid knowledge of machine learning and deep learning fundamentals, in particular MLPs and CNNs
	- Experience with taking deep learning models from conception to production: writing, training, testing, and deploying machine learning models
	- Proficient with Python and one or more deep learning frameworks such as Pytorch, Tensorflow, Caffe, MXNet, Keras, etc
	- Experience working with large data pipelines for analyzing and training
	- Self-directed individual who learns quickly and is comfortable operating in a blank slate environment
	- Excellent communication skills, ability to tailor technical information for different audiences
	- Strong sense of project ownership and personal responsibility
+ skill set:
	- Design, build, evaluate, ship, and refine Neural Magic's machine learning product including libraries, demos, and notebooks
	- Prototype and iterate on state of the art research against proprietary, in-house software
	- Work closely with customers to understand specific needs, implementation details, and successful deployment using Neural Magic's engine
	- Collaborate with a cross functional team about market requirements, best practices and how machine learning is deployed in the wild
	- Be a trusted advisor and partner, providing deep analysis of deep learning approaches, helping to define and conduct pilot tests
	- Several years of experience working with Machine Learning in industry. Master's or PhD degree in ML, computer science or math preferred.
	- Solid knowledge of machine learning and deep learning fundamentals, in particular MLPs, Recommendation Systems, and CNNs
	- Full stack experience of taking deep learning models from conception to production: writing, training, testing, and deploying models
	- Proficient with Python and one or more deep learning frameworks such as Pytorch, Tensorflow, Caffe, MXNet, Keras, etc
	- Experience working with large data pipelines for analyzing and training
	- Self-directed individual who learns quickly and is comfortable operating in a blank slate environment
	- Excellent communication skills, ability to tailor technical information for different audiences
	- Strong sense of project ownership and personal responsibility
+ skill set:
	- develop programs for data transformation, integration, or reduction;
	- prototype and ship production-grade applications for (among others) data coding, entity resolution, predictive analytics, data visualization, data dashboards, and report generation;
	- perform custom data analysis to support all phases of the data collection and research pipeline;
	- design and implement research plans for program evaluation, using both experimental and quasi-experimental methods;
	- participate in business development, including researching and crafting sections of technical proposals;
	- coordinate and conduct internal data science presentations and workshops;
	- communicate research findings to colleagues, clients, and research peers across a range of media;
	- keep current with innovations and trends in data science and survey methodology.
	- An advanced (Masters or above) degree in one of the following fields is required: math, statistics, computer science, data science, or a social science or public policy related field.
	- At least five years' experience in positions of increasing responsibility in a research or data analysis role, which may include graduate-level training at the PhD level or equivalent experience in applied research, is required.
	- Proficiency in statistics and statistical methods (e.g. probability, regression, generalized linear models) is required.
	- Proficiency in at least one of R or Python and their packages for data manipulation, visualization, statistics, and machine learning is required.
	- Experience in communicating scientific research to both specialist and general audiences is required.
	- Familiarity and interest in social scientific and public policy research, as well as applications of data scientific methods to such research, is preferred.
	- Familiarity with methods for causal inference, including the potential outcomes framework, experimental design, and quasi-experimental designs (e.g. difference-in-differences, instrumental variables, regression discontinuity), is preferred.
	- Experience in conducting original research – either independently or as a member of a research group or lab – including research design, novel data collections, data analysis, and preparation of papers, posters, or other presentations, is preferred.
	- Familiarity with areas of data science such as multi-level modeling, machine learning (e.g. methods for regularization, classification, and clustering), natural language processing (text pre-processing, text classification, named entity recognition, etc.), computer vision (e.g. image classification, image object detection), or deep learning libraries is preferred.
	- Familiarity with Bayesian estimation and software for Bayesian inference (e.g. JAGS, Stan) is preferred.
	- Experience with version control workflows and tools (e.g. Git), databases (e.g. SQL), big data technologies (Hadoop, Spark, Hive, etc.), and cloud computing environments (e.g. AWS, Google Cloud, Azure, etc.) is preferred.
+ skill set:
	- Utilize tools such as Pentaho, MSSQL SSIS, Microsoft Power BI, Microsoft  Data Factory, Microsoft Data Analytics to build web usage analytic reports.
	- 4-6 years in developing BI reports, dashboards, KPIs, and scorecards.
	- 4-6 years development experience in Data Analytic technologies such as: Pentaho, Microsoft – Power BI or Tableau
	- Experience with Microsoft Azure platform a plus (Data Factory, Data Lake, Data Analytics, Streams)
	- Experience with Solr is a plus
+ Lifelong-DNNTM (L-DNN) technology, which reduces the data requirements for AI model development and enables continuous learning in the cloud or at the edge
